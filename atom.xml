<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>田佳杰</title>
  <icon>https://www.gravatar.com/avatar/2d54367ed6dc965439f08c9f1b75cea4</icon>
  <subtitle>代码的搬运工</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-06-14T09:00:40.235Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jiajie Tian</name>
    <email>tianjiajie1881090@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>person re-id with memory </title>
    <link href="http://yoursite.com/2019/06/14/person-re-id-with-memory/"/>
    <id>http://yoursite.com/2019/06/14/person-re-id-with-memory/</id>
    <published>2019-06-14T08:18:40.000Z</published>
    <updated>2019-06-14T09:00:40.235Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>CVPR2019 看到好几篇都使用了 memory 的结构来进行 person re-id，所以对这几篇论文总结一下。具体的论文分析在我之前的博客已经有，这里主要介绍这几篇论文对 memory 的使用方法和效果。</p><ul><li>OIM: <a href="https://arxiv.org/abs/1604.01850" target="_blank" rel="noopener">CVPR2017_Joint Detection and Identification Feature Learning for Person Search</a></li><li>DIMN: <a href="http://www.eecs.qmul.ac.uk/~js327/Doc/Publication/2019/cvpr2019_dimn.pdf" target="_blank" rel="noopener">CVPR2019_Generalizable Person Re-identification by Domain-Invariant Mapping Network</a></li><li>MAR: <a href="https://arxiv.org/abs/1903.06325v2" target="_blank" rel="noopener">CVPR2019 Unsupervised Person Re-identification by Soft Multilabel Learning</a></li><li>ECN: <a href="https://arxiv.org/abs/1904.01990" target="_blank" rel="noopener">CVPR2019_Invariance Matters Exemplar Memory for Domain Adaptive Person Re-identification</a></li></ul><a id="more"></a><h2 id="2-memory-的建立"><a href="#2-memory-的建立" class="headerlink" title="2. memory 的建立"></a>2. memory 的建立</h2><img src="/2019/06/14/person-re-id-with-memory/OIM_network.png" title="OIM network"><img src="/2019/06/14/person-re-id-with-memory/DIMN_network.png" title="DIMN network"><img src="/2019/06/14/person-re-id-with-memory/MAR_network.png" title="MAR network"><img src="/2019/06/14/person-re-id-with-memory/ECN_network.png" title="ECN network"><p>通过对比网络结构图可以看出，几种方法各有不同。</p><p>OIM(单数据集): 以 id 为索引，更新方式：running average，存储的是特征</p><p>DIMN: 以 id 为索引，更新方式：running average，存储的是经过线性变化的特征</p><p>MAR: 以 agent 的形式存在，更新方式：reference agent learning，存储的是特征</p><p>ECN: 以 index 为索引，更新方式：running average，存储的是特征</p><h2 id="3-memory-的使用"><a href="#3-memory-的使用" class="headerlink" title="3. memory 的使用"></a>3. memory 的使用</h2><p>OIM:</p><script type="math/tex; mode=display">p_i = \frac{\exp(v_i^T x/\tau)}{\sum_{j=1}^L \exp(v_j^T x/\tau)+ \sum_{k=1}^Q \exp(u_k^T x/\tau)}</script><p>DIMN:</p><p>memory 中的特征是另一张图片的特征，而不是这个公式中$x_i$的特征。</p><script type="math/tex; mode=display">L_{mat}=\sum_{i=1}^{C_b} Cross\_Entropy(y_i, Softmax(\hat{W}g_{\phi}(x_i)))</script><p>MAR:</p><script type="math/tex; mode=display">L_{AL}=\sum_k -\log l(f(z_k), {a_i})^{(w_k)}=\sum_k -\log \frac{\exp(a_{w_k}^T f(z_k))}{\sum_j \exp(a_j^T f(z_k))}</script><p>ECN:</p><script type="math/tex; mode=display">q_i=\frac{exp(z_i/T)}{\sum_j exp(z_j/T)}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1. 前言&quot;&gt;&lt;/a&gt;1. 前言&lt;/h2&gt;&lt;p&gt;CVPR2019 看到好几篇都使用了 memory 的结构来进行 person re-id，所以对这几篇论文总结一下。具体的论文分析在我之前的博客已经有，这里主要介绍这几篇论文对 memory 的使用方法和效果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OIM: &lt;a href=&quot;https://arxiv.org/abs/1604.01850&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2017_Joint Detection and Identification Feature Learning for Person Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DIMN: &lt;a href=&quot;http://www.eecs.qmul.ac.uk/~js327/Doc/Publication/2019/cvpr2019_dimn.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019_Generalizable Person Re-identification by Domain-Invariant Mapping Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MAR: &lt;a href=&quot;https://arxiv.org/abs/1903.06325v2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019 Unsupervised Person Re-identification by Soft Multilabel Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ECN: &lt;a href=&quot;https://arxiv.org/abs/1904.01990&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019_Invariance Matters Exemplar Memory for Domain Adaptive Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="person re-id" scheme="http://yoursite.com/categories/person-re-id/"/>
    
    
      <category term="person re-id" scheme="http://yoursite.com/tags/person-re-id/"/>
    
      <category term="memory" scheme="http://yoursite.com/tags/memory/"/>
    
  </entry>
  
  <entry>
    <title>OIM</title>
    <link href="http://yoursite.com/2019/06/10/OIM/"/>
    <id>http://yoursite.com/2019/06/10/OIM/</id>
    <published>2019-06-10T08:32:49.000Z</published>
    <updated>2019-06-14T08:54:52.916Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul><li>paper: <a href="https://arxiv.org/abs/1604.01850" target="_blank" rel="noopener">CVPR2017_Joint Detection and Identification Feature Learning for Person Search</a></li><li>code: <a href="https://github.com/ShuangLI59/person_search" target="_blank" rel="noopener">caffe</a>, [pytorch][<a href="https://github.com/Cysu/open-reid" target="_blank" rel="noopener">https://github.com/Cysu/open-reid</a>]</li><li>project: <a href="http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html" target="_blank" rel="noopener">End-to-End Deep Learning for Person Search</a></li><li>memory: OIM, DIMN, MAR, ECN</li></ul><a id="more"></a><p>1作: <a href="http://www.mit.edu/~lishuang/" target="_blank" rel="noopener">Shuang Li</a>，跟过汤晓鸥。</p><p>这一篇文章之前就已经看过了，这次的主要目的是 memory 的使用，因为看到 memory 在 ECN、DIMN 中都有使用，所以看看大家都是怎么用的。</p><p>作者提出 <strong>Online Instance Matching (OIM)</strong> loss function 来融合 pedestrian detection and person re-id.</p><p>作者提出了新的数据集，18184 张图片，8432 个行人，96143 个行人框。</p><p>作者重新定义了 test 的过程。在传统的 test 过程中，gallery 中的图片是已经裁剪好的单个行人图片，作者定义 gallery 中的图片是未经裁剪、有多个行人的图片，需要匹配是哪张图片的哪个行人。</p><img src="/2019/06/10/OIM/person_search.png" title="person search"><p>CNN 由两部分组成，给定一张 gallery image, a pedestrian proposal net 用于生成行人的 bounding boxes, 更偏向于召回率而不是精确度, 然后 a identification net 用于提取相应的特征。</p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><img src="/2019/06/10/OIM/framework.png" title="framework"><ul><li>a pedestrian proposal net</li><li>a identification net</li></ul><h3 id="3-1-Online-Instance-Matching-Loss"><a href="#3-1-Online-Instance-Matching-Loss" class="headerlink" title="3.1 Online Instance Matching Loss"></a>3.1 Online Instance Matching Loss</h3><p>a pedestrian proposal net 共检测出三类：labeled identities, unlabeled identities, background clutter. 这里只考虑 labeled identities and unlabeled identities.</p><p>假设 a labeled identity $x\in R^D$，其中 $D$ 表示特征维度，作者建立了一个 <strong>lookup table (LUT)</strong> $V\in R^{D\times L}$ 来储存所有的 feature of all labeled identities，前向传播过程中，计算 cos 距离 $V^T x$，反向传播时 $v_t \gets \gamma v_t (1-\gamma) x$，其中 $\gamma\in [0, 1]$, 并进行归一化。这个过程和 ECN 基本一样。</p><img src="/2019/06/10/OIM/LUT.png" title="LUT"><p>对于 a unlabeled identites，使用 <strong>a circular queue</strong> $U\in R^{D\times Q}$ 来储存 the features of unlabeed identites that appear in recent mini-batches，其中 $Q$ 表示　queue size. 前向传播时，计算 cos 距离 $V^T x$，反向传播时，弹出 queue 顶端的 features，插入当前 batch 的特征。</p><p>通过上面两个结构，作者重新定义了 $x$ 属于某类的概率:</p><script type="math/tex; mode=display">p_i = \frac{\exp(v_i^T x/\tau)}{\sum_{j=1}^L \exp(v_j^T x/\tau)+ \sum_{k=1}^Q \exp(u_k^T x/\tau)}</script><p>其中，更高的 $\tau$ 导致更平缓的分布。</p><script type="math/tex; mode=display">q_i = \frac{\exp(u_i^T x/\tau)}{\sum_{j=1}^L \exp(v_j^T x/\tau)+ \sum_{k=1}^Q \exp(u_k^T x/\tau)}</script><p>损失函数为:</p><script type="math/tex; mode=display">L = E_x [-\log p_t]</script><p>我觉得作者这里写错了，但无伤大雅。</p><p>其对 $x$ 的反向推导很有意思，是：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial x}=-\frac{1}{\tau}[ v_t-\sum_{j=1}^L p_j v_t-\sum_{k=1}^Q q_k u_k ]</script><p>后面会补充整个推导过程</p><p><strong>Why not Softmax loss</strong>: 作者不使用 Softmax loss 有两个方面的原因：第一个原因不是很理解，不是很赞同，第二个原因是 unkown identities 没有 label.</p><p><strong>Scalability</strong>: 随着 id 的增加，分母的计算时间会成为瓶颈，所以采用 sub-sampling 的方法计算，具体见下文</p><p>除此之外，OIM loss 看似和 Softmax 相似，但是OIM loss是非参数的，缺点是容易过拟合，L2-normalized 能减少过拟合。</p><p><strong>Question</strong>: Memory 是否真的比 fc 好用，是否可以单独做一个 memory 和 fc 的对比实验？</p><p>2019-06-12: ICCV 2019 三个 WR, 凉凉。</p><p><strong>补充反向传播推导过程</strong>：</p><p>第一步：假设只有两个变量，很容易就可以推导到多个变量，令</p><script type="math/tex; mode=display">f_1=\frac{e^{x_1}}{e^{x_1}+e^{x_2}}, f_2=\frac{e^{x_2}}{e^{x_1}+e^{x_2}}</script><p>不能随便地使用$f_1+f_2=1$，则</p><script type="math/tex; mode=display">\frac{\partial f_1}{\partial x_1} = \frac{e^{x_1}(e^{x_1}+e^{x_2})-e^{x_1} e^{x_1}}{(e^{x_1}+e^{x_2})^2}=f_1-(f_1)^2</script><script type="math/tex; mode=display">\frac{\partial f_1}{\partial x_2} = -e^{x_1} \frac{e^{x_2}}{(e^{x_1}+e^{x_2})^2}=-f_1 \cdot f_2</script><p>第二步：令 $x_1=v_1 x, x_2=v_2 x$:</p><script type="math/tex; mode=display">f_1=\frac{e^{v_1 x}}{e^{v_1 x}+e^{v_2 x}}, f_2=\frac{e^{v_2 x}}{e^{v_1 x}+e^{v_2 x}}</script><p>则</p><script type="math/tex; mode=display">\frac{\partial f_1}{\partial x} = \frac{\partial f_1}{\partial x_1} \frac{\partial x_1}{\partial x} + \frac{\partial f_1}{\partial x_2} \frac{\partial x_2}{\partial x}=(f_1-(f_1)^2)v_1 + (-f_1 \cdot f_2)v_2</script><script type="math/tex; mode=display">\frac{\partial f_1}{\partial v_1} = \frac{\partial f_1}{\partial x_1} \frac{\partial x_1}{\partial v_1}=(f_1-(f_1)^2)x</script><script type="math/tex; mode=display">\frac{\partial f_1}{\partial v_2} = \frac{\partial f_1}{\partial x_2} \frac{\partial x_2}{\partial v_2}= (-f_1 \cdot f_2)x</script><p>第三步：计算损失函数，假设最优值是第一个， $L=-\log(f_1)$</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial x} &= \frac{\partial L}{\partial f_1} \frac{\partial f_1}{\partial x} \\&= - \frac{1}{f_1} \cdot ((f_1-(f_1)^2)v_1 + (-f_1 \cdot f_2)v_2) \\&= (1-f_1) v_1 - f_2 v_2 \\&= v_1 - (f_1 v_1 + v_2 v_2)\end{aligned}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial v_1} = \frac{\partial L}{\partial f_1} \frac{\partial f_1}{\partial v_1}=-(1-f_1)x</script><script type="math/tex; mode=display">\frac{\partial L}{\partial v_2} = \frac{\partial L}{\partial f_1} \frac{\partial f_1}{\partial v_2}= f_2 x</script><p>第四步：推广到多个变量$v_1, v_2, v_3…$</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial x} &= v1-(\sum_i v_i x) \\\frac{\partial L}{\partial v_1} &= -(1-f_1)x \\\frac{\partial L}{\partial v_2} &= f_2 x \\\frac{\partial L}{\partial v_3} &= f_3 x\end{aligned}</script><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Effectiveness-of-Online-Instance-Matching"><a href="#4-1-Effectiveness-of-Online-Instance-Matching" class="headerlink" title="4.1 Effectiveness of Online Instance Matching"></a>4.1 Effectiveness of Online Instance Matching</h3><img src="/2019/06/10/OIM/OIM_Softmax.png" title="comparisons between the OIM and Softmax loss"><img src="/2019/06/10/OIM/Softmax_OIM.png" title="Softmax OIM for standard person re-id task"><p><strong>Sub-sampling the identities</strong>:</p><img src="/2019/06/10/OIM/sub_sampling.png" title="sub-sampling"><p>当 sub-sampling 的 size 更小的时候，最终性能差不多，但是收敛速度更快，说明了作者提出的方法能有效地处理大规模数据集。</p><p><strong>Low-dimensional subspace</strong>: 作者对比了 128, 256, 512, 1024， 2048-dimention，发现原始的 2048 维特征得到结果不如其他。</p><img src="/2019/06/10/OIM/low_dimensional_subspace.png" title="low dimensional subspace"><h2 id="5-code"><a href="#5-code" class="headerlink" title="5. code"></a>5. code</h2><p>从 open-reid 中只能看到 LUT 的代码，可以看出来，这个代码和 ECN 的代码可以说是一样，牛逼啊。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, autograd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OIM</span><span class="params">(autograd.Function)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lut, momentum=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(OIM, self).__init__()</span><br><span class="line">        self.lut = lut</span><br><span class="line">        self.momentum = momentum</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets)</span>:</span></span><br><span class="line">        self.save_for_backward(inputs, targets)</span><br><span class="line">        outputs = inputs.mm(self.lut.t())</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_outputs)</span>:</span></span><br><span class="line">        inputs, targets = self.saved_tensors</span><br><span class="line">        grad_inputs = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> self.needs_input_grad[<span class="number">0</span>]:</span><br><span class="line">            grad_inputs = grad_outputs.mm(self.lut)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(inputs, targets):</span><br><span class="line">            self.lut[y] = self.momentum * self.lut[y] + (<span class="number">1.</span> - self.momentum) * x</span><br><span class="line">            self.lut[y] /= self.lut[y].norm()</span><br><span class="line">        <span class="keyword">return</span> grad_inputs, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">oim</span><span class="params">(inputs, targets, lut, momentum=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> OIM(lut, momentum=momentum)(inputs, targets)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OIMLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, num_classes, scalar=<span class="number">1.0</span>, momentum=<span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight=None, size_average=True)</span>:</span></span><br><span class="line">        super(OIMLoss, self).__init__()</span><br><span class="line">        self.num_features = num_features</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.momentum = momentum</span><br><span class="line">        self.scalar = scalar <span class="comment"># Temperature</span></span><br><span class="line">        self.weight = weight</span><br><span class="line">        self.size_average = size_average</span><br><span class="line">        <span class="comment"># ECN 这里用的 nn.Parapmeter</span></span><br><span class="line">        self.register_buffer(<span class="string">'lut'</span>, torch.zeros(num_classes, num_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets)</span>:</span></span><br><span class="line">        inputs = oim(inputs, targets, self.lut, momentum=self.momentum)</span><br><span class="line">        inputs *= self.scalar</span><br><span class="line">        loss = F.cross_entropy(inputs, targets, weight=self.weight,</span><br><span class="line">                               size_average=self.size_average)</span><br><span class="line">        <span class="keyword">return</span> loss, inputs</span><br></pre></td></tr></table></figure><p>这里我有点晕，简单理一理。</p><p>第一，定义 operation: $output =  F (input, target;\theta)$。因为 LUT 的前向传播和反向传播不同于一般的 operation，所以需要重新定义 operation，手动实现前向传播和反向传播，前向传播需要 lut, inputs，反向传播对 inputs 求导和 利用 target 对 lut 的更新。</p><p>第二，定义损失函数 Loss. 这个 OIMLoss 用了一个层来包装整个 operation 和 变量，方便管理。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1604.01850&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2017_Joint Detection and Identification Feature Learning for Person Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/ShuangLI59/person_search&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;caffe&lt;/a&gt;, [pytorch][&lt;a href=&quot;https://github.com/Cysu/open-reid&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Cysu/open-reid&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;project: &lt;a href=&quot;http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;End-to-End Deep Learning for Person Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;memory: OIM, DIMN, MAR, ECN&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="person re-id" scheme="http://yoursite.com/categories/person-re-id/"/>
    
    
      <category term="person re-id" scheme="http://yoursite.com/tags/person-re-id/"/>
    
      <category term="memory" scheme="http://yoursite.com/tags/memory/"/>
    
  </entry>
  
  <entry>
    <title>DIMN</title>
    <link href="http://yoursite.com/2019/06/03/DIMN/"/>
    <id>http://yoursite.com/2019/06/03/DIMN/</id>
    <published>2019-06-03T08:16:17.000Z</published>
    <updated>2019-06-14T08:22:04.756Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><ul><li>paper: <a href="http://www.eecs.qmul.ac.uk/~js327/Doc/Publication/2019/cvpr2019_dimn.pdf" target="_blank" rel="noopener">CVPR2019_Generalizable Person Re-identification by Domain-Invariant Mapping Network</a></li><li>code: 暂无</li></ul><a id="more"></a><p>这篇文章和 <a href="https://tjjtjjtjj.github.io/2019/05/06/ECN/#more" target="_blank" rel="noopener">ECN</a> 那篇有相似的地方：Memory 记录所有的特征，并把这些记录的特征直接当做分类器使用，我觉得这样的使用应该有更合理的解释，不同点在于使用 Memory 的具体方式不同，我暂时觉得这里面还有东西可以挖。我记得我之前读过一篇 也是用 Memory 用于分类的论文，那篇直接把已知的和未知的特征都放在 memory 中，更简单巧妙。</p><p>该论文首次提到 <strong>meta-learning, few-shot learning</strong>，我简单查了查。人工智能的理论进展呈现：</p><blockquote><p>Artificial Intelligence—&gt;Machine Learning—&gt;Deep Learning—&gt;Deep Reinforcement Learning—&gt;Deep Meta Learning</p></blockquote><p>深度学习可以在特定的 task 上得到一个很好的模型，但是在其他 task 上效果就会很差，是否可以理解成过拟合呢？应该和过拟合不是一个角度，过拟合指得的是训练集和测试集不可能同时达到各自的最优解，但是 meta learning 更偏向于当出现一个新的少量的样本的时候，不需要巨量的训练，原始数据训练得到的模型作为先验知识，有助于学习新的样本，有点 <a href="https://tjjtjjtjj.github.io/2019/05/29/FUNIT/#more" target="_blank" rel="noopener">FUNIt</a> 的味道，又有点像之前看过的一篇 <a href="https://tjjtjjtjj.github.io/2019/03/27/One-Example-reID/#more" target="_blank" rel="noopener">TIP2019 Progressive Learning for Person Re-Identification with One Example</a>，暂时还说不上来这种感觉。郑哲东那个团队是直接 meta-learing 过来的思路吗？不得而知了。</p><p>举个例子：分类器可以很好地区分在各种环境下的猫狗之类的动物，这个时候给一张站着的老虎图片用于二次训练，使得二次训练后的分类器既能对于分猫狗的性能不下降，又对各种环境下的老虎图片也有相同的区分能力。如果直接从头开始训练，训练集为一张老虎+各种环境下的猫狗，显然对于老虎的区分能力很弱。</p><p>meta-leaning 现在的研究思路</p><blockquote><ul><li>基于记忆Memory的方法</li><li>基于预测梯度的方法</li><li>利用Attention注意力机制的方法</li><li>借鉴LSTM的方法</li><li>面向RL的Meta Learning方法</li><li>通过训练一个好的base model的方法，并且同时应用到监督学习和增强学习</li><li>利用WaveNet的方法</li><li>预测Loss的方法</li></ul></blockquote><p>参考链接: <a href="https://zhuanlan.zhihu.com/p/28639662" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28639662</a></p><p>示例视频: <a href="https://zhuanlan.zhihu.com/p/46002992" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46002992</a></p><p>其他资料: <a href="https://www.zhihu.com/topic/20140823/hot" target="_blank" rel="noopener">https://www.zhihu.com/topic/20140823/hot</a></p><p>三个趋势: 自动驾驶，强化学习，元学习</p><p>作者 <a href="http://www.eecs.qmul.ac.uk/~js327/#RESOURCE" target="_blank" rel="noopener">Jifei Song</a>，博三，敦煌玛丽女王大学的 <a href="http://sketchx.eecs.qmul.ac.uk/people/" target="_blank" rel="noopener">SketchX 实验室</a>，16年 BMVC，17年 ICCV，18年 CPVR，19年 CVPR，大佬发文章都白送的吗，刚不过刚不过。</p><h2 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h2><p>作者希望可以在 a set of source domains 上一次训练，直接直接应用到 unseen dataset 上，称之为 <strong>Domain-Invariant Mapping Network (DIMN)</strong>，遵循 meta-learning pipeline amd sample a subset of source domain training tasks(identities) during each training episode。</p><p>现有的 cross domain person Re-ID 可以分为 unsupervised domain adaptation (UDA)，domain generalization(DG)(few-shot meta learning)。</p><blockquote><p>However, existing DG methods [20, 34, 23, 39] assume that the source and target domain have the same label space; whilst existing metalearning models [49, 49, 10, 35, 40] assume a fixed number of classes for target domains and are trained specifically for that number using source data.</p></blockquote><p>补充: 之前看过的 UDA 论文旨在寻找 domain-invariant feature space，而 DG(domain generalization) 旨在更好地模型迅速地适应新的 domain。</p><p>DIMN 的目的是学习 <strong>a mapping between a person image and its identity classifier weight vector</strong>.</p><p>DIMN 不同于传统的 meta-learning ：</p><ol><li>在 target domain 中不需要更新，这一点和 ECN 有点区别，ECN 使用了 target domain 的信息</li><li>不同的 training tasks share a memory bank, memory bank 使用 running average strategy 的方式更新，这一点和 ECN 差不多</li><li>训练得到的模型可以应用于有任意数量行人的 target domain，这一点和 ECN 不一样，ECN 基本只保证了在哪训练用在哪</li></ol><p>感觉这个目标定的要比 ECN 更高一些。</p><p>作者在实际实验的时候使用5个 datasets 作为 source domains，另外4个 datasets 作为 target domains，实验结果不是很全，比如没有做只有一个 dataset 的 source domain 的效果。所以不能完全排除因为扩大数据集而造成的性能提高。但是根据现有的使用过 memory 的论文，我觉得还是可信的。</p><h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h2><p>背景：已知有标签的 M 个 datasets(domains), $D_1, D_2,…, D_M$，每个 domain 有自己的 label space，希望在这些数据集上训练得到的模型直接应用到新的 domain/dataset 得到好的效果，而不需要更新模型。</p><img src="/2019/06/03/DIMN/Network.png" title="Network"><p>The network:</p><ul><li>the encoding subnets: $g_{\phi}$</li><li>mapping subnets: gallery image embedding—&gt;classifier’s weight vector</li><li>memory bank: store all classifiers in training domains</li></ul><p><strong>Encoding Subnet</strong>(最基本的分类损失): MobilenetV2，这个也是 (MobilenetV2，另外一篇 distill 也是) 。假设在融合后的 $M$ 个数据集中一共有 $C$ 个行人，每个 mini-batch 中取 $C_b$ 个行人，对于每个行人 $l_i$，取两张图片，分别设置为 gallery $\tilde{x}_i$ 和 probe $x_i$，所以在每个 batch 中有 $2C_b$ 个行人图片，组成 $2C_b$ pairs(两张图片可以调过来)。</p><p>最基本的分类损失：</p><script type="math/tex; mode=display">L_{id}=\sum_{i=1}^{C_b} Cross\_Entropy(l_i, Softmax(f_{\theta}(g_{\phi}(x_i)))</script><p>其中 $x_i$ 表示输入图片，$l_i$ 表示标签，$g_{\phi}$ 表示 encoding subnet，得到的是 D 维特征向量，$f_{\theta}$表示分类器，$\theta \in R^{D\times C}$</p><p><strong>Mapping Subnet</strong>：相对于传统的分类权重 $\theta_{\cdot,j}$ 作为模型参数的一部分，作者直接生成分类权重 identity classifier weight vector：</p><script type="math/tex; mode=display">\hat{\theta}_{\cdot,j}=h_w(g_{\phi}(\tilde{x}_i))</script><p>其中 mapping subnet $h_w$ 可以理解成 hyper-network(不懂，难道为其他网络生成参数就是 hyper-network)，这里可以使用简单的 multi-layer perception (MLP)。</p><p>对于给定的 gallery image $\tilde{x}_j$ and probe image $x_i$，得到 $\hat{\theta}_{\cdot,j}$，进而得到 logit vector $p$:</p><script type="math/tex; mode=display">p_j=h_w(g_{\phi}(\tilde{x}_ j))\cdot g_{\phi}(x_i)</script><p>然后对 $p_j$ 进行softmax，这是表示对于 probe image $x_i$, 所有的 gallery images $\tilde{x}_j$ 根据相似度进行排序，亦或者可以理解成 gallery images 为基本分类向量，判断 probe image 属于哪类，真值为如果 probe image 和 gallery image 是同一类，设为1，否则设为0，根据之前写的 mini-batch 的取法，只有一个为1，剩下都为0，可以理解成基本的分类损失。</p><p>这里 logit vector $p$ 有两种写法，一种是 $p\in R^C$，表示 probe image 属于哪类，优点是具有 discriminative power，一种是 $p\in R^{C_b}$，表示 probe image 与 mini-batch 的哪张相似，优点是容易收敛训练。为了兼顾 scalability and discriminativity，引入 memory bank.</p><p><strong>Memory Bank</strong>：Memory bank is a weight matrix$W\in R^{D\times C}$。每个 mini-batch 的 gallery branch 中，有 $C_b$ 个不同 id 的样本，$[ \tilde{x}_1, \tilde{x}_2,…, \tilde{x}_{C _b}]$，经过 encoding subnet $g _{\phi}$ and the mapping subnet $h_w$，得到 $C_b$ 个 predicted weight vector $\lbrace \hat{\theta}_{\cdot, j}, j=[ 1,2,…, C_b] \rbrace$，即 $\hat{\theta}\in R^{D\times C_b}$.</p><p>然后根据 id 更新 $W$，这里的更新还蛮奇怪的，和 ECN 的更新不一样。</p><p>先得到 $\hat{W}$: 复制 $W$，$\hat{W} \gets W$，再复制 $\hat{\theta}$，$\hat{W}_ {\cdot, L(j)} \gets \hat{\theta}_{\cdot, j}, \forall j\in [ 1,2,…,C_b]$($L(j)$ 表示第 $j$ 张图片的 id)，</p><p>然后用类似 ECN 那样，用分类损失</p><script type="math/tex; mode=display">L_{mat}=\sum_{i=1}^{C_b} Cross\_Entropy(y_i, Softmax(\hat{W}g_{\phi}(x_i)))</script><p>与 ECN 不同地是，第一，ECN 没有真值，所以只能自己像自己，DIMN 有真值，第二，ECN 用的是未更新的 memory，DIMN 用的是半旧半新的 memory。</p><p>最后更新:</p><script type="math/tex; mode=display">W\gets (1-\alpha)W+\alpha \hat{W}</script><p>作者发现有两个 trick 可以帮助稳定训练：对 $W$ 进行二范约束；$W$ 更新前后变化比较小。</p><script type="math/tex; mode=display">L_{reg}=\sum_{i=1}^{C_b} \parallel W_{\cdot, L(j)} - \hat{\theta}_{\cdot, j} \parallel_2^2</script><p>Question: 对 $L_{reg}$ 的出现表示存疑，感觉出现的很勉强</p><p><strong>Training Objective</strong>: 三元组损失，对于 $x_i$，$\tilde{x}_i$ 为正样本，其他的 $\hat{W} _{\cdot, j’}| _{j’\not = i}$ 为负样本，得到 $p=h_w(g_{\phi}(\tilde{x_i}))\cdot g_{\phi}(x_i)$，$n=\hat{W} _{\cdot, j’}\cdot g _{\phi}(x_i)|_{j’\not = i}$，归一化为 $S(x_i, \tilde{x}_i)$, $S(x_i, \tilde{x}_j’| _{j’\not = i})$，从而有</p><script type="math/tex; mode=display">L_{tri}=\sum_{i=1}^{C_b} \max (0, \triangle - \max S(x_i, \tilde{x}_j'| _{j'\not = i}) + S(x_i, \tilde{x}_i)</script><p><strong>Overall</strong>：</p><script type="math/tex; mode=display">L_{full}=L_{id}+\lambda_1 L_{mat} +\lambda_2 L_{reg} + \lambda_3 L_{tri}</script><p><strong>备注 ECN</strong>:</p><ol><li>memory 的使用：ECN 先计算损失后更新， DIMN 半更新计算损失后全更新</li><li>损失函数: ECN 使用 memory 进行了两次分类损失和一次假标签分类损失，DIMN 使用 memory 进行了一次分类损失和一次三元组损失。</li><li>memory 的输入: ECN 中 memory 和基本分类损失用的是同一张图片，DIMN 中 memory 和基本分类损失用的不是同一张图片</li><li>memory 的特征：ECN 中直接共享网络提取到的，DIMN 又经过了一次 mapping subnet.</li></ol><p><strong>Model Testing</strong>: 上述的损失函数可以学习到 encoding subnet $g_{\phi}(\cdot)$ and mapping subnet $h_w(\cdot)$，在测试阶段，给定 query image $x_i$ and gallery image $\tilde{x}_j$，两张图片的相似度定义为 $h_w(g_{\phi}(\tilde{x}_j))\cdot g _{\phi}(x_i)$.</p><p>备注：</p><ol><li>DIMN 是一个 DG method 而不是一个 one-shot learning，因为只在 source domain 上训练</li><li>传统的 deep ReID 的相似度计算是 $g_{\phi}(\tilde{x}_j)\cdot g _{\phi}(x_i)$</li></ol><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Datasets-and-Settings"><a href="#4-1-Datasets-and-Settings" class="headerlink" title="4.1 Datasets and Settings"></a>4.1 Datasets and Settings</h3><p><strong>A Large-Scale ReID benchmark</strong>:</p><ul><li>source datasets: CUHK02, CUHK03, Market-1501, DukeMTMC-ReID, CUHK-SYSU PersonSearch</li><li>test datasets: VIPeR, PRID, GRID, i-LIDS</li></ul><img src="/2019/06/03/DIMN/dataset.png" title="dataset"><p><strong>Implementation Details</strong>:</p><ul><li>encoding subnet: MobileNetV2, output: 1792</li><li>mapping subnet: a single fully-connected layer</li><li>running average parameter $\alpha=0.5$</li><li>triplet loss margin $\triangle=0.8$</li><li>weight of loss $\lambda_1=\lambda_3=1, \lambda_2=0.01$</li></ul><h3 id="4-3-Ablation-Study"><a href="#4-3-Ablation-Study" class="headerlink" title="4.3 Ablation Study"></a>4.3 Ablation Study</h3><img src="/2019/06/03/DIMN/ablation.png" title="ablation study"><p><strong>Question</strong>: 直接使用 memory 进行分类损失和使用 fc 进行分类损失有区别吗？前向计算过程是一样的，memory 的权重等同于 fc 的权重。反向求导过程不一样，memory 使用 running average 并进行二范归一化进行更新，fc 的权重使用链式求导进行更新，memory 和 fc 的 feature 反向求导也是一样的公式。也就是说两种唯一的不同在于权重的更新方式，memory 更新时其权重是固定的，fc 更新时其权重是变化的，memory 更新时会进行二范归一化，fc 更新时没有这个操作。这些区别足以产生这么大的影响吗？可能更需要其他理论上的支持</p><p>从 ablation study 上看，缺失每一个对最终结果的影响都不大。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1. 前言&quot;&gt;&lt;/a&gt;1. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;http://www.eecs.qmul.ac.uk/~js327/Doc/Publication/2019/cvpr2019_dimn.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019_Generalizable Person Re-identification by Domain-Invariant Mapping Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: 暂无&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="person re-id" scheme="http://yoursite.com/categories/person-re-id/"/>
    
    
      <category term="meta-learning" scheme="http://yoursite.com/tags/meta-learning/"/>
    
      <category term="few-shot learning" scheme="http://yoursite.com/tags/few-shot-learning/"/>
    
      <category term="cross-domain person re-id" scheme="http://yoursite.com/tags/cross-domain-person-re-id/"/>
    
  </entry>
  
  <entry>
    <title>ArcFace</title>
    <link href="http://yoursite.com/2019/05/30/ArcFace/"/>
    <id>http://yoursite.com/2019/05/30/ArcFace/</id>
    <published>2019-05-30T07:41:52.000Z</published>
    <updated>2019-05-31T02:20:55.383Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1801.07698" target="_blank" rel="noopener">CVPR2019_ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a></li><li>code: <a href="https://github.com/deepinsight/insightface" target="_blank" rel="noopener">MXNet</a>, <a href="https://github.com/wujiyang/Face_Pytorch" target="_blank" rel="noopener">pytorch</a>, <a href="https://github.com/ronghuaiyang/arcface-pytorch" target="_blank" rel="noopener">pytorch</a>, <a href="https://github.com/luckycallor/InsightFace-tensorflow" target="_blank" rel="noopener">tensorflow</a></li><li>author: <a href="https://ibug.doc.ic.ac.uk/people/jdeng" target="_blank" rel="noopener">邓健康</a></li></ul><p>开源代码中称之为 insightface，思路简单，效果却很好，并且与其他变种做了详尽的对比。(这篇论文原名是ArcFace，但是由于与虹软重名，后改名为Insight Face)</p><a id="more"></a><p>帝国理工学院博士生，CVPR17, 18, 19 都有他的文章，牛逼。</p><p>此外，他们在ICCV 2019 举办了 Lightweight Face Recognition Challenge Workshop(LFR19) ，LFR19 是第一个强调“轻量级推理” 和“可重现系统” 的人脸识别比赛。竞赛于 4 月 25 日~7 月 10 日进行，比赛结果将于 10 月 28 日在韩国首尔举办的 ICCV 2019 会议上进行公布，欢迎前往<a href="https://ibug.doc.ic.ac.uk/resources/lightweight-face-recognition-challenge-workshop/" target="_blank" rel="noopener">比赛主页</a>了解详细规则.</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p><a href="https://www.wengbi.com/thread_68343_1.html" target="_blank" rel="noopener">腾讯AI lab CVPR paper把玩：cosFace、ArcFace—additive margin softmax</a></p><blockquote><p>深度学习界从开始到现在，所有的论文基本就干了两件事情，一、发现一个牛逼的网络结构，提高了网络的表达能力；二、发现了一个好玩的损失函数，提升了模型的鲁棒性和精度。人脸识别的优化方向就两个，一是不断增大不同人之间的距离，二是不断降低一个人不同人脸照片之间的距离。 center loss 和 tripleloss都是干这个事情。但他们损失函数的根基还是softmax的交叉熵.</p></blockquote><ul><li>主流: the softmax-loss-based methods and the triplet-loss-based methods</li><li>softmax: angular margin penalty: Sphereface, CosFace, ArcFace</li></ul><img src="/2019/05/30/ArcFace/network.png" title="network"><h2 id="2-Proposed-Approach"><a href="#2-Proposed-Approach" class="headerlink" title="2. Proposed Approach"></a>2. Proposed Approach</h2><h3 id="2-1-ArcFace"><a href="#2-1-ArcFace" class="headerlink" title="2.1 ArcFace"></a>2.1 ArcFace</h3><script type="math/tex; mode=display">L_1=-\frac{1}{N} \sum_{i=1}^N \log \frac{e^{W_{y_i}^T x_i+b_{y_i}}}{\sum_{j=1}^n e^{W_j^T x_i+b_j}}</script><p>令 $b_j=0, \parallel W_j \parallel=1, \parallel x_i \parallel=s, W_j^T x_i=s \cos \theta_j$</p><script type="math/tex; mode=display">L_2=-\frac{1}{N} \sum_{i=1}^N \log \frac{e^{s \cos \theta_{y_i}}}{e^{s \cos \theta_{y_i}}+\sum_{j=1,j\not =y_i}^n e^{s \cos \theta_j}}</script><p>angular margin penalty:</p><script type="math/tex; mode=display">L_3=-\frac{1}{N} \sum_{i=1}^N \log \frac{e^{s \cos (\theta_{y_i}+m)}}{e^{s \cos (\theta_{y_i}+m)}+\sum_{j=1,j\not =y_i}^n e^{s \cos \theta_j}}</script><img src="/2019/05/30/ArcFace/softmax_ArcFace.png" title="softmax and ArcFace"><h3 id="2-2-Comparison-with-SphereFace-and-CosFace"><a href="#2-2-Comparison-with-SphereFace-and-CosFace" class="headerlink" title="2.2 Comparison with SphereFace and CosFace"></a>2.2 Comparison with SphereFace and CosFace</h3><ul><li>SphereFace: multiplicative angular margin $m_1$</li><li>ArcFace: additive angular margin $m_2$</li><li>CosFace: additive cosine margin $m_3$</li></ul><p><strong>补充</strong>：按照作者的说法，ArcFace 相当于融合了 softmax 和 triplet，形成了 image-to-class 的 margin 损失，softmax 是 image-to-class 损失， triplet 是 image-to-image 的 margin 损失。</p><img src="/2019/05/30/ArcFace/target_logit_analysis.png" title="target logit analysis"><img src="/2019/05/30/ArcFace/decision_margin.png" title="decision margin"><p>各种实验结果也证明了这个损失很强。</p><p><strong>补充</strong>：作者在直播中还提到了 RetinaFace Face Detector，他也参加了一些比赛，结果也说明了他们提出的整体框架效果很好。</p><img src="/2019/05/30/ArcFace/RetinaFace.png" title="RetinaFace"><p>自己标了1w多张人脸的五点图，大佬大佬。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1801.07698&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019_ArcFace: Additive Angular Margin Loss for Deep Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/deepinsight/insightface&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MXNet&lt;/a&gt;, &lt;a href=&quot;https://github.com/wujiyang/Face_Pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;, &lt;a href=&quot;https://github.com/ronghuaiyang/arcface-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;, &lt;a href=&quot;https://github.com/luckycallor/InsightFace-tensorflow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;author: &lt;a href=&quot;https://ibug.doc.ic.ac.uk/people/jdeng&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;邓健康&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;开源代码中称之为 insightface，思路简单，效果却很好，并且与其他变种做了详尽的对比。(这篇论文原名是ArcFace，但是由于与虹软重名，后改名为Insight Face)&lt;/p&gt;
    
    </summary>
    
      <category term="face recognition" scheme="http://yoursite.com/categories/face-recognition/"/>
    
    
      <category term="face recognition" scheme="http://yoursite.com/tags/face-recognition/"/>
    
      <category term="softmax" scheme="http://yoursite.com/tags/softmax/"/>
    
  </entry>
  
  <entry>
    <title>FUNIT</title>
    <link href="http://yoursite.com/2019/05/29/FUNIT/"/>
    <id>http://yoursite.com/2019/05/29/FUNIT/</id>
    <published>2019-05-29T03:08:24.000Z</published>
    <updated>2019-05-30T06:21:58.515Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1905.01723" target="_blank" rel="noopener">FUNIT: Few-Shot Unsupervised Image-to-Image Translation</a></li><li>code: <a href="https://github.com/NVlabs/FUNIT" target="_blank" rel="noopener">code</a></li></ul><p>代码还需要等段时间，才能下载到所有代码。</p><p>这篇文章和郑哲东的 <a href="https://arxiv.org/abs/1904.07223" target="_blank" rel="noopener">Joint Discriminative and Generative Learning for Person Re-identification</a> 这篇文章看着有点像，果然是高手的思路都是相同的。不对，两个都是 NVIDIA 的，哈哈哈。</p><p>这篇文章思路奇特，实现简单。</p><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>传统的 GAN 在训练阶段需要 many images in both source and target classes (其中 source 提供 content, target 提供 classes)(这里的 content 可以粗略的理解成形态或者表情或者属性或者姿势这种与类别无关的内容，classes 表示与类别相关的内容)，并且测试图片需要来自训练时所使用的 source and target classes.而作者提出的方法在训练阶段需要很少的 images in both source and target classes 并且测试时可以提供训练未曾见到过的 target classes.</p><p>其基本假设是当人类见到 new object (target class) 时，可以根据以往的经验推断出它的其他形态，比如曾经见过猫的站立和趴着的状态，见到老虎时，能自动脑补出老虎站立和趴着的状态。</p><p>Question: 如果见过猫的站立和趴着的状态，看到蛇会咋想，蛇没有站或者趴这种说法，这种转换会考虑到这种情况吗？还是会生成一个站着的蛇？哈哈哈？</p><p>Question: 测试时如果把 target dataset 作为 content image，不知道会是什么效果。</p><p>Question: 作者是否隐藏了一个假设：content image 提供的信息或者属性是所有 object 都共享的？</p><img src="/2019/05/29/FUNIT/example.png" title="example"><h2 id="2-Few-shot-Unsupervised-Image-Translation"><a href="#2-Few-shot-Unsupervised-Image-Translation" class="headerlink" title="2. Few-shot Unsupervised Image Translation"></a>2. Few-shot Unsupervised Image Translation</h2><img src="/2019/05/29/FUNIT/network.png" title="network"><p>明确几个相关定义：</p><ul><li>数据集一共分为两个：source dataset (source class images), target dataset (target class images), 训练时只使用 source dataset, 测试时使用 source dataset 和 target dataset。</li><li>对于模型而言，输入分为一张 content image 和多张 class image，content image 提供姿势等信息，class image 提供类别等信息.</li><li>模型在训练时，content image 和 class image 都来自 source dataset，在测试时，content image 来自 source dataset，class image 来自 target dataset.</li></ul><p>generator G 的输入是 一张 content image $x$ and a set of K class images $\lbrace y_1,…,y_k \rbrace$, 从而得到输出图片 $\bar{x}$:</p><script type="math/tex; mode=display">\bar{x}=G(x, \lbrace y_1,...,y_k \rbrace)</script><p>其中，content image 属于 object class $c_x$，K 张 class images 属于 object class $c_y$，一般情况下，K 比较小并且 $c_x$ 不等于 $c_y$，称 G 为 few-shot image translator.得到的结果在外形看更像 $c_y$，在姿势等方面更像 $c_x$.</p><p>令 $\mathbb{S}$ and $\mathbb{T}$ 表示 the set of source classes and the set of target classes.在训练阶段，随机从 $\mathbb{S}$ 中选取两类图片 $c_x, c_y \in \mathbb{S}$ 输入 G，在测试时，class image 选自 $\mathbb{T}$，content image 选自 $\mathbb{S}$.</p><h3 id="2-1-Few-shot-Image-Translator"><a href="#2-1-Few-shot-Image-Translator" class="headerlink" title="2.1 Few-shot Image Translator"></a>2.1 Few-shot Image Translator</h3><ul><li>G: generator, consists of a content encoder $E_x$, a class encoder $E_y$, and a decoder $F_x$</li><li>$E_x: x \to z_x$: class-invariant latent representation, determines the local structure</li><li>$E_y: \lbrace y_1,…,y_k \rbrace \to z_y$: class-specific latent representation, control the global look, K 可大可小</li><li>$F_x: (z_x, z_y)\to \bar{x}$</li></ul><h3 id="2-2-Multi-task-Adversarial-Discriminator"><a href="#2-2-Multi-task-Adversarial-Discriminator" class="headerlink" title="2.2 Multi-task Adversarial Discriminator"></a>2.2 Multi-task Adversarial Discriminator</h3><ul><li>D: discriminator, multiple adversarial binary classification task</li><li>D 输入是一张图片，得到 $|\mathbb{S}|$ 个输出</li><li>当输入图片取自 real image of source class $c_x$，希望第 $c_x$ 个输出为真</li><li>当输入图片取自 fake image of translation out $c_x$，希望第 $c_x$ 个输出为假</li><li>属于 $c_x$ 类的图片预测为其他类 $(\mathbb{S}\setminus {c_x})$ 是真是假，则不关心</li><li>更新 G 时，仅仅希望第 $c_x$ 个输出为假</li><li>作者通过经验发现 $|\mathbb{S}|$ 个二分类器比一个 $|\mathbb{S}|$ 分类器得到的效果更好</li></ul><p>Question: 这种设置分类器的方法倒是没想过，不知道作者的思路由来，感觉这种多个二分类器要比多分类器更容易扩展，如果多一个类别，可以在不改变原有分类器的情况下加一个即可，但是多分类器的话需要全部替换</p><h3 id="2-3-Learning"><a href="#2-3-Learning" class="headerlink" title="2.3 Learning"></a>2.3 Learning</h3><p><strong>GAN loss</strong>:</p><script type="math/tex; mode=display">L_{GAN}(G,D)=E_x[-\log D^{c_x}(x)]+E_{x,\lbrace y_1,...,y_k \rbrace}[\log (1-D^{c_y}(\bar{x}))]</script><p><strong>content reconstruction loss</strong>:</p><script type="math/tex; mode=display">L_{R}(G)=E_x[\parallel x-G(x, \lbrace x \rbrace ) \parallel]</script><p>Question: 这个损失是为了约束 $\bar{x}$ 与 $x$ 相似吗？</p><p><strong>feature matching loss</strong>:</p><p>记 $D_f$ 为 feature extractor, 即移除 D 的分类器。</p><script type="math/tex; mode=display">L_F(G)=E_{x,\lbrace y_1,...,y_k \rbrace}[ \parallel D_f(\bar{x}-\sum _k \frac{D_f(y_k)}{K}) \parallel _1^1]</script><p>Question: 按理来说，通过分类损失就已经可以约束 $\bar{x}$ 和 $\lbrace y_1,…,y_k \rbrace$ 相似了，再加上 feature matching 的效果更明显吗？</p><p>Question: 是怎么约束 $\bar{x}$ 与 $x$ 在姿势等方面相似的？</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><p>训练时 $K=1$，测试时 $K=1, 5, 10, 15, 20$</p><p><strong>Baseline</strong>: 作者把 target class image 在训练阶段是否出现划分为 fair (unavailable) and unfair (available)</p><ul><li>Fair: StarGAN-Fair-K</li><li>Unfair: StarGAN-Unfair-K, CycleGAN-Unfair-K, UNIT-Unfair-K, MUNIT-Unfair-K</li></ul><img src="/2019/05/29/FUNIT/visual_comparison.png" title="visual comparison"><p>至于关于 GAN 的其他指标，则不列了，一是肯定效果好，二是自己也不是特别懂这些指标的意义和难度。</p><p>训练时 source classes 越多，效果越好。</p><p>在跨物种时，效果也会很差，只能改变 content image 的颜色。其类别没有发生变化。</p><img src="/2019/05/29/FUNIT/limitation.png" title="limitation">]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1905.01723&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;FUNIT: Few-Shot Unsupervised Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/NVlabs/FUNIT&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;代码还需要等段时间，才能下载到所有代码。&lt;/p&gt;
&lt;p&gt;这篇文章和郑哲东的 &lt;a href=&quot;https://arxiv.org/abs/1904.07223&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Joint Discriminative and Generative Learning for Person Re-identification&lt;/a&gt; 这篇文章看着有点像，果然是高手的思路都是相同的。不对，两个都是 NVIDIA 的，哈哈哈。&lt;/p&gt;
&lt;p&gt;这篇文章思路奇特，实现简单。&lt;/p&gt;
    
    </summary>
    
      <category term="GAN" scheme="http://yoursite.com/categories/GAN/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="few-shot" scheme="http://yoursite.com/tags/few-shot/"/>
    
  </entry>
  
  <entry>
    <title>STGAN</title>
    <link href="http://yoursite.com/2019/05/27/STGAN/"/>
    <id>http://yoursite.com/2019/05/27/STGAN/</id>
    <published>2019-05-27T03:03:40.000Z</published>
    <updated>2019-05-28T08:11:37.231Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1904.09709" target="_blank" rel="noopener">CVPR2019_STGAN: A Unified Selective Transfer Network for Arbitrary Image Attribute Editing</a></li><li>code: <a href="https://github.com/csmliu/STGAN" target="_blank" rel="noopener">tensorflow</a>, <a href="https://github.com/bluestyle97/STGAN-pytorch" target="_blank" rel="noopener">pytorch</a></li></ul><a id="more"></a><ul><li>reference: <a href="https://arxiv.org/abs/1711.09020" target="_blank" rel="noopener">CVPR2018_StarGAN：StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</a></li><li>code: <a href="https://github.com/yunjey/StarGAN" target="_blank" rel="noopener">pytorch</a></li><li>reference: <a href="https://arxiv.org/abs/1711.10678" target="_blank" rel="noopener">Valse2018_AttGAN: Facial Attribute Editing by Only Changing What You Want</a></li><li>code: <a href="https://github.com/LynnHo/AttGAN-Tensorflow" target="_blank" rel="noopener">tensorflow</a>, <a href="https://github.com/elvisyjlin/AttGAN-PyTorch" target="_blank" rel="noopener">pytorch</a></li></ul><p>这也是一篇做GAN的文章，根据论文的叙述，一直和 StarGAN, AttGAN 做对比。我只看过 StarGANs，没有看过 AttGAN.</p><!--more--><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>作者首先发现了 skip connection 和 target attribute vector 的缺点: skip connection 对重建有用，但是对于 attribute manipulation 有害，target attribute vector 对重建有害。</p><p>作者提出自己的创新点时论据很充分。</p><p>作者在使用GAN时，发现 skip connection 可以提高重建图片质量(图片属性和生成图片属性一致时)(reconstruction)，但是会减弱属性变化的程度(图片属性和生成图片不一致时)(attribute manipulation)。</p><p>作者从 selective transfer 角度来解决这些问题:</p><ul><li>selective: a. 仅仅考虑改变的属性；b. 有选择地拼接 encoder feature and decoder feature</li><li>transfer: 对局部和全局属性提供一个统一的框架</li></ul><p>Question: transfer 的解释不是很理解，看后面如何应用吧。</p><p>创新点：</p><ul><li>输入的是 <strong>different attribute vector</strong></li><li>z针对skip connection, 使用 <strong>selective transfer units</strong>，同时提高输入属性的影响力和图片质量</li></ul><h2 id="2-Proposed-Method"><a href="#2-Proposed-Method" class="headerlink" title="2. Proposed Method"></a>2. Proposed Method</h2><h3 id="2-1-Limitation-of-Skip-Connection-in-AttGAN"><a href="#2-1-Limitation-of-Skip-Connection-in-AttGAN" class="headerlink" title="2.1 Limitation of Skip Connection in AttGAN"></a>2.1 Limitation of Skip Connection in AttGAN</h3><img src="/2019/05/27/STGAN/AttGAN1.png" title="AttGAN"><p>可以看出，增加 skip connections 有益于重建图像(图片属性和生成属性一致)(reconstruction)，但是，会生成其他属性的图片时的精度会降低(图片属性和生成属性不一致)(attribute manipulation ability)。主要原因是直接 concatenates encoder and decoder features.</p><p>作者为了解决这个问题，使用 selective transfer units to adaptively transform encoder features guided by attributes to be changed.</p><h3 id="2-2-Taking-Difference-Attribute-Vector-as-Input"><a href="#2-2-Taking-Difference-Attribute-Vector-as-Input" class="headerlink" title="2.2 Taking Difference Attribute Vector as Input"></a>2.2 Taking Difference Attribute Vector as Input</h3><img src="/2019/05/27/STGAN/reconstruction.png" title="reconstruction"><p>StarGAN and AttGAN 是把 target attribute vector $att_t$ 做为输入，作者通过观察 StarGAN and AttGAN 的重建图片，发现 attribute vector 会对重建图片有害。其实也不难理解，要想不发生变换，最好是 x0，但是 attribute vector 肯定不是0，卷积层也不可能是0，自然 attribute vector 会改变生成的图片，尤其是重建图片，因为此时是希望完全不变的，要求更高一些。</p><p>作者针对 target attribute vector 的缺点提出 difference attribute vector。</p><script type="math/tex; mode=display">att_{diff}=att_t-att_s</script><h3 id="2-3-Selective-Transfer-Units-STU"><a href="#2-3-Selective-Transfer-Units-STU" class="headerlink" title="2.3 Selective Transfer Units(STU)"></a>2.3 Selective Transfer Units(STU)</h3><p>作者使用修改版的 GRU 来建立 STUs 作为 skip connection.</p><p>设第 $l$ 层的 encoder feature 表示为 $f_{enc}^l$，第 $l+1$ 层的 hidden state 为 $s^{l+1}$，从而有下述方程，更新 hidden state $s^l$ 和 transformed encoder feature $f_t^l$:</p><script type="math/tex; mode=display">\begin{aligned}\hat{s}^{l+1}&=W_{t*T}[s^{l+1}, att_{diff}]\\r^l &= \sigma(W_r * [f_{enc}^l, \hat{s}^{l+1}])\\z^l &= \sigma(W_z * [f_{enc}^l, \hat{s}^{l+1}])\\s^l&=r^l\circ \hat{s}^{l+1}\\\hat{f}_t^l &= tanh(W_h*[f_{enc}^l, s^l])\\f_t^l &= (1-z^l)\circ\hat{s}^{l+1}+z^l\circ \hat{f}_t^l\end{aligned}</script><p>其中 $[\cdot ,\cdot]$ 表示 concatenation operation，$*T$ 表示 transposed convolution.</p><p>$*$ 表示 convolution operation，$\circ$ 表示 entry-wise product，$\sigma(\cdot)$ 表示 sigmoid function.</p><p>看着很像常规意义上的 LSTM 的方程，但是自己在这方面接触比较少。所以简单地记录下GRU的原始公式。</p><img src="/2019/05/27/STGAN/GRU1.png" title="GRU"><p>两者的公式基本上是一致的，但是不太懂其原理或者可以达到的效果。另外其含义也发生了一些变化，主要是 GRU 中，$f_t^l$ 是 output of hidden state，作者把 $s^l$ 作为 output of hidden state，$f_t^l$ 作为 output of transformed encoder feature。</p><p>不懂。但我觉得能提出这个，就很厉害。</p><h3 id="2-4-Network-Architecture"><a href="#2-4-Network-Architecture" class="headerlink" title="2.4 Network Architecture"></a>2.4 Network Architecture</h3><p>STGAN 分为两部分: generator G and discriminator D.</p><p>G: $G_{enc}$ and $G_{dec}$，都分别有5个卷积操作。STU应用在前四个 encoder layers,即 </p><script type="math/tex; mode=display">(f_t^l, s^l)=G_{st}^l(f_{enc}^l, s^{l+1}, att_{diff})</script><p>D: $D_{adv}$ and $D_{att}$. $D_{adv}$ and $D_{att}$ 共享前五个卷积层，分别有两个全连接层用于预测。</p><h3 id="2-5-Loss-Functions"><a href="#2-5-Loss-Functions" class="headerlink" title="2.5 Loss Functions"></a>2.5 Loss Functions</h3><p>第一步: 给定一张图片 $x$，可以得到 encoder features:</p><script type="math/tex; mode=display">f=\lbrace f_{enc}^1, f_{enc}^2,..., f_{enc}^5 \rbrace=G_{enc}(x)</script><p>第二步: 进而通过 STUs 得到 transform encoder features:</p><script type="math/tex; mode=display">(f_t^l, s^l)=G_{st}^l(f_{enc}^l, s^{l+1}, att_{diff})</script><p>其中，不同的 STU 之间不共享参数。<br>即：</p><script type="math/tex; mode=display">f_t=\lbrace f_t^1, f_t^2, f_t^3, f_t^4 \rbrace</script><p>第三步: 得到结果,</p><script type="math/tex; mode=display">\hat{y}=G_{dec}(f_{enc}^5, f_t)</script><p>即:</p><script type="math/tex; mode=display">\hat{y}=G(x, att_{diff})</script><p><strong>Reconstruction loss</strong>:</p><script type="math/tex; mode=display">L_{rec}=\parallel x-G(x,0) \parallel_1</script><p><strong>Adversarial loss</strong>:</p><script type="math/tex; mode=display">\max_{D_{adv}} L_{D_{adv}} = \mathbb{E}_x D _{adv}(x)-\mathbb{E} _{\hat{y}} D _{adv}(\hat{y})+\lambda \mathbb{E} [ (\parallel \nabla _{\hat{x}} D _{adv}(\hat{x}) \parallel_2 -1)^2 ]</script><script type="math/tex; mode=display">\max_{G} L_{G_{adv}} = \mathbb{E}_{x, att _{diff}} D _{adv}(G(x, att _{diff}))</script><p>其中 $\hat{x}$ 表示真和生成图片的线性插值，在 StarGAN 中也见到过。</p><p><strong>Attribute manipulation loss</strong>:</p><script type="math/tex; mode=display">L_{D_{att}}=- \sum _{i=1}^c[ att_s^{(i)}\log D_{att}^{(i)}(x)+(1-att_s^{(i)})\log (1-D_{att}^{(i)}(x)) ]</script><script type="math/tex; mode=display">L_{G_{att}}=- \sum _{i=1}^c[ att_t^{(i)}\log D_{att}^{(i)}(\hat{y})+(1-att_s^{(i)})\log (1-D_{att}^{(i)}(\hat{y})) ]</script><p>这里应该只是一个简单的分类器。</p><p>Question: 为什么还会有第二项？</p><p><strong>Model Objective</strong>:</p><script type="math/tex; mode=display">\min_D L_D= -L_{D_{adv}} + \lambda_1 L_{D_{att}}</script><script type="math/tex; mode=display">\min_G L_G= -L_{G_{adv}} + \lambda_2 L_{G_{att}} + \lambda_3 L_{rec}</script><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><p>结果很棒，在真实性和分类准确率上都要比其它的GAN好很多。</p><h2 id="5-Ablation-Study"><a href="#5-Ablation-Study" class="headerlink" title="5. Ablation Study"></a>5. Ablation Study</h2><img src="/2019/05/27/STGAN/ablation_study.png" title="ablation study"><ul><li>STGAN: original STGAN</li><li>STGAN-dst: target attribute vector, not difference attribute vector</li><li>STGAN-conv: conv(encoder feature and difference attribute vector), not STU</li><li>STGAN-conv-res: residual learning formulation to learn the convolution operator in STGAN-conv</li><li>STGAN-gru: GRU not STU</li><li>STGAN-res: residual learning formulation to learn the STU in STGAN</li></ul><p>作者的对比实验做的是真多。</p><p><strong>Difference attribute vector vs. target attribute vector</strong>: 可以看出，AttGAN, StarGAN, STGAN 三个模型上 difference attribute vector 都要好于 target attribute vector。</p><p><strong>Selective Transfer Unit vs. its variants</strong>: STGAN-conv, STGAN-conv-res 性能很低，STGAN-gru, STGAN-res, STGAN 三种方法的性能差不太多，可能具体到某个属性会略有区别，因为作者没有一个最终的指标，所以说不上来到底差多少，但是从各个属性上看，STGAN最好，其他两个略微低一些。</p><p>Question: 具体 GRU, STU 的工作机制有机会的话还是需要多了解一下。</p><h2 id="6-code"><a href="#6-code" class="headerlink" title="6. code"></a>6. code</h2><p>因为最近在跑实验，所以没法具体跑论文的代码，以后再看情况把。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1904.09709&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019_STGAN: A Unified Selective Transfer Network for Arbitrary Image Attribute Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/csmliu/STGAN&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;tensorflow&lt;/a&gt;, &lt;a href=&quot;https://github.com/bluestyle97/STGAN-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="GAN" scheme="http://yoursite.com/categories/GAN/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Deep CV-MIML</title>
    <link href="http://yoursite.com/2019/05/22/Deep-CV-MIML/"/>
    <id>http://yoursite.com/2019/05/22/Deep-CV-MIML/</id>
    <published>2019-05-22T02:26:35.000Z</published>
    <updated>2019-05-25T09:17:05.233Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1904.03832" target="_blank" rel="noopener">CVPR2019 Weakly Supervised Person Re-Identification</a></li><li>code:</li></ul><a id="more"></a><p>这篇论文主要的创新点是：不再需要在 video 的每一帧都给出行人的位置和 id，而是直接赋予一段 video 一个 video-level label，即这段 video 中有哪些人。主要目的也是为了降低标注成本。有点重新定义行人重识别问题的意思。我觉得很有意思，也更贴近实际一些。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>在传统的行人重识别设定中，每张图片表示用方框圈出来的行人。因此，作者提出了 <strong>weakly supervised setting</strong>，即在训练阶段，只有 video-level label，因此此时每段 video 中有多个行人，有个标签，即 multi-instance multi-label(<strong>MIML</strong>)。</p><p><strong>订正(2019-05-24)</strong>：论文中的 train set 分为：probe set 和 gallery set，probe set 每个行人的一系列图片组成一个 bag ，有准确的标签，gallery set 一段 video 组成一个 bag，取出这段 video 每一帧自动检测出的行人图片，有 video level label，但是不知道具体哪个行人对应哪个标签。</p><p>提出：</p><ul><li>intra-bag alignment</li><li>cross-view bag alignment</li></ul><img src="/2019/05/22/Deep-CV-MIML/weakly_supervised_setting.png" title="weakly supervised setting"><p>基本定义：</p><ul><li>gallery set 中的 every video clip 为一个 bag</li><li>每个 bag 包含多个行人的图片，并标记 video-level label</li><li>probe set 中是经过手工标记的图片，为了统一，也视这些图片为一个 bag</li></ul><p>解决的问题：</p><ul><li>每个 bag 中包含相同的行人: intra-bag alignment</li><li>bag 与 bag （跨摄像头）之间也包含相同的行人: cross-view bag alignment</li></ul><p>现有的 MIML 直接应用到 person re-id 中存在的问题</p><ul><li>现有的 MIML 忽略了 intra-bag variation，即 bag 内会有相同类别的实例</li><li>现有的 MIML 假设 instance-level label 是高度相关的，但是 person re-id 中的行人都相互独立的</li><li>现有的 MIML 没有考虑到 bag 之间的 (cross-view) 相关性</li></ul><h2 id="2-The-Proposed-Approach"><a href="#2-The-Proposed-Approach" class="headerlink" title="2. The Proposed Approach"></a>2. The Proposed Approach</h2><h3 id="2-1-Problem-Statement-and-Notation"><a href="#2-1-Problem-Statement-and-Notation" class="headerlink" title="2.1 Problem Statement and Notation"></a>2.1 Problem Statement and Notation</h3><p>在 weakly supervised person re-id setting 中，不再是以图搜图，而是以图搜视频，或者以视频搜视频，即给定同一个行人组成的视频，在只经过自动画框的视频中找出这个行人。</p><div class="table-container"><table><thead><tr><th style="text-align:left">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">$C$</td><td style="text-align:left">identities</td></tr><tr><td style="text-align:left">$V$</td><td style="text-align:left">camera views</td></tr><tr><td style="text-align:left">$\tilde{C}$</td><td style="text-align:left">$\tilde{C}=C+1$</td></tr><tr><td style="text-align:left">$\cal{X}$</td><td style="text-align:left">the training set</td></tr><tr><td style="text-align:left">$N_{\cal{X}}$</td><td style="text-align:left">$N_{\cal{X}}$ video in the training set</td></tr><tr><td style="text-align:left">$\cal{X}_p, \cal{X}_g$</td><td style="text-align:left">probe set and gallery set,and $\cal{X}_p + \cal{X}_g=\cal{X}$</td></tr><tr><td style="text-align:left">$\cal{X}_p=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_p}$</td><td style="text-align:left">probe set</td></tr><tr><td style="text-align:left">$\cal{X}_g=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_g}$</td><td style="text-align:left">gallery set</td></tr><tr><td style="text-align:left">$v_b \in \lbrace 1,2,…,V \rbrace$</td><td style="text-align:left">摄像头</td></tr><tr><td style="text-align:left">$y_b=[y_b^0, y_b^1,…, y_b^C]\in\lbrace 0,1 \rbrace^{\tilde{C}}$</td><td style="text-align:left">行人标签</td></tr><tr><td style="text-align:left">$X_b=\lbrace x_{b,1}, x_{b,2},…, x_{b,n_b} \rbrace $</td><td style="text-align:left">多实例</td></tr><tr><td style="text-align:left">$x_{b,i}=f_e(I_b;\theta)\in \mathbb{R}^d$</td><td style="text-align:left">图片 $I_b$，特征提取方程 $f_e(\cdot;\theta)$</td></tr><tr><td style="text-align:left">$f_c(\cdot;W)$</td><td style="text-align:left">classifier</td></tr><tr><td style="text-align:left">$y_{b,i}=f_c(x_{b,i};W)$</td><td style="text-align:left">分类器</td></tr></tbody></table></div><p>基本保证每个行人至少在两个摄像头下出现过，并视在 untrimmed videos 中的出现的未知行人为新的一类，即第0类，所以 $\tilde{C}=C+1$</p><p>Question: probe set 是图片，gallery set 是视频，那training set 是什么？也是视频吗？ gallery set 和 training set 是一个集合还是完全不同的集合？</p><p>gallery set 中的视频没有经过人工画框，只有 video-level weak label，其中的行人是自动检测出的，没有人工标定行人标签，所以具体到行人对应哪个标签也是未知的。</p><p>probe set 中，each query 由一系列同一个行人的检测图片组成。为了统一，视 probe set 中的 each query 也为一个 bag，此时 video-level 应该是单实例的。</p><p>所以，可以统一为：$\cal{X}=\lbrace \cal{X}_p, \cal{X}_g \rbrace$，其中 $\cal{X}_p=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_p}$，$\cal{X}_g=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_g}$.其中 probe set 的 bag 是单标签的，gallery set 的 bag label 为0时，表示不确定这个行人是否在这个 video 中出现过。</p><p>重新定义问题需要重新定义好多概念。</p><h3 id="2-2-Cross-View-MIML-for-Person-Re-id"><a href="#2-2-Cross-View-MIML-for-Person-Re-id" class="headerlink" title="2.2 Cross-View MIML for Person Re-id"></a>2.2 Cross-View MIML for Person Re-id</h3><h4 id="2-2-1-Weakly-Supervised-Person-Re-id-MIML"><a href="#2-2-1-Weakly-Supervised-Person-Re-id-MIML" class="headerlink" title="2.2.1 Weakly Supervised Person Re-id MIML"></a>2.2.1 Weakly Supervised Person Re-id MIML</h4><p>令 classifier $f_c(\cdot;W)$, 对于 probe set $\mathcal{X}_p$，一个 bag 内的所有实例 $\lbrace x _{b,i} \rbrace _{i=1}^{n_b}$ 标签都是一样的 label $y_b$，此时可以可以直接使用交叉熵损失函数。对于 gallery set $\mathcal{X}_g$，共享 weak video-level label $y_b$，很明显这时候不能直接使用交叉熵损失函数，所以 probe set 和 gallery set 需要分开训练。</p><p>对于 <strong>probe set</strong> 中的 bag，可以直接使用 instance-level 交叉熵损失函数。</p><script type="math/tex; mode=display">L_p=\frac{1}{N_p}\displaystyle \sum_{X_b \in \mathcal{X}_p} \sum _{i\in \lbrace 1,...,n_b \rbrace} \sum _{c\in \lbrace 0,...,C \rbrace} (-y_b^c\log \tilde{y}_{b,i}^c)</script><p>其中，$y_b^c$ 表示第 b 个 bag 的第 c 类的真实 bag 标签，$\tilde{y}_{b,i}^c$ 表示第 b 个包的第 i 个图片的第 c 类的预测标签。</p><p>Question: probe set 中，一个 bag 是由行人的图片组成的像上面的一样，而 gallery set 中，一个bag的组成图片是一大张，行人在其中只占到很小的比例，明显两个不是一个尺度的，所以(根据 gallery set 的预测函数得出的猜测)猜测 gallery set 在进行训练时，是把每一帧中的行人(已经自动检测出)图片抠出来，然后送入网络，那么 gallery set 中，bag 大小不是表示这个 video 的帧数，而是这段 video 自动检测出来的行人图片个数，如果是这样的话，那怎么解决检测出的未知人(第0类)和丢失的行人。</p><p>对于 <strong>gallery set</strong> 中的 bag，因为是多实例多标签，所以用的方法略微不同，之前没见过，先举个例子解释下。</p><p>假设一个 bag 内有两个图片，label 是三类，即: $X_b=\lbrace x_{b,1}, x_{b,2} \rbrace $, $y_b=[1,0,1]$，预测标签是 $\tilde{y}_ {b,1}=[0.1, 0.2, 0.7], \tilde{y}_{b,2}=[0.3, 0.4, 0.3]$，对于每一类，先找出该类预测概率最大的图片，即 $q_1=2, q_2=2, q_3=1$，即第一类和第二类在第二张图片出现的概率最大，第三类在第一张图片出现的概率最大(纵向比较，而非常规意义上的横向比较)，从而得到bag的预测概率 $\tilde{y}=[0,3, 0.4, 0.7]$，然后再做个交叉熵。</p><p>Question:其理论基础是什么。</p><p>选择第 c 类后验概率最大的实例，记为 <strong>seed instance</strong> $x_{b, q_c}$，其中 $q_c$ 的定义为：</p><script type="math/tex; mode=display">q_c= \mathop{argmax}_{i\in \lbrace 1,2,...,n_b \rbrace} \lbrace \tilde{y}_{b,i}^c \rbrace</script><p>然后定义 bag-level 分类损失:</p><script type="math/tex; mode=display">L_g=\frac{1}{N_g}\displaystyle \sum_{X_b \in \mathcal{X}_g} \sum _{c\in \lbrace 0,...,C \rbrace} (-y_b^c\log \max \lbrace \tilde{y}_ {b,1}^c, \tilde{y}_ {b,2}^c, ..., \tilde{y}_{b,n_b}^c \rbrace )</script><p>从而得到 <strong>MIML classification loss</strong>:</p><script type="math/tex; mode=display">L_C=L_p+L_g</script><h3 id="2-2-2-Intra-bag-Alignment"><a href="#2-2-2-Intra-bag-Alignment" class="headerlink" title="2.2.2 Intra-bag Alignment"></a>2.2.2 Intra-bag Alignment</h3><img src="/2019/05/22/Deep-CV-MIML/intra_bag_alignment.png" title="intra bag alignment"><p>应该是指一段视频中即一个 bag，因为行人出现在连续的帧中，所以每一帧的行人分布是差不多的，因此可以尽可能地将属于同一个人的图片聚在一起。</p><p>在 gallery set 中，对于第 c 类已经找到 seed instance $x_{b, q_c}$，即对于第 c 类，这张图片比其他图片更像，从而构建一个 group 包含类似第 c 类的图片</p><script type="math/tex; mode=display">G_{b,c}=\lbrace p|x_{b,p}\in \mathcal{N}_{q_c} \text{ and } \tilde{y}_{b,p}^c \ge \gamma \tilde{y} _{b,q_c}^c \rbrace</script><p>其中，$\mathcal{N}_{q_c}$ 表示 $x_{b,q_c}$ 的 K-近邻，$\gamma\in (0, 1)$，上述的公式表示这个 group 中的图片既需要满足在特征上相近，也要满足预测概率大于预定值。</p><p>Question: soft multi-label, ECN 在选择正样本和负样本的时候的构建规则和这个就不太一样，或者说这么多构建最相似的规则，其区别大概在什么地方，特征相似，概率相似。不太懂，可能还是自己见得少吧。</p><p>因此可以得到 <strong>intra-bag alignment loss</strong>:</p><script type="math/tex; mode=display">L_{IA}=\frac{1}{N_{IA}} \displaystyle\sum_{X_b\in \mathcal{X}_g} \sum _{c\in \lbrace 0,...C \rbrace} \sum _{p \in G _{b,c}} y_b^c D_{KL}(\tilde{y}_{b,p}||\tilde{y} _{b,q_c})</script><script type="math/tex; mode=display">D_{KL}(\tilde{y}_{b,p}||\tilde{y} _{b,q_c})=\sum_{c\in\lbrace 0,...,C \rbrace}\tilde{y}_{b,p}^c(\log \tilde{y} _{b,p}^c -\log \tilde{y} _{b,q_c}^c)</script><p>这个是用KL散度衡量分布一致性。</p><h3 id="2-2-3-Cross-view-Bag-Alignment"><a href="#2-2-3-Cross-view-Bag-Alignment" class="headerlink" title="2.2.3 Cross-view Bag Alignment"></a>2.2.3 Cross-view Bag Alignment</h3><p>intra-bag alignment and cross-view bag alignment 都是在尽量找到同一个行人的不同图片并且使这些图片得到特征相似，预测类别相似。</p><img src="/2019/05/22/Deep-CV-MIML/cross_view_bag_alignment.png" title="cross view bag alignment"><p>理论上来说，不同 view 下的同一个行人的特征应该也是相似的，因此，对每个行人引入 <strong>distribution prototype</strong>，并且同一个行人的图片特征应该近似 distribution prototype 的特征。公式定义为：第 t 个 epoch 时 第 c 类行人的 distribution prototype 为 $\hat{p}_c^t$:</p><script type="math/tex; mode=display">p_c^t=\frac{1}{|V_c|} \sum_{v\in V_c} (\frac{1}{|I_{c,v}|} \sum_{i\in I_{c,v}}\tilde{y}_i )</script><script type="math/tex; mode=display">\hat{p}_c^t=\alpha \hat{p}_c^{t-1}+(1-\alpha)p_c^t</script><p>其中，$V_c$ 表示所有的 camera view，$I_{c,v}$ 表示在第 v 个 camera view 下第 c 类的所有实例，可以理解成，在 gallery set 中，每个 bag 是一段视频，视频的每一帧是一张大图片，然后从上面扣下来行人图片，即行人图片是指第几个 bag 的第几帧的第几个行人，同时 bag 还有 camera view 信息，即第几个摄像头拍的。</p><p>Question: $I_{c,v}$ 是怎么得到的，在训练阶段， probe set 是可以具体到 instance-level label，在 gallery set 只有 bag-level label，那怎么得出属于同一类的 instance，是通过 $G_{b,c}$ 吗？总感觉怪怪的。</p><p>从而得到对齐损失函数：</p><script type="math/tex; mode=display">L_{CA}=\frac{1}{N_{CA}} \sum_{X_b\in(\mathcal{X}_p \bigcup \mathcal{X}_g)} \sum _{c\in \lbrace 0,...C \rbrace} \sum _{i \in I_c} y_b^c D_{KL}(\tilde{y}_{b,i}||\hat{p}_c^t)</script><p>$I_c$ 表示第 c 类的所有 instance.</p><p>需要注意的是：两次求 KL-散度 都在前面乘以了 bag-level label 用以衡量当前 video 当前类的重要性。</p><p>这两个公式都写的很长哈。而且是那种需要把全部图片遍历一遍才可以计算的那种。</p><h3 id="2-3-Deep-Cross-view-MIML-Model"><a href="#2-3-Deep-Cross-view-MIML-Model" class="headerlink" title="2.3 Deep Cross-view MIML Model"></a>2.3 Deep Cross-view MIML Model</h3><p>作者提出了一个新的 <strong>entropy regularization</strong> term，在 gallery set 中，总是会存在一些 outlier instance，在 intra-bag and cross-view alignment 中远离 data group，为了避免这种情况，设计了个正则项:</p><script type="math/tex; mode=display">L_E=\frac{1}{N_E} \sum_{X_b\in \mathcal{X}_g} \sum _{i\in \lbrace 1,...n_b \rbrace} \sum _{c\in \lbrace 0,...C \rbrace} (-\tilde{y} _{b,i}^c \log \tilde{y} _{b,i}^c )</script><p>这是信息熵公式，应该是想尽可能使预测的概率陡峭，或为1或为0.</p><p>这个公式来得有点突然啊，是有什么实验证明吗？</p><p><strong>Cross-view Multi-label Multi-Instance learning (CV-MIML)</strong>:</p><script type="math/tex; mode=display">L_{CV-MIML}=L_C+\delta (L_{IA}+L_{CA}+L{E})</script><h3 id="2-4-Implementation-Details"><a href="#2-4-Implementation-Details" class="headerlink" title="2.4 Implementation Details"></a>2.4 Implementation Details</h3><p>实现还是有点意思的，希望看到代码吧。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><img src="/2019/05/22/Deep-CV-MIML/ablation_study.png" title="ablation study"><img src="/2019/05/22/Deep-CV-MIML/comparison.png" title="comparison">]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1904.03832&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019 Weakly Supervised Person Re-Identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code:&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="person re-id" scheme="http://yoursite.com/categories/person-re-id/"/>
    
    
      <category term="person re-id" scheme="http://yoursite.com/tags/person-re-id/"/>
    
  </entry>
  
  <entry>
    <title>BNNeck</title>
    <link href="http://yoursite.com/2019/05/17/BNNeck/"/>
    <id>http://yoursite.com/2019/05/17/BNNeck/</id>
    <published>2019-05-17T08:20:21.000Z</published>
    <updated>2019-05-18T07:02:18.096Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1903.07071" target="_blank" rel="noopener">Bag of Tricks and A Strong Baseline for Deep Person Re-identification</a></li><li>code:　<a href="https://github.com/michuanhaohao/reid-strong-baseline" target="_blank" rel="noopener">pytorch</a></li></ul><p>这篇论文是旷视科技 Video Team 做的。</p><p>这篇论文主要介绍 re-id 代码中的各种 trick 的作用。</p><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>作者总结了各种 trick 并与 ECCV2018 CVPR2018 的论文做了比较，实验证明，只需要通过各种 trick 的组合，就比提出的算法高很多。</p><img src="/2019/05/17/BNNeck/strong_baseline.png" title="strong baseline"><h2 id="2-Standard-Baseline"><a href="#2-Standard-Baseline" class="headerlink" title="2. Standard Baseline"></a>2. Standard Baseline</h2><ol><li>ResNet50 为基础网络，修改最后一个 fc 层的输出维度为行人类别 $N$.</li><li>随机取 $P$ 个人， 每个人 $K$ 张图片，所以每个 batch $B=P\times K$，作者设置 $P=16, K=4$.</li><li>图片 resize 成 256x128，并添加10个0元素的 pad，然后随机 crop 成 256x128.</li><li>图片随机水平翻转概率设置为0.5.</li><li>图片的像素值转化为 [0,1]，并且 normalize: mean=0.485, 0.456, 0.406, std=0.229, 0.224, 0.225.</li><li>模型提取特征记为 $f$，ID 的预测值为 $p$.</li><li>ReID features $f$ 用于计算 triplet loss，ID prediction logits $p$ 用于计算交叉熵，margin $m=0.3$</li><li>优化器: Adam: lr=0.00035, epoch=120, 第40和第70个 epoch 乘 0.1.</li></ol><h2 id="3-Training-Tricks"><a href="#3-Training-Tricks" class="headerlink" title="3. Training Tricks"></a>3. Training Tricks</h2><img src="/2019/05/17/BNNeck/pipelines_of_baseline.png" title="pipelines of baseline"><h3 id="3-1-Warmup-Learning-Rate"><a href="#3-1-Warmup-Learning-Rate" class="headerlink" title="3.1 Warmup Learning Rate"></a>3.1 Warmup Learning Rate</h3><p>Question: 前10个epoch是不是写错了，应该是-4，不是-5</p><img src="/2019/05/17/BNNeck/warmup.png" title="warmup"><script type="math/tex; mode=display">lr(t)=\begin{cases}3.5\times 10^{-5}\times \frac{t}{10}, &\text{if } 10 \ge t \ge 1 \\3.5\times 10^{-4}, &\text{if } 40 \ge t \ge 10 \\3.5\times 10^{-5}, &\text{if } 70 \ge t \ge 40 \\3.5\times 10^{-6}, &\text{if } 120 \ge t \ge 70 \\\end{cases}</script><h3 id="3-2-Random-Erasing-Augmentation"><a href="#3-2-Random-Erasing-Augmentation" class="headerlink" title="3.2 Random Erasing Augmentation"></a>3.2 Random Erasing Augmentation</h3><img src="/2019/05/17/BNNeck/REA.png" title="REA"><ul><li>probability: $p_e=0.5$</li><li>rectangle region $I_e: S_e=W_e\times H_e$, $0.02&lt;S_e&lt;0.4$</li><li>area ratio: $r_1&lt; r_e=\frac{S_e}{S} &lt; r_2$, $r_1=0.3, r_2=3.33$</li></ul><h3 id="3-3-Label-Smoothing"><a href="#3-3-Label-Smoothing" class="headerlink" title="3.3 Label Smoothing"></a>3.3 Label Smoothing</h3><script type="math/tex; mode=display">q_i=\begin{cases}1-\frac{N-1}{N}\epsilon, &\text{if } i=y \\\frac{\epsilon}{N}, &\text{otherwise}\end{cases}</script><p>$\epsilon=0.1$</p><h3 id="3-4-Last-Stride"><a href="#3-4-Last-Stride" class="headerlink" title="3.4 Last Stride"></a>3.4 Last Stride</h3><ul><li>stride=2: 256x128-&gt;8x4</li><li>stride=1: 256x128-&gt;16x8</li></ul><h3 id="3-5-BNNeck"><a href="#3-5-BNNeck" class="headerlink" title="3.5 BNNeck"></a>3.5 BNNeck</h3><ul><li>ID loss: 更偏向于 cosine distance</li><li>triplet loss: 更偏向于 Euclidean distance</li></ul><img src="/2019/05/17/BNNeck/BNNeck.png" title="BNNeck"><img src="/2019/05/17/BNNeck/loss.png" title="loss"><h3 id="3-6-Center-Loss"><a href="#3-6-Center-Loss" class="headerlink" title="3.6 Center Loss"></a>3.6 Center Loss</h3><p>三元组损失只能使一个 batch 内的正负样本的值相差比较大，却不能考虑全局的正负样本值。</p><p>Triplet loss:</p><script type="math/tex; mode=display">T_{Tri}=[ d_p-d_n+\alpha ]_+</script><p>Center loss:</p><script type="math/tex; mode=display">L_{C}=\frac{1}{2} \sum_{j=1}^B \parallel f_{t_j} -c_{y_j} \parallel_2^2</script><p>其中 $y_j$ 是第 j 张图片的label，$c_{y_j}$ 表示第 $y_j$ 类的特征的中心，$f_{t_j}$ 表示提取的特征 $f_t$.</p><p><strong>Overall</strong>:</p><script type="math/tex; mode=display">L=L_{ID}+L_{Triplet}+\beta L_{C}</script><p>$\beta=0.0005$</p><h2 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4. Experimental Results"></a>4. Experimental Results</h2><p>作者一共做了两组实验，一组是在 source damain 上的，一组是在 cross domain 上的。</p><h3 id="4-1-Influences-of-Each-Trich-Same-domain"><a href="#4-1-Influences-of-Each-Trich-Same-domain" class="headerlink" title="4.1 Influences of Each Trich (Same domain)"></a>4.1 Influences of Each Trich (Same domain)</h3><img src="/2019/05/17/BNNeck/same_domain.png" title="same domain"><p>这些应该是在前面各种 trick 已经有的情况再加一个得到的结果。</p><h3 id="4-2-Analysis-of-BNNeck"><a href="#4-2-Analysis-of-BNNeck" class="headerlink" title="4.2 Analysis of BNNeck"></a>4.2 Analysis of BNNeck</h3><img src="/2019/05/17/BNNeck/ablation_study_of_BNNeck.png" title="ablation study of BNNeck"><p>我觉得这个表格说明了 bn 层是用的，但是在测试的时候取 $f_t$ 还是 $f_i$ ，用 cosine 还是 Euclidean 是无所谓的。</p><h3 id="4-3-Influences-of-Each-Trick-Cross-domain"><a href="#4-3-Influences-of-Each-Trick-Cross-domain" class="headerlink" title="4.3 Influences of Each Trick　(Cross domain)"></a>4.3 Influences of Each Trick　(Cross domain)</h3><img src="/2019/05/17/BNNeck/cross_domain.png" title="cross domain"><p>warmup and label smoothing 更有用一些，stride=1, center loss 没啥用，REA 有负作用。</p><h2 id="5-Supplementary-Experiments"><a href="#5-Supplementary-Experiments" class="headerlink" title="5. Supplementary Experiments"></a>5. Supplementary Experiments</h2><h3 id="5-1-Influences-of-the-Number-of-Batch-Size"><a href="#5-1-Influences-of-the-Number-of-Batch-Size" class="headerlink" title="5.1 Influences of the Number of Batch Size"></a>5.1 Influences of the Number of Batch Size</h3><img src="/2019/05/17/BNNeck/batch_size.png" title="batch size"><p>差别也就在2个点足有，不是特别大，但是更大的 batch 是更有用的。</p><h3 id="5-2-Influences-of-Image-Size"><a href="#5-2-Influences-of-Image-Size" class="headerlink" title="5.2 Influences of Image Size"></a>5.2 Influences of Image Size</h3><img src="/2019/05/17/BNNeck/image_size.png" title="image size"><p>也差不太多</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1903.07071&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bag of Tricks and A Strong Baseline for Deep Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code:　&lt;a href=&quot;https://github.com/michuanhaohao/reid-strong-baseline&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这篇论文是旷视科技 Video Team 做的。&lt;/p&gt;
&lt;p&gt;这篇论文主要介绍 re-id 代码中的各种 trick 的作用。&lt;/p&gt;
    
    </summary>
    
      <category term="re-id" scheme="http://yoursite.com/categories/re-id/"/>
    
    
      <category term="re-id" scheme="http://yoursite.com/tags/re-id/"/>
    
  </entry>
  
  <entry>
    <title>MAR</title>
    <link href="http://yoursite.com/2019/05/13/MAR/"/>
    <id>http://yoursite.com/2019/05/13/MAR/</id>
    <published>2019-05-13T03:29:27.000Z</published>
    <updated>2019-05-15T10:48:41.828Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1903.06325v2" target="_blank" rel="noopener">CVPR2019 Unsupervised Person Re-identification by Soft Multilabel Learning</a></li><li>code: <a href="https://github.com/KovenYu/MAR" target="_blank" rel="noopener">pytorch</a></li><li>参考链接: <a href="https://www.cnblogs.com/Thinker-pcw/p/10807681.html" target="_blank" rel="noopener">https://www.cnblogs.com/Thinker-pcw/p/10807681.html</a></li></ul><p>出发点 Multi-label 很强，效果的确好，就是论文看得有点头晕，有些公式自己之前从来没见过，并且有些公式的出发点没有实验证明。</p><p>这篇论文是腾讯的，今年腾讯优图实验室25篇、腾讯AILab33篇共计55篇论文被 CVPR 2019 录取。</p><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>这是篇跨数据集的行人重识别，source 是 MSMT，target 是 Market 和 Duke。</p><p>作者针对跨数据集行人重识别没有标签问题，提出四个工作：</p><ol><li>soft multilabel learning, 示例如下图</li><li>soft multilabel-guided hard negative mining</li><li>cross-view consistent soft multi-label learning</li><li>reference agent learning</li></ol><img src="/2019/05/13/MAR/soft_multilabel_learning.png" title="soft multilabel learning"><img src="/2019/05/13/MAR/network.png" title="network"><ol><li>用分类结果作为图片的真值，即 soft multilabel</li><li>确定正负样本：特征相似但是 soft multilabel 不相似的作为负样本，特征相似且 soft multilabel 相似的作为正样本</li><li>跨摄像头的一致性，摄像头同一数据集下，应该是不管哪个摄像头下，得到的 soft multilabel 的分布是近似的，</li><li>reference agent 应该满足：有标签数据集中，和 agent 同类别的图片得到的特征应该和 agent 相似，不同类别的图片得到的特征应该不相似，在无标签数据集中，图片得到的特征和 agent 应该不相似。</li></ol><p>这几个点感觉完全没有联系啊，作者是咋想到的并放在一起的呢？</p><p>剩下的三个创新点后面依次阐述，其理解还是有点费劲的。</p><p>注:</p><ul><li>本论文中的 auxiliary dataset 等价于 source dataset</li><li>agent 是一个单独的 classx2048 维的数据，代码中是直接调用的 fc.weight。</li></ul><h2 id="2-Deep-Soft-Multilabel-Reference-Learning"><a href="#2-Deep-Soft-Multilabel-Reference-Learning" class="headerlink" title="2. Deep Soft Multilabel Reference Learning"></a>2. Deep Soft Multilabel Reference Learning</h2><h3 id="2-1-Problem-formulation-and-Overview"><a href="#2-1-Problem-formulation-and-Overview" class="headerlink" title="2.1 Problem formulation and Overview"></a>2.1 Problem formulation and Overview</h3><div class="table-container"><table><thead><tr><th style="text-align:left">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">$X=\lbrace x_i \rbrace_{i=1}^{N_u}$</td><td style="text-align:left">没有标签的数据集，$N_u$张图片</td></tr><tr><td style="text-align:left">$Z=\lbrace z_i, w_i \rbrace_{i=1}^{N_a}, \text{where }w_i=1,2,…, N_p$</td><td style="text-align:left">有标签的数据集 auxiliary，$z_i$表示图片，$w_i$表示 label，$N_a$ 张图片，$N_p$个人，有标签数据集和无标签数据集人物没有重叠</td></tr><tr><td style="text-align:left">$f(\cdot)$</td><td style="text-align:left">discriminative deep feature embedding，应该是特征提取模型，即 $f(x)$，满足 $\parallel f(\cdot) \parallel_2=1$</td></tr><tr><td style="text-align:left">$\lbrace a_i \rbrace_{i=1}^{N_p}$</td><td style="text-align:left">reference person feature, $\parallel a_i \parallel_2=1$</td></tr><tr><td style="text-align:left">$y=l(f(x),\lbrace a_i \rbrace_{i=1}^{N_p})\in R^{N_p}$</td><td style="text-align:left">soft multilabel function $l(\cdot)$, where $y=(y^{(1)}, y^{(2)}, …, y^{(N_p)})$, $\sum_i^{N_p} y^{(i)}=1, y^{(i)}\in [0, 1]$</td></tr></tbody></table></div><p>一共有两个内容需要学习: $f(\cdot)$, $\lbrace a_i \rbrace_{i=1}^{N_p}$</p><h3 id="2-2-Soft-multilabel-guided-hard-negative-mining"><a href="#2-2-Soft-multilabel-guided-hard-negative-mining" class="headerlink" title="2.2 Soft multilabel-guided hard negative mining"></a>2.2 Soft multilabel-guided hard negative mining</h3><p>大哥，你的上下标能不能提前说清楚啊[捂脸]，算了算了，腾讯的，惹不起惹不起。</p><img src="/2019/05/13/MAR/soft_multilabel_guided_hard_negative_mining.png" title="soft multilabel guided hard negative mining"><p><strong>定义</strong> soft multilabel function:</p><script type="math/tex; mode=display">y^{(k)}=l(f(x),\lbrace a_i \rbrace_{i=1}^{N_p})^{(k)}=\frac{\exp(a_k^Tf(x))}{\sum_i \exp(a_i^Tf(x))}</script><p>也就是说，用 $f(x)$ 去依次点乘 $a_k$， 然后用一个 softmax。</p><p>按照这个公式来说的话，是在特征空间上做的操作，有点类似 ECN 中的预测概率，越相似，值越大，而不是通过分类器预测概率。</p><p>代码中有温度T。</p><p><strong>假设</strong>：如果一对样本 $x_i, x_j$ 有很高的特征相似性, 即 $f(x_i)^Tf(x_j)$，称之为相似样本。如果这对相似样本的其他特性也相似，则大概率为一对正样本，如果其他特性不相似，则大概率为一个难负样本 hard negative pair。</p><p>注：这里并没有实验证明一对 hard negative pair 的其他特性（主要指下文提到的 soft multilabel agreement）大概率不相似。所以表示存疑其假设的正确性。又想了想，在难采样三元组损失中，hard negative pair 就是指特征相似但是 label 不同的样本，positive pair 指 label 相同的样本的，easy positive pair 指 label 相同特征相似的样本，hard positive pair 指 label 相同特征不相似的样本，这样的话可以把 soft multilabel 看成样本的 label 的话，也是可以说得通的。</p><p><strong>引理1：其他特性的相似性</strong>：作者选用 soft multilabel 作为其他特性，soft multilabel agreement $A(\cdot, \cdot)$ 表示作为其他特性的相似性。定义为</p><script type="math/tex; mode=display">A(y_i,y_j)=y_i \land y_j=\sum_k \min(y_i^{(k)},y_j^{(k)})=1-\frac{\parallel y_i-y_j \parallel_1}{2} \in [0,1]</script><p>越相似，值越大。最后一个等号通过画图很容易求得，就不解释了。Question: 这里的相似性定义成了向量之间的一范，没有定义成熟悉的点积，暂时不知道原因。</p><p><strong>引理2： hard negative pair</strong>：对于无标签数据集 $X$ 的所有样本对 $M=N_u\times (N_u-1)/2$，设置比例 $p$，取 $pM$ 个特征最相似的样本对，即 $\hat{M}=\lbrace (i,j)|f(x_i)^T f(x_j)\ge S\rbrace, \parallel \hat{M} \parallel=pM$, 其中 $S$ 表示 $pM$ 个特征最相似样本对的阈值，动态变化，不是很重要的，重要的是取 $pM$个样本对。然后根据 label 的相似性将这些样本对划分为 positive set $P$ and hard negative set $N$，即</p><script type="math/tex; mode=display">P=\lbrace (i,j)|f(x_i)^T f(x_j)\ge S, A(y_i, y_j)\ge T \rbrace</script><script type="math/tex; mode=display">N=\lbrace (k,l)|f(x_k)^T f(x_l)\ge S, A(y_k, y_l)< T \rbrace</script><p>其中 $T$ 表示 soft multilabel agreement 的阈值。会更新。</p><p><strong>loss</strong>：soft Multilabel-guided Discriminative embedding Learning:</p><script type="math/tex; mode=display">L_{MDL}=-\log \frac{\bar{P}}{\bar{P}+\bar{N}}</script><p>where,</p><script type="math/tex; mode=display">\bar{P}=\frac{1}{|P|}\sum_{(i,j)\in P}\exp(-\parallel f(x_i)-f(x_j) \parallel_2^2)</script><script type="math/tex; mode=display">\bar{N}=\frac{1}{|N|}\sum_{(k,l)\in N}\exp(-\parallel f(x_k)-f(x_l) \parallel_2^2)</script><p>so,</p><script type="math/tex; mode=display">\begin{aligned}L_{MDL}&=-\log \frac{\bar{P}}{\bar{P}+\bar{N}} \\       &=-\log \frac{\frac{1}{|P|}\sum_{(i,j)\in P}\exp(-\parallel f(x_i)-f(x_j) \parallel_2^2)}{\frac{1}{|P|}\sum_{(i,j)\in P}\exp(-\parallel f(x_i)-f(x_j) \parallel_2^2)+\frac{1}{|N|}\sum_{(k,l)\in N}\exp(-\parallel f(x_k)-f(x_l) \parallel_2^2)}\end{aligned}</script><p>Question: 这是个啥公式啊，都没有见过类似的公式，作者也没有给出解释。</p><p>此时固定 agent $\lbrace a_i \rbrace_{i=1}^{N_p}$ ，学习 $f(\cdot)$.</p><p>实际训练时，$M=M_{batch}=N_{batch}\times (N_{batch}-1)/2$</p><h3 id="2-3-Cross-view-consistent-soft-multilabel-learning"><a href="#2-3-Cross-view-consistent-soft-multilabel-learning" class="headerlink" title="2.3 Cross-view consistent soft multilabel learning"></a>2.3 Cross-view consistent soft multilabel learning</h3><p>因为行人重识别要求跨摄像头识别，所以考虑到行人的分布应该与摄像头无关。</p><p><strong>Loss</strong>:</p><script type="math/tex; mode=display">L_{CML}=\sum_v d(P_v(y), P(y))^2</script><p>其中，$P(y)$ 表示数据集 $X$ 的 soft multilabel 分布，$P_v(y)$ 表示数据集 $X$ 在摄像头 $v$ 的 soft multilabel 分布，$d(\cdot, \cdot)$ 表示分布的距离，可以是 KL divergence 或者 <a href="https://blog.csdn.net/yzxnuaa/article/details/79725014" target="_blank" rel="noopener">Wasserstein distance</a>.因为实际观察到服从 log-normal 分布，所以采取 simplified 2-Wasserstein distance。</p><script type="math/tex; mode=display">L_{CML}=\sum_v \parallel \mu_v-\mu \parallel_2^2 + \parallel \sigma_v-\sigma\parallel_2^2</script><p>其中，$\mu/\sigma$表示总体数据集的 log-soft multilabel 的均值和方差，$\mu_v/\sigma_v$表示总体数据集在摄像头$v$的 log-soft multilabel 的均值和方差.<br>Question: 这个公式又是咋推出来的，这是妥妥地写出来也看不懂系列。</p><p>此时固定 agent $\lbrace a_i \rbrace_{i=1}^{N_p}$ ，学习 $f(\cdot)$.</p><h3 id="2-4-Reference-agent-Learning"><a href="#2-4-Reference-agent-Learning" class="headerlink" title="2.4 Reference agent Learning"></a>2.4 Reference agent Learning</h3><p>考虑到 referentce agent 需要与 soft multilabel function $l(\cdot, \cdot)$ 有关，因此得到损失函数</p><script type="math/tex; mode=display">L_{AL}=\sum_k -\log l(f(z_k), {a_i})^{(w_k)}=\sum_k -\log \frac{\exp(a_{w_k}^T f(z_k))}{\sum_j \exp(a_j^T f(z_k))}</script><p>其中，$z_k$ 表示有标签数据集 $Z$ 中标签为 $w_k$ 的第 $k$ 张图片。</p><p>这里可以理解成 $z_k$ 的预测概率和真实概率的交叉熵损失。这个损失函数不仅训练 $a_i$ 更接近第i个人的所有图片的特征，也训练 feature embedding $f$，使 $l(\cdot, \cdot)$ 得到的标签更具有表示同一个人的能力，符合 soft multilabel-guided hard negative mining 的假设：特征相似，但是 soft multilabel 不相似的为 hard negative mining。</p><p>这个公式更新的是 $f$ 和 agent $a_i$.</p><p>注：该论文中的公式其实按照从广义的定义到实际的应用的具体化过程，所以刚开始才会感觉有点乱，公式里面的字符也会一变再变，其实是从理论的公式到具体化实际代码的过程。</p><p><strong>Joint embedding learning for reference comparability</strong>: 为了更好地提高 soft multilabel function 表示无标签数据集图片的正确性，提出 Joint embedding learning for reference comparability，为了修正 domain shift，利用无标签数据集 $f(x)$ 和 $a_i$ 肯定不是一对，提出 loss:</p><script type="math/tex; mode=display">L_{RJ}=\sum_i \sum_{j\in M_i} \sum_{k:w_k=i}[m-\parallel a_i-f(x_j) \parallel_2^2]_+ + \parallel a_i-f(z_k) \parallel_2^2</script><p>其中，其目的是为了保证$a_i$所表示的有标签数据集中的同一id的图片和$a_i$特征相似，$a_i$和所有无标签数据集中的图片特征都不相似。 $M_i=\lbrace j| \parallel a_i-f(x_j) \parallel_2^2 &lt; m \rbrace$，表示对第 $i$ 个 agent $a_i$ 而言，特征最为相似 ($\parallel a_i-f(x_j) \parallel_2^2=2(1-a_i^Tf(x_j))$, 越相似，值越小) 的无标签数据集中的图片，按照作者推荐的论文 <a href="https://arxiv.org/abs/1704.06369" target="_blank" rel="noopener">44:Normface: l2 hypersphere embedding for face verification</a>，建议 $m=1$。</p><p>此时固定 agent $\lbrace a_i \rbrace_{i=1}^{N_p}$ ，学习 $f(\cdot)$.</p><p>对 $L_{RJ}$ 根据代码再次做出解释，$L_{RJ}$ 的目的是学习更好的特征提取器 $f$，使有标签数据集提取出的特征与同类别的 agent 的特征相似，与不同类别的agent不相似，无标签数据集提取出的特征与 agent 都不相似，有点三元组损失的意思。此时的 $a_i$ 是常量，不进行反向求导的。其二，对于任意一个 agent $a_i$，有标签数据集中，label 等于 $i$ 的图片视为正样本，其他图片视为负样本，对于无标签数据集，则直接视为 $a_i$ 的负样本。具体来说，就是对每一张有标签数据集的图片，$a_{label}$ 为正， 其余 $a_i$ 为负，对于每一张无标签数据集的图片，$a_i$ 都为负。</p><p>所以总的 reference agent learning loss为:</p><script type="math/tex; mode=display">L_{RAL}=L_{AL}+\beta L_{RJ}</script><h3 id="2-5-1-Model-training-and-testing"><a href="#2-5-1-Model-training-and-testing" class="headerlink" title="2.5.1 Model training and testing"></a>2.5.1 Model training and testing</h3><script type="math/tex; mode=display">L_{MAR}=L_{MDL}+\lambda_1 L_{CML}+\lambda_2 L_{RAL}</script><h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h2><p>MSMT17 为辅助数据集， Market-1501、Duke 为无标签数据集。</p><img src="/2019/05/13/MAR/comparison.png" title="comparison"><img src="/2019/05/13/MAR/Ablation.png" title="Ablation study"><p>备注: 论文中的 agent 其实并不是之前以为通过图片输入模型得到的特征求出来的，而是 ResNet-50 的 fc.weight(classx2048) ，也就是分类器的分类向量。和 ECN 论文中的使用方法有很大的不同吧。在 ECN 中，使用的就是图片输入模型得到的特征，可能是因为 ECN 中一张图片对应一个特征，而本论文中是多个图片对应一个特征。</p><h2 id="4-code"><a href="#4-code" class="headerlink" title="4. code"></a>4. code</h2><h3 id="4-1-Logger"><a href="#4-1-Logger" class="headerlink" title="4.1 Logger"></a>4.1 Logger</h3><p>两种logger</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种：定义简单，使用繁琐</span></span><br><span class="line"><span class="comment"># 定义</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_string</span><span class="params">()</span>:</span></span><br><span class="line">    ISOTIMEFORMAT = <span class="string">'%Y-%m-%d %X'</span></span><br><span class="line">    string = <span class="string">'[&#123;&#125;]'</span>.format(time.strftime(ISOTIMEFORMAT, time.localtime(time.time())))</span><br><span class="line">    <span class="keyword">return</span> string</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Logger</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, save_path)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(save_path):</span><br><span class="line">            os.makedirs(save_path)</span><br><span class="line">        self.file = open(os.path.join(save_path, <span class="string">'log_&#123;&#125;.txt'</span>.format(time_string())), <span class="string">'w'</span>)</span><br><span class="line">        self.print_log(<span class="string">"python version : &#123;&#125;"</span>.format(sys.version.replace(<span class="string">'\n'</span>, <span class="string">' '</span>)))</span><br><span class="line">        self.print_log(<span class="string">"torch  version : &#123;&#125;"</span>.format(torch.__version__))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_log</span><span class="params">(self, string)</span>:</span></span><br><span class="line">        self.file.write(<span class="string">"&#123;&#125;\n"</span>.format(string))</span><br><span class="line">        self.file.flush()</span><br><span class="line">        print(string)</span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line">logger = Logger(args.save_path)</span><br><span class="line">logger.print_log(<span class="string">"=&gt; loading checkpoint '&#123;&#125;'"</span>.format(load_path))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二种，定义繁琐，使用简单，重定向</span></span><br><span class="line"><span class="comment"># 推荐用这种</span></span><br><span class="line"><span class="comment"># 定义</span></span><br><span class="line"><span class="comment"># .\logging.py</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> errno</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkdir_if_missing</span><span class="params">(dir_path)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        os.makedirs(dir_path)</span><br><span class="line">    <span class="keyword">except</span> OSError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">if</span> e.errno != errno.EEXIST:</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Logger</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, fpath=None)</span>:</span></span><br><span class="line">        self.console = sys.stdout</span><br><span class="line">        self.file = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> fpath <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            mkdir_if_missing(os.path.dirname(fpath))</span><br><span class="line">            self.file = open(fpath, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(self, msg)</span>:</span></span><br><span class="line">        self.console.write(msg)</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.write(msg)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">flush</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.console.flush()</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.flush()</span><br><span class="line">            os.fsync(self.file.fileno())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.console.close()</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.close()</span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line">sys.stdout = Logger(osp.join(args.logs_dir, <span class="string">'log.txt'</span>))</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><h3 id="4-2-model"><a href="#4-2-model" class="headerlink" title="4.2 model"></a>4.2 model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 3x384x128</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        feature_maps = self.layer4(x)</span><br><span class="line">        <span class="comment"># 2048x12x4</span></span><br><span class="line">        x = self.avgpool(feature_maps)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># bx2048</span></span><br><span class="line">        <span class="comment"># renorm 使每一行的向量的2范进行了截断处理</span></span><br><span class="line">        <span class="comment"># 使之变成[0,1e-5]，再线性变成[0,1]</span></span><br><span class="line">        <span class="comment"># 这里的renorm可以暂时理解成进行了二范处理，</span></span><br><span class="line">        feature = x.renorm(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1e-5</span>).mul(<span class="number">1e5</span>)</span><br><span class="line">        <span class="comment"># bx2048</span></span><br><span class="line">        w = self.fc.weight</span><br><span class="line">        <span class="comment"># 注: 这个self.fc.weight(classx2048)就是论文中的agent</span></span><br><span class="line">        ww = w.renorm(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1e-5</span>).mul(<span class="number">1e5</span>)</span><br><span class="line">        sim = feature.mm(ww.t())</span><br><span class="line">        <span class="comment"># sim: bxclass</span></span><br><span class="line">        <span class="comment"># feature(f): bx2048, sim(y): bxclass, feature_maps: 2048x12x4</span></span><br><span class="line">        <span class="keyword">return</span> feature, sim, feature_maps</span><br></pre></td></tr></table></figure><h3 id="4-3-optim"><a href="#4-3-optim" class="headerlink" title="4.3 optim"></a>4.3 optim</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bn_params, other_params = partition_params(self.net, <span class="string">'bn'</span>)</span><br><span class="line">self.optimizer = torch.optim.SGD([&#123;<span class="string">'params'</span>: bn_params, <span class="string">'weight_decay'</span>: <span class="number">0</span>&#125;,</span><br><span class="line">                                  &#123;<span class="string">'params'</span>: other_params&#125;], lr=args.lr, momentum=<span class="number">0.9</span>, weight_decay=args.wd)</span><br></pre></td></tr></table></figure><h3 id="4-4-trainer-init-losses"><a href="#4-4-trainer-init-losses" class="headerlink" title="4.4 trainer/init_losses"></a>4.4 trainer/init_losses</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReidTrainer</span><span class="params">(Trainer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args, logger)</span>:</span></span><br><span class="line">        self.al_loss = nn.CrossEntropyLoss().cuda()</span><br><span class="line">        self.rj_loss = JointLoss(args.margin).cuda()</span><br><span class="line">        self.cml_loss = MultilabelLoss(args.batch_size).cuda()  <span class="comment"># L_CML</span></span><br><span class="line">        self.mdl_loss = DiscriminativeLoss(args.mining_ratio).cuda() <span class="comment"># L_MDL</span></span><br><span class="line">        self.net = resnet50(pretrained=<span class="keyword">False</span>, num_classes=self.args.num_classes)</span><br><span class="line">        self.multilabel_memory = torch.zeros(N_target_samples, <span class="number">4101</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_losses</span><span class="params">(self, target_loader)</span>:</span></span><br><span class="line">        self.logger.print_log(<span class="string">'initializing centers/threshold ...'</span>)</span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(self.args.ml_path):</span><br><span class="line">            (multilabels, views, pairwise_agreements) = torch.load(self.args.ml_path)</span><br><span class="line">            self.logger.print_log(<span class="string">'loaded ml from &#123;&#125;'</span>.format(self.args.ml_path))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.logger.print_log(<span class="string">'not found &#123;&#125;. computing ml...'</span>.format(self.args.ml_path))</span><br><span class="line">            sim, _, views = extract_features(target_loader, self.net, index_feature=<span class="number">1</span>, return_numpy=<span class="keyword">False</span>)</span><br><span class="line">            <span class="comment"># sim: bxclass, views: bx1</span></span><br><span class="line">            multilabels = F.softmax(sim * self.args.scala_ce, dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># multilabels: bxclass 这里应该对于soft multilabel funtion得到的结果y^&#123;(k)&#125;</span></span><br><span class="line">            <span class="comment"># Question: sim*self.args.scala_ce 是什么意思</span></span><br><span class="line">            ml_np = multilabels.cpu().numpy()</span><br><span class="line">            pairwise_agreements = <span class="number">1</span> - pdist(ml_np, <span class="string">'minkowski'</span>, p=<span class="number">1</span>)/<span class="number">2</span></span><br><span class="line">            <span class="comment"># pairwise_agreements: soft multilabel agreement A(.,.) 公式2</span></span><br><span class="line">        log_multilabels = torch.log(multilabels)</span><br><span class="line">        self.cml_loss.init_centers(log_multilabels, views)</span><br><span class="line">        self.logger.print_log(<span class="string">'initializing centers done.'</span>)</span><br><span class="line">        self.mdl_loss.init_threshold(pairwise_agreements)</span><br><span class="line">        self.logger.print_log(<span class="string">'initializing threshold done.'</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_epoch</span><span class="params">(self, source_loader, target_loader, epoch)</span>:</span></span><br><span class="line">        self.lr_scheduler.step()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.cml_loss.initialized <span class="keyword">or</span> <span class="keyword">not</span> self.mdl_loss.initialized:</span><br><span class="line">            self.init_losses(target_loader)</span><br><span class="line">        batch_time_meter = AverageMeter()</span><br><span class="line">        stats = (<span class="string">'loss_source'</span>, <span class="string">'loss_st'</span>, <span class="string">'loss_ml'</span>, <span class="string">'loss_target'</span>, <span class="string">'loss_total'</span>)</span><br><span class="line">        meters_trn = &#123;stat: AverageMeter() <span class="keyword">for</span> stat <span class="keyword">in</span> stats&#125;</span><br><span class="line">        self.train()</span><br><span class="line"></span><br><span class="line">        end = time.time()</span><br><span class="line">        target_iter = iter(target_loader)</span><br><span class="line">        <span class="keyword">for</span> i, source_tuple <span class="keyword">in</span> enumerate(source_loader):</span><br><span class="line">            imgs = source_tuple[<span class="number">0</span>].cuda()</span><br><span class="line">            labels = source_tuple[<span class="number">1</span>].cuda()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                target_tuple = next(target_iter)</span><br><span class="line">            <span class="keyword">except</span> StopIteration:</span><br><span class="line">                target_iter = iter(target_loader)</span><br><span class="line">                target_tuple = next(target_iter)</span><br><span class="line">            imgs_target = target_tuple[<span class="number">0</span>].cuda()</span><br><span class="line">            labels_target = target_tuple[<span class="number">1</span>].cuda()</span><br><span class="line">            views_target = target_tuple[<span class="number">2</span>].cuda()</span><br><span class="line">            idx_target = target_tuple[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">            features, similarity, _ = self.net(imgs)</span><br><span class="line">            features_target, similarity_target, _ = self.net(imgs_target)</span><br><span class="line">            <span class="comment"># features: bx2048, similarity: bxclass</span></span><br><span class="line">            scores = similarity * self.args.scala_ce</span><br><span class="line">            loss_source = self.al_loss(scores, labels) <span class="comment"># 公式7，同时训练 agent 和 f()</span></span><br><span class="line">            agents = self.net.module.fc.weight.renorm(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1e-5</span>).mul(<span class="number">1e5</span>)</span><br><span class="line">            <span class="comment"># features: bx2048, agents: classx2048, labels: bx1, similarity: bxclass</span></span><br><span class="line">            loss_st = self.rj_loss(features, agents.detach(), labels, similarity.detach(), features_target, similarity_target.detach())</span><br><span class="line">            multilabels = F.softmax(features_target.mm(agents.detach().t_()*self.args.scala_ce), dim=<span class="number">1</span>)</span><br><span class="line">            loss_ml = self.cml_loss(torch.log(multilabels), views_target)</span><br><span class="line">            <span class="keyword">if</span> epoch &lt; <span class="number">1</span>:</span><br><span class="line">                loss_target = torch.Tensor([<span class="number">0</span>]).cuda()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                multilabels_cpu = multilabels.detach().cpu()</span><br><span class="line">                is_init_batch = self.initialized[idx_target]</span><br><span class="line">                initialized_idx = idx_target[is_init_batch]</span><br><span class="line">                uninitialized_idx = idx_target[~is_init_batch]</span><br><span class="line">                self.multilabel_memory[uninitialized_idx] = multilabels_cpu[~is_init_batch]</span><br><span class="line">                self.initialized[uninitialized_idx] = <span class="number">1</span></span><br><span class="line">                self.multilabel_memory[initialized_idx] = <span class="number">0.9</span> * self.multilabel_memory[initialized_idx] \</span><br><span class="line">                                                          + <span class="number">0.1</span> * multilabels_cpu[is_init_batch]</span><br><span class="line">                loss_target = self.mdl_loss(features_target, self.multilabel_memory[idx_target], labels_target)</span><br><span class="line"></span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            loss_total = loss_target + self.args.lamb_1 * loss_ml + self.args.lamb_2 * \</span><br><span class="line">                         (loss_source + self.args.beta * loss_st)</span><br><span class="line">            loss_total.backward()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> stats:</span><br><span class="line">                v = locals()[k]</span><br><span class="line">                meters_trn[k].update(v.item(), self.args.batch_size)</span><br><span class="line"></span><br><span class="line">            batch_time_meter.update(time.time() - end)</span><br><span class="line">            freq = self.args.batch_size / batch_time_meter.avg</span><br><span class="line">            end = time.time()</span><br><span class="line">            <span class="keyword">if</span> i % self.args.print_freq == <span class="number">0</span>:</span><br><span class="line">                self.logger.print_log(<span class="string">'  Iter: [&#123;:03d&#125;/&#123;:03d&#125;]   Freq &#123;:.1f&#125;   '</span>.format(</span><br><span class="line">                    i, len(source_loader), freq) + create_stat_string(meters_trn) + time_string())</span><br><span class="line"></span><br><span class="line">        save_checkpoint(self, epoch, os.path.join(self.args.save_path, <span class="string">"checkpoints.pth"</span>))</span><br><span class="line">        <span class="keyword">return</span> meters_trn</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L_CML 公式6</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultilabelLoss</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, batch_size, use_std=True)</span>:</span></span><br><span class="line">        super(MultilabelLoss, self).__init__()</span><br><span class="line">        self.use_std = use_std</span><br><span class="line">        self.moment = batch_size / <span class="number">10000</span></span><br><span class="line">        self.initialized = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_centers</span><span class="params">(self, log_multilabels, views)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param log_multilabels: shape=(N, n_class)</span></span><br><span class="line"><span class="string">        :param views: (N,)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        # 用于初始化全局的均值和方差</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        univiews = torch.unique(views)</span><br><span class="line">        mean_ml = []</span><br><span class="line">        std_ml = []</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> univiews:</span><br><span class="line">            ml_in_v = log_multilabels[views == v]</span><br><span class="line">            mean = ml_in_v.mean(dim=<span class="number">0</span>)</span><br><span class="line">            std = ml_in_v.std(dim=<span class="number">0</span>)</span><br><span class="line">            mean_ml.append(mean)</span><br><span class="line">            std_ml.append(std)</span><br><span class="line">        center_mean = torch.mean(torch.stack(mean_ml), dim=<span class="number">0</span>)</span><br><span class="line">        center_std = torch.mean(torch.stack(std_ml), dim=<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'center_mean'</span>, center_mean)</span><br><span class="line">        self.register_buffer(<span class="string">'center_std'</span>, center_std)</span><br><span class="line">        self.initialized = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_centers</span><span class="params">(self, log_multilabels, views)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param log_multilabels: shape=(BS, n_class)</span></span><br><span class="line"><span class="string">        :param views: shape=(BS,)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        univiews = torch.unique(views)</span><br><span class="line">        means = []</span><br><span class="line">        stds = []</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> univiews:</span><br><span class="line">            ml_in_v = log_multilabels[views == v]</span><br><span class="line">            <span class="keyword">if</span> len(ml_in_v) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            mean = ml_in_v.mean(dim=<span class="number">0</span>)</span><br><span class="line">            means.append(mean)</span><br><span class="line">            <span class="keyword">if</span> self.use_std:</span><br><span class="line">                std = ml_in_v.std(dim=<span class="number">0</span>)</span><br><span class="line">                stds.append(std)</span><br><span class="line">        new_mean = torch.mean(torch.stack(means), dim=<span class="number">0</span>)</span><br><span class="line">        self.center_mean = self.center_mean * (<span class="number">1</span> - self.moment) + new_mean * self.moment</span><br><span class="line">        <span class="keyword">if</span> self.use_std:</span><br><span class="line">            new_std = torch.mean(torch.stack(stds), dim=<span class="number">0</span>)</span><br><span class="line">            self.center_std = self.center_std * (<span class="number">1</span> - self.moment) + new_std * self.moment</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, log_multilabels, views)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param log_multilabels: shape=(BS, n_class)</span></span><br><span class="line"><span class="string">        :param views: shape=(BS,)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._update_centers(log_multilabels.detach(), views)</span><br><span class="line"></span><br><span class="line">        univiews = torch.unique(views)</span><br><span class="line">        loss_terms = []</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> univiews:</span><br><span class="line">            ml_in_v = log_multilabels[views == v]</span><br><span class="line">            <span class="keyword">if</span> len(ml_in_v) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            mean = ml_in_v.mean(dim=<span class="number">0</span>)</span><br><span class="line">            loss_mean = (mean - self.center_mean).pow(<span class="number">2</span>).sum()</span><br><span class="line">            loss_terms.append(loss_mean)</span><br><span class="line">            <span class="keyword">if</span> self.use_std:</span><br><span class="line">                std = ml_in_v.std(dim=<span class="number">0</span>)</span><br><span class="line">                loss_std = (std - self.center_std).pow(<span class="number">2</span>).sum()</span><br><span class="line">                loss_terms.append(loss_std)</span><br><span class="line">        loss_total = torch.mean(torch.stack(loss_terms))</span><br><span class="line">        <span class="keyword">return</span> loss_total</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L_MDL 公式4</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiscriminativeLoss</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mining_ratio=<span class="number">0.001</span>)</span>:</span></span><br><span class="line">        super(DiscriminativeLoss, self).__init__()</span><br><span class="line">        self.mining_ratio = mining_ratio</span><br><span class="line">        self.register_buffer(<span class="string">'n_pos_pairs'</span>, torch.Tensor([<span class="number">0</span>]))</span><br><span class="line">        self.register_buffer(<span class="string">'rate_TP'</span>, torch.Tensor([<span class="number">0</span>]))</span><br><span class="line">        self.moment = <span class="number">0.1</span></span><br><span class="line">        self.initialized = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_threshold</span><span class="params">(self, pairwise_agreements)</span>:</span></span><br><span class="line">        <span class="comment"># Question：论文中还有一个限制条件，f(x_i)f(x_j)&gt;S，代码只考虑了A(y_i, y_j)</span></span><br><span class="line">        pos = int(len(pairwise_agreements) * self.mining_ratio)</span><br><span class="line">        sorted_agreements = np.sort(pairwise_agreements)</span><br><span class="line">        t = sorted_agreements[-pos]</span><br><span class="line">        self.register_buffer(<span class="string">'threshold'</span>, torch.Tensor([t]).cuda())</span><br><span class="line">        self.initialized = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, multilabels, labels)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param features: shape=(BS, dim)</span></span><br><span class="line"><span class="string">        :param multilabels: (BS, n_class)</span></span><br><span class="line"><span class="string">        :param labels: (BS,)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        P, N = self._partition_sets(features.detach(), multilabels, labels)</span><br><span class="line">        <span class="keyword">if</span> P <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            pos_exponant = torch.Tensor([<span class="number">1</span>]).cuda()</span><br><span class="line">            num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sdist_pos_pairs = []</span><br><span class="line">            <span class="keyword">for</span> (i, j) <span class="keyword">in</span> zip(P[<span class="number">0</span>], P[<span class="number">1</span>]):</span><br><span class="line">                sdist_pos_pair = (features[i] - features[j]).pow(<span class="number">2</span>).sum()</span><br><span class="line">                sdist_pos_pairs.append(sdist_pos_pair)</span><br><span class="line">            pos_exponant = torch.exp(- torch.stack(sdist_pos_pairs)).mean()</span><br><span class="line">            num = -torch.log(pos_exponant)</span><br><span class="line">        <span class="keyword">if</span> N <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            neg_exponant = torch.Tensor([<span class="number">0.5</span>]).cuda()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sdist_neg_pairs = []</span><br><span class="line">            <span class="keyword">for</span> (i, j) <span class="keyword">in</span> zip(N[<span class="number">0</span>], N[<span class="number">1</span>]):</span><br><span class="line">                sdist_neg_pair = (features[i] - features[j]).pow(<span class="number">2</span>).sum()</span><br><span class="line">                sdist_neg_pairs.append(sdist_neg_pair)</span><br><span class="line">            neg_exponant = torch.exp(- torch.stack(sdist_neg_pairs)).mean()</span><br><span class="line">        den = torch.log(pos_exponant + neg_exponant)</span><br><span class="line">        loss = num + den</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_partition_sets</span><span class="params">(self, features, multilabels, labels)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        partition the batch into confident positive, hard negative and others</span></span><br><span class="line"><span class="string">        :param features: shape=(BS, dim)</span></span><br><span class="line"><span class="string">        :param multilabels: shape=(BS, n_class)</span></span><br><span class="line"><span class="string">        :param labels: shape=(BS,)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        P: positive pair set. tuple of 2 np.array i and j.</span></span><br><span class="line"><span class="string">            i contains smaller indices and j larger indices in the batch.</span></span><br><span class="line"><span class="string">            if P is None, no positive pair found in this batch.</span></span><br><span class="line"><span class="string">        N: negative pair set. similar to P, but will never be None.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        f_np = features.cpu().numpy()</span><br><span class="line">        ml_np = multilabels.cpu().numpy()</span><br><span class="line">        p_dist = pdist(f_np)</span><br><span class="line">        p_agree = <span class="number">1</span> - pdist(ml_np, <span class="string">'minkowski'</span>, p=<span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">        sorting_idx = np.argsort(p_dist)</span><br><span class="line">        n_similar = int(len(p_dist) * self.mining_ratio)</span><br><span class="line">        similar_idx = sorting_idx[:n_similar]</span><br><span class="line">        is_positive = p_agree[similar_idx] &gt; self.threshold.item()</span><br><span class="line">        pos_idx = similar_idx[is_positive]</span><br><span class="line">        neg_idx = similar_idx[~is_positive]</span><br><span class="line">        P = dist_idx_to_pair_idx(len(f_np), pos_idx)</span><br><span class="line">        N = dist_idx_to_pair_idx(len(f_np), neg_idx)</span><br><span class="line">        self._update_threshold(p_agree)</span><br><span class="line">        self._update_buffers(P, labels)</span><br><span class="line">        <span class="keyword">return</span> P, N</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_threshold</span><span class="params">(self, pairwise_agreements)</span>:</span></span><br><span class="line">        pos = int(len(pairwise_agreements) * self.mining_ratio)</span><br><span class="line">        sorted_agreements = np.sort(pairwise_agreements)</span><br><span class="line">        t = torch.Tensor([sorted_agreements[-pos]]).cuda()</span><br><span class="line">        self.threshold = self.threshold * (<span class="number">1</span> - self.moment) + t * self.moment</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_buffers</span><span class="params">(self, P, labels)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> P <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            self.n_pos_pairs = <span class="number">0.9</span> * self.n_pos_pairs</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        n_pos_pairs = len(P[<span class="number">0</span>])</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> (i, j) <span class="keyword">in</span> zip(P[<span class="number">0</span>], P[<span class="number">1</span>]):</span><br><span class="line">            count += labels[i] == labels[j]</span><br><span class="line">        rate_TP = float(count) / n_pos_pairs</span><br><span class="line">        self.n_pos_pairs = <span class="number">0.9</span> * self.n_pos_pairs + <span class="number">0.1</span> * n_pos_pairs</span><br><span class="line">        self.rate_TP = <span class="number">0.9</span> * self.rate_TP + <span class="number">0.1</span> * rate_TP</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L_RJ 公式 8</span></span><br><span class="line"><span class="comment"># 与公式8略有不同</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JointLoss</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, margin=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(JointLoss, self).__init__()</span><br><span class="line">        self.margin = margin</span><br><span class="line">        self.sim_margin = <span class="number">1</span> - margin / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, agents, labels, similarity, features_target, similarity_target)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param features: shape=(BS/2, dim)</span></span><br><span class="line"><span class="string">        :param agents: shape=(n_class, dim)</span></span><br><span class="line"><span class="string">        :param labels: shape=(BS/2,)</span></span><br><span class="line"><span class="string">        :param features_target: shape=(BS/2, n_class)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss_terms = []</span><br><span class="line">        arange = torch.arange(len(agents)).cuda()</span><br><span class="line">        zero = torch.Tensor([<span class="number">0</span>]).cuda()</span><br><span class="line">        <span class="keyword">for</span> (f, l, s) <span class="keyword">in</span> zip(features, labels, similarity):</span><br><span class="line">            loss_pos = (f - agents[l]).pow(<span class="number">2</span>).sum() <span class="comment"># 公式8的最后一项，a_i-f(z_k)</span></span><br><span class="line">            loss_terms.append(loss_pos)</span><br><span class="line">            neg_idx = arange != l</span><br><span class="line">            <span class="comment"># 从agent中选出与当前图片特征相似度高于阈值，但不是同一类的的agent</span></span><br><span class="line">            hard_agent_idx = neg_idx &amp; (s &gt; self.sim_margin) <span class="comment"># 越相似，值越大</span></span><br><span class="line">            <span class="keyword">if</span> torch.any(hard_agent_idx):</span><br><span class="line">                hard_neg_sdist = (f - agents[hard_agent_idx]).pow(<span class="number">2</span>).sum(dim=<span class="number">1</span>)</span><br><span class="line">                loss_neg = torch.max(zero, self.margin - hard_neg_sdist).mean()</span><br><span class="line">                loss_terms.append(loss_neg)</span><br><span class="line">        <span class="keyword">for</span> (f, s) <span class="keyword">in</span> zip(features_target, similarity_target):</span><br><span class="line">            hard_agent_idx = s &gt; self.sim_margin</span><br><span class="line">            <span class="keyword">if</span> torch.any(hard_agent_idx):</span><br><span class="line">                hard_neg_sdist = (f - agents[hard_agent_idx]).pow(<span class="number">2</span>).sum(dim=<span class="number">1</span>)</span><br><span class="line">                loss_neg = torch.max(zero, self.margin - hard_neg_sdist).mean()</span><br><span class="line">                loss_terms.append(loss_neg)</span><br><span class="line">        loss_total = torch.mean(torch.stack(loss_terms))</span><br><span class="line">        <span class="keyword">return</span> loss_total</span><br></pre></td></tr></table></figure><p>根据代码，重新明确两个定义:</p><ul><li>similarity 指的是图片的 feature1 和 agent 的 feature2 的特征相似性: feature1*feature2</li><li>multilabels 指的是 similarity.mul(self.args.scala_ce) 再softmax得到的</li></ul><p>算了，有些代码还是跑的时候看吧，因为有些更新方式有些看不懂。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1903.06325v2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019 Unsupervised Person Re-identification by Soft Multilabel Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/KovenYu/MAR&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;参考链接: &lt;a href=&quot;https://www.cnblogs.com/Thinker-pcw/p/10807681.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/Thinker-pcw/p/10807681.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;出发点 Multi-label 很强，效果的确好，就是论文看得有点头晕，有些公式自己之前从来没见过，并且有些公式的出发点没有实验证明。&lt;/p&gt;
&lt;p&gt;这篇论文是腾讯的，今年腾讯优图实验室25篇、腾讯AILab33篇共计55篇论文被 CVPR 2019 录取。&lt;/p&gt;
    
    </summary>
    
      <category term="deep learning" scheme="http://yoursite.com/categories/deep-learning/"/>
    
    
      <category term="person re-id" scheme="http://yoursite.com/tags/person-re-id/"/>
    
  </entry>
  
  <entry>
    <title>residual_attention</title>
    <link href="http://yoursite.com/2019/05/10/residual-attention_and_CBAM_GCNet/"/>
    <id>http://yoursite.com/2019/05/10/residual-attention_and_CBAM_GCNet/</id>
    <published>2019-05-10T02:12:48.000Z</published>
    <updated>2019-05-10T08:14:31.646Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1704.06904" target="_blank" rel="noopener">商汤 CVPR2017 Residual Attention_Network for Image Classification</a></li><li>code: <a href="https://github.com/fwang91/residual-attention-network" target="_blank" rel="noopener">caffe</a>, <a href="http://ethereon.github.io/netscope/#/editor" target="_blank" rel="noopener">caffe网络可视化工具 Netscope</a>, <a href="https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch" target="_blank" rel="noopener">pytorch</a></li><li>paper: <a href="https://arxiv.org/abs/1807.06521" target="_blank" rel="noopener">ECCV2018_CBAM: Convolutional Block Attention Module</a></li><li>code: <a href="https://github.com/luuuyi/CBAM.PyTorch" target="_blank" rel="noopener">pytorch</a></li><li>paper: <a href="https://arxiv.org/abs/1904.11492" target="_blank" rel="noopener">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li>code: <a href="https://github.com/xvjiarui/GCNet" target="_blank" rel="noopener">pytorch</a><a id="more"></a>这篇论文也是讲 attention map 的，主要用于分类，说实话，没有太理解其中的创新点，可能是因为不懂整个 attention map 的进程，前人做到了什么地步，从效果图上看，感觉比 Dual Attention 那篇论文还是差一些。不过能上 CVPR 的肯定有牛的地方，只是自己水平不够。</li></ul><h2 id="1-Residual-Attention-Network"><a href="#1-Residual-Attention-Network" class="headerlink" title="1. Residual Attention Network"></a>1. Residual Attention Network</h2><p>每个 Attention Module 都分为两个分支：mask branch and trunk branch。</p><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/mask_trunk_branch.png" title="mask branch and trunk branch"><script type="math/tex; mode=display">H_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)</script><p>其中，c表示通道，i表示所有的位置，$M_{i,c}(x)\in[0,1]$</p><h3 id="1-1-Spatial-Attention-and-Channel-Attention"><a href="#1-1-Spatial-Attention-and-Channel-Attention" class="headerlink" title="1.1 Spatial Attention and Channel Attention"></a>1.1 Spatial Attention and Channel Attention</h3><p>Mixed attention:</p><script type="math/tex; mode=display">f_1(x_{i,c})=\frac{1}{1+\exp (-x_{i,c})}</script><p>Channel Attention:</p><script type="math/tex; mode=display">f_2(x_{i,c})=\frac{x_{i,c}}{\parallel x_i \parallel}</script><p>Spatial Attention:</p><script type="math/tex; mode=display">f_3(x_{i,c})=\frac{1}{1+\exp (-(x_{i,c}-mean_c)/std_c)}</script><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/activation_function.png" title="different activation functions"><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/network.png" title="Residual Attention Network"><h2 id="2-code"><a href="#2-code" class="headerlink" title="2. code"></a>2. code</h2><p>代码有点绕，暂时没有看懂。</p><h2 id="3-CBAM"><a href="#3-CBAM" class="headerlink" title="3. CBAM"></a>3. CBAM</h2><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/CBAM.png" title="CBAM"><h2 id="4-code"><a href="#4-code" class="headerlink" title="4. code"></a>4. code</h2><p>简单明了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChannelAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_planes, ratio=<span class="number">16</span>)</span>:</span></span><br><span class="line">        super(ChannelAttention, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1   = nn.Conv2d(in_planes, in_planes // <span class="number">16</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2   = nn.Conv2d(in_planes // <span class="number">16</span>, in_planes, <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))</span><br><span class="line">        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))</span><br><span class="line">        out = avg_out + max_out</span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(out)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, planes, stride=<span class="number">1</span>, downsample=None)</span>:</span></span><br><span class="line">        super(Bottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                               padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv3 = nn.Conv2d(planes, planes * <span class="number">4</span>, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(planes * <span class="number">4</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        self.ca = ChannelAttention(planes * <span class="number">4</span>)</span><br><span class="line">        self.sa = SpatialAttention()</span><br><span class="line"></span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        out = self.ca(out) * out</span><br><span class="line">        out = self.sa(out) * out</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="5-GCNet"><a href="#5-GCNet" class="headerlink" title="5. GCNet"></a>5. GCNet</h2><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/GCNet.png" title="GCNet"><p>综合考虑了 SE-block 和 NL-block。</p><p>代码有点绕，暂时不看。</p><h2 id="5-优化器-optimizer"><a href="#5-优化器-optimizer" class="headerlink" title="5. 优化器 optimizer"></a>5. 优化器 optimizer</h2><p><a href="https://zhuanlan.zhihu.com/p/64882877?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=589386839161311232" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/64882877?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=589386839161311232</a></p><p><a href="https://gitlab.com/snowhitiger/learn_deep_learning/blob/master/sgd_visualization.ipynb" target="_blank" rel="noopener">code</a></p><p>不同的优化器，对结果也有很大的影响，先只记录一下 SGD, Adam 的常见参数配置。</p><p><strong>原始 SGD</strong>：</p><ul><li>学习率太小 0.20 ，有限步内无法达到最优，全程都没有震荡</li><li>学习率太大 0.61 ，会造成剧烈震荡，随着学习率的增加，后期震荡逐渐变大，0.52时后期震荡就已经无法收敛了</li><li>学习率适中 0.42 ，会成功达到最优点，前期会有震荡，后期没有震荡</li><li>对学习率很敏感</li></ul><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/optimizer_ori_SGD.jpg" title="optimizer_ori_SGD"><p><strong>带动量的 SGD</strong>：</p><ul><li>适中的动量 beta=0.42 可以减少大学习率 lr=0.61 的震荡，达到最优点</li><li>较大的动量 beta=0.81 会对大学习率 lr=0.61 引入更大的震荡</li><li>一般的做法是大的动量 beta=0.9 和小的学习率 lr=0.02 or 0.03，会以比较平缓的方式加速达到最优</li><li>当 lr=0.01 时，beta不管怎么取都达不到最优，或者没到，或者超过</li><li>当 beta=0.9 时，对学习率也比较敏感，0.01会到不了最优点、0.02会到最优点前面、0.03会到最优点后面，区别比较大</li><li>常用设置：beta=0.9，weight-decay=5e-4，nesterov=True，之后再调节 lr 吧。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.optim.SGD(params, lr, momentum=<span class="number">0.9</span>,  weight_decay=<span class="number">5e-4</span>, nesterov=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><strong>Adam</strong>：</p><ul><li>对学习率不敏感</li><li>训练稳定</li><li>在最优点附件</li><li>参数设置：beta1=0.9, beta2=0.999, eta=1e-8</li></ul><p>虽然 Adam 对初始学习率不敏感，训练也比较稳定，但最终能达到的精度没有手动调好的SGD来得高。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.optim.Adam(params, lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>, weight_decay=<span class="number">1e-4</span>)</span><br><span class="line"><span class="comment"># 更推荐 0.5, 0.999</span></span><br><span class="line">torch.optim.Adam(params, lr=<span class="number">0.001</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1704.06904&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;商汤 CVPR2017 Residual Attention_Network for Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/fwang91/residual-attention-network&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;caffe&lt;/a&gt;, &lt;a href=&quot;http://ethereon.github.io/netscope/#/editor&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;caffe网络可视化工具 Netscope&lt;/a&gt;, &lt;a href=&quot;https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1807.06521&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ECCV2018_CBAM: Convolutional Block Attention Module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/luuuyi/CBAM.PyTorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1904.11492&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/xvjiarui/GCNet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;
    
    </summary>
    
      <category term="attention" scheme="http://yoursite.com/categories/attention/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>ECN</title>
    <link href="http://yoursite.com/2019/05/06/ECN/"/>
    <id>http://yoursite.com/2019/05/06/ECN/</id>
    <published>2019-05-06T07:32:26.000Z</published>
    <updated>2019-06-03T09:20:33.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>钟准团队的CCPR2019文章，牛逼。思想有一丢丢类似聚类，但是比聚类强的是，赋予了id。与HHL文章有很大的不同。什也不说了，虚心学习。</p><ul><li>paper: <a href="https://arxiv.org/abs/1904.01990" target="_blank" rel="noopener">CVPR2019_Invariance Matters Exemplar Memory for Domain Adaptive Person Re-identification</a></li><li>code: <a href="https://github.com/zhunzhong07/ECN" target="_blank" rel="noopener">pytorch</a></li></ul><a id="more"></a><ul><li>[x] 2019-06-03: 这篇文章和 <a href="https://tjjtjjtjj.github.io/2019/03/27/One-Example-reID/#more" target="_blank" rel="noopener">One Example reID</a> 有些相似啊。感觉大家的论文之间的联系错综复杂。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>作者的注意力还是集中在如何区分 target domain 中的图片。考虑了三个因素，exemplar-invariance, camera-invariance and neighborhood-invariance. 可以简单地理解成每个人是一类，StarGAN生成的图片是一类，特征相似的是一类，并且为了实现上述目标，需要存储 target domain 中 train 数据集的所有特征，这一点有点疑问，需要的存储空间和计算时间消耗大吗？作者在第一节的最后有解答，消耗的空间和时间都很小。等自己实验的时候就知道了。它是用了两个分类器，一个用于训练 source 数据集， label 是 source 的label，一个用于训练 target 数据集， label 是 target 的label，两个label不一样。网络模型与HHL略有不同。</p><p>第一，通过分类损失使 target domain 的图片全部可分。分类模型学到的更可能是图片视觉上的相似性而不是语义相似性，并且数据集中同一id的图片在视觉上变化很大，所以，索性将target数据集上的每张图片作为一类，迫使不同id的相似图片变得不相似。行吧，这个理由，(⊙o⊙)…，怎么有点牵强吧，因为和后面的特征相似的作为一类有一丢丢矛盾。不对，有点类似既然不知道你们的id，那就把让你们之间先全部可分，这个思路在哪篇论文见过，想不起来了。</p><p>第二，同一 content 不同 camera style 的图片视为一类。这个和HHL的数据集处理方法一样，用 StarGAN 生成同一图片在不同 camera 下的图片，并视为一类，因为刚刚对图片使用了分类损失，所以这时候可以直接用到分类损失上，而不用再考虑三元组损失，(我觉得可能是基于分类损失的作用一直大于二元组损失，三元组损失)，这时可以保证同一 content 不同 camera style 的图片视为一类。这时 StarGAN 生成图片中的人的形态等基本保留，感觉更多是亮度上的变化(肉眼可见)，偶尔会有衣服颜色的变换，但是不能变换这个人的姿势，比如正面走变成侧面走这种，HHL 已经证明了 camera style 的影响，但是我觉得姿势的变换也是一个重要的因素才对，因为分类模型的起家就是识别同一类物体不同形态的图片。好像 CVPR 2019 这个团队有做这方面的内容，抽空膜拜一波。</p><p>第三，k近邻的图片视为一类。这应该像是之间看到的一些关于聚类的或者k近邻的论文，总体思想是因为不知道图片的真id，那就可能赋予一个比较接近真实的id，又因为同一id的图片的特征应该是临近的，所以把临近的图片作为同一id也不过分，其实有的论文用的是聚类，但本质是一样的。</p><p>我觉得钟准厉害的地方不仅仅是能想到这三个创新点，更重要的是能把这三个创新点融合到一起，因为这三个创新点在别的论文里或多或少都可以看到影子，但是因为实现的的方法、思路、具体过程都不一样，很难看见一个创新点就把融到一起，看到一个想法就放到自己的模型里，或者看到一个idea就觉得有用并且一试还真有用，并能给出属于自己的解释，而不是明显的生搬硬造，比如这里的 memory module 很有想法。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>Unsupervised domain adaptation. 一种方法是对齐两个域的特征，其基本假设是源域和目标域的类别一样。另一种方法是丢弃目标域的未知类别样本，学习源域到目标域的映射。</p><p>Unsupervised person re-identification. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.pdf" target="_blank" rel="noopener">ECCV2018_HHL</a> 可能忽略了 target domain 中其他因素对 id 的影响，比如姿势等。与 <a href="https://arxiv.org/pdf/1808.07301.pdf" target="_blank" rel="noopener">BMVC2018_DAL</a> 不同的是，作者设计了软分类损失。</p><h2 id="3-The-Proposed-Method"><a href="#3-The-Proposed-Method" class="headerlink" title="3. The Proposed Method"></a>3. The Proposed Method</h2><p>简单定义下数学符号，直接用英文吧，能知道就行。</p><div class="table-container"><table><thead><tr><th style="text-align:center">数学符号</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">$\lbrace X_s, Y_s \rbrace, N_s$</td><td style="text-align:center">source domain, $N_s$ person images, $(x_{si}, y_{s,i})$, $M$ person identities</td></tr><tr><td style="text-align:center">$X_t, N_t$</td><td style="text-align:center">target domain, $N_t$ person images</td></tr></tbody></table></div><h3 id="3-1-Overview-of-Framework"><a href="#3-1-Overview-of-Framework" class="headerlink" title="3.1 Overview of Framework"></a>3.1 Overview of Framework</h3><img src="/2019/05/06/ECN/framework.png" title="Framework of the proposed approach"><p>其实模型没有画的这么复杂，基本还是保留ResNet-50的基本网络，后面加个FC-4049，FC-#id/ exemplar memory，其中FC-#id用于 source domain 的数据， exemplar memory 用于计算 target domain 的数据，等下直接看代码一目了然。</p><h3 id="3-2-Supervised-Learning-for-Source-Domain"><a href="#3-2-Supervised-Learning-for-Source-Domain" class="headerlink" title="3.2 Supervised Learning for Source Domain"></a>3.2 Supervised Learning for Source Domain</h3><p>这个容易理解， source domain 直接用个分类损失。</p><script type="math/tex; mode=display">L_{src}=-\frac{1}{n_s}\sum_{i=1}^{n_s}\log p(y_{s,i}|x_{s,i})</script><h3 id="3-3-Exemplar-Memory"><a href="#3-3-Exemplar-Memory" class="headerlink" title="3.3 Exemplar Memory"></a>3.3 Exemplar Memory</h3><p>Exemplar Memory 借鉴的是 <a href="https://arxiv.org/abs/1604.01850v3" target="_blank" rel="noopener">Joint Detection and Identification Feature Learning for Person Search</a>，（已经有两篇论文没有看过了，自己菜不是没有原因的），采用的是 key-value 架构 $K,V$，其中在 key-memory 中存储每张图片的 FC-4096 特征， value-memory 中存储图片的lable，其中视每张图片为一类，index 即为 label。key-memory 中的特征初始化成0，value-memory 初始化成 $V[i]=i$，并且在整个训练过程中保持不变。key-memory 的更新策略是:</p><script type="math/tex; mode=display">K[i]\gets \alpha K[i]+(1-\alpha)f(x_{t,i})</script><p>其中$\alpha\in [0,1]$，$K[i]\gets \parallel K[i] \parallel_2$，也就是每次都会进行一次归一化。这里的 $\alpha$ 是线性变化的，初始化为 0.01, 后续变化为 $\alpha = 0.01*epoch $。其中每次更新都是发生在反向传播时，顺便进行更新的。写的很巧妙。</p><h3 id="3-4-Invariance-Learning-for-Target-Domain"><a href="#3-4-Invariance-Learning-for-Target-Domain" class="headerlink" title="3.4 Invariance Learning for Target Domain"></a>3.4 Invariance Learning for Target Domain</h3><p>开始祭出三板斧了。</p><p><strong>Exemplar-invariance</strong>: 因为同一id的图片在外观上也会有很大的变化，即每一张图片都应该只与自己相似，与其他图片差别很大，因此，可以视每一张图片为一类。具体过程是，对于给定图片$x_{t,i}$，先计算 $x_{t,i}$ 的特征与其他图片在 key-memory 存储的特征的 cosine 距离，距离越大越相似。然后用这个距离去预测 $x_{t,i}$ 属于第 i 类的概率。这种预测与我见过的分类不太一样，因为分类模型一般都是直接用特征去预测类别，但这里是用距离去预测类别，或者说用距离当预测概率，还是第一次见。</p><script type="math/tex; mode=display">p(i|x_{t,i})=\frac{exp(K[i]^T f(x_{t,i})/\beta)}{\sum_{j=1}^{N_t}exp(K[j]^T f(x_{t,i})/\beta)}</script><p>其中 $\beta\in (0,1]$ ，用于平衡分布的趋势，记为 temperature，$\beta=0.05$，根据代码来看，$K[i]$ 表示当前 model 运行之前存储的， $f(x_{t,i})$表示当前 model 的结果，也就是说两者不一样。下面求概率的过程同理。</p><p>这个公式和 distillation 很像。</p><script type="math/tex; mode=display">q_i=\frac{exp(z_i/T)}{\sum_j exp(z_j/T)}</script><p>其中，$z_i$是类别预测概率。</p><p>所以分类损失可以写成：</p><script type="math/tex; mode=display">L_{ei}=-\log p(i|x_{t,i})</script><p><strong>Camera-invariance</strong>：视经过 StarGAN 转换的图片$\hat{x}_{t,i}$是同一类，与HHL一样，但是与HHL不同的是，HHL用于三元组损失，ECN用于分类损失，类似 Exemplar-invariance 的概率求解和loss损失，得：</p><script type="math/tex; mode=display">p(i|\hat{x} _ {t,i})=\frac{exp(K[i]^T f(\hat{x} _ {t,i})/\beta)}{\sum_{j=1}^{N_t}exp(K[j]^T f(\hat{x}_{t,i})/\beta)}</script><script type="math/tex; mode=display">L_{ci}=-\log p(i|\hat{x}_{t,i})</script><p>其中，$\hat{x}_ {t,i}$是在生成的C张图片中$\lbrace \hat{x}_ {t,i,1},…, \hat{x}_{t,i,C} \rbrace$随机选了一张。</p><p>这里有个问题，Exemplar Memory 中是否包含 CamStyle 生成的图片。等看代码就知道了。</p><p><strong>Neighborhood-invariance</strong>: 对每一张图片，在数据集中一定至少存在一张与之相同id的图片，如果能得到这些图片，那就更好了，这样就可以获得同一id不同姿态的图片。具体方法是在 exemplar memory 中计算 cos 距离，用k近邻方法得到最近的k张图片的index，记为 $M(x_{t,i},k)$，显然最近的是i. $k=6$</p><p>基于假设k近邻得到的图片$M(x_{t,i},k)$属于同一类，因此可以得到 $x_{t,i}$ 属于第 j 类的概率的权重是:</p><script type="math/tex; mode=display">w_{i,j}=\begin{cases}\frac{1}{k}, j\not = i \\1, j=i\end{cases}, \forall j \in M(x_{t,i},k)</script><p>这种赋予权重的方法是把 $x_{t,i}$ 属于另一张图片类别的概率，而不是我之前以为的 $M(x_{t,i},k)$ 中的图片属于 $x_{t,i}$ 类别的概率。</p><p>因此损失可以写成：</p><script type="math/tex; mode=display">L_{n,i}=-\sum_{j\not = i} w_{i,j} \log p(j|x_{t,i}), \forall j \in M(x_{t,i},k)</script><p>其中，为了区分 Exemplar-invariance 和 Neighborhood-invariance ，这里没有再次计算 $x_{t,i}$ 属于第 i 类的损失。</p><p><strong>Overall loss of invariance learning</strong>: invariance learning 的总损失可以表示成：</p><script type="math/tex; mode=display">L_{tgt}=-\frac{1}{n_t} \sum_{i=1}^{n_t} \sum_{j \in M(x_{t,i},k)} w_{i,j} \log p(j|x^*_{t,i})</script><p>其中，$x^*_ {t,i}$ 表示随机从$\lbrace x_{t,i}, \hat{x}_ {t,i,1},…, \hat{x}_{t,i,C}, \rbrace$ 选一张。</p><p>看看代码再说这个损失的具体实现吧。</p><h3 id="3-5-Final-Loss-for-Network"><a href="#3-5-Final-Loss-for-Network" class="headerlink" title="3.5 Final Loss for Network"></a>3.5 Final Loss for Network</h3><script type="math/tex; mode=display">L=(1-\lambda)L_{src}+\lambda L_{tgt}</script><p>其中，$\lambda \in (0, 1]$， $\lambda=0.3$</p><p>这里的损失没有用常见的加法，而是一个线性组合？</p><h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4. Experiment"></a>4. Experiment</h2><h3 id="4-1-Experiment-Setting"><a href="#4-1-Experiment-Setting" class="headerlink" title="4.1 Experiment Setting"></a>4.1 Experiment Setting</h3><p>前5个 epoch 只训练 exemplar-invariance and camera-invariance，也就是通用的交叉熵损失，5个epoch之后再加入 neighborhood-invariance。</p><h3 id="4-2-Parameter-Analysis"><a href="#4-2-Parameter-Analysis" class="headerlink" title="4.2 Parameter Analysis"></a>4.2 Parameter Analysis</h3><p>temperature fact $\beta$, weight of loss $\lambda$, number of candidate positive samples $k$.</p><p><strong>Temperature fact $\beta$</strong></p><img src="/2019/05/06/ECN/temperature.png" title="different values of $\beta$"><p>当 $\beta$ 比较小的时候，分布越陡，值越大，损失越小，结果也越好。最好的结果是 $\beta$ 在0.05 左右。同时通过表格可以看出，当 $\beta$ 变化从0.05到0.5时，rank-1下降了17。但同时，在最优解的附近也接近最优解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在蒸馏网络中，T越大，分布越平缓</span></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">fun1 = F.softmax</span><br><span class="line">aa = torch.tensor([<span class="number">0.2</span>, <span class="number">0.8</span>, <span class="number">0.2</span>])</span><br><span class="line">fun1(aa, dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">0.2616</span>, <span class="number">0.4767</span>, <span class="number">0.2616</span>])</span><br><span class="line">bb = torch.tensor([<span class="number">0.2</span>, <span class="number">0.8</span>, <span class="number">0.2</span>])/<span class="number">0.1</span></span><br><span class="line">fun1(bb, dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">0.0025</span>, <span class="number">0.9951</span>, <span class="number">0.0025</span>])</span><br></pre></td></tr></table></figure><p><strong>The weight of source and target losses $\lambda$</strong>:</p><img src="/2019/05/06/ECN/weight.png" title="The weight of source and target losses $\lambda$"><p>可以看出即便只有 target loss， 性能也超过 baseline， 当取其他值的时候，性能几乎不变。</p><p><strong>Number of positive samples $k$</strong>:</p><img src="/2019/05/06/ECN/number.png" title="Number of positive samples $k$"><p>可以看出，性能对k挺敏感的。</p><h3 id="4-4-Evaluation"><a href="#4-4-Evaluation" class="headerlink" title="4.4 Evaluation"></a>4.4 Evaluation</h3><p>通过表格中的数据集可以计算出，C+N&gt;C,N，C和N共同使用互相还有促进作用。</p><p>作者也在 MSMT 数据集上做了训练测试。</p><img src="/2019/05/06/ECN/MSMT.png" title="performance evaluation when tested on MSMT17"><p>感觉大家都开始在 MSMT17 上做实验了，不怎么管 CUHK 了。</p><h2 id="5-code"><a href="#5-code" class="headerlink" title="5. code"></a>5. code</h2><p>代码和HHL的差不多</p><h3 id="5-1-model"><a href="#5-1-model" class="headerlink" title="5.1 model"></a>5.1 model</h3><h4 id="5-1-1-baseline"><a href="#5-1-1-baseline" class="headerlink" title="5.1.1 baseline"></a>5.1.1 baseline</h4><p>有以下几个改变：</p><ul><li>conv1 ~ layer2 的参数不再更新</li><li>source: layer4-avgpool-Linear(4096)-bn-relu-drop(0.5)-Linear(num_class)</li><li>target: layer4-avgpool-Linear(4096)-bn-F.normalize-drop(0.5)</li></ul><p>Question：先normalize后drop的话就不满足归一化的定义了</p><p>Question: conv1~layer2 固定参数是有什么含义吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    __factory = &#123;</span><br><span class="line">        <span class="number">18</span>: torchvision.models.resnet18,</span><br><span class="line">        <span class="number">34</span>: torchvision.models.resnet34,</span><br><span class="line">        <span class="number">50</span>: torchvision.models.resnet50,</span><br><span class="line">        <span class="number">101</span>: torchvision.models.resnet101,</span><br><span class="line">        <span class="number">152</span>: torchvision.models.resnet152,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, depth, pretrained=True, cut_at_pooling=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_features=<span class="number">0</span>, norm=False, dropout=<span class="number">0</span>, num_classes=<span class="number">0</span>, num_triplet_features=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.depth = depth</span><br><span class="line">        self.pretrained = pretrained</span><br><span class="line">        self.cut_at_pooling = cut_at_pooling</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Construct base (pretrained) resnet</span></span><br><span class="line">        <span class="keyword">if</span> depth <span class="keyword">not</span> <span class="keyword">in</span> ResNet.__factory:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">"Unsupported depth:"</span>, depth)</span><br><span class="line">        self.base = ResNet.__factory[depth](pretrained=pretrained)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Fix layers [conv1 ~ layer2]</span></span><br><span class="line">        fixed_names = []</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.base._modules.items():</span><br><span class="line">            <span class="keyword">if</span> name == <span class="string">"layer3"</span>:</span><br><span class="line">                <span class="comment"># assert fixed_names == ["conv1", "bn1", "relu", "maxpool", "layer1", "layer2"]</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            fixed_names.append(name)</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> module.parameters():</span><br><span class="line">                param.requires_grad = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.cut_at_pooling:</span><br><span class="line">            self.num_features = num_features</span><br><span class="line">            self.norm = norm</span><br><span class="line">            self.dropout = dropout</span><br><span class="line">            self.has_embedding = num_features &gt; <span class="number">0</span></span><br><span class="line">            self.num_classes = num_classes</span><br><span class="line">            self.num_triplet_features = num_triplet_features</span><br><span class="line"></span><br><span class="line">            self.l2norm = Normalize(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            out_planes = self.base.fc.in_features</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Append new layers</span></span><br><span class="line">            <span class="keyword">if</span> self.has_embedding:</span><br><span class="line">                self.feat = nn.Linear(out_planes, self.num_features)</span><br><span class="line">                self.feat_bn = nn.BatchNorm1d(self.num_features)</span><br><span class="line">                init.kaiming_normal_(self.feat.weight, mode=<span class="string">'fan_out'</span>)</span><br><span class="line">                init.constant_(self.feat.bias, <span class="number">0</span>)</span><br><span class="line">                init.constant_(self.feat_bn.weight, <span class="number">1</span>)</span><br><span class="line">                init.constant_(self.feat_bn.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Change the num_features to CNN output channels</span></span><br><span class="line">                self.num_features = out_planes</span><br><span class="line">            <span class="keyword">if</span> self.dropout &gt;= <span class="number">0</span>:</span><br><span class="line">                self.drop = nn.Dropout(self.dropout)</span><br><span class="line">            <span class="keyword">if</span> self.num_classes &gt; <span class="number">0</span>:</span><br><span class="line">                self.classifier = nn.Linear(self.num_features, self.num_classes)</span><br><span class="line">                init.normal_(self.classifier.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                init.constant_(self.classifier.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.pretrained:</span><br><span class="line">            self.reset_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, output_feature=None)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.base._modules.items():</span><br><span class="line">            <span class="keyword">if</span> name == <span class="string">'avgpool'</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = module(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.cut_at_pooling:</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        x = F.avg_pool2d(x, x.size()[<span class="number">2</span>:])</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_feature == <span class="string">'pool5'</span>:</span><br><span class="line">            x = F.normalize(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.has_embedding:</span><br><span class="line">            x = self.feat(x)</span><br><span class="line">            x = self.feat_bn(x)</span><br><span class="line">            tgt_feat = F.normalize(x)</span><br><span class="line">            tgt_feat = self.drop(tgt_feat)</span><br><span class="line">            <span class="keyword">if</span> output_feature == <span class="string">'tgt_feat'</span>:</span><br><span class="line">                <span class="keyword">return</span> tgt_feat</span><br><span class="line">        <span class="keyword">if</span> self.norm:</span><br><span class="line">            x = F.normalize(x)</span><br><span class="line">        <span class="keyword">elif</span> self.has_embedding:</span><br><span class="line">            x = F.relu(x)</span><br><span class="line">        <span class="keyword">if</span> self.dropout &gt; <span class="number">0</span>:</span><br><span class="line">            x = self.drop(x)</span><br><span class="line">        <span class="keyword">if</span> self.num_classes &gt; <span class="number">0</span>:</span><br><span class="line">            x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                init.kaiming_normal(m.weight, mode=<span class="string">'fan_out'</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    init.constant(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                init.constant(m.weight, <span class="number">1</span>)</span><br><span class="line">                init.constant(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</span><br><span class="line">                init.normal(m.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    init.constant(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>L2 正则化定义了专门的层，但没有使用，可能是嫌慢吧。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Normalize</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, power=<span class="number">2</span>)</span>:</span></span><br><span class="line">        super(Normalize, self).__init__()</span><br><span class="line">        self.power = power</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        norm = x.pow(self.power).sum(<span class="number">1</span>, keepdim=<span class="keyword">True</span>).pow(<span class="number">1.</span>/self.power)</span><br><span class="line">        out = x.div(norm)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h4 id="5-1-2-InvNet"><a href="#5-1-2-InvNet" class="headerlink" title="5.1.2 InvNet"></a>5.1.2 InvNet</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pytorch&gt;=1.0.0</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, autograd</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable, Function</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment"># 这应该是最新版的自定义 Function 的实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExemplarMemory</span><span class="params">(Function)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, em, alpha=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        super(ExemplarMemory, self).__init__()</span><br><span class="line">        self.em = em</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: b*2048, targets: b*1 (index)</span></span><br><span class="line">        self.save_for_backward(inputs, targets)</span><br><span class="line">        <span class="comment"># outputs: b*12936</span></span><br><span class="line">        outputs = inputs.mm(self.em.t())</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_outputs)</span>:</span></span><br><span class="line">        <span class="comment"># 这个backward 主要用于更新 em</span></span><br><span class="line">        inputs, targets = self.saved_tensors</span><br><span class="line">        grad_inputs = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> self.needs_input_grad[<span class="number">0</span>]:</span><br><span class="line">            grad_inputs = grad_outputs.mm(self.em)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(inputs, targets):</span><br><span class="line">            self.em[y] = self.alpha * self.em[y] + (<span class="number">1.</span> - self.alpha) * x</span><br><span class="line">            self.em[y] /= self.em[y].norm()</span><br><span class="line">        <span class="keyword">return</span> grad_inputs, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Invariance learning loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InvNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, num_classes, beta=<span class="number">0.05</span>, knn=<span class="number">6</span>, alpha=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        super(InvNet, self).__init__()</span><br><span class="line">        self.device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">        self.num_features = num_features</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.alpha = alpha  <span class="comment"># Memory update rate</span></span><br><span class="line">        self.beta = beta  <span class="comment"># Temperature fact</span></span><br><span class="line">        self.knn = knn  <span class="comment"># Knn for neighborhood invariance</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Exemplar memory, 12936x2048</span></span><br><span class="line">        self.em = nn.Parameter(torch.zeros(num_classes, num_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets, epoch=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: b*2048, targets: b*1 (index)</span></span><br><span class="line">        alpha = self.alpha * epoch</span><br><span class="line">        <span class="comment"># 每次都是重建一个 ExemplarMemory，我觉得可能是因为alpha每次要改变。</span></span><br><span class="line">        inputs = ExemplarMemory(self.em, alpha=alpha)(inputs, targets)</span><br><span class="line">        <span class="comment"># inputs: b*12936</span></span><br><span class="line"></span><br><span class="line">        inputs /= self.beta</span><br><span class="line">        <span class="keyword">if</span> self.knn &gt; <span class="number">0</span> <span class="keyword">and</span> epoch &gt; <span class="number">4</span>:</span><br><span class="line">            <span class="comment"># With neighborhood invariance</span></span><br><span class="line">            loss = self.smooth_loss(inputs, targets)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Without neighborhood invariance</span></span><br><span class="line">            loss = F.cross_entropy(inputs, targets)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">smooth_loss</span><span class="params">(self, inputs, targets)</span>:</span></span><br><span class="line">        <span class="comment"># overall loss of invariance loss</span></span><br><span class="line">        <span class="comment"># inputs: b*12936, targets: b*1 (index)</span></span><br><span class="line">        targets = self.smooth_hot(inputs.detach().clone(), targets.detach().clone(), self.knn)</span><br><span class="line">        <span class="comment"># targets: b*12936, weights</span></span><br><span class="line">        outputs = F.log_softmax(inputs, dim=<span class="number">1</span>)</span><br><span class="line">        loss = - (targets * outputs)</span><br><span class="line">        loss = loss.sum(dim=<span class="number">1</span>)</span><br><span class="line">        loss = loss.mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">smooth_hot</span><span class="params">(self, inputs, targets, k=<span class="number">6</span>)</span>:</span></span><br><span class="line">        <span class="comment"># Sort</span></span><br><span class="line">        <span class="comment"># inputs: b*12936, targets: b*1 (index)</span></span><br><span class="line">        <span class="comment"># targets_onehot: b*12936</span></span><br><span class="line">        <span class="comment"># targets_onehot: 记录的是当前样本属于其他类的概率的权重 1/k or 1</span></span><br><span class="line">        _, index_sorted = torch.sort(inputs, dim=<span class="number">1</span>, descending=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        ones_mat = torch.ones(targets.size(<span class="number">0</span>), k).to(self.device)</span><br><span class="line">        targets = torch.unsqueeze(targets, <span class="number">1</span>)</span><br><span class="line">        targets_onehot = torch.zeros(inputs.size()).to(self.device)</span><br><span class="line">        <span class="comment"># 这里的 weights 应该是 1/k， 因为每个值都一样，所以 softmax 之后就是 1/k</span></span><br><span class="line">        <span class="comment"># 猜测作者这里刚开始不是直接想要 1/k 的权重，而是根据距离远近赋予权重，比如选</span></span><br><span class="line">        <span class="comment"># 择6个最近的，然后根据相似性赋予权重（等同于概率的求解）</span></span><br><span class="line">        <span class="comment"># Question:weights = F.softmax(ones_mat, dim=1)</span></span><br><span class="line">        <span class="comment"># targets_onehot.scatter_(1, index_sorted[:, 0:k], ones_mat * weights)</span></span><br><span class="line">        weights = F.softmax(ones_mat, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 根据位置填充权重</span></span><br><span class="line">        targets_onehot.scatter_(<span class="number">1</span>, index_sorted[:, <span class="number">0</span>:k], ones_mat * weights)</span><br><span class="line">        <span class="comment"># Question: 怎么保证前k个就一定没有index？</span></span><br><span class="line">        targets_onehot.scatter_(<span class="number">1</span>, targets, float(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> targets_onehot</span><br></pre></td></tr></table></figure><p>补充 <strong>gather</strong> 和 <strong>scatter</strong> 的用法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gather</span></span><br><span class="line"><span class="comment"># torch.gather(input, dim, index, out=None)</span></span><br><span class="line"><span class="comment"># output[i][j][k]=input[index[[i][j][k]]][j][k] # if dim=0</span></span><br><span class="line"><span class="comment"># output[i][j][k]=input[i][index[[i][j][k]]][k] # if dim=1</span></span><br><span class="line"><span class="comment"># output[i][j][k]=input[i][j][index[[i][j][k]]] # if dim=2</span></span><br><span class="line"><span class="comment"># 光看公式很绕，</span></span><br><span class="line"><span class="comment"># 其中 out 的size和 index 的size相同</span></span><br><span class="line"><span class="comment"># input: a0*a1*a2*...ai-1*ai*ai+1,...,an-1</span></span><br><span class="line"><span class="comment"># index: a0*a1*a2*...ai-1*y*ai+1,...,an-1</span></span><br><span class="line"><span class="comment"># output:a0*a1*a2*...ai-1*y*ai+1,...,an-1</span></span><br><span class="line"><span class="comment"># 除了第dim维，其他维度上 index 的size和input的size相同</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者我们换个角度，不要管公式，把index分成y份，index_0，index_1, ..., index_y-1</span></span><br><span class="line"><span class="comment"># index_0: a0*a1*a2*...ai-1*1*ai+1,...,an-1</span></span><br><span class="line"><span class="comment"># index_0 就是在第i个维度上选取对应位置第y[0]个通道的数字，</span></span><br><span class="line"><span class="comment"># 以此类推，下面用二维矩阵做个示范</span></span><br><span class="line"><span class="comment"># 可以理解成降维</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]])</span><br><span class="line"><span class="comment"># [0,0] 表示选取第0维第0个通道的数字，第0维第0个通道的数字</span></span><br><span class="line">torch.gather(input, <span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">0</span>]]) )</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>]])</span><br><span class="line"><span class="comment"># [0,0] 表示选取第0维第0个通道的数字，第0维第1个通道的数字</span></span><br><span class="line">torch.gather(input, <span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>]]) )</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">4.</span>]])</span><br><span class="line">torch.gather(input, <span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>]]) )</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">4.</span>]])</span><br><span class="line"><span class="comment"># 三维矩阵</span></span><br><span class="line"><span class="comment"># 三维矩阵更好理解，可以通过投射的方式理解</span></span><br><span class="line">a = torch.arange(<span class="number">0</span>,<span class="number">27</span>).view(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 第0维第0个通道</span></span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>]],</span><br><span class="line"><span class="comment"># 第0维第1个通道</span></span><br><span class="line">        [[ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">         [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>],</span><br><span class="line">         [<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>]],</span><br><span class="line"><span class="comment"># 第0维第2个通道</span></span><br><span class="line">        [[<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>],</span><br><span class="line">         [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>]]])</span><br><span class="line"><span class="comment"># 在对应位置，分别选取第0维的第0个，第1个，第2个通道的数字</span></span><br><span class="line">c = [[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]]]</span><br><span class="line">index = torch.LongTensor(c)</span><br><span class="line">a.gather(<span class="number">0</span>, index)</span><br><span class="line">tensor([[[ <span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">         [ <span class="number">3</span>, <span class="number">13</span>, <span class="number">23</span>],</span><br><span class="line">         [ <span class="number">6</span>, <span class="number">16</span>, <span class="number">26</span>]]])</span><br><span class="line"><span class="comment"># output的第0维第0个通道：分别选取0,1,2</span></span><br><span class="line"><span class="comment"># output的第0维第1个通道：分别选取0,1,0</span></span><br><span class="line">c = [[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]], [[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]]</span><br><span class="line">index = torch.LongTensor(c)</span><br><span class="line">a.gather(<span class="number">0</span>, index)</span><br><span class="line">tensor([[[ <span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">         [ <span class="number">3</span>, <span class="number">13</span>, <span class="number">23</span>],</span><br><span class="line">         [ <span class="number">6</span>, <span class="number">16</span>, <span class="number">26</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0</span>, <span class="number">10</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>, <span class="number">13</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>, <span class="number">16</span>,  <span class="number">8</span>]]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scatter</span></span><br><span class="line"><span class="comment"># torch.scatter_(dim, index, src)</span></span><br><span class="line"><span class="comment"># 可以理解成gather的相反操作</span></span><br><span class="line"><span class="comment"># src 和 index 的size相同</span></span><br><span class="line"><span class="comment"># out[index[i][j][k]][j][k]=src[i][j][k] if dim=0</span></span><br><span class="line"><span class="comment"># out[i][index[i][j][k]][k]=src[i][j][k] if dim=1</span></span><br><span class="line"><span class="comment"># out[i][j][index[i][j][k]]=src[i][j][k] if dim=2</span></span><br><span class="line">x = torch.rand(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"> <span class="number">0.4319</span>  <span class="number">0.6500</span>  <span class="number">0.4080</span>  <span class="number">0.8760</span>  <span class="number">0.2355</span></span><br><span class="line"> <span class="number">0.2609</span>  <span class="number">0.4711</span>  <span class="number">0.8486</span>  <span class="number">0.8573</span>  <span class="number">0.1029</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">2</span>x5]</span><br><span class="line">torch.zeros(<span class="number">3</span>, <span class="number">5</span>).scatter_(<span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]), x)</span><br><span class="line"> <span class="number">0.4319</span>  <span class="number">0.4711</span>  <span class="number">0.8486</span>  <span class="number">0.8760</span>  <span class="number">0.2355</span></span><br><span class="line"> <span class="number">0.0000</span>  <span class="number">0.6500</span>  <span class="number">0.0000</span>  <span class="number">0.8573</span>  <span class="number">0.0000</span></span><br><span class="line"> <span class="number">0.2609</span>  <span class="number">0.0000</span>  <span class="number">0.4080</span>  <span class="number">0.0000</span>  <span class="number">0.1029</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">3</span>x5]</span><br></pre></td></tr></table></figure><h3 id="5-2-data"><a href="#5-2-data" class="headerlink" title="5.2 data"></a>5.2 data</h3><p>对于 target domain 中的图片，每次都是从原始图片和生成图片组成的集合中随机取一张图片，当选取的 camid 是原始图片的 camid 时，取原始图片，当取到的 camid 不是原始图片的 camid 时，取生成图片，实际只使用了 C-1 张生成图片和 1 张原始图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># reid/utils/data/preprocessor.py</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_single_item</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        fname, pid, camid = self.dataset[index]</span><br><span class="line">        sel_cam = torch.randperm(self.num_cam)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> sel_cam == camid:</span><br><span class="line">            fpath = osp.join(self.root, fname)</span><br><span class="line">            img = Image.open(fpath).convert(<span class="string">'RGB'</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'msmt'</span> <span class="keyword">in</span> self.root:</span><br><span class="line">                fname = fname[:<span class="number">-4</span>] + <span class="string">'_fake_'</span> + str(sel_cam.numpy() + <span class="number">1</span>) + <span class="string">'.jpg'</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                fname = fname[:<span class="number">-4</span>] + <span class="string">'_fake_'</span> + str(camid + <span class="number">1</span>) + <span class="string">'to'</span> + str(sel_cam.numpy() + <span class="number">1</span>) + <span class="string">'.jpg'</span></span><br><span class="line">            fpath = osp.join(self.camstyle_root, fname)</span><br><span class="line">            img = Image.open(fpath).convert(<span class="string">'RGB'</span>)</span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        <span class="keyword">return</span> img, fname, pid, index</span><br></pre></td></tr></table></figure><h3 id="5-3-optimizer"><a href="#5-3-optimizer" class="headerlink" title="5.3 optimizer"></a>5.3 optimizer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个写的可以，清晰明了，只是在HHL中，只有classifier层设置为1.0，其他新层则是0.0，这里则把新层全部设置成了1.0，应该也是实验所得吧。</span></span><br><span class="line">base_param_ids = set(map(id, model.module.base.parameters()))</span><br><span class="line"></span><br><span class="line">base_params_need_for_grad = filter(<span class="keyword">lambda</span> p: p.requires_grad, model.module.base.parameters())</span><br><span class="line"></span><br><span class="line">new_params = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span></span><br><span class="line">                id(p) <span class="keyword">not</span> <span class="keyword">in</span> base_param_ids]</span><br><span class="line">param_groups = [</span><br><span class="line">    &#123;<span class="string">'params'</span>: base_params_need_for_grad, <span class="string">'lr_mult'</span>: <span class="number">0.1</span>&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: new_params, <span class="string">'lr_mult'</span>: <span class="number">1.0</span>&#125;]</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(param_groups, lr=args.lr,</span><br><span class="line">                            momentum=args.momentum,</span><br><span class="line">                            weight_decay=args.weight_decay,</span><br><span class="line">                            nesterov=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h3 id="5-4-train"><a href="#5-4-train" class="headerlink" title="5.4 train"></a>5.4 train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># main.py and reid/trainers.py</span></span><br><span class="line"><span class="comment"># self.model: ResNet-50</span></span><br><span class="line"><span class="comment"># self.model_inv : InvNet</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># source domain 的交叉熵损失</span></span><br><span class="line">inputs, pids = self._parse_data(inputs)</span><br><span class="line">outputs = self.model(inputs)</span><br><span class="line">source_pid_loss = self.pid_criterion(outputs, pids)</span><br><span class="line"><span class="comment"># target domain 的损失</span></span><br><span class="line">outputs = self.model(inputs_target, <span class="string">'tgt_feat'</span>)</span><br><span class="line">loss_un = self.model_inv(outputs, index_target, epoch=epoch)</span><br><span class="line"><span class="comment"># overall loss</span></span><br><span class="line">loss = (<span class="number">1</span> - self.lmd) * source_pid_loss + self.lmd * loss_un</span><br></pre></td></tr></table></figure><p>知识点补充：因为 ResNet-50 的 conv1~layer2 的参数已经设置不参与更新，自然这些层的 bn 层也应该是 eval 状态。</p><p>Question: 上面的知识点补充是否正确？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_model_train</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.model.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fix first BN</span></span><br><span class="line">    fixed_bns = []</span><br><span class="line">    <span class="keyword">for</span> idx, (name, module) <span class="keyword">in</span> enumerate(self.model.module.named_modules()):</span><br><span class="line">        <span class="keyword">if</span> name.find(<span class="string">"layer3"</span>) != <span class="number">-1</span>:</span><br><span class="line">            <span class="comment"># assert len(fixed_bns) == 22</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> name.find(<span class="string">"bn"</span>) != <span class="number">-1</span>:</span><br><span class="line">            fixed_bns.append(name)</span><br><span class="line">            module.eval()</span><br><span class="line">len(fixed_bns)</span><br><span class="line"><span class="number">22</span></span><br><span class="line">[<span class="string">'bn1'</span>,</span><br><span class="line"> <span class="string">'layer1.0.bn1'</span>,</span><br><span class="line"> <span class="string">'layer1.0.bn2'</span>,</span><br><span class="line"> <span class="string">'layer1.0.bn3'</span>,</span><br><span class="line"> <span class="string">'layer1.1.bn1'</span>,</span><br><span class="line"> <span class="string">'layer1.1.bn2'</span>,</span><br><span class="line"> <span class="string">'layer1.1.bn3'</span>,</span><br><span class="line"> <span class="string">'layer1.2.bn1'</span>,</span><br><span class="line"> <span class="string">'layer1.2.bn2'</span>,</span><br><span class="line"> <span class="string">'layer1.2.bn3'</span>,</span><br><span class="line"> <span class="string">'layer2.0.bn1'</span>,</span><br><span class="line"> <span class="string">'layer2.0.bn2'</span>,</span><br><span class="line"> <span class="string">'layer2.0.bn3'</span>,</span><br><span class="line"> <span class="string">'layer2.1.bn1'</span>,</span><br><span class="line"> <span class="string">'layer2.1.bn2'</span>,</span><br><span class="line"> <span class="string">'layer2.1.bn3'</span>,</span><br><span class="line"> <span class="string">'layer2.2.bn1'</span>,</span><br><span class="line"> <span class="string">'layer2.2.bn2'</span>,</span><br><span class="line"> <span class="string">'layer2.2.bn3'</span>,</span><br><span class="line"> <span class="string">'layer2.3.bn1'</span>,</span><br><span class="line"> <span class="string">'layer2.3.bn2'</span>,</span><br><span class="line"> <span class="string">'layer2.3.bn3'</span>]</span><br></pre></td></tr></table></figure><p>到此应该代码全部看完了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;钟准团队的CCPR2019文章，牛逼。思想有一丢丢类似聚类，但是比聚类强的是，赋予了id。与HHL文章有很大的不同。什也不说了，虚心学习。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1904.01990&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019_Invariance Matters Exemplar Memory for Domain Adaptive Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/zhunzhong07/ECN&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="preson re-id" scheme="http://yoursite.com/categories/preson-re-id/"/>
    
    
      <category term="person re-id" scheme="http://yoursite.com/tags/person-re-id/"/>
    
      <category term="cross domain" scheme="http://yoursite.com/tags/cross-domain/"/>
    
  </entry>
  
  <entry>
    <title>Dual Attention Network for Scene Segmentation</title>
    <link href="http://yoursite.com/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/"/>
    <id>http://yoursite.com/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/</id>
    <published>2019-05-05T02:21:36.000Z</published>
    <updated>2019-05-20T07:45:23.148Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>这篇文章的重点在于 dual attention 的作用，并且attention的使用和之前看到的 SE block 还不太一样。dual attention 主要解决了全局依赖性，即其他位置的物体对当前位置的的物体的特征的影响。重点不是场景分割，自己也不是很懂分割的代码和实现，暂时对分割不做过多研究。</p><ul><li>paper: <a href="https://arxiv.org/pdf/1809.02983.pdf" target="_blank" rel="noopener">CPVR2019: Dual Attention Network for Scene Segmentation</a></li><li>code: <a href="https://github.com/junfu1115/DANet/" target="_blank" rel="noopener">pytorch</a></li><li>team: 中科院自动化所图像与视频分析团队（IVA），隶属于模式识别国家重点实验室，在 ICCV 2017 COCO-Places 场景解析竞赛、京东 AI 时尚挑战赛和阿里巴巴大规模图像搜索大赛踢馆赛等多次拔得头筹。嗯，一句话，很牛逼。</li><li><a href="https://blog.csdn.net/P_LarT/article/details/89043620" target="_blank" rel="noopener">解读</a></li></ul><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>作者提出了 <strong>Dual Attention Network(DANet)</strong> 来融合局部特征。具体的过程是在 dilated FCN 上添加了两种 attention modules: <strong>the position attention module</strong> and <strong>the channel attention module</strong>，这两个attention主要解决的是全局依赖性。</p><p>场景分割需要解决的两个问题：区分相似的东西(田地和草)，识别不同大小外观的同一个东西(车)。因此，场景分割模型需要提高像素级别识别的特征表示。</p><p>一种方法是<strong>多尺度</strong>融合来识别不同大小的物体，但是不能以全局地角度来很好地处理物体与物体之间的关系。应该是指最后的特征的感受野有大有小，可以理解成不同大小的物体都能识别到。</p><p>还有一种方法是利用了LSTM来实现 long-range dependencies，可以理解成物体的识别不仅依靠自己的特征，还依赖于其他物体的的特征，即<strong>全局依赖</strong>或者<strong>空间依赖性</strong>。</p><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/Dual_Attention_Network.png" title="Dual Attention Network"><p>其中 the position attention module 主要用于解决全局的空间位置依赖问题，the channel attention module 解决的是全局的通道依赖性。</p><p>所以作者主要解决的是全局依赖性，并没有考虑不同大小的物体的分割问题。</p><p>按照作者的说法，DANet 有两种作用：第一，可以避免显眼的大的物体的特征影响不起眼的小的物体的标签；第二，可以在一定程度上融合不同尺寸的物体的相似特征；第三，利用空间和通道的依懒性解决全局依赖问题。</p><h2 id="2-Dual-Attention-Network"><a href="#2-Dual-Attention-Network" class="headerlink" title="2. Dual Attention Network"></a>2. Dual Attention Network</h2><h3 id="3-1-Overview"><a href="#3-1-Overview" class="headerlink" title="3.1 Overview"></a>3.1 Overview</h3><p>基准网络是 dilated FCN.</p><h3 id="3-2-Position-Attention-Module"><a href="#3-2-Position-Attention-Module" class="headerlink" title="3.2 Position Attention Module"></a>3.2 Position Attention Module</h3><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/Position_and_Channel_Attention_Module.png" title="Position and Channel Attention Module"><p>通过特征提取网络得到特征图 $A\in R^{C\times H\times W}$，分别通过一个卷积层得两个特征图 $\lbrace B,C \rbrace \in R^{C\times H\times W}$，并且 reshape 成 $R^{C\times N}$，其中$N=H\times W$，然后得到$S\in R^{N\times N}$，此时把$R^{C}$看成这个位置的特征。下面阐述下具体的过程：</p><script type="math/tex; mode=display">\begin{aligned}\hat{S}&=C^T\times B, (where, \hat{S}\in R^{N\times N}) \\       &=[C_1, C_2,..., C_N]^T\times [B_1, B_2,..., B_N] \\       &=\begin{bmatrix}        C_1^T\times B_1 & C_1^T\times B_2 & ... & C_1^T\times B_N \\        C_2^T\times B_1 & C_2^T\times B_2 & ... & C_2^T\times B_N \\        ... & ... & ... & ... \\        C_N^T\times B_1 & C_N^T\times B_2 & ... & C_N^T\times B_N       \end{bmatrix}(where,C_j,B_i\in R^C)       \\\hat{s}_{ji}&=B_i^T \times C_j\end{aligned}</script><p>从而得到$S$</p><script type="math/tex; mode=display">s_{ji}=\frac{\exp(B_i \cdot C_j)}{\sum_{i=1}^N \exp(B_i \cdot C_j)}</script><p>可以理解成对$\hat{S}$的每一行都做一次softmax，即 S 的每一行和为1，可以解释成C中的点与B中所有点的相似性，越相似值越大。其中B和C是对称的。</p><p>同时，将A送进第三个滤波器得到 $D\in R^{C\times H\times W}$ 并且 reshape 成 $R^{C\times N}$ ，从而得到最后的输出$E$，下面阐述具体计算过程:</p><script type="math/tex; mode=display">\begin{aligned}\hat{E}&=D\times S^T (where,\hat{E}\in R^{C\times N}, D\in R^{C\times N}, C\in R^{N\times N})\\       &=[D_1, D_2,..., D_N] \times \begin{bmatrix}        s_{11} & s_{12} & ... & s_{1N} \\        s_{21} & s_{22} & ... & s_{2N} \\        ... & ... & ... & ... \\        s_{N1} & s_{N2} & ... & s_{NN}       \end{bmatrix}^T(where,D_i\in R^C) \\       &=[D_1\cdot s_{11}+D_2\cdot s_{12}+...+D_N\cdot s_{1N}, ..., D_1\cdot s_{N1}+D_2\cdot s_{N2}+...+D_N\cdot s_{NN}] \\       &=[\sum_i s_{1i}D_i, \sum_i s_{2i}D_i,... ,\sum_i s_{Ni}D_i] \\\hat{e}_{j}&=\sum_i^N(s_{ji}D_i)\end{aligned}</script><p>从而得到$E\in R^{C\times N}$,</p><script type="math/tex; mode=display">E_j=\alpha \sum_i^N(s_{ji}D_i)+A_j</script><p>相当于 $\hat{E}$ 与 $A$ 进行了线性组合，并对其reshape变成$E\in R^{C\times H\times W}$，其中$\alpha$是一个可学习参数，网络自动学习，初始化为0。</p><p>如果不考虑其中的 softmax, 可以写成:</p><script type="math/tex; mode=display">E=\alpha D\times (C^T \times B)^T+A</script><h3 id="3-3-Channel-Attention-Module"><a href="#3-3-Channel-Attention-Module" class="headerlink" title="3.3 Channel Attention Module"></a>3.3 Channel Attention Module</h3><p>Position Attention Module 是把每个位置的通道作为其特征 $R^C$ ，Channel Attention Module 是把每个通道的特征图作为其特征 $R^N$。</p><p>与 Position Attention Module 不同的地方还有没有经过三个滤波器得到 $B,C,D$ ，而是直接使用A。</p><p>仍然是先把 A reshape 成 $A\in R^{C\times N}$，然后进行和上述类似的操作，可以令$B,C,D=A^T$下面阐述具体过程:</p><script type="math/tex; mode=display">\begin{aligned}\hat{X}&=A\cdot A^T (where, A\in R^{C\times N}, X\in R^{C\times C}) \\\hat{x}_{ji}&=A_i^T \cdot A_j (where, A_i \in R^N)\end{aligned}</script><p>结合后面的代码分析，从而得到$X\in R^{C\times C}$:</p><script type="math/tex; mode=display">x_{ji}=\frac{\exp(-A_i \cdot A_j)}{\sum_{i=1}^N \exp(-A_i \cdot A_j)}</script><p>同样可以理解成对 $\hat{X}$ 的每一行做一次softmax，可以理解成A的自相关性。结合后面的 channel attention 的可视化，不同通道代表的类别不同，所以这里应该是越不相似值越大。</p><p>然后类似地我们得到$E\in R^{C\times H \times W}$:</p><script type="math/tex; mode=display">E_j=\beta \sum_i^N(x_{ji}A_i)+A_j</script><p>如果不考虑其中的 softmax, 可以写成:</p><script type="math/tex; mode=display">E=\beta (A \times A^T)\times A+A</script><p>这里给我的感觉更多地是在加法，而不是 SE block 用的乘法。</p><p>其实看到这里我是表示很怀疑的，这种 attention 能有效果吗？后面的可视化证明了作者的思路是正确的。</p><p>其中$\beta$也是一个可学习参数，网络自动学习，初始化为0。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Implementation-Details"><a href="#4-1-Implementation-Details" class="headerlink" title="4.1 Implementation Details"></a>4.1 Implementation Details</h3><p>学习率: 多项式衰减</p><script type="math/tex; mode=display">(1-\frac{iter}{total iter})^{0.9}</script><p>下面专门做一个学习率衰减的情况。</p><h3 id="4-2-Results-on-Datasets"><a href="#4-2-Results-on-Datasets" class="headerlink" title="4.2 Results on Datasets"></a>4.2 Results on Datasets</h3><h4 id="4-2-1-Ablation-Study-for-Attention-Modules"><a href="#4-2-1-Ablation-Study-for-Attention-Modules" class="headerlink" title="4.2.1 Ablation Study for Attention Modules"></a>4.2.1 Ablation Study for Attention Modules</h4><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/ablation_on_Cityscapes.png" title="ablation study on Cityscapes"><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/visualization_on_Cityscapes.png" title="visualization results on Cityscapes"><p>从实验结果可以看出， the position module 和 the channel module 互为补充，两个合起来后的提升效果没有单个的提升效果明显。</p><h3 id="4-2-3-Visualization-of-Attention-Module"><a href="#4-2-3-Visualization-of-Attention-Module" class="headerlink" title="4.2.3 Visualization of Attention Module"></a>4.2.3 Visualization of Attention Module</h3><p>这一小节很有意思的。</p><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/visualization_of_attention_module.png" title="visualization of attention module"><p>对于 position attention，得到的 $E\in R^{(H\times W)\times (H\times W)}$ ，可以理解点与点之间的相似性，对每个图片，选两个点，记为 ( #1 and #2 )，并且展示这两个点的 position attention map. 第一张图 #1 标记的是建筑物， #2 标记的是车，第二张图分别标记的是交通标记和行人，第三行标记的是植物和行人。可以看出来，同一类事物哪怕离得远也可以标记出来，不同事物哪怕离得近也标记不出来。或者说， position attention 具有在全局的角度来标记同一类事物，哪怕离得远，哪怕事物很小，同时区分近距离的不同事物。</p><p>对于 channel attention, 从图片中可以看出来，主要是同一通道得到的是同一类别。</p><p>现在还不知道是怎么可视化的。</p><h3 id="4-2-4-Comparing-with-State-of-the-art"><a href="#4-2-4-Comparing-with-State-of-the-art" class="headerlink" title="4.2.4 Comparing with State-of-the-art"></a>4.2.4 Comparing with State-of-the-art</h3><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/results.png" title="results"><p>嗯，比其他方法都强。</p><p>作者一共在四个数据集上做了实验，说明是真的强。</p><h2 id="5-Learning-rate"><a href="#5-Learning-rate" class="headerlink" title="5. Learning rate"></a>5. Learning rate</h2><p>以前虽然一直在用一些学习率衰减方式，但是都不系统。</p><h3 id="5-1-fixed"><a href="#5-1-fixed" class="headerlink" title="5.1 fixed"></a>5.1 fixed</h3><script type="math/tex; mode=display">lr=base\_lr</script><h3 id="5-2-step"><a href="#5-2-step" class="headerlink" title="5.2 step"></a>5.2 step</h3><p>离散的学习率变化策略</p><script type="math/tex; mode=display">lr=base\_lr\cdot \gamma^{epoch//step\_size}</script><p>其中，向下取整，并且 $\gamma$ 和 step_size 都需要设置<br><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/step.png" title="step"><br>gamma一般取0.1， step_wise一般取40</p><h3 id="5-3-exp"><a href="#5-3-exp" class="headerlink" title="5.3 exp"></a>5.3 exp</h3><script type="math/tex; mode=display">lr=base\_lr\cdot \gamma^{epoch}</script><p>其中 $\gamma$ 需要设置<br><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/exp.jpg" title="exp"><br>gamma一般取0.99</p><h3 id="5-4-inv"><a href="#5-4-inv" class="headerlink" title="5.4 inv"></a>5.4 inv</h3><script type="math/tex; mode=display">lr=base\_lr\cdot (1+y\cdot epoch)^{-power}</script><p>其中$\gamma$和power都需要设置<br><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/inv.jpg" title="inv"><br>gamma控制下降速率，power控制曲线在饱和状态下学习率达到的最低值。可以理解成当epoch达到最大值的时候，学习率在不同的power下最低值不一样。</p><h3 id="5-5-multistep"><a href="#5-5-multistep" class="headerlink" title="5.5 multistep"></a>5.5 multistep</h3><p>多次step，只是学习率改变的迭代次数不均匀</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lr_policy: <span class="string">"multistep"</span></span><br><span class="line">gamma: <span class="number">0.5</span></span><br><span class="line">stepvalue: <span class="number">10000</span></span><br><span class="line">stepvalue: <span class="number">30000</span></span><br><span class="line">stepvalue: <span class="number">60000</span></span><br></pre></td></tr></table></figure><h3 id="5-6-poly"><a href="#5-6-poly" class="headerlink" title="5.6 poly"></a>5.6 poly</h3><script type="math/tex; mode=display">lr=base\_lr\cdot (1-epoch/max\_epoch)^{power}</script><p>其中，power需要设置，并且epoch为0时，lr是base_lr，当达到最大次数时，学习率变成0.</p><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/poly.jpg" title="poly"><h3 id="5-7-sigmoid"><a href="#5-7-sigmoid" class="headerlink" title="5.7 sigmoid"></a>5.7 sigmoid</h3><script type="math/tex; mode=display">lr=base\_lr\cdot \frac{1}{1+exp^{-\gamma \cdot (epoch-step\_size)}}</script><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/sigmoid.jpg" title="sigmoid"><p>其中step_size控制sigmoid为0.5的位置，gamma学习率的变化速率。</p><h3 id="5-8-warm-up"><a href="#5-8-warm-up" class="headerlink" title="5.8 warm up"></a>5.8 warm up</h3><p>在前10个epoch使用较小的lr，之后正常使用</p><h3 id="5-9-all"><a href="#5-9-all" class="headerlink" title="5.9 all"></a>5.9 all</h3><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/all.jpg" title="all"><p>其中，step和multi_step最好，其次是exp,ploy，最差的是 inv,sigmoid.</p><h3 id="5-10-code"><a href="#5-10-code" class="headerlink" title="5.10 code"></a>5.10 code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = list(range(<span class="number">1000</span>))</span><br><span class="line">base_lr=<span class="number">0.01</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_lr</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    step_wise=<span class="number">50</span></span><br><span class="line">    gamma=<span class="number">0.1</span></span><br><span class="line">    <span class="keyword">return</span> base_lr*gamma**(epoch//step_wise)</span><br><span class="line">y = [step_lr(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x,y)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exp_lr</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    gamma=<span class="number">0.999</span></span><br><span class="line">    <span class="keyword">return</span> base_lr*gamma**epoch</span><br><span class="line">y = [exp_lr(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x,y)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inv_lr</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    gamma=<span class="number">0.1</span></span><br><span class="line">    power=<span class="number">0.75</span></span><br><span class="line">    <span class="keyword">return</span> base_lr*(<span class="number">1</span>+gamma*epoch)**(-power)</span><br><span class="line">y = [inv_lr(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x,y)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_lr</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    step_wise1=<span class="number">200</span></span><br><span class="line">    step_wise2=<span class="number">300</span></span><br><span class="line">    step_wise3=<span class="number">400</span></span><br><span class="line">    gamma=<span class="number">0.5</span></span><br><span class="line">    power = [<span class="number">0</span>]*step_wise1 + [<span class="number">1</span>]*step_wise2+[<span class="number">2</span>]*step_wise3</span><br><span class="line">    <span class="keyword">if</span> epoch&lt;len(power):</span><br><span class="line">        <span class="keyword">return</span> base_lr*gamma**power[epoch]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> base_lr*gamma**<span class="number">3</span></span><br><span class="line">y = [multi_lr(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x,y)</span><br></pre></td></tr></table></figure><h2 id="6-code"><a href="#6-code" class="headerlink" title="6. code"></a>6. code</h2><p>这次的代码很有含金量，用到了多GPU。</p><p>一是涉及到的代码有点多，二是自己没有跑过分割的代码，不清楚具体的代码组织形式。所以下面从小到大一个个讲关键的地方。有些代码和作者的论文描述不是非常一致，但不影响总体。</p><h3 id="6-1-PAM-and-CAM"><a href="#6-1-PAM-and-CAM" class="headerlink" title="6.1 PAM and CAM"></a>6.1 PAM and CAM</h3><p>position attention and channel attention</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PAM_Module</span><span class="params">(Module)</span>:</span></span><br><span class="line">    <span class="string">""" Position attention module"""</span></span><br><span class="line">    <span class="comment">#Ref from SAGAN</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim)</span>:</span></span><br><span class="line">        super(PAM_Module, self).__init__()</span><br><span class="line">        self.chanel_in = in_dim</span><br><span class="line"></span><br><span class="line">        self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//<span class="number">8</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//<span class="number">8</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.gamma = Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.softmax = Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            inputs :</span></span><br><span class="line"><span class="string">                x : input feature maps( B X C X H X W)</span></span><br><span class="line"><span class="string">            returns :</span></span><br><span class="line"><span class="string">                out : attention value + input feature</span></span><br><span class="line"><span class="string">                attention: B X (HxW) X (HxW)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># C: 512, C//8: 64</span></span><br><span class="line">        m_batchsize, C, height, width = x.size()</span><br><span class="line">        <span class="comment"># x: B,C,H,W</span></span><br><span class="line">        proj_query = self.query_conv(x).view(m_batchsize, <span class="number">-1</span>, width*height).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># C': B,HxW,C//8</span></span><br><span class="line">        proj_key = self.key_conv(x).view(m_batchsize, <span class="number">-1</span>, width*height)</span><br><span class="line">        <span class="comment"># B: B,C//8,HxW</span></span><br><span class="line">        energy = torch.bmm(proj_query, proj_key)</span><br><span class="line">        <span class="comment"># \hat&#123;S&#125; = C'xB : B,HxW,HxW</span></span><br><span class="line">        attention = self.softmax(energy)</span><br><span class="line">        <span class="comment"># S: B,HxW,HxW</span></span><br><span class="line">        proj_value = self.value_conv(x).view(m_batchsize, <span class="number">-1</span>, width*height)</span><br><span class="line">        <span class="comment"># D: B,C,HxW</span></span><br><span class="line">        out = torch.bmm(proj_value, attention.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># \hat&#123;E&#125; = DxS': B,C,HxW</span></span><br><span class="line">        out = out.view(m_batchsize, C, height, width)</span><br><span class="line">        <span class="comment"># \hat&#123;E&#125; : B,C,H,W</span></span><br><span class="line">        out = self.gamma*out + x</span><br><span class="line">        <span class="comment"># E: B,C,H,W</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CAM_Module</span><span class="params">(Module)</span>:</span></span><br><span class="line">    <span class="string">""" Channel attention module"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim)</span>:</span></span><br><span class="line">        super(CAM_Module, self).__init__()</span><br><span class="line">        self.chanel_in = in_dim</span><br><span class="line">        self.gamma = Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line">        self.softmax  = Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            inputs :</span></span><br><span class="line"><span class="string">                x : input feature maps( B X C X H X W)</span></span><br><span class="line"><span class="string">            returns :</span></span><br><span class="line"><span class="string">                out : attention value + input feature</span></span><br><span class="line"><span class="string">                attention: B X C X C</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        m_batchsize, C, height, width = x.size()</span><br><span class="line">        <span class="comment"># x: B,C,H,W</span></span><br><span class="line">        proj_query = x.view(m_batchsize, C, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># A: B,C,HxW</span></span><br><span class="line">        proj_key = x.view(m_batchsize, C, <span class="number">-1</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># A': B,HxW,C</span></span><br><span class="line">        energy = torch.bmm(proj_query, proj_key)</span><br><span class="line">        <span class="comment"># \hat&#123;X&#125; = AxA': B,C,C</span></span><br><span class="line">        energy_new = torch.max(energy, <span class="number">-1</span>, keepdim=<span class="keyword">True</span>)[<span class="number">0</span>].expand_as(energy)-energy</span><br><span class="line">        <span class="comment"># note that, 作者在这里用了一次 max-v_i，而不是常见的v_i-max，按照github上的解释，</span></span><br><span class="line">        <span class="comment"># 作者选用前者而不是后者的原因是后者的效果不好，不知道该怎么反驳，</span></span><br><span class="line">        <span class="comment"># channel attention 主要衡量的是通道与通道之间的相似性，</span></span><br><span class="line">        <span class="comment"># 按照这个公式，结合channel的可视化，只能强行解释成，希望通道之间不相似，越不相似给的值越高，</span></span><br><span class="line">        attention = self.softmax(energy_new)</span><br><span class="line">        <span class="comment"># X: B,C,C</span></span><br><span class="line">        proj_value = x.view(m_batchsize, C, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># D: B,C,HxW</span></span><br><span class="line">        out = torch.bmm(attention, proj_value)</span><br><span class="line">        <span class="comment"># \hat&#123;E&#125; = XxD : B,C,HxW，这里也满足\hat&#123;E&#125;中的每个元素的系数之后为1</span></span><br><span class="line">        out = out.view(m_batchsize, C, height, width)</span><br><span class="line">        <span class="comment"># \hat&#123;E&#125;: B,C,H,W</span></span><br><span class="line">        out = self.gamma*out + x</span><br><span class="line">        <span class="comment"># E: B,C,H,W</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="6-2-DANetHead"><a href="#6-2-DANetHead" class="headerlink" title="6.2 DANetHead"></a>6.2 DANetHead</h3><p>从代码上看，过程大概是：</p><ol><li>是在进入 attention module 会进行一次通道缩小，2048-&gt;512，</li><li>position attention module: sa_conv, channel attention module: sc_conv</li><li>得到三种预测的类别：sa_conv+sc_conv-&gt;sasc_output, sa_conv-&gt;sa_output, sc_conv-&gt;sc_output</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DANetHead</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, norm_layer)</span>:</span></span><br><span class="line">        <span class="comment"># in_channels: 2048</span></span><br><span class="line">        <span class="comment"># out_channels: dataset.num_classes</span></span><br><span class="line">        super(DANetHead, self).__init__()</span><br><span class="line">        inter_channels = in_channels // <span class="number">4</span></span><br><span class="line">        self.conv5a = nn.Sequential(nn.Conv2d(in_channels, inter_channels, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                                   norm_layer(inter_channels),</span><br><span class="line">                                   nn.ReLU())</span><br><span class="line">        self.sa = PAM_Module(inter_channels)</span><br><span class="line">        self.conv51 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                                   norm_layer(inter_channels),</span><br><span class="line">                                   nn.ReLU())</span><br><span class="line">        self.conv6 = nn.Sequential(nn.Dropout2d(<span class="number">0.1</span>, <span class="keyword">False</span>), nn.Conv2d(<span class="number">512</span>, out_channels, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.conv5c = nn.Sequential(nn.Conv2d(in_channels, inter_channels, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                                   norm_layer(inter_channels),</span><br><span class="line">                                   nn.ReLU())</span><br><span class="line">        self.sc = CAM_Module(inter_channels)</span><br><span class="line">        self.conv52 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                                   norm_layer(inter_channels),</span><br><span class="line">                                   nn.ReLU())</span><br><span class="line">        self.conv7 = nn.Sequential(nn.Dropout2d(<span class="number">0.1</span>, <span class="keyword">False</span>), nn.Conv2d(<span class="number">512</span>, out_channels, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.conv8 = nn.Sequential(nn.Dropout2d(<span class="number">0.1</span>, <span class="keyword">False</span>), nn.Conv2d(<span class="number">512</span>, out_channels, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x: B,C,H,W: C 2048</span></span><br><span class="line">        feat1 = self.conv5a(x)</span><br><span class="line">        <span class="comment"># feat1: B,C//4,H,W</span></span><br><span class="line">        sa_feat = self.sa(feat1)</span><br><span class="line">        <span class="comment"># sa_feat: B,C//4,H,W</span></span><br><span class="line">        sa_conv = self.conv51(sa_feat)</span><br><span class="line">        <span class="comment"># sa_conv: B,C//4,H,W</span></span><br><span class="line">        sa_output = self.conv6(sa_conv)</span><br><span class="line">        <span class="comment"># sa_output: B,C_out,H,W</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># x: B,C,H,W: C 2048</span></span><br><span class="line">        feat2 = self.conv5c(x)</span><br><span class="line">        <span class="comment"># feat2: B,C//4,H,W</span></span><br><span class="line">        sc_feat = self.sc(feat2)</span><br><span class="line">        <span class="comment"># sc_feat: B,C//4,H,W</span></span><br><span class="line">        sc_conv = self.conv52(sc_feat)</span><br><span class="line">        <span class="comment"># sc_conv: B,C//4,H,W</span></span><br><span class="line">        sc_output = self.conv7(sc_conv)</span><br><span class="line">        <span class="comment"># sc_output: B,C_out,H,W</span></span><br><span class="line"></span><br><span class="line">        feat_sum = sa_conv+sc_conv</span><br><span class="line">        <span class="comment"># feat_sum: B,C//4,H,W</span></span><br><span class="line">        sasc_output = self.conv8(feat_sum)</span><br><span class="line">        <span class="comment"># sasc_output: B,C_out,H,W</span></span><br><span class="line"></span><br><span class="line">        output = [sasc_output]</span><br><span class="line">        output.append(sa_output)</span><br><span class="line">        output.append(sc_output)</span><br><span class="line">        <span class="comment"># output:[sasc_output, sa_output, sc_output]: 3,B,C_out,H,W</span></span><br><span class="line">        <span class="keyword">return</span> tuple(output)</span><br></pre></td></tr></table></figure><h3 id="6-3-BaseNet"><a href="#6-3-BaseNet" class="headerlink" title="6.3 BaseNet"></a>6.3 BaseNet</h3><p>以ResNet-50为例，相当于求得每一个layer的输出 [c1, c2, c3, c4]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nclass, backbone, aux, se_loss, dilated=True, norm_layer=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 base_size=<span class="number">576</span>, crop_size=<span class="number">608</span>, mean=[<span class="number">.485</span>, <span class="number">.456</span>, <span class="number">.406</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                 std=[<span class="number">.229</span>, <span class="number">.224</span>, <span class="number">.225</span>], root=<span class="string">'./pretrain_models'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 multi_grid=False, multi_dilation=None)</span>:</span></span><br><span class="line">        super(BaseNet, self).__init__()</span><br><span class="line">        self.nclass = nclass</span><br><span class="line">        self.aux = aux</span><br><span class="line">        self.se_loss = se_loss</span><br><span class="line">        self.mean = mean</span><br><span class="line">        self.std = std</span><br><span class="line">        self.base_size = base_size</span><br><span class="line">        self.crop_size = crop_size</span><br><span class="line">        <span class="comment"># copying modules from pretrained models</span></span><br><span class="line">        <span class="keyword">if</span> backbone == <span class="string">'resnet50'</span>:</span><br><span class="line">            self.pretrained = resnet.resnet50(pretrained=<span class="keyword">True</span>, dilated=dilated,</span><br><span class="line">                                              norm_layer=norm_layer, root=root,</span><br><span class="line">                                              multi_grid=multi_grid, multi_dilation=multi_dilation)</span><br><span class="line">        <span class="keyword">elif</span> backbone == <span class="string">'resnet101'</span>:</span><br><span class="line">            self.pretrained = resnet.resnet101(pretrained=<span class="keyword">True</span>, dilated=dilated,</span><br><span class="line">                                               norm_layer=norm_layer, root=root,</span><br><span class="line">                                               multi_grid=multi_grid,multi_dilation=multi_dilation)</span><br><span class="line">        <span class="keyword">elif</span> backbone == <span class="string">'resnet152'</span>:</span><br><span class="line">            self.pretrained = resnet.resnet152(pretrained=<span class="keyword">True</span>, dilated=dilated,</span><br><span class="line">                                               norm_layer=norm_layer, root=root,</span><br><span class="line">                                               multi_grid=multi_grid, multi_dilation=multi_dilation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'unknown backbone: &#123;&#125;'</span>.format(backbone))</span><br><span class="line">        <span class="comment"># bilinear upsample options</span></span><br><span class="line">        self._up_kwargs = up_kwargs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">base_forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pretrained.conv1(x)</span><br><span class="line">        x = self.pretrained.bn1(x)</span><br><span class="line">        x = self.pretrained.relu(x)</span><br><span class="line">        x = self.pretrained.maxpool(x)</span><br><span class="line">        c1 = self.pretrained.layer1(x)</span><br><span class="line">        c2 = self.pretrained.layer2(c1)</span><br><span class="line">        c3 = self.pretrained.layer3(c2)</span><br><span class="line">        c4 = self.pretrained.layer4(c3)</span><br><span class="line">        <span class="keyword">return</span> c1, c2, c3, c4</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, x, target=None)</span>:</span></span><br><span class="line">        pred = self.forward(x)</span><br><span class="line">        <span class="keyword">if</span> isinstance(pred, (tuple, list)):</span><br><span class="line">            pred = pred[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> target <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> pred</span><br><span class="line">        correct, labeled = batch_pix_accuracy(pred.data, target.data)</span><br><span class="line">        inter, union = batch_intersection_union(pred.data, target.data, self.nclass)</span><br><span class="line">        <span class="keyword">return</span> correct, labeled, inter, union</span><br></pre></td></tr></table></figure><h3 id="6-5-DANet"><a href="#6-5-DANet" class="headerlink" title="6.5 DANet"></a>6.5 DANet</h3><p>相当于求这三种的预测：sa_conv+sc_conv-&gt;sasc_output, sa_conv-&gt;sa_output, sc_conv-&gt;sc_output</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DANet</span><span class="params">(BaseNet)</span>:</span></span><br><span class="line">    <span class="string">r"""Fully Convolutional Networks for Semantic Segmentation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    nclass : int</span></span><br><span class="line"><span class="string">        Number of categories for the training dataset.</span></span><br><span class="line"><span class="string">    backbone : string</span></span><br><span class="line"><span class="string">        Pre-trained dilated backbone network type (default:'resnet50'; 'resnet50',</span></span><br><span class="line"><span class="string">        'resnet101' or 'resnet152').</span></span><br><span class="line"><span class="string">    norm_layer : object</span></span><br><span class="line"><span class="string">        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Reference:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks</span></span><br><span class="line"><span class="string">        for semantic segmentation." *CVPR*, 2015</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nclass, backbone, aux=False, se_loss=False, norm_layer=nn.BatchNorm2d, **kwargs)</span>:</span></span><br><span class="line">        super(DANet, self).__init__(nclass, backbone, aux, se_loss, norm_layer=norm_layer, **kwargs)</span><br><span class="line">        self.head = DANetHead(<span class="number">2048</span>, nclass, norm_layer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 具体的图片大小还是需要看图像分割的输入，这里以标准的224为例</span></span><br><span class="line">        <span class="comment"># x: 3,H,W &amp;&amp; 224, 224</span></span><br><span class="line">        imsize = x.size()[<span class="number">2</span>:]</span><br><span class="line">        _, _, c3, c4 = self.base_forward(x)</span><br><span class="line">        <span class="comment"># c3, c4: ResNet-50 的 layer3 和 layer 4 的输出</span></span><br><span class="line">        <span class="comment"># c3: 1024, H//16, W//16 &amp;&amp; 1024, 14, 14=224//16</span></span><br><span class="line">        <span class="comment"># c4: 2018, H//32, W//32 &amp;&amp; 7, 7=224//32</span></span><br><span class="line">        x = self.head(c4)</span><br><span class="line">        <span class="comment"># x: [sasc_output, sa_output, sc_output]: 3,B,dataset.num_classes,H//32, W//32 &amp;&amp; 7, 7=224//32</span></span><br><span class="line">        x = list(x)</span><br><span class="line">        x[<span class="number">0</span>] = upsample(x[<span class="number">0</span>], imsize, **self._up_kwargs)</span><br><span class="line">        x[<span class="number">1</span>] = upsample(x[<span class="number">1</span>], imsize, **self._up_kwargs)</span><br><span class="line">        x[<span class="number">2</span>] = upsample(x[<span class="number">2</span>], imsize, **self._up_kwargs)</span><br><span class="line">        <span class="comment"># 上采样</span></span><br><span class="line">        <span class="comment"># x: [sasc_output, sa_output, sc_output]: 3,B,dataset.num_classes,H,W &amp;&amp; 224, 224</span></span><br><span class="line">        outputs = [x[<span class="number">0</span>]]</span><br><span class="line">        outputs.append(x[<span class="number">1</span>])</span><br><span class="line">        outputs.append(x[<span class="number">2</span>])</span><br><span class="line">        <span class="comment"># x: [sasc_output, sa_output, sc_output]: 3,B,dataset.num_classes,H,W &amp;&amp; 224, 224</span></span><br><span class="line">        <span class="keyword">return</span> tuple(outputs)</span><br></pre></td></tr></table></figure><h3 id="6-6-SegmentationMultiLosses"><a href="#6-6-SegmentationMultiLosses" class="headerlink" title="6.6 SegmentationMultiLosses"></a>6.6 SegmentationMultiLosses</h3><p>希望 position+channel attetion, position attention, channel attention 三种预测都准确</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SegmentationMultiLosses</span><span class="params">(CrossEntropyLoss)</span>:</span></span><br><span class="line">    <span class="string">"""2D Cross Entropy Loss with Multi-L1oss"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nclass=<span class="number">-1</span>, weight=None,size_average=True, ignore_index=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        super(SegmentationMultiLosses, self).__init__(weight, size_average, ignore_index)</span><br><span class="line">        self.nclass = nclass</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, *inputs)</span>:</span></span><br><span class="line"></span><br><span class="line">        *preds, target = tuple(inputs)</span><br><span class="line">        pred1, pred2 ,pred3= tuple(preds)</span><br><span class="line">        <span class="comment"># sa_conv+sc_conv-&gt;sasc_output, sa_conv-&gt;sa_output, sc_conv-&gt;sc_output</span></span><br><span class="line">        loss1 = super(SegmentationMultiLosses, self).forward(pred1, target)</span><br><span class="line">        loss2 = super(SegmentationMultiLosses, self).forward(pred2, target)</span><br><span class="line">        loss3 = super(SegmentationMultiLosses, self).forward(pred3, target)</span><br><span class="line">        loss = loss1 + loss2 + loss3</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="6-7-其他"><a href="#6-7-其他" class="headerlink" title="6.7 其他"></a>6.7 其他</h3><p>其他的代码暂时就不看了，只是记录一个自己没有看到过的函数</p><p>Synchronized Cross-GPU Batch Normalization functions</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;这篇文章的重点在于 dual attention 的作用，并且attention的使用和之前看到的 SE block 还不太一样。dual attention 主要解决了全局依赖性，即其他位置的物体对当前位置的的物体的特征的影响。重点不是场景分割，自己也不是很懂分割的代码和实现，暂时对分割不做过多研究。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/1809.02983.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CPVR2019: Dual Attention Network for Scene Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/junfu1115/DANet/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;team: 中科院自动化所图像与视频分析团队（IVA），隶属于模式识别国家重点实验室，在 ICCV 2017 COCO-Places 场景解析竞赛、京东 AI 时尚挑战赛和阿里巴巴大规模图像搜索大赛踢馆赛等多次拔得头筹。嗯，一句话，很牛逼。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/P_LarT/article/details/89043620&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;解读&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="segmentation" scheme="http://yoursite.com/categories/segmentation/"/>
    
    
      <category term="segmentation" scheme="http://yoursite.com/tags/segmentation/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>SE block</title>
    <link href="http://yoursite.com/2019/05/04/SE-block/"/>
    <id>http://yoursite.com/2019/05/04/SE-block/</id>
    <published>2019-05-04T08:15:35.000Z</published>
    <updated>2019-05-04T12:42:48.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>这篇文章主要是通过在神经网络中加入 SE-block 来加强通道之间的关系，提高性能，理论上讲可以加入任意网络任意任务，并且这篇文章获得了 ImageNet2017 的冠军。很牛逼。</p><ul><li>paper: <a href="https://arxiv.org/pdf/1709.01507.pdf" target="_blank" rel="noopener">ECCV2018_Squeeze-and-Excitation Networks</a></li><li>code: <a href="https://github.com/hujie-frank/SENet" target="_blank" rel="noopener">Caffe, TensorFlow, MatConvNet, MXNet, Pytorch, Chainer</a>, <a href="https://github.com/miraclewkf/SENet-PyTorch" target="_blank" rel="noopener">pytorch</a></li></ul><p>这篇文章清晰易懂，讲得很细(没用的话也比较多)，很work，一起拜读一下。<br><a id="more"></a></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul><li>SE block: Squeeze and Excitation block</li></ul><img src="/2019/05/04/SE-block/squeeze_and_excitation_block.png" title="Squeeze and Excitation block"><p>符号表达：$F_{tr}:X\to U, X\in R^{H’×W’×C’}, U\in R^{H×W×C}$</p><p>SE block 在底层时更偏向于提取任务之间的共享特征，在高层时更偏向于提取任务相关的特征。</p><h2 id="3-Squeeze-and-Excitation-Blocks"><a href="#3-Squeeze-and-Excitation-Blocks" class="headerlink" title="3. Squeeze and Excitation Blocks"></a>3. Squeeze and Excitation Blocks</h2><h3 id="3-1-Squeeze-Global-Information-Embedding"><a href="#3-1-Squeeze-Global-Information-Embedding" class="headerlink" title="3.1 Squeeze: Global Information Embedding"></a>3.1 Squeeze: Global Information Embedding</h3><script type="math/tex; mode=display">z_c=F_{sq}(u_c)=\frac{1}{H×W}\sum_{i=1}^H \sum_{j=1}^W u_c(i,j)</script><p>其中，$z\in R^C$</p><h3 id="3-2-Excitation-Adaptive-Recalibration"><a href="#3-2-Excitation-Adaptive-Recalibration" class="headerlink" title="3.2 Excitation: Adaptive Recalibration"></a>3.2 Excitation: Adaptive Recalibration</h3><script type="math/tex; mode=display">s=F_{ex}(z,W)=\sigma(g(z,W))=\sigma(W_2\delta (W_1z))</script><p>其中，$\delta$表示ReLU函数，$W_1\in R^{\frac{C}{r}×C}$ 并且 $W_2\in R^{C×\frac{C}{r}}$，也就是两个FC层。</p><script type="math/tex; mode=display">\tilde{x}_c=F_{scale}(u_c, s_c)=s_c\cdot u_c</script><h3 id="3-3-Instantiations"><a href="#3-3-Instantiations" class="headerlink" title="3.3 Instantiations"></a>3.3 Instantiations</h3><img src="/2019/05/04/SE-block/Instantiations.png" title="instantiation"><img src="/2019/05/04/SE-block/concrete_examples.png" title="concrete_examples of ResNet"><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><p>主要是从分类等场景出发，说明了 SE block 在ResNet,Inception等各种网络和ImageNet, cifar-100等数据集上表现都好。</p><h2 id="5-Ablation-Study"><a href="#5-Ablation-Study" class="headerlink" title="5. Ablation Study"></a>5. Ablation Study</h2><h3 id="5-1-Reduction-ratio"><a href="#5-1-Reduction-ratio" class="headerlink" title="5.1 Reduction ratio"></a>5.1 Reduction ratio</h3><p>Reduction ratio r in Excitation</p><img src="/2019/05/04/SE-block/different_reduction_ratio.png" title="different reduction ratio"><p>作者设置为r=16.</p><h3 id="5-2-Squeeze-Operator"><a href="#5-2-Squeeze-Operator" class="headerlink" title="5.2 Squeeze Operator"></a>5.2 Squeeze Operator</h3><p>作者只比较了max pooling 和avg pooling.这两种方法差不多。</p><h3 id="5-3-Excitation-Operator"><a href="#5-3-Excitation-Operator" class="headerlink" title="5.3 Excitation Operator"></a>5.3 Excitation Operator</h3><img src="/2019/05/04/SE-block/different_non-linearities.png" title="different non-linearities"><p>作者比较了 ReLU, Tanh, Sigmoid三种函数，实验证明 Sigmoid 函数更好一些，这里指的是第二个激活函数。</p><h3 id="5-4-Different-stages"><a href="#5-4-Different-stages" class="headerlink" title="5.4 Different stages"></a>5.4 Different stages</h3><img src="/2019/05/04/SE-block/different_stages.png" title="different stage"><h3 id="5-5-Integration-strategy"><a href="#5-5-Integration-strategy" class="headerlink" title="5.5 Integration strategy"></a>5.5 Integration strategy</h3><img src="/2019/05/04/SE-block/different_integration_designs.png" title="different integration designs"><img src="/2019/05/04/SE-block/effect_integration_designs.png" title="effect integration designs"><h2 id="6-code"><a href="#6-code" class="headerlink" title="6. code"></a>6. code</h2><p>代码还是很简单的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SE ResNet50</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SELayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channel, reduction=<span class="number">16</span>)</span>:</span></span><br><span class="line">        super(SELayer, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(channel, channel // reduction, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Linear(channel // reduction, channel, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y = self.avg_pool(x).view(b, c)</span><br><span class="line">        y = self.fc(y).view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x * y.expand_as(x)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SEBasicBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, planes, stride=<span class="number">1</span>, downsample=None, reduction=<span class="number">16</span>)</span>:</span></span><br><span class="line">        super(SEBasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes, <span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.se = SELayer(planes, reduction)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        residual = x</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.se(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;这篇文章主要是通过在神经网络中加入 SE-block 来加强通道之间的关系，提高性能，理论上讲可以加入任意网络任意任务，并且这篇文章获得了 ImageNet2017 的冠军。很牛逼。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/1709.01507.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ECCV2018_Squeeze-and-Excitation Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/hujie-frank/SENet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Caffe, TensorFlow, MatConvNet, MXNet, Pytorch, Chainer&lt;/a&gt;, &lt;a href=&quot;https://github.com/miraclewkf/SENet-PyTorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这篇文章清晰易懂，讲得很细(没用的话也比较多)，很work，一起拜读一下。&lt;br&gt;
    
    </summary>
    
      <category term="attention" scheme="http://yoursite.com/categories/attention/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="SE" scheme="http://yoursite.com/tags/SE/"/>
    
  </entry>
  
  <entry>
    <title>Reality Oriented Adaptation</title>
    <link href="http://yoursite.com/2019/05/03/Reality-Oriented-Adaptation/"/>
    <id>http://yoursite.com/2019/05/03/Reality-Oriented-Adaptation/</id>
    <published>2019-05-03T07:52:58.000Z</published>
    <updated>2019-05-04T08:18:41.523Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><h3 id="0-0-前言"><a href="#0-0-前言" class="headerlink" title="0.0 前言"></a>0.0 前言</h3><p>这篇文章主要解决的问题是：当语义分割网络在合成数据集(有标签)上训练好，在真实数据集(没有标签)上性能下降比较多。作者认为有两个原因：对合成数据过拟合，合成数据与真实数据存在分布差异。(好吧，我认为这两是一个原因)。作者提出target guided distillation 和 spatial-aware adaptation 来改进性能，效果还挺好的。我主要看target guided distillaiton。</p><ul><li>paper: <a href="https://arxiv.org/pdf/1711.11556.pdf" target="_blank" rel="noopener">CVPR2018_ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes</a></li><li>code: 无</li></ul><h3 id="0-1-HydraPlus-Net"><a href="#0-1-HydraPlus-Net" class="headerlink" title="0.1 HydraPlus-Net"></a>0.1 HydraPlus-Net</h3><p>顺便记录下刚看的论文 HydraPlus-Net，因为这篇论文是caffe代码且对我的帮助不大，所以只是简单地记录下其中的创新点。</p><a id="more"></a><ul><li>paper: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_HydraPlus-Net_Attentive_Deep_ICCV_2017_paper.pdf" target="_blank" rel="noopener">ICCV2017: HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</a></li><li>code: <a href="https://github.com/xh-liu/HydraPlus-Net" target="_blank" rel="noopener">caffe</a></li></ul><img src="/2019/05/03/Reality-Oriented-Adaptation/HydraPlus.png" title="HydraPlus network"><p>其主要创新点在于：</p><ul><li>attention不仅可以用于本block，也可以用于其他block</li><li>一个block可以生成多个attention map</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>针对合成数据集的模型在真实数据集上性能很差，作者提出原因可能是：过拟合和分布不一致，因此提出模型：ROAD-Net。下面对作者提出的名词做出解释</p><ul><li>Reality Oriented Adaptation Networks(ROAD-Net)</li><li>real style orientation &amp; target guided distillation: 为了避免过拟合合成数据集，使用 distillation 使 model 的输出和预训练的模型的输出一致，注意，这里 distillation 针对的是真实数据集，而不是合成数据集，这种方法称为 target guided distillation.</li><li>real distribution orientation &amp; spatial-aware adaptation &amp; domain classifier: 因为合成数据集和真实数据集的分布不一致，提出 <a href="https://arxiv.org/pdf/1409.7495v2.pdf" target="_blank" rel="noopener">DANN</a> 也就是 domain classifier，其实类似GAN的D，来使得合成数据集和真实数据集的特征分布一致。主要过程就是将合成数据集和真实数据集的特征图分割成几个区域，然后判断这几个区域是不是同一个domain。</li></ul><img src="/2019/05/03/Reality-Oriented-Adaptation/ROAD_Net.png" title="ROAD_Net"><h2 id="2-Reality-Oriented-Adaptation-Networks"><a href="#2-Reality-Oriented-Adaptation-Networks" class="headerlink" title="2. Reality Oriented Adaptation Networks"></a>2. Reality Oriented Adaptation Networks</h2><h3 id="2-1-Target-Guided-Distillation"><a href="#2-1-Target-Guided-Distillation" class="headerlink" title="2.1 Target Guided Distillation"></a>2.1 Target Guided Distillation</h3><img src="/2019/05/03/Reality-Oriented-Adaptation/target_guided_distillation.png" title="illustration of target guided distillaion"><p>其中，pretrained model 在训练的时候不更新.</p><p><strong>distillation loss</strong>：</p><script type="math/tex; mode=display">L_{dist}=\frac{1}{N}\sum_{i,j}\parallel x_{i,j}-z_{i,j} \parallel _2</script><p>其中，$x_{i,j}, z_{i,j}$分别表示 segmentation model 和 pretrained model 得到的 feature map 在位置 $(i,j)$ 的值，二范是简单的欧式距离。这个损失简单粗暴，后续的实验也证明了这种方法的确要更好一些。</p><p>我觉得这个是一个思路，这种 distillation 可以在一定程度上使得 segmentation model 学习到真实数据集的特征分布。</p><p>当然，除了 distillation loss 用来防止过拟合，也有其他方法用来防止过拟合，比如冻结一些层然后循环，或者用 source (合成数据集) 用来进行 distillation，是 <a href="https://arxiv.org/pdf/1606.09282.pdf" target="_blank" rel="noopener">learning without forgetting</a>，这篇文章也很有用，等下简单地讲下这篇文章。</p><p>其实这里大有文章可做。</p><h3 id="2-2-Spatial-Aware-Adaptation"><a href="#2-2-Spatial-Aware-Adaptation" class="headerlink" title="2.2 Spatial-Aware Adaptation"></a>2.2 Spatial-Aware Adaptation</h3><img src="/2019/05/03/Reality-Oriented-Adaptation/spatial_aware_adaptation.png" title="spatial-aware adaptation"><p>假设把特征图分割成了$m=1,…,M$块，每一块的区域坐标集合表示为$(u,v)\in R_m$，记点(u,v)对应的特征图为$x_{u,v}$，记区域对应的特征图为$X_m^s={x_{u,v}^s | (u,v)\in R_m}$和$X_m^t={x_{u,v}^t | (u,v)\in R_m}$，定义其loss为:</p><script type="math/tex; mode=display">L_{spt}=\sum_{m=1}^M L_{da}(X_m^s, X_m^t)</script><p>其中，$L_{da}$表示 domain adaptation loss，其实就是domain classifier loss,具体表示如下。</p><script type="math/tex; mode=display">L_H(X^s, X^t)=\frac{1}{|X|}\sum_{x\in X} l(h(x),d)</script><p>其中$h:x\to {0,1}$，采用的<a href="https://arxiv.org/pdf/1409.7495v2.pdf" target="_blank" rel="noopener">DANN</a>模型，应该是类似GAN中的D，$d\in {0,1}$.</p><p>这个domain classifer的目的是为了使生成的source和target生成的特征尽量相似。</p><h3 id="2-3-Network-Overview"><a href="#2-3-Network-Overview" class="headerlink" title="2.3 Network Overview"></a>2.3 Network Overview</h3><script type="math/tex; mode=display">L_{ROAD}=L_{seg}+\lambda_1 L_{dist}+\lambda_2 L_{spt}</script><h2 id="3-Experimental-Results"><a href="#3-Experimental-Results" class="headerlink" title="3. Experimental Results"></a>3. Experimental Results</h2><h3 id="3-1-Experimental-Results"><a href="#3-1-Experimental-Results" class="headerlink" title="3.1 Experimental Results"></a>3.1 Experimental Results</h3><ul><li>dst: target guided distillation</li><li>spt: spatial-aware adaptation</li></ul><img src="/2019/05/03/Reality-Oriented-Adaptation/results.png" title="results"><p>通过结果可以看出，这两个创新点是有用的。</p><h3 id="3-2-Analysis-on-Real-Style-Orientation"><a href="#3-2-Analysis-on-Real-Style-Orientation" class="headerlink" title="3.2 Analysis on Real Style Orientation"></a>3.2 Analysis on Real Style Orientation</h3><img src="/2019/05/03/Reality-Oriented-Adaptation/td.png" title="target domain distillation"><h3 id="3-3-Analysis-on-Real-Distribution-Orientation"><a href="#3-3-Analysis-on-Real-Distribution-Orientation" class="headerlink" title="3.3 Analysis on Real Distribution Orientation"></a>3.3 Analysis on Real Distribution Orientation</h3><img src="/2019/05/03/Reality-Oriented-Adaptation/spt.png" title="spatial-aware adaptation"><h2 id="4-Others"><a href="#4-Others" class="headerlink" title="4. Others"></a>4. Others</h2><p>这里主要简单介绍下论文中提到的几篇参考文献的主要内容，并没有细读这几篇参考文献。</p><h3 id="4-1-Domain-Adaptation"><a href="#4-1-Domain-Adaptation" class="headerlink" title="4.1 Domain Adaptation"></a>4.1 Domain Adaptation</h3><p><a href="https://arxiv.org/pdf/1409.7495v2.pdf" target="_blank" rel="noopener">Unsupervised Domain Adaptation by Backpropagation</a></p><img src="/2019/05/03/Reality-Oriented-Adaptation/domain_classifier.png" title="domain classifier"><p><a href="https://blog.csdn.net/MataFela/article/details/77827217" target="_blank" rel="noopener">Domain Adaptation</a></p><h3 id="4-2-Distillation"><a href="#4-2-Distillation" class="headerlink" title="4.2 Distillation"></a>4.2 Distillation</h3><script type="math/tex; mode=display">q_i=\frac{exp(z_i/T)}{\sum_j exp(z_j/T)}</script><script type="math/tex; mode=display">L(p,q)=\sum_i p_i * (-\log q_i)</script><p><a href="https://github.com/dkozlov/awesome-knowledge-distillation" target="_blank" rel="noopener">Awesome Knowledge Distillation</a>中有相关的论文和一部分实现。</p><p><a href="https://arxiv.org/pdf/1606.09282.pdf" target="_blank" rel="noopener">Learning without Forgetting</a></p><p><a href="https://www.jianshu.com/p/e73851f32c9f" target="_blank" rel="noopener">模型压缩总览</a></p><p><a href="https://blog.csdn.net/nature553863/article/details/80568658" target="_blank" rel="noopener">知识蒸馏</a></p><p><a href="https://blog.csdn.net/qzrdypbuqk/article/details/81482598" target="_blank" rel="noopener">Knowledge Distillation</a></p><p>终于找到一个 KD (knowledge-distillation) loss 的<a href="https://github.com/peterliht/knowledge-distillation-pytorch" target="_blank" rel="noopener">代码</a>. <strong>这个我看懂了</strong>。</p><script type="math/tex; mode=display">D_{KL}(P||Q)=\sum P(i)\frac{P(i)}{Q(i)}</script><script type="math/tex; mode=display">H(p,q)=E_p[-\log q]=H(p)+D_{KL}(P||Q)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准的KD，利用的是交叉熵求KD</span></span><br><span class="line"><span class="comment"># 但速度很慢</span></span><br><span class="line">x, target = x.to(device), target.to(device)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    out = teacher(x)</span><br><span class="line">    soft_target = F.softmax(out/T, dim=<span class="number">1</span>)</span><br><span class="line">hard_target = target</span><br><span class="line">out = student(x)  <span class="comment">## this is the input to softmax</span></span><br><span class="line">logp = F.log_softmax(out/T, dim=<span class="number">1</span>)</span><br><span class="line">loss_soft_target = -torch.mean(torch.sum(soft_target * logp, dim=<span class="number">1</span>))</span><br><span class="line">loss_hard_target = nn.CrossEntropyLoss()(out, hard_target)</span><br><span class="line">loss = loss_soft_target * T * T + alpha * loss_hard_target</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">L_{KD}=\alpha T^2 KLdiv(Q_S^T,Q_T^T)+(1-\alpha)CrossEntrop(Q_s,y_{true})</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用KL散度求KD</span></span><br><span class="line"><span class="comment"># 理论上和实际上KL散度等同于交叉熵，速度很快，结果相同</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn_kd</span><span class="params">(outputs, labels, teacher_outputs, params)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the knowledge-distillation (KD) loss given outputs, labels.</span></span><br><span class="line"><span class="string">    "Hyperparameters": temperature and alpha</span></span><br><span class="line"><span class="string">    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher</span></span><br><span class="line"><span class="string">    and student expects the input tensor to be log probabilities! See Issue #2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    alpha = params.alpha</span><br><span class="line">    T = params.temperature</span><br><span class="line">    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=<span class="number">1</span>),</span><br><span class="line">                             F.softmax(teacher_outputs/T, dim=<span class="number">1</span>)) * (alpha * T * T) + \</span><br><span class="line">              F.cross_entropy(outputs, labels) * (<span class="number">1.</span> - alpha)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> KD_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这段代码证明了DL散度和交叉熵的反向传播是一样的</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># sample number</span></span><br><span class="line">N = <span class="number">10</span></span><br><span class="line"><span class="comment"># category number</span></span><br><span class="line">C = <span class="number">5</span></span><br><span class="line"><span class="comment"># softmax output of teacher</span></span><br><span class="line">p = torch.softmax(torch.rand(N, C), dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># logit output of student</span></span><br><span class="line">s = torch.rand(N, C, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># softmax output of student, T = 1</span></span><br><span class="line">q = torch.softmax(s, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># KL Diverse</span></span><br><span class="line"><span class="comment"># this is the implementation of the author's</span></span><br><span class="line"><span class="comment"># torch will do element mean because it is the default option</span></span><br><span class="line"><span class="comment"># kl_loss = nn.KLDivLoss()(torch.log(q), p)</span></span><br><span class="line"><span class="comment"># I think this should be the right solution</span></span><br><span class="line">kl_loss = (nn.KLDivLoss(reduction=<span class="string">'none'</span>)(torch.log(q), p)).sum(dim=<span class="number">1</span>).mean()</span><br><span class="line">kl_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'grad using KL DivLoss'</span></span><br><span class="line"><span class="keyword">print</span> s.grad</span><br><span class="line"><span class="comment"># clear the grad</span></span><br><span class="line">s.grad.zero_()</span><br><span class="line"><span class="comment"># bug2: should not do element wise mean operation</span></span><br><span class="line"><span class="comment"># ce_loss = torch.mean(-torch.log(q) * p)</span></span><br><span class="line">ce_loss = torch.mean(torch.sum(-torch.log(q) * p, dim=<span class="number">1</span>))</span><br><span class="line">ce_loss.backward()</span><br><span class="line"><span class="keyword">print</span> <span class="string">'grad using ce loss'</span></span><br><span class="line"><span class="keyword">print</span> s.grad</span><br><span class="line"><span class="comment"># the real gradient of s should be `(q - p) / batch_size`</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'real grad, should be (q-p) / batch_size'</span></span><br><span class="line"><span class="keyword">print</span> (q - p) / N</span><br></pre></td></tr></table></figure><p>或者也可以参考: <a href="https://github.com/PolarisShi/distillation/blob/master/student.py" target="_blank" rel="noopener">code</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;h3 id=&quot;0-0-前言&quot;&gt;&lt;a href=&quot;#0-0-前言&quot; class=&quot;headerlink&quot; title=&quot;0.0 前言&quot;&gt;&lt;/a&gt;0.0 前言&lt;/h3&gt;&lt;p&gt;这篇文章主要解决的问题是：当语义分割网络在合成数据集(有标签)上训练好，在真实数据集(没有标签)上性能下降比较多。作者认为有两个原因：对合成数据过拟合，合成数据与真实数据存在分布差异。(好吧，我认为这两是一个原因)。作者提出target guided distillation 和 spatial-aware adaptation 来改进性能，效果还挺好的。我主要看target guided distillaiton。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/1711.11556.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2018_ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: 无&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;0-1-HydraPlus-Net&quot;&gt;&lt;a href=&quot;#0-1-HydraPlus-Net&quot; class=&quot;headerlink&quot; title=&quot;0.1 HydraPlus-Net&quot;&gt;&lt;/a&gt;0.1 HydraPlus-Net&lt;/h3&gt;&lt;p&gt;顺便记录下刚看的论文 HydraPlus-Net，因为这篇论文是caffe代码且对我的帮助不大，所以只是简单地记录下其中的创新点。&lt;/p&gt;
    
    </summary>
    
      <category term="semantice segmentation" scheme="http://yoursite.com/categories/semantice-segmentation/"/>
    
    
      <category term="semantice segmentation" scheme="http://yoursite.com/tags/semantice-segmentation/"/>
    
      <category term="domain guided distillation" scheme="http://yoursite.com/tags/domain-guided-distillation/"/>
    
  </entry>
  
  <entry>
    <title>Domain Guided Dropout</title>
    <link href="http://yoursite.com/2019/04/30/Domain-Guided-Dropout/"/>
    <id>http://yoursite.com/2019/04/30/Domain-Guided-Dropout/</id>
    <published>2019-04-30T07:38:36.000Z</published>
    <updated>2019-05-02T12:05:58.980Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0.前言"></a>0.前言</h2><p>这篇文章的主要任务是，给定多个有标签行人重识别数据集，如何训练一个模型，在各个数据集上表现都好，并且这个模型能“隐式识别出”（Domain Guided Dropout）是哪个数据集。</p><ul><li>paper: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Xiao_Learning_Deep_Feature_CVPR_2016_paper.pdf" target="_blank" rel="noopener">CVPR2016 Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification</a></li><li>code: <a href="https://github.com/Cysu/dgd_person_reid" target="_blank" rel="noopener">caffe</a></li></ul><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>对于单个数据集，模型已经能达到很好的效果，对于多个数据集，训练一个模型，怎么才能达到也是很好的效果。</p><p>思路分为两步：</p><ul><li>第一步，将多个数据集的训练集融合到一起，用单分类器训练一个模型baseline</li><li>第二步，用Domain Guided Dropout(DGD)替换标准的dropout，再训练几个epoch，相当于用DGD对模型有针对性的微调。</li></ul><p>结论：fine-tune+DGD的组合作用更大一些。</p><p>DGD有两种方案：deterministic DGD 和 stochastic DGD.</p><p>等会儿看看能不能从caffe代码中找到一些代码，毕竟看不懂caffe代码很伤啊。</p><p>实验结果证明了：1. 多个数据集混合训练得到的baseline比单数据集的模型效果要好；2. deterministic DGD 对baseline有提升，而且stochastic DGD 提升效果更好。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>现在对于多个数据集训练一个模型的方法集中在如何学习一个共同的域不变的特征表示空间，但是作者这篇论文提出的模型允许模型学习到与域相关的components和域无关的特征表示空间，我觉得这个思路要更好一些，因为测试是在各个数据集上分别进行测试的，那么如果能学到一些与域相关的components，将特征共享空间与component组合之后得到域相关空间。</p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><img src="/2019/04/30/Domain-Guided-Dropout/pipeline.png" title="overview of pipeline"><p>针对这张图，我是这么理解的，假设模型在训练时特征提取了2048维特征，那么这2048维特征可能前1500维特征是和 domain A 有关，而后1500维特征和 domain B 有关，这里的有关可以暂时理解成对后续的分类结果有比较大的影响，举个极端的例子，在 domain A 中，主要根据第100维特征分类，那么第100维特征的存在对分类结果有很大的影响，而其他维的特征对 domain A 中的数据集分类无关，那么模型只需要每次都把其他维的特征直接变成0即可，只是因为其他域的存在，使得模型不能把其他维的特征每次都变成0，从而影响了 domain A 的特征提取。</p><p>后面作者也专门做了研究特征的影响。</p><h3 id="3-1-公式定义"><a href="#3-1-公式定义" class="headerlink" title="3.1 公式定义"></a>3.1 公式定义</h3><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:center">D</td><td style="text-align:left">D个数据集</td></tr><tr><td style="text-align:center">$\lbrace (x_i^j, y_i^j)_{j=1}^{N_i}\rbrace_{i=1}^D$</td><td style="text-align:left">数据集的表示，第i个数据集第j个图片</td></tr><tr><td style="text-align:center">$g(\cdot)$</td><td style="text-align:left">generic feature extractor</td></tr></tbody></table></div><h3 id="3-2-baseline"><a href="#3-2-baseline" class="headerlink" title="3.2 baseline"></a>3.2 baseline</h3><p>baseline 值得注意的是作者没有采用多个分类器的方法，而是用一个分类器，把所有数据集混在一起，重新排id，来训练。</p><p>多个分类器$f_1, f_2, …, f_D$</p><script type="math/tex; mode=display">\sum_{i=1}^D \sum_{j=1}^{N_i} L(f_i(g(x_i^j)), y_i^j)</script><p>单分类器$f$</p><script type="math/tex; mode=display">\sum_{i=1}^D \sum_{j=1}^{N_i} L(f(g(x_i^j)), y_i^j)</script><p>按照作者的说法，这种单分类器能同时学习到domain biases and person id，后续有实验分析证明这个理论。</p><p><strong>网络架构</strong>：</p><img src="/2019/04/30/Domain-Guided-Dropout/structure.png" title="structure of CNN"><p>其中M表示行人类别。其中 fc7 包括：Linear, ReLU, Dropout, 再结合后续的DGD，不难猜测测试特征就是fc7的输出，也就是g(x)，这里的dropout灵活处理，因为DGD就是取代的dropout.</p><h3 id="3-3-Domain-Guided-Dropout"><a href="#3-3-Domain-Guided-Dropout" class="headerlink" title="3.3 Domain Guided Dropout"></a>3.3 Domain Guided Dropout</h3><p>定义：$g(x)\in R^d$，第i个元素的影响因子定义为:</p><script type="math/tex; mode=display">s_i=L(g(x)_{\i})-L(g(x))</script><p>其中$g(x)_{\i}$表示将i个元素变成0.</p><p>对于每个域，遍历所有图片得到第i个元素的影响因子:</p><script type="math/tex; mode=display">\bar{s}_i=E_{x\in D}[s_i]</script><img src="/2019/04/30/Domain-Guided-Dropout/impact_scores.png" title="impact scores"><p>这张图片可以清晰地表明对于不同的 domain，第i个特征的影响程度不同，换句话说，有些特征对 domain A 影响大，对 domain B 影响小，如果能自动学习到哪些特征影响作用大，那么就可以有针对性地训练这些特征并减少其他特征对测试的影响。作者的思路很清晰。</p><p>作者实际是用的二阶泰勒展开逼近的$s_i$:</p><script type="math/tex; mode=display">s_i \approx -\frac{\partial L}{\partial g(x)_i}g(x)_i+\frac{1}{2}\frac{\partial^2 L}{\partial g(x)_i^2}g(x)_i^2</script><p>实验证明，这种逼近的精度是可以满足需求的：</p><img src="/2019/04/30/Domain-Guided-Dropout/approximated.png" title="true and approximated"><p>定义：mask m，根据上一步得到的s得到mask m，有两种方案：</p><p>方案一：deterministic</p><script type="math/tex; mode=display">m_i = \begin{cases}1 &\text{if} s_i>0 \\0 &\text{if} s_i\eqslantless 0\end{cases}</script><p>方法二：stochastic</p><script type="math/tex; mode=display">p(m_i=1)=\frac{1}{1+e^{-s_i/T}}</script><p>然后用mask m代替fc7的dropout，再训练10个epoch.在测试的时候，特征也是需要经过mask m的。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><ul><li>JSTL: jointly with a single-task learning objective</li><li>DGD: Domain Guided Dropout</li><li>JSTL+DGD: improve JSTL with deterministic Domain Guided Dropout: work on all the domains simultaneously.</li><li>FT-JSTL+DGD: fine-tune the CNN separately on each domain with the stochastic Domain Guided Dropout</li><li>FT-JSTL: fine-tune the CNN separately on each domain with standard dropout</li></ul><img src="/2019/04/30/Domain-Guided-Dropout/different_methods.png" title="different methods"><p>从上图中可以看出，联合训练对大数据集的作用不明显，对小数据集的作用比较明显，从随机初始化开始训练的话，deterministic DGD的作用几乎没有，fine-tune的作用是有的，fine-tune和stochastic DGD 共同起的作用更大一些，也就是说fine-tune本身的作用最大，如果在fine-tune的过程中加入DGD，发挥的作用会更好一些，</p><img src="/2019/04/30/Domain-Guided-Dropout/different_Dropout.png" title="different Dropout"><p>其中a图表示对所有的域同时训练，b图表示分别对域微调得到不同的模型。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0.前言&quot;&gt;&lt;/a&gt;0.前言&lt;/h2&gt;&lt;p&gt;这篇文章的主要任务是，给定多个有标签行人重识别数据集，如何训练一个模型，在各个数据集上表现都好，并且这个模型能“隐式识别出”（Domain Guided Dropout）是哪个数据集。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Xiao_Learning_Deep_Feature_CVPR_2016_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2016 Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/Cysu/dgd_person_reid&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;caffe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="person re-id" scheme="http://yoursite.com/categories/person-re-id/"/>
    
    
      <category term="person re-id" scheme="http://yoursite.com/tags/person-re-id/"/>
    
      <category term="multi-domain" scheme="http://yoursite.com/tags/multi-domain/"/>
    
  </entry>
  
  <entry>
    <title>One_Example_reID</title>
    <link href="http://yoursite.com/2019/03/27/One-Example-reID/"/>
    <id>http://yoursite.com/2019/03/27/One-Example-reID/</id>
    <published>2019-03-27T07:25:37.000Z</published>
    <updated>2019-03-29T10:33:27.247Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这篇文章主要的任务也是为了解决re-ID中需要全部标签的问题，核心思想只对一个摄像头下的全部行人各取一张图片（对于没有出现的行人，从其他摄像头下取一张），然后通过训练模型，聚类给假标签，逐步识别出所有图片。</p><ul><li>paper: <a href="https://yu-wu.net/pdf/TIP2019_One-Example-reID.pdf" target="_blank" rel="noopener">TIP 2019 Progressive Learning for Person Re-Identification with One Example</a></li><li>author: Yu Wu, Yutian Lin, Yi Yang (University of Technology Sydney (UTS))</li><li>code: <a href="https://github.com/Yu-Wu/One-Example-Person-ReID" target="_blank" rel="noopener">https://github.com/Yu-Wu/One-Example-Person-ReID</a></li><li>project: <a href="https://zhuanlan.zhihu.com/p/54576174" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/54576174</a></li></ul><a id="more"></a><p>关于这篇文章的作者也多提几句。</p><p>关于行人重识别(person re-ID)，我看到的多是<strong>悉尼科技大学的郑良团队</strong>，所以简单介绍一下郑良团队的成员。</p><ul><li>郑良：<a href="http://www.liangzheng.com.cn/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/liangzheng06" target="_blank" rel="noopener">github</a>，2015年清华博士毕业，有学生郑哲东，钟准，孙奕帆。</li><li>郑哲东：<a href="http://www.zdzheng.xyz/" target="_blank" rel="noopener">主页</a>, <a href="https://github.com/layumi?tab=repositories" target="_blank" rel="noopener">github</a>，悉尼科技大学博三。</li><li>钟准：<a href="http://zhunzhong.site/" target="_blank" rel="noopener">主页</a>, <a href="https://github.com/zhunzhong07" target="_blank" rel="noopener">github</a>（鸣人），厦门大学交换生博四。</li><li>孙奕帆：<a href="https://syfafterzy.github.io/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/syfafterzy" target="_blank" rel="noopener">github</a>，清华博二。</li><li>林雨恬：<a href="https://vana77.github.io/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/vana77" target="_blank" rel="noopener">github</a>，很漂亮也很厉害的一个小姐姐.</li><li>Wu Yu：<a href="https://yu-wu.net/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/Yu-Wu" target="_blank" rel="noopener">github</a>，<a href="https://www.zhihu.com/people/wu2008yu/activities" target="_blank" rel="noopener">知乎</a>，悉尼科技大学博一。</li></ul><p>人家牛逼，虚心学习。</p><p>[-] 2019-03-29 </p><p>看到一个大神廖星宇的github,中国科学技术大学,应用数学研究生,旷世科技工程师,他对re-id的研究的浓缩,我觉得就已经可以秒杀90%的论文了.</p><ul><li>廖星宇:<a href="https://l1aoxingyu.github.io/" target="_blank" rel="noopener">主页</a>, <a href="https://github.com/L1aoXingyu/reid_baseline" target="_blank" rel="noopener">github</a></li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>person re-ID 有监督模型，半监督模型。作者采用的是 one-shot image-based setting，也就是每个行人只有一个样例。</p><p>具体来说，作者将数据集分为三部分：labeled data, selected pseudo-labeled data and index-labeled data，其中 labeled data 和 selected pseudo-labeled data 用分类损失，index-labeled data 用 exclusive loss。exclusive loss 的目标是尽可能地使图片之间都离得比较远。</p><img src="/2019/03/27/One-Example-reID/join_training_procedure.png" title="join training procedure"><p>作者主要的<strong>改进点</strong>在于提出了 <strong>exclusive loss</strong> 和 label estimation</p><p>作者的另一篇相关文章是在视频上做的</p><ul><li>paper: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf" target="_blank" rel="noopener">CVPR 2018 Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning</a></li><li>知乎: <a href="https://www.leiphone.com/news/201806/o8a3H5um1H2zXrof.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201806/o8a3H5um1H2zXrof.html</a></li><li>code: <a href="https://github.com/Yu-Wu/Exploit-Unknown-Gradually" target="_blank" rel="noopener">https://github.com/Yu-Wu/Exploit-Unknown-Gradually</a></li></ul><h1 id="2-The-progressive-model"><a href="#2-The-progressive-model" class="headerlink" title="2. The progressive model"></a>2. The progressive model</h1><h2 id="2-1-Framework-overview"><a href="#2-1-Framework-overview" class="headerlink" title="2.1 Framework overview"></a>2.1 Framework overview</h2><p>训练模型分为两步：在三个数据集上训练CNN模型—&gt;在 unlabeled data 上选择一些数据放到 pseudo-labeled data 上。</p><h2 id="2-2-Preliminaries"><a href="#2-2-Preliminaries" class="headerlink" title="2.2 Preliminaries"></a>2.2 Preliminaries</h2><ul><li>labeled data set $\mathcal{L}$</li><li>unlabeled data set $\mathcal{U}$</li><li><ul><li>pseudo-labeled data set $\mathcal{S}^t$</li></ul></li><li><ul><li>index-labeled data set $\mathcal{M}^t$</li></ul></li></ul><h2 id="2-3-The-joint-learning-method"><a href="#2-3-The-joint-learning-method" class="headerlink" title="2.3 The joint learning method"></a>2.3 The joint learning method</h2><p><strong>The Exclusive Loss</strong>: index-labeled data set $\mathcal{M}^t$,</p><script type="math/tex; mode=display">\max \limits_{\theta}\parallel \phi(\theta;x_i) - \phi(\theta;x_j)\parallel</script><p>令$||v_i||=\tilde{\phi}(\theta;x_i)$是$x_i$的归一化后的特征，因此可以将最大化欧氏距离近似为最小化cos距离。</p><ul><li><strong>这个公式没有推导出来，还在询问作者的答案</strong>.</li><li>在<a href="https://arxiv.org/pdf/1604.01850.pdf" target="_blank" rel="noopener">Joint Detection and Identification Feature Learning for Person Search</a>看到了类似的公式，并且有了新理解，在下面补充。</li></ul><script type="math/tex; mode=display">l(V)=-log \frac{exp(v_i^T\tilde{\phi}(x_i)/\tau)}{\sum_{j=1}^{|M^t|}exp(v_j^T\tilde{\phi}(x_i)/\tau)}</script><p>其中，V是所有数据的归一化特征，更高的$\tau$导致softer probability distribution.在更新时，先计算当前数据与所有数据的cos距离，在反向传播时，$v_i=1\2*(v_i+\tilde{\phi}(x_i)$并且归一化。</p><p><strong>The cross-entropy loss</strong>:labeled data set $\mathcal{L}$ and pseudo-labeled data set $\mathcal{S}^t$</p><p>不再陈述</p><p><strong>The total loss</strong>:</p><script type="math/tex; mode=display">\min \lambda l(\mathcal{L}) + \lambda l(\mathcal{S}^t) + (1-\lambda) l(V)(\mathcal{M}^t)</script><h2 id="2-4-The-effective-sampling-criterion"><a href="#2-4-The-effective-sampling-criterion" class="headerlink" title="2.4 The effective sampling criterion"></a>2.4 The effective sampling criterion</h2><p>没有采用分类损失，而是利用最近邻赋予假标签并且假标签的真实性为与最近邻真值的距离，每次取$m_t = m_{t-1}+p\cdot n_u$</p><p>验证集是另外一个re-ID的训练集。</p><p>整体算法如下：</p><img src="/2019/03/27/One-Example-reID/Algorithm.png" title="algorithm"><h2 id="2-5-补充公式"><a href="#2-5-补充公式" class="headerlink" title="2.5 补充公式"></a>2.5 补充公式</h2><p><a href="https://arxiv.org/pdf/1604.01850.pdf" target="_blank" rel="noopener">Joint Detection and Identification Feature Learning for Person Search</a></p><p>定义：n个数据，每个数据表示成向量，即L-2归一化向量$V={v_i}_{i=1}^{n}$，则新向量$x$属于$v_i$类的概率定义为:</p><script type="math/tex; mode=display">p_i=\frac{exp(v_i^Tx/\tau)}{\sum_{j=1}^{n}exp(v_i^Tx/\tau)}</script><p>其中，$\tau$更高，概率分布更平缓，设为0.1.</p><p>这里有两种含义。</p><p>第一种，$v_i$有标签，但$V$的类别多，属于同一类的数据较少，$x$不属于$V$，$x$有标签t，那么n个数据的概率最大值为x的标签，目标应该是最大化x属于t类的概率，其目的是为了分类更加准确。</p><p>第二种，$v_i$没有标签，$x=v_k$属于$V$，目标是最大化x属于第k个元素的概率，最大也就是1，此时$x=v_k$与其他向量$v_j$都正交，其目的是为了令各个向量都离得比较远，也就是作者的目的。作者这里没有用正交来做损失函数，而是用了softmax，很厉害。</p><h1 id="3-code"><a href="#3-code" class="headerlink" title="3. code"></a>3. code</h1><h2 id="3-1-数据集加载"><a href="#3-1-数据集加载" class="headerlink" title="3.1 数据集加载"></a>3.1 数据集加载</h2><p>两个文件 ./reid/datasets/duke.py ./reid/utils/data/dataset.py</p><p>./reid/datasets/duke.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Duke</span><span class="params">(Dataset)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 预处理数据集,使其变成统一形式,从而使用load进行加载</span></span><br></pre></td></tr></table></figure><p>./reid/utils/data/dataset.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self, verbose=True)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_check_integrity</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 检查文件的完整性 images  meta.json splits.json</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这篇文章主要的任务也是为了解决re-ID中需要全部标签的问题，核心思想只对一个摄像头下的全部行人各取一张图片（对于没有出现的行人，从其他摄像头下取一张），然后通过训练模型，聚类给假标签，逐步识别出所有图片。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://yu-wu.net/pdf/TIP2019_One-Example-reID.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TIP 2019 Progressive Learning for Person Re-Identification with One Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;author: Yu Wu, Yutian Lin, Yi Yang (University of Technology Sydney (UTS))&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/Yu-Wu/One-Example-Person-ReID&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Yu-Wu/One-Example-Person-ReID&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;project: &lt;a href=&quot;https://zhuanlan.zhihu.com/p/54576174&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/54576174&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="re-ID" scheme="http://yoursite.com/categories/re-ID/"/>
    
    
      <category term="one_example" scheme="http://yoursite.com/tags/one-example/"/>
    
      <category term="re-ID" scheme="http://yoursite.com/tags/re-ID/"/>
    
  </entry>
  
  <entry>
    <title>GANimation</title>
    <link href="http://yoursite.com/2019/01/24/GANimation/"/>
    <id>http://yoursite.com/2019/01/24/GANimation/</id>
    <published>2019-01-24T07:53:06.000Z</published>
    <updated>2019-03-28T08:15:53.709Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这篇文章是根据GANnotation的一个公式查过来的，感觉还挺厉害。</p><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><ul><li>paper: <a href="https://www.albertpumarola.com/publications/files/pumarola2018ganimation.pdf" target="_blank" rel="noopener">GANimation Anatomically-aware Facial Animation from a Single Image</a></li><li>github: <a href="https://github.com/albertpumarola/GANimation" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation</a></li><li>project: <a href="https://www.albertpumarola.com/research/GANimation/index.html" target="_blank" rel="noopener">https://www.albertpumarola.com/research/GANimation/index.html</a></li></ul><p>关键词：starGAN的改进、连续的表情变换、贴回去能够一致</p><a id="more"></a><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>在人脸转换中，StarGAN是最成功的GAN，但是只能生成离散的人脸。作者要做的就是生成连续的表情变化。</p><h1 id="2-Problem-Formulation"><a href="#2-Problem-Formulation" class="headerlink" title="2. Problem Formulation"></a>2. Problem Formulation</h1><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">$\mathrm{I}_{y_r}\in \mathbb{R}^{H×W×3}$</td><td style="text-align:center">输入图片</td></tr><tr><td style="text-align:center">$\mathrm{y}_r=(y_1,…,y_N)^T$</td><td style="text-align:center">其中，每一个$y_i$表示第i个action unit的程度，在0~1之间</td></tr><tr><td style="text-align:center">$\mathrm{I}_{y_g}$</td><td style="text-align:center">输出图片</td></tr><tr><td style="text-align:center">$\mathcal{M}$</td><td style="text-align:center">映射函数M: $(\mathrm{I}_{y_r},\mathrm{y}_g)$—&gt;$\mathrm{I}_{y_g}$</td></tr></tbody></table></div><p>非成对图片</p><h1 id="3-Our-Approach"><a href="#3-Our-Approach" class="headerlink" title="3. Our Approach"></a>3. Our Approach</h1><p><img src="./GANimation/GANimation1.png" alt="网络结构"><br><img src="/2019/01/24/GANimation/GANimation1.png" title="网络结构"></p><h2 id="3-1-Network-Architechture"><a href="#3-1-Network-Architechture" class="headerlink" title="3.1 Network Architechture"></a>3.1 Network Architechture</h2><h3 id="3-1-1-Generator"><a href="#3-1-1-Generator" class="headerlink" title="3.1.1 Generator"></a>3.1.1 Generator</h3><p>对于G的改进，为了能够使G只聚焦于对于新表情的生成，而保留其他元素，引入attention机制，也就是G生成的不是一整张图片，而是两个mask，color mask C 和 attention mask A.即：</p><script type="math/tex; mode=display">\mathrm{I}_{\mathrm{y}_f}=(1-A)\cdot C + A\cdot \mathrm{I}_{\mathrm{y}_o}</script><p>其中，$\mathrm{A}=G_A(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f)\in {0,…,1}^{H×W}$，$\mathrm{C}=G_C(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f)\in {0,…,1}^{H×W×3}$</p><p><img src="./GANimation/GANimation2.png" alt="生成器结构"><br><img src="/2019/01/24/GANimation/GANimation2.png" title="生成器结构"></p><h3 id="3-1-2-Conditional-Critic"><a href="#3-1-2-Conditional-Critic" class="headerlink" title="3.1.2 Conditional Critic"></a>3.1.2 Conditional Critic</h3><p>PatchGAN: 输入图像 $\mathrm{I}\dashrightarrow \mathrm{Y}_{\mathrm{I}}\in \mathbb{R}^{H/2^6×W/2^6}$</p><p>并且对判别器进行改进，加入额外的回归判别类别。</p><h2 id="3-2-Learning-the-model"><a href="#3-2-Learning-the-model" class="headerlink" title="3.2 Learning the model"></a>3.2 Learning the model</h2><p>损失函数</p><h3 id="3-2-1-Image-Adversarial-Loss"><a href="#3-2-1-Image-Adversarial-Loss" class="headerlink" title="3.2.1 Image Adversarial Loss"></a>3.2.1 Image Adversarial Loss</h3><p>判断图片是生成的还是真实的。</p><p>和StarGAN的损失一样。</p><script type="math/tex; mode=display">L_I(G,D_I,I_{y_o},y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[D_\mathrm{I}(G(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f))]-\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[D_\mathrm{I}(\mathrm{I}_{\mathrm{y}_o})]+\lambda_{gp} \mathbb{E}_{\tilde{I}\thicksim \mathbb{P}_{\tilde{I}}}[(\parallel \nabla_{\tilde{I}}D_I(\tilde{I}) \parallel _2-1)^2]</script><h3 id="3-2-2-Attention-Loss"><a href="#3-2-2-Attention-Loss" class="headerlink" title="3.2.2 Attention Loss"></a>3.2.2 Attention Loss</h3><p>这个损失是针对attention mask A 和 color mask C.</p><p>Total Variation Regularization</p><script type="math/tex; mode=display">L_A(G,I_{y_o},y_f)=\lambda_{TV}\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\sum_{i,j}^{H,W}[(A_{i+1,j}-A_{i,j})^2+(A_{i,j+1}-A_{i,j})^2]]+\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel A \parallel_2]</script><p>这个公式的初步感觉是A要尽可能平缓，并且A中的元素尽可能小。</p><p>根据作者的说法，是为了保证A不变成全是1的矩阵，并且为了保证更加平滑的空间结合。以代码为准。</p><h3 id="3-2-3-Conditional-Expression-Loss"><a href="#3-2-3-Conditional-Expression-Loss" class="headerlink" title="3.2.3 Conditional Expression Loss"></a>3.2.3 Conditional Expression Loss</h3><p>这个应该和starGAN的判断图片属性分类正确损失是一样的。</p><script type="math/tex; mode=display">L_y(G,D_y,I_{y_o},y_o,y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel D_y(G(I_{y_o}|y_f))-y_f \parallel]+\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel D_y(I_{y_o})-y_o \parallel]</script><h3 id="3-2-4-Identity-Loss"><a href="#3-2-4-Identity-Loss" class="headerlink" title="3.2.4 Identity Loss"></a>3.2.4 Identity Loss</h3><p>这个应该就是starGAN的重构损失</p><script type="math/tex; mode=display">L_{idg}(G,I_{y_o},y_o,y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel G(G(I_{y_o}|y_f)|y_o)-I_{y_o} \parallel _1]</script><p>这个损失是为了保证生成前后图片的id是一样的。</p><h3 id="3-2-5-Full-Loss"><a href="#3-2-5-Full-Loss" class="headerlink" title="3.2.5 Full Loss"></a>3.2.5 Full Loss</h3><script type="math/tex; mode=display">L=L_I+\lambda_y L_y+\lambda_A (L_A(G,I_{y_g},y_r)+L_A(G,I_{y_r},y_g))+\lambda_{idt}L_{idt}</script><script type="math/tex; mode=display">\lambda_{gp}=10, \lambda_A=0.1, \lambda_{TV}=0.0001, \lambda_y=4000, \lambda_{idt}=10</script><h1 id="4-Implementation-Details"><a href="#4-Implementation-Details" class="headerlink" title="4. Implementation Details"></a>4. Implementation Details</h1><p><strong>The attention mechanism guaranties a smooth transition between the morphed cropped face and the original image.</strong></p><p>也就是说 attention mechanism 能够保证生成的图片很好地再贴回去。</p><h2 id="4-1-Single-Action-Units-Edition"><a href="#4-1-Single-Action-Units-Edition" class="headerlink" title="4.1 Single Action Units Edition"></a>4.1 Single Action Units Edition</h2><p><img src="./GANimation/GANimation3.png" alt="Single Action Units Edition"><br><img src="/2019/01/24/GANimation/GANimation3.png" title="Single Action Units Edition"></p><ul><li>[x] 这里的AU是什么？ intensity怎么理解？</li></ul><p>AU:<a href="https://www.cs.cmu.edu/~face/facs.htm" target="_blank" rel="noopener">https://www.cs.cmu.edu/~face/facs.htm</a></p><p>intensity: <a href="https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units" target="_blank" rel="noopener">https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units</a></p><p><img src="./GANimation/GANimation4.png" alt="Attention Model"><br><img src="/2019/01/24/GANimation/GANimation4.png" title="Attention Model"></p><h2 id="4-2-Simultaneous-Edition-of-Multiple-AUs"><a href="#4-2-Simultaneous-Edition-of-Multiple-AUs" class="headerlink" title="4.2 Simultaneous Edition of Multiple AUs"></a>4.2 Simultaneous Edition of Multiple AUs</h2><p><img src="./GANimation/GANimation5.png" alt="Facial animation from a single image"><br><img src="/2019/01/24/GANimation/GANimation5.png" title="Facial animation from a single image"></p><script type="math/tex; mode=display">\alpha y_g+(1-\alpha)y_r</script><h2 id="4-3-Discrete-Emotions-Editing"><a href="#4-3-Discrete-Emotions-Editing" class="headerlink" title="4.3 Discrete Emotions Editing"></a>4.3 Discrete Emotions Editing</h2><p><img src="./GANimation/GANimation6.png" alt="Qualitative comparison"><br><img src="/2019/01/24/GANimation/GANimation6.png" title="Qualitative comparison"></p><p>作者生成的图片比StarGAN更清晰。</p><h2 id="4-4-High-Expressions-Variability"><a href="#4-4-High-Expressions-Variability" class="headerlink" title="4.4 High Expressions Variability"></a>4.4 High Expressions Variability</h2><h2 id="4-5-Images-in-the-Wild"><a href="#4-5-Images-in-the-Wild" class="headerlink" title="4.5 Images in the Wild"></a>4.5 Images in the Wild</h2><p><img src="./GANimation/GANimation7.png" alt="Qualitative evaluation on images in the wild"><br><img src="/2019/01/24/GANimation/GANimation7.png" title="Qualitative evaluation on images in the wild"></p><p>作者先检测到人脸，然后扣下来，做训练测试，然后再贴回去，与原图保持了一样的清晰度，个人猜测是因为表情的变化只在人脸的中央就可以完成，不涉及到背景的变换，如果涉及到背景的变换，那么是否还能保证贴回去与原图保持一致性。</p><h2 id="4-6-Pushing-the-Limits-of-the-Model"><a href="#4-6-Pushing-the-Limits-of-the-Model" class="headerlink" title="4.6 Pushing the Limits of the Model"></a>4.6 Pushing the Limits of the Model</h2><p><img src="./GANimation/GANimation8.png" alt="Success and Failure Cases"><br><img src="/2019/01/24/GANimation/GANimation8.png" title="Success and Failure Cases."></p><h1 id="5-code"><a href="#5-code" class="headerlink" title="5. code"></a>5. code</h1><h2 id="5-1-生成器Generator"><a href="#5-1-生成器Generator" class="headerlink" title="5.1 生成器Generator"></a>5.1 生成器Generator</h2><p>GANimation的Generator的主体网络和starGAN的Generator的主体网络一致，只是多加了一个conv</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(NetworkBase)</span>:</span></span><br><span class="line">    <span class="string">"""Generator. Encoder-Decoder Architecture."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conv_dim=<span class="number">64</span>, c_dim=<span class="number">5</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self._name = <span class="string">'generator_wgan'</span></span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">3</span>+c_dim, conv_dim, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.InstanceNorm2d(conv_dim, affine=<span class="keyword">True</span>))</span><br><span class="line">        layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Down-Sampling</span></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            layers.append(nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">            layers.append(nn.InstanceNorm2d(curr_dim*<span class="number">2</span>, affine=<span class="keyword">True</span>))</span><br><span class="line">            layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Bottleneck</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(repeat_num):</span><br><span class="line">            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Up-Sampling</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">            layers.append(nn.InstanceNorm2d(curr_dim//<span class="number">2</span>, affine=<span class="keyword">True</span>))</span><br><span class="line">            layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.main = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(curr_dim, <span class="number">3</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.Tanh())</span><br><span class="line">        self.img_reg = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.Sigmoid())</span><br><span class="line">        self.attetion_reg = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, c)</span>:</span></span><br><span class="line">        <span class="comment"># replicate spatially and concatenate domain information</span></span><br><span class="line">        c = c.unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">        c = c.expand(c.size(<span class="number">0</span>), c.size(<span class="number">1</span>), x.size(<span class="number">2</span>), x.size(<span class="number">3</span>))</span><br><span class="line">        x = torch.cat([x, c], dim=<span class="number">1</span>)</span><br><span class="line">        features = self.main(x)</span><br><span class="line">        <span class="keyword">return</span> self.img_reg(features), self.attetion_reg(features)</span><br></pre></td></tr></table></figure><h2 id="5-2-Discriminator"><a href="#5-2-Discriminator" class="headerlink" title="5.2 Discriminator"></a>5.2 Discriminator</h2><p>Discriminator和StarGAN 的Discriminator完全一样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(NetworkBase)</span>:</span></span><br><span class="line">    <span class="string">"""Discriminator. PatchGAN."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_size=<span class="number">128</span>, conv_dim=<span class="number">64</span>, c_dim=<span class="number">5</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self._name = <span class="string">'discriminator_wgan'</span></span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">3</span>, conv_dim, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.LeakyReLU(<span class="number">0.01</span>, inplace=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, repeat_num):</span><br><span class="line">            layers.append(nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">            layers.append(nn.LeakyReLU(<span class="number">0.01</span>, inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        k_size = int(image_size / np.power(<span class="number">2</span>, repeat_num))</span><br><span class="line">        self.main = nn.Sequential(*layers)</span><br><span class="line">        self.conv1 = nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=k_size, bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        h = self.main(x)</span><br><span class="line">        out_real = self.conv1(h)</span><br><span class="line">        out_aux = self.conv2(h)</span><br><span class="line">        <span class="keyword">return</span> out_real.squeeze(), out_aux.squeeze()</span><br></pre></td></tr></table></figure><h2 id="5-3-train-D"><a href="#5-3-train-D" class="headerlink" title="5.3 train D"></a>5.3 train D</h2><p>这里训练D的过程和starGAN有所不同，并且超参数也有所不同。</p><script type="math/tex; mode=display">\lambda_{D-cond}=4000, \lambda_{gp}=10</script><p>starGAN:</p><script type="math/tex; mode=display">\lambda_{cls}=1, \lambda_{gp}=10</script><ul><li>[ ] 为什么和怎么使用的MSELoss</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># starGAN是把Image Adversarial Loss的三项一起反向传播，但GANimation是分开反向传播的，不确定这么做是否有影响。</span></span><br><span class="line">loss_D, fake_imgs_masked = self._forward_D()</span><br><span class="line">self._optimizer_D.zero_grad()</span><br><span class="line">loss_D.backward()</span><br><span class="line">self._optimizer_D.step()</span><br><span class="line"></span><br><span class="line">loss_D_gp= self._gradinet_penalty_D(fake_imgs_masked)</span><br><span class="line">self._optimizer_D.zero_grad()</span><br><span class="line">loss_D_gp.backward()</span><br><span class="line">self._optimizer_D.step()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_forward_D</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># generate fake images</span></span><br><span class="line">    fake_imgs, fake_img_mask = self._G.forward(self._real_img, self._desired_cond)</span><br><span class="line">    fake_img_mask = self._do_if_necessary_saturate_mask(fake_img_mask, saturate=self._opt.do_saturate_mask)</span><br><span class="line">    fake_imgs_masked = fake_img_mask * self._real_img + (<span class="number">1</span> - fake_img_mask) * fake_imgs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># D(real_I)</span></span><br><span class="line">    <span class="comment"># 识别真图片为真，(Image Adversarial Loss)</span></span><br><span class="line">    <span class="comment"># 图片类别分类准确，这里的分类用的不是交叉熵，而是MSELoss，(Conditional Expression Loss)</span></span><br><span class="line">    <span class="comment"># 刚刚发现一个问题，如果是分类损失，MSELoss的输入必须是同样大小的，按照starGAN，D的输出是类别大小(batch*classification)，G的输入是(batch*1)，那这个样子肯定是没法进行MSELoss的，所以还需要看了数据的处理之后才能明白怎么回事。</span></span><br><span class="line">    <span class="comment"># self._criterion_D_cond = torch.nn.MSELoss().cuda()</span></span><br><span class="line">    d_real_img_prob, d_real_img_cond = self._D.forward(self._real_img)</span><br><span class="line">    self._loss_d_real = self._compute_loss_D(d_real_img_prob, <span class="keyword">True</span>) * self._opt.lambda_D_prob</span><br><span class="line">    self._loss_d_cond = self._criterion_D_cond(d_real_img_cond, self._real_cond) / self._B * self._opt.lambda_D_cond</span><br><span class="line"></span><br><span class="line">    <span class="comment"># D(fake_I)</span></span><br><span class="line">    <span class="comment"># 识别假图片为假，(Image Adversarial Loss)</span></span><br><span class="line">    d_fake_desired_img_prob, _ = self._D.forward(fake_imgs_masked.detach())</span><br><span class="line">    self._loss_d_fake = self._compute_loss_D(d_fake_desired_img_prob, <span class="keyword">False</span>) * self._opt.lambda_D_prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># combine losses</span></span><br><span class="line">    <span class="keyword">return</span> self._loss_d_real + self._loss_d_cond + self._loss_d_fake, fake_imgs_masked</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_loss_D</span><span class="params">(self, estim, is_real)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -torch.mean(estim) <span class="keyword">if</span> is_real <span class="keyword">else</span> torch.mean(estim)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gradinet_penalty_D</span><span class="params">(self, fake_imgs_masked)</span>:</span></span><br><span class="line">    <span class="comment"># (Image Adversarial Loss)的第三项</span></span><br><span class="line">    <span class="comment"># interpolate sample</span></span><br><span class="line">    alpha = torch.rand(self._B, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).cuda().expand_as(self._real_img)</span><br><span class="line">    interpolated = Variable(alpha * self._real_img.data + (<span class="number">1</span> - alpha) * fake_imgs_masked.data, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">    interpolated_prob, _ = self._D(interpolated)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute gradients</span></span><br><span class="line">    grad = torch.autograd.grad(outputs=interpolated_prob,</span><br><span class="line">                                inputs=interpolated,</span><br><span class="line">                                grad_outputs=torch.ones(interpolated_prob.size()).cuda(),</span><br><span class="line">                                retain_graph=<span class="keyword">True</span>,</span><br><span class="line">                                create_graph=<span class="keyword">True</span>,</span><br><span class="line">                                only_inputs=<span class="keyword">True</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># penalize gradients</span></span><br><span class="line">    grad = grad.view(grad.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    grad_l2norm = torch.sqrt(torch.sum(grad ** <span class="number">2</span>, dim=<span class="number">1</span>))</span><br><span class="line">    self._loss_d_gp = torch.mean((grad_l2norm - <span class="number">1</span>) ** <span class="number">2</span>) * self._opt.lambda_D_gp</span><br></pre></td></tr></table></figure><h2 id="5-4-train-G"><a href="#5-4-train-G" class="headerlink" title="5.4 train G"></a>5.4 train G</h2><p>这一部分和starGAN的训练类似，比starGAN多一个mask的平滑loss。</p><script type="math/tex; mode=display">\lambda_{D-cond}=4000, \lambda_{cyc}=10, \lambda_{mask}=0.1, \lambda_{mask-smooth}=1*e^{-5}</script><p>starGAN:</p><script type="math/tex; mode=display">\lambda_{cls}=1, \lambda_{cyc}=10</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_forward_G</span><span class="params">(self, keep_data_for_visuals)</span>:</span></span><br><span class="line">    <span class="comment"># generate fake images</span></span><br><span class="line">    fake_imgs, fake_img_mask = self._G.forward(self._real_img, self._desired_cond)</span><br><span class="line">    fake_img_mask = self._do_if_necessary_saturate_mask(fake_img_mask, saturate=self._opt.do_saturate_mask)</span><br><span class="line">    fake_imgs_masked = fake_img_mask * self._real_img + (<span class="number">1</span> - fake_img_mask) * fake_imgs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># D(G(Ic1, c2)*M) masked</span></span><br><span class="line">    <span class="comment"># 生成图片为真 (Image Adversarial Loss)</span></span><br><span class="line">    <span class="comment"># 生成图片的属性为真 (Conditional Expression Loss)</span></span><br><span class="line">    d_fake_desired_img_masked_prob, d_fake_desired_img_masked_cond = self._D.forward(fake_imgs_masked)</span><br><span class="line">    self._loss_g_masked_fake = self._compute_loss_D(d_fake_desired_img_masked_prob, <span class="keyword">True</span>) * self._opt.lambda_D_prob</span><br><span class="line">    self._loss_g_masked_cond = self._criterion_D_cond(d_fake_desired_img_masked_cond, self._desired_cond) / self._B * self._opt.lambda_D_cond</span><br><span class="line"></span><br><span class="line">    <span class="comment"># G(G(Ic1,c2), c1)</span></span><br><span class="line">    <span class="comment"># 重构损失 (Identity Loss)</span></span><br><span class="line">    rec_real_img_rgb, rec_real_img_mask = self._G.forward(fake_imgs_masked, self._real_cond)</span><br><span class="line">    rec_real_img_mask = self._do_if_necessary_saturate_mask(rec_real_img_mask, saturate=self._opt.do_saturate_mask)</span><br><span class="line">    rec_real_imgs = rec_real_img_mask * fake_imgs_masked + (<span class="number">1</span> - rec_real_img_mask) * rec_real_img_rgb</span><br><span class="line"></span><br><span class="line">    <span class="comment"># l_cyc(G(G(Ic1,c2), c1)*M)</span></span><br><span class="line">    self._loss_g_cyc = self._criterion_cycle(rec_real_imgs, self._real_img) * self._opt.lambda_cyc</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss mask</span></span><br><span class="line">    <span class="comment"># (Attention Loss) 不仅对生成的mask进行了平滑，也对重构生成的mask进行了平滑损失</span></span><br><span class="line">    self._loss_g_mask_1 = torch.mean(fake_img_mask) * self._opt.lambda_mask</span><br><span class="line">    self._loss_g_mask_2 = torch.mean(rec_real_img_mask) * self._opt.lambda_mask</span><br><span class="line">    self._loss_g_mask_1_smooth = self._compute_loss_smooth(fake_img_mask) * self._opt.lambda_mask_smooth</span><br><span class="line">    self._loss_g_mask_2_smooth = self._compute_loss_smooth(rec_real_img_mask) * self._opt.lambda_mask_smooth</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_loss_smooth</span><span class="params">(self, mat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.sum(torch.abs(mat[:, :, :, :<span class="number">-1</span>] - mat[:, :, :, <span class="number">1</span>:])) + \</span><br><span class="line">            torch.sum(torch.abs(mat[:, :, :<span class="number">-1</span>, :] - mat[:, :, <span class="number">1</span>:, :]))</span><br></pre></td></tr></table></figure><h2 id="5-5-保存图片"><a href="#5-5-保存图片" class="headerlink" title="5.5 保存图片"></a>5.5 保存图片</h2><p>这个保存图片在starGAN就没有太理解，在这里又看到了类似的，才理解这是对输入图片归一化的反向操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># starGAN</span></span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">denorm</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="string">"""Convert the range from [-1, 1] to [0, 1]."""</span></span><br><span class="line">    out = (x + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> out.clamp_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GANimation</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">mean = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line">std = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line"><span class="keyword">for</span> i, m, s <span class="keyword">in</span> zip(img, mean, std):</span><br><span class="line">  i.mul_(s).add_(m)</span><br><span class="line">image_numpy = img.numpy()</span><br><span class="line">image_numpy_t = np.transpose(image_numpy, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">image_numpy_t = image_numpy_t*<span class="number">254.0</span></span><br><span class="line">image_numpy_t.astype(np.uint8)</span><br></pre></td></tr></table></figure><h2 id="5-6-其他"><a href="#5-6-其他" class="headerlink" title="5.6 其他"></a>5.6 其他</h2><p>没有实际跑这个代码，所以对于一些细节不是很清晰，尤其是数据处理那里，暂时根据查到的AU资料理解成17个AU(但1, 2, 4, 5, 6, 7, 9, 10, 12, 14, 15, 17, 20, 23, 25, 26, 28, and 45是18个AU)，每个AU是一个0~5的数字。</p><p>但是对于作者所说的能够生成连续的表情变换，这一点只能在测试代码中看出，但是在训练的时候并没有特意去表示连续的变化，暂时对于连续的变化存疑。</p><p>主要是openface这个库有点晕，等数据集下载之后试试。</p><p><a href="https://github.com/albertpumarola/GANimation/issues/45" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/45</a><br><a href="https://github.com/albertpumarola/GANimation/issues/62" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/62</a><br><a href="https://github.com/albertpumarola/GANimation/issues/43" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/43</a><br><a href="https://github.com/albertpumarola/GANimation/issues/32" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/32</a><br><a href="https://github.com/albertpumarola/GANimation/issues/25" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/25</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这篇文章是根据GANnotation的一个公式查过来的，感觉还挺厉害。&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{I}=(1-M) \circ C + M \circ I&lt;/script&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://www.albertpumarola.com/publications/files/pumarola2018ganimation.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GANimation Anatomically-aware Facial Animation from a Single Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;github: &lt;a href=&quot;https://github.com/albertpumarola/GANimation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/albertpumarola/GANimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;project: &lt;a href=&quot;https://www.albertpumarola.com/research/GANimation/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.albertpumarola.com/research/GANimation/index.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关键词：starGAN的改进、连续的表情变换、贴回去能够一致&lt;/p&gt;
    
    </summary>
    
      <category term="GAN" scheme="http://yoursite.com/categories/GAN/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="mask and colour" scheme="http://yoursite.com/tags/mask-and-colour/"/>
    
  </entry>
  
  <entry>
    <title>GANnotation</title>
    <link href="http://yoursite.com/2019/01/17/GANnotation/"/>
    <id>http://yoursite.com/2019/01/17/GANnotation/</id>
    <published>2019-01-17T09:00:01.000Z</published>
    <updated>2019-01-24T07:25:49.642Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这是对GAN应用到人脸合成的改进，只是在arxiv上，先看看再说。</p><a id="more"></a><ul><li>paper: <a href="https://arxiv.org/abs/1811.03492" target="_blank" rel="noopener">https://arxiv.org/abs/1811.03492</a></li><li>github: <a href="https://github.com/ESanchezLozano/GANnotation" target="_blank" rel="noopener">https://github.com/ESanchezLozano/GANnotation</a></li><li>youtube: <a href="https://youtu.be/-8r7zexg4yg" target="_blank" rel="noopener">https://youtu.be/-8r7zexg4yg</a></li></ul><p>key word: self-consistency loss, triple consistency loss, progressive image translation</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>作者对 <strong>self-consistency loss</strong> 的有效性进行了论证， self-consistency loss 可以有效地使图片转换前后 <strong>preserve identity</strong>，并且提出了新的loss <strong>triple consistency loss</strong>.</p><p><img src="./GANnotation/GANnotation2.png" alt="triple consistency loss"><br><img src="/2019/01/17/GANnotation/GANnotation2.png" title="triple consistency loss"></p><p>作者观察到当生成的图片再次经过生成另一种属性的图片时，生成的效果很差。记生成的图片再次经过网络生成新的图片的过程为 <strong>“progressive image translation”</strong>。比如输入一张图片a，先通过网络生成图片b，再将图片b送入网络，得到生成图片c。</p><p><img src="./GANnotation/GANnotation1.png" alt="progressive image translation"><br><img src="/2019/01/17/GANnotation/GANnotation1.png" title="progressive image translation"></p><p>并且作者提出了<strong>GAN-notation</strong>，即 <strong>unconstrained landmark guided face-to-face synthesis</strong>, 同时改变人脸的姿势和表情(simultaneous change in pose and expression)， 论证了 triple consistency loss 的有效性。</p><h1 id="2-Proposed-approach"><a href="#2-Proposed-approach" class="headerlink" title="2. Proposed approach"></a>2. Proposed approach</h1><p><img src="./GANnotation/GANnotation3.png" alt="Proposed approach"><br><img src="/2019/01/17/GANnotation/GANnotation3.png" title="Proposed approach"></p><h2 id="2-1-Notation"><a href="#2-1-Notation" class="headerlink" title="2.1 Notation"></a>2.1 Notation</h2><p>符号说明：</p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">$I\in \mathcal{I}, w × h$</td><td style="text-align:center">人脸图片$I$</td></tr><tr><td style="text-align:center">$s_i \in \mathbb{R}^{2n}$</td><td style="text-align:center">$n$个有序点的集合</td></tr><tr><td style="text-align:center">$H(s_t) \in \mathcal{H}, H(s_t) \in \mathbb{R}^{n×w×h}$</td><td style="text-align:center">$s_i$编码成 heatmap，以均值为点的高斯分布的形式呈现</td></tr><tr><td style="text-align:center">$G: \mathcal{I}×\mathcal{H} \to \hat{\mathcal{I}}$</td><td style="text-align:center">$\hat{\mathcal{I}}$是生成图片的集合</td></tr><tr><td style="text-align:center">$\hat{I} = G(I;H(s_t))$</td><td style="text-align:center">其中$I$和$H(s_t)$是在通道维拼接而成的</td></tr><tr><td style="text-align:center">$\mathbb{P}_{\mathcal{I}}$</td><td style="text-align:center">图片$I$的分布</td></tr><tr><td style="text-align:center">$\mathbb{P}_{\hat{\mathcal{I}}}$</td><td style="text-align:center">图片$\hat{I}$的分布</td></tr></tbody></table></div><h2 id="2-2-Architecture"><a href="#2-2-Architecture" class="headerlink" title="2.2 Architecture"></a>2.2 Architecture</h2><p><strong>G</strong>:</p><ul><li>two spatial downsampling convolutions</li><li>followed by a set of residual blocks</li><li>two spatial upsampling blocks with 1=2 strided convolutions</li><li>output: colour image $C$ and a mask $M$.</li></ul><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><p><strong>D</strong>: PatchGAN, 128×128—&gt;4×4×512</p><h2 id="2-3-Training"><a href="#2-3-Training" class="headerlink" title="2.3 Training"></a>2.3 Training</h2><h3 id="2-3-1-Adversarial-loss"><a href="#2-3-1-Adversarial-loss" class="headerlink" title="2.3.1 Adversarial loss"></a>2.3.1 Adversarial loss</h3><p>作者使用的是 hinge adversarial loss。</p><p>我在参考<a href="https://arxiv.org/pdf/1705.02894.pdf" target="_blank" rel="noopener">文献19</a>中没有找到类似的loss，自己理解的是，对于判别器D而言，希望真图片的真值是$y \leqslant -1$，生成图片的真值是$y \geqslant 1$， 对于生成器G而言，希望生成的图片的真值是$y \geqslant 0$</p><script type="math/tex; mode=display">\begin{align}L_{adv}(D) = &-\mathbb{E}_{\hat{I}\backsim \mathbb{P}_{\hat{\mathcal{I}}}}[min(0, -1+D(\hat{I}))]+ \\&-\mathbb{E}_{I\backsim \mathbb{P}_{\mathcal{I}}}[min(0, -1+D(\hat{I}))]\end{align}</script><script type="math/tex; mode=display">L_{adv}(G)=-\mathbb{E}_{I \backsim \mathbb{P}_I } [D(\hat{I})]</script><h3 id="2-3-2-Pixel-loss"><a href="#2-3-2-Pixel-loss" class="headerlink" title="2.3.2 Pixel loss"></a>2.3.2 Pixel loss</h3><p>这个公式应该需要成对图片</p><script type="math/tex; mode=display">L_{pix} = \parallel G(I;H(s_t)) - I_t \parallel</script><h3 id="2-3-3-Consistency-loss"><a href="#2-3-3-Consistency-loss" class="headerlink" title="2.3.3 Consistency loss"></a>2.3.3 Consistency loss</h3><script type="math/tex; mode=display">L_{self}=\parallel G(G(I;H(s_t)); H(s_i)) - I \parallel</script><h3 id="2-3-4-Triple-Consistency-loss"><a href="#2-3-4-Triple-Consistency-loss" class="headerlink" title="2.3.4 Triple Consistency loss"></a>2.3.4 Triple Consistency loss</h3><script type="math/tex; mode=display">L_{triple}=\parallel G(\hat{I};H(s_n)) - G(I;H(s_n)) \parallel ^2</script><p>其中，$ \hat{I} = G(I;H(s_t)) $.</p><h3 id="2-3-5-Identity-preserving-loss"><a href="#2-3-5-Identity-preserving-loss" class="headerlink" title="2.3.5 Identity preserving loss"></a>2.3.5 Identity preserving loss</h3><script type="math/tex; mode=display">L_{id}=\sum_{l=fc,p} \parallel \Phi_{CNN}^l(I) - \Phi_{CNN}^l(\hat{I})  \parallel</script><p>其中，这个公式的目的是为了preserve the identity(???), 使用 Light CNN 的 fully connected layer 和 last pooling layer 提取出的特征。</p><p>不懂，为啥子呢？之前看StarGAN已经不使用这个公式了。</p><h3 id="2-3-6-Perceptual-loss"><a href="#2-3-6-Perceptual-loss" class="headerlink" title="2.3.6 Perceptual loss"></a>2.3.6 Perceptual loss</h3><p>这个应该就是之前见过的Vgg提取特征做损失。</p><script type="math/tex; mode=display">\begin{align}L_{pp} =& \sum_{l} \parallel \Phi_{VGG}^l(I) \Phi_{VGG}^l(\hat{I}) \parallel \\&+\parallel  \Gamma(\Phi_{VGG}^{relu3_3}(I)) - \Phi_{VGG}^{relu3_3}(\hat{I}) \parallel_F\end{align}</script><p>其中，$l=\lbrace relu1_2, relu2_2, relu3_3, relu4_3\rbrace$</p><p>感觉用到的损失太多了。</p><h3 id="2-3-7-Full-loss"><a href="#2-3-7-Full-loss" class="headerlink" title="2.3.7 Full loss"></a>2.3.7 Full loss</h3><script type="math/tex; mode=display">\begin{align}L(G) =& \lambda_{adv}L_{adv} + \lambda_{pix}L_{pix}+\lambda_{self}L_{self}+\\&\lambda_{triple}L_{triple}+\lambda_{id}L_{id}+\lambda_{pp}L_{pp}+\lambda_{tv}L_{tv}\end{align}</script><p>其中，$\lambda_{adv}=1, \lambda_{pix}=10, \lambda_{self}=100, \lambda_{triple}=100, \lambda_{id}=1, \lambda_{pp}=10, \lambda_{tv}=10^{-4} $</p><p>在作者给的<a href="https://arxiv.org/pdf/1603.08155.pdf" target="_blank" rel="noopener">参考文献14</a>中，也没有明确找到$L_{tv}$的表达式。之后看代码再确定一下吧。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h1><ul><li>Adam, $\beta_1=0.5, \beta_2=0.999$</li></ul><h2 id="3-1-On-the-use-of-a-triple-consistency-loss"><a href="#3-1-On-the-use-of-a-triple-consistency-loss" class="headerlink" title="3.1 On the use of a triple consistency loss"></a>3.1 On the use of a triple consistency loss</h2><p><img src="./GANnotation/GANnotation4.png" alt="StarGAN with/without triple consistency loss"><br><img src="/2019/01/17/GANnotation/GANnotation4.png" title="StarGAN with/without triple consistency loss"></p><p>可以看到即使是 one-to-one ， with triple consistency loss 的效果也要更好一些。</p><h2 id="3-2-GANnotation"><a href="#3-2-GANnotation" class="headerlink" title="3.2 GANnotation"></a>3.2 GANnotation</h2><p>StarGAN只能对很粗的概念进行转换，但是GANnotation可以直接对比较细的属性pose进行转换，而且还可以连续变换，更厉害一些。</p><p><img src="./GANnotation/GANnotation5.png" alt="GANnotation"><br><img src="/2019/01/17/GANnotation/GANnotation5.png" title="GANnotation"></p><p><img src="./GANnotation/GANnotation6.png" alt="GANnotation.vs.CR-GAN"><br><img src="/2019/01/17/GANnotation/GANnotation6.png" title="GANnotation.vs.CR-GAN"></p><h1 id="4-code"><a href="#4-code" class="headerlink" title="4. code"></a>4. code</h1><h2 id="4-1-生成器G"><a href="#4-1-生成器G" class="headerlink" title="4.1 生成器G"></a>4.1 生成器G</h2><p>生成器G输入是一张3通道的人脸图片和66通道的目标人脸关键点热力图，输出是3通道的colour image 和1通道的mask图。</p><p>生成器G共分为5部分：down_conv、bottleneck、feature_layer、colour_layer、mask_layer。</p><ul><li>[x] colour_image 和 mask 怎么理解？</li></ul><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conv_dim=<span class="number">64</span>, c_dim=<span class="number">66</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        initial_layer = [nn.Conv2d(<span class="number">3</span>+c_dim, conv_dim, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>,padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">        initial_layer += [nn.InstanceNorm2d(conv_dim, affine=<span class="keyword">True</span>)]</span><br><span class="line">        initial_layer += [nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>)]</span><br><span class="line"></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            initial_layer += [nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">            initial_layer += [nn.InstanceNorm2d(curr_dim*<span class="number">2</span>, affine=<span class="keyword">True</span>)]</span><br><span class="line">            initial_layer += [nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>)]</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.down_conv = nn.Sequential(*initial_layer)</span><br><span class="line"></span><br><span class="line">        bottleneck = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(repeat_num):</span><br><span class="line">            bottleneck += [ResidualBlock(curr_dim)]</span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(*bottleneck)</span><br><span class="line"></span><br><span class="line">        features = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            features += [nn.ConvTranspose2d(curr_dim, curr_dim//<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">            features += [nn.InstanceNorm2d(curr_dim//<span class="number">2</span>, affine=<span class="keyword">True</span>)]</span><br><span class="line">            features += [nn.LeakyReLU(<span class="number">0.2</span>,inplace=<span class="keyword">True</span>)]</span><br><span class="line">            curr_dim = curr_dim // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.feature_layer = nn.Sequential(*features)</span><br><span class="line"></span><br><span class="line">        colour = [nn.Conv2d(curr_dim, <span class="number">3</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">        colour += [nn.Tanh()]</span><br><span class="line">        self.colour_layer = nn.Sequential(*colour)</span><br><span class="line"></span><br><span class="line">        mask = [nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">        mask += [nn.Sigmoid()]</span><br><span class="line">        self.mask_layer = nn.Sequential(*mask)</span><br><span class="line">        init_weights(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">( self, x )</span>:</span></span><br><span class="line">        down = self.down_conv(x)</span><br><span class="line">        bottle = self.bottleneck(down)</span><br><span class="line">        features = self.feature_layer(bottle)</span><br><span class="line">        col = self.colour_layer(features)</span><br><span class="line">        mask = self.mask_layer(features)</span><br><span class="line">        output = mask * ( x[:,<span class="number">0</span>:<span class="number">3</span>,:,:] - col ) + col</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(net, init_type=<span class="string">'normal'</span>, gain=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_func</span><span class="params">(m)</span>:</span></span><br><span class="line">        classname = m.__class__.__name__</span><br><span class="line">        <span class="keyword">if</span> hasattr(m, <span class="string">'weight'</span>) <span class="keyword">and</span> (classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span> <span class="keyword">or</span> classname.find(<span class="string">'Linear'</span>) != <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> init_type == <span class="string">'normal'</span>:</span><br><span class="line">                weight_init.normal_(m.weight.data, <span class="number">0.0</span>, gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'xavier'</span>:</span><br><span class="line">                weight_init.xavier_normal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'kaiming'</span>:</span><br><span class="line">                weight_init.kaiming_normal_(m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_in'</span>)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'orthogonal'</span>:</span><br><span class="line">                weight_init.orthogonal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> NotImplementedError(<span class="string">'initialization method [%s] is not implemented'</span> % init_type)</span><br><span class="line">            <span class="keyword">if</span> hasattr(m, <span class="string">'bias'</span>) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                weight_init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm2d'</span>) != <span class="number">-1</span>:</span><br><span class="line">            weight_init.normal_(m.weight.data, <span class="number">1.0</span>, gain)</span><br><span class="line">            weight_init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'initialize network with %s'</span> % init_type)</span><br><span class="line">    net.apply(init_func)</span><br></pre></td></tr></table></figure><p>np.loadtxt</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">points = np.loadtxt(<span class="string">'test_images/test_1.txt'</span>).transpose().reshape(<span class="number">66</span>,<span class="number">2</span>,<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="4-2-其他"><a href="#4-2-其他" class="headerlink" title="4.2 其他"></a>4.2 其他</h2><p>作者现在只公布了demo代码。但不久就会公布训练代码，搓搓小手手期待中。</p><h2 id="4-3-StarGAN-with-Triple-Consistency-Loss"><a href="#4-3-StarGAN-with-Triple-Consistency-Loss" class="headerlink" title="4.3 StarGAN-with-Triple-Consistency-Loss"></a>4.3 StarGAN-with-Triple-Consistency-Loss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate target domain labels randomly.</span></span><br><span class="line">rand_idx = torch.randperm(label_org.size(<span class="number">0</span>))</span><br><span class="line">label_trg = label_org[rand_idx]</span><br><span class="line"></span><br><span class="line">rand_idx = torch.randperm(label_org.size(<span class="number">0</span>))</span><br><span class="line">label_third = label_org[rand_idx]</span><br><span class="line"><span class="comment"># ================================================================================#</span></span><br><span class="line"><span class="comment">#                               Triple consistency loss.                          #</span></span><br><span class="line"><span class="comment"># ================================================================================#</span></span><br><span class="line">x_third = self.G(x_fake, c_third)</span><br><span class="line">x_third_real = self.G(x_real, c_third)</span><br><span class="line">g_loss_triple = torch.mean(torch.abs(x_third - x_third_real))</span><br></pre></td></tr></table></figure><h1 id="5-答疑解惑"><a href="#5-答疑解惑" class="headerlink" title="5. 答疑解惑"></a>5. 答疑解惑</h1><h2 id="5-1-colour-image-和-mask-怎么理解？"><a href="#5-1-colour-image-和-mask-怎么理解？" class="headerlink" title="5.1 colour_image 和 mask 怎么理解？"></a>5.1 colour_image 和 mask 怎么理解？</h2><p>这个公式来源于</p><p>paper: <a href="http://arxiv.org/abs/1807.09251" target="_blank" rel="noopener">Ganimation: Anatomically-aware facial animation from a single image</a></p><p>github: <a href="https://github.com/albertpumarola/GANimatio" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimatio</a></p><p>project: <a href="http://www.albertpumarola.com/#projects" target="_blank" rel="noopener">http://www.albertpumarola.com/#projects</a></p><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><p>网络生成器没有直接回归整个图像，而是输出两个掩码，一个着色掩码C和一个注意力掩码A，其中，掩码A表示C的每个像素在多大程度上对输出图像有贡献，这样生成器就无需渲染与表情无关的元素，仅聚焦于定义了人脸表情的像素上。</p><p>这个公式貌似比StarGAN还牛逼。能不能放在StarGAN上呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这是对GAN应用到人脸合成的改进，只是在arxiv上，先看看再说。&lt;/p&gt;
    
    </summary>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="face synthesis" scheme="http://yoursite.com/tags/face-synthesis/"/>
    
  </entry>
  
  <entry>
    <title>tensorboard</title>
    <link href="http://yoursite.com/2019/01/15/tensorboard/"/>
    <id>http://yoursite.com/2019/01/15/tensorboard/</id>
    <published>2019-01-15T09:15:48.000Z</published>
    <updated>2019-01-17T08:20:10.969Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>新装pytorch之后，可视化工具visdom就不能用了，所以改用tensorboard，以下是tensorboard及其变体的使用命令。</p><a id="more"></a><h1 id="1-原理"><a href="#1-原理" class="headerlink" title="1. 原理"></a>1. 原理</h1><p>tensorboard与visdom不同，前者是直接读取文件进行展示，需要程序先把要展示的内容保存成文件，然后tensorboard再读取文件进行展示，后者是代码直接展示，在程序中就直接传递给visdom了。</p><p>相同点是两者都需要在程序外面启动。</p><h1 id="2-API以及使用流程"><a href="#2-API以及使用流程" class="headerlink" title="2. API以及使用流程"></a>2. API以及使用流程</h1><h2 id="2-1-API"><a href="#2-1-API" class="headerlink" title="2.1 API"></a>2.1 API</h2><ul><li>tf.summary.FileWriter——用于将汇总数据写入磁盘 </li><li>tf.summary.scalar——对标量数据汇总和记录 </li><li>tf.summary.histogram——记录数据的直方图 </li><li>tf.summary.image——将图像写入summary </li><li>tf.summary.merge——对各类的汇总进行一次合并 </li><li>tf.summary.merge_all——合并默认图像中的所有汇总</li></ul><h2 id="2-2-使用流程"><a href="#2-2-使用流程" class="headerlink" title="2.2 使用流程"></a>2.2 使用流程</h2><ol><li>添加记录节点：tf.summary.scalar/image/histogram()等</li><li>汇总记录节点：merged = tf.summary.merge_all()</li><li>运行汇总节点：summary = sess.run(merged)，得到汇总结果</li><li>日志书写器实例化：summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph)，实例化的同时传入 graph 将当前计算图写入日志</li><li>调用日志书写器实例对象summary_writer的add_summary(summary, global_step=i)方法将所有汇总日志写入文件</li><li>调用日志书写器实例对象summary_writer的close()方法写入内存，否则它每隔120s写入一次， close() 之后就无法再次写入了，需要重新打开reopen()，这里可以替代为summary_writer.flush()。</li></ol><p><img src="./tensorboard/tensorboard.jpg" alt="tensorboard使用流程图"><br><img src="/2019/01/15/tensorboard/tensorboard.jpg" title="tensorboard使用流程图"></p><h1 id="3-启动"><a href="#3-启动" class="headerlink" title="3. 启动"></a>3. 启动</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir /path/to/log</span><br></pre></td></tr></table></figure><h1 id="4-初始化"><a href="#4-初始化" class="headerlink" title="4. 初始化"></a>4. 初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.summary.FileWriter(logdir, graph=None, flush_secs=120, max_queue=10)</span></span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>, sess.graph(), flush_secs = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">writer.add_graph(sess.graph())</span><br></pre></td></tr></table></figure><p>其他常用API</p><ul><li>add_event(event)：Adds an event to the event file</li><li>add_graph(graph, global_step=None)：Adds a Graph to the event file，Most users pass a graph in the constructor instead</li><li>add_summary(summary, global_step=None)：Adds a Summary protocol buffer to the event file，一定注意要传入 global_step</li><li>close()：Flushes the event file to disk and close the file</li><li>flush()：Flushes the event file to disk</li><li>add_meta_graph(meta_graph_def,global_step=None)</li><li>add_run_metadata(run_metadata, tag, global_step=None)</li></ul><p>备注：这个event还是没有看懂怎么使用。</p><h1 id="5-scalar"><a href="#5-scalar" class="headerlink" title="5. scalar"></a>5. scalar</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 三个工具：writer用于指定路径，sess用于运行，summary_op用于执行。</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy) <span class="comment"># tag, value</span></span><br><span class="line"></span><br><span class="line">summary_op = tf.summary.merge_all()</span><br><span class="line">summary_str = sess.run(summary_op)</span><br><span class="line">summary_writer.add_summary(summary_str, global_step) <span class="comment"># y, x</span></span><br><span class="line"><span class="comment"># 备注：global_step总是int型，即使输入0.4，在图像显示也是0.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or family</span></span><br><span class="line">tf.summary.scalar(<span class="string">'loss1'</span>, <span class="number">1</span>, family=<span class="string">'loss'</span>)</span><br><span class="line">tf.summary.scalar(<span class="string">'loss2'</span>, <span class="number">2</span>, family=<span class="string">'loss'</span>)</span><br><span class="line">tf.summary.scalar(<span class="string">'loss3'</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 示意如下图</span></span><br></pre></td></tr></table></figure><p><img src="./tensorboard/tensorboard3.png" alt="family"><br><img src="/2019/01/15/tensorboard/tensorboard3.png" title="family"></p><p>或者自定义数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 两个工具 summary_writer: 用于连接路径， summary: 用于指定y值</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">summary = tf.Summary(value=[</span><br><span class="line">  tf.Summary.Value(tag=<span class="string">'test2'</span>, simple_value=<span class="number">0</span>),</span><br><span class="line">  tf.Summary.Value(tag=<span class="string">'test3'</span>, simple_value=<span class="number">1</span>),</span><br><span class="line">])</span><br><span class="line">summary_writer.add_summary(summary, global_step) <span class="comment"># y, x</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">summary = tf.Summary()</span><br><span class="line">summary.value.add(tag=<span class="string">'test4'</span>, simple_value=<span class="number">0</span>)</span><br><span class="line">summary.value.add(tag=<span class="string">'test5'</span>, simple_value=<span class="number">1</span>)</span><br><span class="line">sumamry_writer.add_summary(summary, global_step)</span><br></pre></td></tr></table></figure><h1 id="6-histogram"><a href="#6-histogram" class="headerlink" title="6. histogram"></a>6. histogram</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.histogram(<span class="string">'layer'</span>+str(i+<span class="number">1</span>)+<span class="string">'weights'</span>,weights)</span><br></pre></td></tr></table></figure><p>这里的weights可以是list型，也可以是pytorch的tensor型，其他类型没试，但可以推断，一般情况下的都可以，猜测会统一转换为numpy型。</p><h1 id="7-image"><a href="#7-image" class="headerlink" title="7. image"></a>7. image</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.image(<span class="string">'input'</span>, x_image, max_outputs=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tf.summary.image(<span class="string">'test'</span>, tf.reshape(images, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]), <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tf.summary.image(<span class="string">'test1'</span>, torch.rand(<span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>备注：x_image必须是uint8或者float32型的4-D Tensor[batch_size, height, width, channels]，其中channels为1， 3 或者4</p><p>对于numpy</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line">image_ = Image.open(<span class="string">'path'</span>)</span><br><span class="line">image_numpy = numpy.array(image_) <span class="comment"># 271, 108, 3</span></span><br><span class="line">tf.summary.image(<span class="string">'test1'</span>, image_numpy.reshape(<span class="number">1</span>, <span class="number">271</span>, <span class="number">108</span>,<span class="number">3</span>))</span><br><span class="line">summary_op = tf.summary.merge_all()</span><br><span class="line">summary_str = sess.run(summary_op)</span><br><span class="line">summary_writer.add_summary(summary_str, global_step)</span><br></pre></td></tr></table></figure><p>对于pytorch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image_ = Image.open(<span class="string">'path'</span>) <span class="comment"># 3, 271, 108</span></span><br><span class="line">image_tensor = T.ToTensor()(image_)</span><br><span class="line">tf.summary.image(<span class="string">'test9'</span>, image_tensor.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">271</span>, <span class="number">108</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 备注：将permute改为reshape之后不知道为什么会展示9空格的相同的黑白图片</span></span><br><span class="line">summary_op = tf.summary.merge_all()</span><br><span class="line">summary_str = sess.run(summary_op)</span><br><span class="line">summary_writer.add_summary(summary_str, global_step)</span><br></pre></td></tr></table></figure><p>uint8或者float32型的4-D Tensor[batch_size, height, width, channels]，其中channels为1， 3 或者4</p><h1 id="8-多个event的可视化"><a href="#8-多个event的可视化" class="headerlink" title="8. 多个event的可视化"></a>8. 多个event的可视化</h1><p>如果 logdir 目录的子目录中包含另一次运行时的数据(多个 event)，那么 TensorBoard 会展示所有运行的数据(主要是scalar)，这样可以用于比较不同参数下模型的效果，调节模型的参数，让其达到最好的效果！</p><p>这个还有待测试</p><h1 id="9-tensorboard-logger"><a href="#9-tensorboard-logger" class="headerlink" title="9. tensorboard_logger"></a>9. tensorboard_logger</h1><p>tensorboard_logger可以暂时理解成简化版的tensorboard，或者是tensorboard的高级API。</p><p>tensorboard_logger依赖于tensorboard。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">pip install tensorboard_logger</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">tensorboard --logdir /path/to/log --port /port <span class="comment"># 实际还是通过tensorboard来使用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">from</span> tensorboard_logger <span class="keyword">import</span> Logger</span><br><span class="line">logger = Logger(logdir=<span class="string">'./logs'</span>, flush_secs=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># logger.logdir = './logs2'  # 可以修改，尝试着修改了一下，发现没用，暂时用不着。</span></span><br><span class="line"><span class="comment"># logger.flush_secs = 3   # 可以修改</span></span><br><span class="line"><span class="comment"># API</span></span><br><span class="line">logger.log_value(<span class="string">'loss'</span>, <span class="number">10</span>, step=<span class="number">3</span>) <span class="comment"># step 可以不写，推荐写</span></span><br><span class="line"><span class="comment"># image_path = 'test.jpg'</span></span><br><span class="line"><span class="comment"># image_ = Image.open(image_path)</span></span><br><span class="line"><span class="comment"># image_numpy = np.array(image_)</span></span><br><span class="line">logger.log_images(<span class="string">'images'</span>, image_numpy.reshape(<span class="number">1</span>,<span class="number">271</span>,<span class="number">168</span>,<span class="number">3</span>), step=<span class="number">2</span>) <span class="comment"># 规则同上</span></span><br><span class="line">logger.log_histogram(<span class="string">'weights'</span>, torch.rand(<span class="number">2</span>,<span class="number">3</span>)+<span class="number">10</span>, step=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数tensorboard_logger.configure, tensorboard_logger.log_value与tensorboard_logger.Logger可以达到一样的效果</span></span><br><span class="line"><span class="keyword">from</span> tensorboard_logger <span class="keyword">import</span> configure, log_value</span><br><span class="line">configure(<span class="string">"runs/run-1234"</span>, flush_secs=<span class="number">5</span>)</span><br><span class="line">log_value(<span class="string">'v1'</span>, v1, step)</span><br></pre></td></tr></table></figure><h1 id="10-tensorboardX"><a href="#10-tensorboardX" class="headerlink" title="10. tensorboardX"></a>10. tensorboardX</h1><p><a href="https://github.com/lanpa/tensorboardX" target="_blank" rel="noopener">https://github.com/lanpa/tensorboardX</a></p><p>tensorboardX也依赖于tensorflow和tensorboard。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">pip install tensorboardX</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">tensorboard --logdir /path/to/log --port /port</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(logdir = <span class="string">'tensorboard7'</span>, comment = <span class="string">'test1'</span>, flush_secs = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 当有logdir时，comment被忽略，</span></span><br><span class="line"><span class="comment"># 当没有logdir时，只有comment(writer = SummaryWriter(comment = 'tensorboard7_test1', flush_secs = 1))会作为补充自动生成这样的文件目录：</span></span><br><span class="line"><span class="comment"># ./runs/Jan17_11-24-12_zbp-PowerEdge-T630tensorboard7_test1/events....</span></span><br><span class="line"><span class="comment"># 此外，任意tf.summary.FileWriter的参数都可以加上去。</span></span><br><span class="line">writer.close()</span><br><span class="line"><span class="comment"># 没有writer.flush()</span></span><br></pre></td></tr></table></figure><p>另一种写法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> SummaryWriter(logdir = <span class="string">'tensorboard7'</span>) <span class="keyword">as</span> w:</span><br><span class="line">    w.add_something()</span><br></pre></td></tr></table></figure><p>不推荐with的写法，会自动重新创建一个文件，乱。</p><h2 id="10-1-add-scalar"><a href="#10-1-add-scalar" class="headerlink" title="10.1 add_scalar"></a>10.1 add_scalar</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># API</span></span><br><span class="line">writer.add_scalar(<span class="string">'loss1'</span>, <span class="number">2</span>, global_step=<span class="number">2</span>)</span><br><span class="line">writer.add_scalars(<span class="string">'loss'</span>, &#123;<span class="string">'loss1'</span>:<span class="number">2</span>, <span class="string">'loss2'</span>:<span class="number">3</span>&#125;, global_step=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 示例如下图，add_scalars可以将多个loss显示同一个图上，实现类似visdom的功能。</span></span><br><span class="line"><span class="comment"># add_scalars(main_tag,tag_scalar_dict,global_step=None)</span></span><br></pre></td></tr></table></figure><p><img src="./tensorboard/tensorboard1.png" alt="add_scalar&amp;add_scalars"><br><img src="/2019/01/15/tensorboard/tensorboard1.png" title="add_scalar&add_scalars"></p><p><strong>备注：</strong>:</p><ol><li><strong>SummaryWriter以及之前的类似Writer都可以自动创建文件夹</strong>。</li><li><strong>tensorboard —logdir /path/to/log会自动迭代该文件下的所有文件夹和文件，将event展示出来，并且将同名字的scalar放在一起，可以用于对比修改前后的结果</strong>。</li></ol><h2 id="10-2-add-image"><a href="#10-2-add-image" class="headerlink" title="10.2 add_image"></a>10.2 add_image</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">writer.add_image(tag, img_tensor, global_step=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># img_tensor: 图像数据，shape（3，H，W) 配合torchvision.utils.make_grid使用</span></span><br><span class="line"><span class="comment"># writer.add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW'),</span></span><br><span class="line"><span class="comment">#  dataformats: 'CHW', 'HWC', 'HW'</span></span><br><span class="line"></span><br><span class="line">image_path = <span class="string">'test.jpg'</span></span><br><span class="line">image_ = Image.open(image_path)</span><br><span class="line">image_tensor = T.ToTensor()(image_)</span><br><span class="line">writer.add_image(<span class="string">'test1'</span>, image_tensor, global_step=<span class="number">2</span>)</span><br><span class="line">writer.add_image(<span class="string">'test2'</span>, torchvision.utils.make_grid(torch.rand(<span class="number">16</span>, <span class="number">3</span>, <span class="number">256</span>,<span class="number">256</span>), nrow=<span class="number">8</span>, padding=<span class="number">20</span>), global_step=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="10-3-add-histogram"><a href="#10-3-add-histogram" class="headerlink" title="10.3 add_histogram"></a>10.3 add_histogram</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">writer.add_histogram(tag,values,global_step=<span class="keyword">None</span>,bins=<span class="string">'tensorflow'</span>)</span><br><span class="line"><span class="comment"># bins:  (string): one of &#123;'tensorflow','auto', 'fd', ...&#125;, this determines</span></span><br><span class="line"><span class="comment"># how the bins are made. You can find other options in:</span></span><br><span class="line"><span class="comment"># https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html</span></span><br><span class="line">writer.add_histogram(<span class="string">'test1'</span>, torch.rand(<span class="number">16</span>, <span class="number">3</span>, <span class="number">256</span>,<span class="number">256</span>), global_step=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 感觉 bins 这个参数也差不多么</span></span><br></pre></td></tr></table></figure><h2 id="10-4-add-graph"><a href="#10-4-add-graph" class="headerlink" title="10.4 add_graph"></a>10.4 add_graph</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net1</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net1, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x )</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">10</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = torch.nn.functional.softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">input = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">model = Net1()</span><br><span class="line">writer.add_graph(model, (input,))</span><br><span class="line"></span><br><span class="line">add_graph(model, input_to_model, verbose=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># verbose 用于是否print</span></span><br></pre></td></tr></table></figure><p><img src="./tensorboard/tensorboard4.png" alt="add_graph"><br><img src="/2019/01/15/tensorboard/tensorboard4.png" title="add_graph"></p><p>终于pytorch也能显示graph了，不容易，应该提前熟悉这些API的。</p><h1 id="11-结论"><a href="#11-结论" class="headerlink" title="11. 结论"></a>11. 结论</h1><p>综上所述，我觉得：</p><ul><li>tensorboardX最适合非tensorflow的深度学习框架</li><li>tensorboard适合tensoflow</li><li>tensorboard_logger也适合非tensorflow的深度学习框架，可以看成简化版的tensorboardX</li><li>tensorboardX和tensorboard_logger可以互相替换</li><li>开始使用tensorboardX作为我的新的可视化工具。</li></ul><p>不太懂tensorboardX和tensorboard_logger之间的区别，难道是开发的人不一样？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;新装pytorch之后，可视化工具visdom就不能用了，所以改用tensorboard，以下是tensorboard及其变体的使用命令。&lt;/p&gt;
    
    </summary>
    
      <category term="tensorboard" scheme="http://yoursite.com/categories/tensorboard/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
      <category term="tensorboard" scheme="http://yoursite.com/tags/tensorboard/"/>
    
  </entry>
  
  <entry>
    <title>SyRI</title>
    <link href="http://yoursite.com/2019/01/14/SyRI/"/>
    <id>http://yoursite.com/2019/01/14/SyRI/</id>
    <published>2019-01-14T03:32:20.000Z</published>
    <updated>2019-01-14T07:07:45.946Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这也是一篇合成数据集的论文。</p><a id="more"></a><p>paper: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Slawomir_Bak_Domain_Adaptation_through_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Domain Adaptation through Synthesis for Unsupervised Person Re-identification</a></p><p>github: 无</p><p>项目地址: 无</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>针对的问题是行人重识别数据缺乏在不同光照条件下的多样性，所以作者进行了数据合成。</p><p><strong>HDR maps</strong>: high dynamic range environment maps.</p><p><strong>three-step domain adaption technique</strong>:</p><p><img src="./SyRI/SyRI1.png" alt="three-step domain adaption technique"><br><img src="/2019/01/14/SyRI/SyRI1.png" title="three-step domain adaption technique"></p><ol><li>Illumination inference</li><li>Domain translation</li><li>Fine-tuning</li></ol><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>这篇文章看得我有点迷迷糊糊的，感觉其中用到了cycleGAN，优化了cycleGAN的一个损失函数，但是怎么进行的illuminaion inference、fine-tuning，没有充分理解。</p><h1 id="罗浩对这篇文章的解读"><a href="#罗浩对这篇文章的解读" class="headerlink" title="罗浩对这篇文章的解读"></a>罗浩对这篇文章的解读</h1><p><a href="https://zhuanlan.zhihu.com/p/44212707" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44212707</a></p><blockquote><p>由于训练数据的缺乏，以及3D建模的技术增强，利用游戏等合成的逼真数据来进行视觉研究已经逐渐打入各个领域。这一篇就是利用电脑合成数据在进行ReID的论文。论文提出了新的合成数据集SyRI dataset，该数据集通过改变HDR参数等，一个行人ID可以拥有一百多个环境下图像数据。此外，为了在未见过的真实场景上实现更好的域适应效果，论文基于这个合成数据集提出了一种新的方法。</p><p>整个Pipeline包括三个环节。第一步是拿到了target domain $R_{M+1}$ 的一些未标注图片之后，要对光照进行一个推理，在合成数据集 $S_{k^*}$ 里面找到最接近样本 。然后利用CycleGAN将合成数据生成target domain style的数据。最后利用生成的数据对ReID网络进行fune-tuning。整体来说pipeline比较简单.</p><p>例如，在CycleGAN生成图环节，为了得到更高质量的图像，论文使用了和PTGAN一样的前景mask的思想。不同点在于PTGAN是借助于语义分割网络得到一个行人前景的mask，本文是直接使用了一个2D高斯核作为mask</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这也是一篇合成数据集的论文。&lt;/p&gt;
    
    </summary>
    
      <category term="person re-id" scheme="http://yoursite.com/categories/person-re-id/"/>
    
    
      <category term="person re-identification" scheme="http://yoursite.com/tags/person-re-identification/"/>
    
      <category term="domain adaption" scheme="http://yoursite.com/tags/domain-adaption/"/>
    
  </entry>
  
</feed>
