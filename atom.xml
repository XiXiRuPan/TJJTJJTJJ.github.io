<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>田佳杰</title>
  <icon>https://www.gravatar.com/avatar/cbd320e406f4c9571bb798e8810c4d18</icon>
  <subtitle>记录一些学习到的东西和论文记录</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-12-17T09:44:32.796Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jiajie Tian</name>
    <email>18810906582@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CASN</title>
    <link href="http://yoursite.com/2018/12/17/CASN/"/>
    <id>http://yoursite.com/2018/12/17/CASN/</id>
    <published>2018-12-17T03:42:25.000Z</published>
    <updated>2018-12-17T09:44:32.796Z</updated>
    
    <content type="html"><![CDATA[<p>CASN: <a href="https://arxiv.org/abs/1811.07487" target="_blank" rel="noopener">Re-Identification with Consistent Attentive Siamese Networks</a></p><p>Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J. Radke</p><a id="more"></a><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>这篇论文；</p><ul><li>采用了Grad-CAM作为attention机制</li><li>attention consistency: 采用Siamese-Net来使同一个人的attention位置是一样的</li></ul><h1 id="2-The-Consistent-Attentive-Siamese-Network"><a href="#2-The-Consistent-Attentive-Siamese-Network" class="headerlink" title="2. The Consistent Attentive Siamese Network"></a>2. The Consistent Attentive Siamese Network</h1><p>整体网络架构如图所示：</p><p><img src="./CASN/CASN1.png" alt="CASN的整体网络架构"><br><img src="/2018/12/17/CASN/CASN1.png" title="CASN的整体网络架构"></p><ul><li>整体网络架构以IDE为基准网络，分为两部分:</li><li><ul><li>Identification Module</li></ul></li><li><ul><li>Siamese Module</li></ul></li><li>Identification 和 Siamese 的特征提取网络共享，不同的只是fc层</li></ul><h2 id="2-1-The-Identification-Module"><a href="#2-1-The-Identification-Module" class="headerlink" title="2.1 The Identification Module"></a>2.1 The Identification Module</h2><p>通过<a href="https://tjjtjjtjj.github.io/2018/12/14/Grad-CAM/#more" target="_blank" rel="noopener">Grad-CAM</a>的学习，已经知道了Grad-CAM的作用。</p><p><strong>Identification loss:</strong><br>$$L_{ide}=-\sum_{n=1}^N log \frac{exp(y_{c_n})}{\sum_j exp(y_j)}$$</p><p>Identification loss 更偏向于不同行人之间的判别信息。</p><p><strong>Identification attention loss:</strong><br>$$L_{ia}=\overline{y_{c_n}}$$</p><p>其中，给定一张图片$I_n$和类别$c_n$，Grad-CAM得到attention map $M_n$，做归一化操作，令$\Sigma(M_n)=sigmoid(\alpha(M_n-\beta))$，从而得到去掉attention区域的新图片$\overline{I_n}=I_n*(1-\Sigma(M_n))$，$\overline{y_{c_n}}$是$\overline{I_n}$的预测值。</p><p>Identification attention loss 更偏向于行人的全部信息。</p><p>我的理解是$\overline{I_n}$中尽可能包含少的ID信息，所以预测出是$c_n$的概率更小，得到的attention区域尽可能地包括全部信息。</p><p><strong>两种loss的效果对比图</strong><br><img src="./CASN/CASN2.png" alt="两种loss的效果对比图"><br><img src="/2018/12/17/CASN/CASN2.png" title="两种loss的效果对比图"></p><h2 id="2-2-The-Siamese-Module"><a href="#2-2-The-Siamese-Module" class="headerlink" title="2.2 The Siamese Module"></a>2.2 The Siamese Module</h2><p>Siamese Module 结构图<br><img src="./CASN/CASN3.png" alt="Siamese Module 结构图"><br><img src="/2018/12/17/CASN/CASN3.png" title="Siamese Module 结构图"></p><p><strong>Siamese loss</strong><br>$$L_{bce}=-\sum_p log(\frac{exp(z_{c_p})}{exp(z_0)+exp(z_1)})$$</p><p><strong>Siamese attention loss</strong><br>$$\alpha_i=<br>\begin{cases}<br>1, \mbox{if} f_i^-&gt;0 \<br>0, otherwise<br>\end{cases}<br>$$<br>$$s_1=&lt;\alpha, f_1&gt;, s_2=&lt;\alpha, f_2&gt;(dot products)$$<br>$$\alpha_1^k=GAP(\frac{\partial s_1}{\partial A_1}), \alpha_2^k=GAP(\frac{\partial s_2}{\partial A_2}) \tag{5}$$<br>$$M_1=ReLU(\sum_k \alpha_1^k A_1^k),M_2=ReLU(\sum_k \alpha_2^k A_2^k)$$<br>$$L_{sa}=L_{bce}+\alpha \parallel M_{m1}^{resize}-M_{m2}^{resize} \parallel _2$$<br>$$\alpha=0.2$$</p><p>其中，$M_{m1}$是$M_1$中超过阈值t的元素，$M_{m1}^{resize}$是$M_{m1}$resize成相同大小的元素，主要是为了解决对齐问题。</p><h2 id="2-3-Overall-Design-of-the-CASN"><a href="#2-3-Overall-Design-of-the-CASN" class="headerlink" title="2.3 Overall Design of the CASN"></a>2.3 Overall Design of the CASN</h2><p>CASN的整体架构<br><img src="./CASN/CASN4.png" alt="CASN的整体架构"><br><img src="/2018/12/17/CASN/CASN4.png" title="CASN的整体架构"></p><p><strong>The overall loss</strong><br>$$L=L_{ide}+\lambda_1 L_{ia}+\lambda_2 L_{sa}$$</p><h3 id="3-Experiments-and-Results"><a href="#3-Experiments-and-Results" class="headerlink" title="3. Experiments and Results"></a>3. Experiments and Results</h3><h4 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h4><ul><li>input: 288x144</li><li>SGD: momentum=0.9</li><li>lr=0.03</li><li>epoch=40</li><li>lr decay=0.1 after 30</li><li>baseline: IDE and PCB( input: 384x128)</li><li>batch=16</li><li>test:  we send the query and gallery as pair inputs to obtain attention maps $\parallel M_{m1}^{resize}-M_{m2}^{resize} \parallel _2$</li></ul><h4 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h4><p><img src="./CASN/CASN5.png" alt="Results"><br><img src="/2018/12/17/CASN/CASN5.png" title="Results"></p><p><img src="./CASN/CASN6.png" alt="Ablation Study"><br><img src="/2018/12/17/CASN/CASN6.png" title="Ablation Study"></p><p>通过 Ablation Study , 对比CASN(IDE)、PCB，可以看出IA或者SA的作用和简单地分成6块达到的效果是类似的，这是不是说明了这种attention机制没有很大的作用，或者说分成6块就已经是一种很好的attention机制。</p><p>另外，+IA、+SA、CASN的对比，感觉IA或者SA一种机制就已经足够了，两者达到的效果是一样的，只使用一种就可以了。</p><h3 id="4-Others"><a href="#4-Others" class="headerlink" title="4. Others"></a>4. Others</h3><p>这篇论文不懂的地方：</p><ul><li>$L_{ia}$为什么可以直接这么写，不需要经过softmax之类的，或者不应该是每类的概率差不多么，</li><li>$L_{ia}$还是经过相同的网络得到的吗？反向求导要怎么写？</li><li>根据Grad-CAM的以类别反向求导，方程5给我的感觉更像是$f$的特征和作为输入图片的分类预测值，合理性站不住脚。</li><li>在测试时，需要每次输入一对图片，是不是太慢了。</li><li>如果实验结果可以复现的话，那么IA我觉得还是很有用的，解释性也强。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CASN: &lt;a href=&quot;https://arxiv.org/abs/1811.07487&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Re-Identification with Consistent Attentive Siamese Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J. Radke&lt;/p&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="person-reid" scheme="http://yoursite.com/tags/person-reid/"/>
    
      <category term="Grad-CAM" scheme="http://yoursite.com/tags/Grad-CAM/"/>
    
  </entry>
  
  <entry>
    <title>Grad-CAM</title>
    <link href="http://yoursite.com/2018/12/14/Grad-CAM/"/>
    <id>http://yoursite.com/2018/12/14/Grad-CAM/</id>
    <published>2018-12-14T01:50:53.000Z</published>
    <updated>2018-12-17T07:54:49.406Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1610.02391.pdf" target="_blank" rel="noopener">Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization</a></p><a id="more"></a><p>Gradient-weighted Class Activation Mapping</p><p>Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam</p><p>code-torch: <a href="https://github.com/ramprs/grad-cam/" target="_blank" rel="noopener">https://github.com/ramprs/grad-cam/</a></p><p>code-pytorch: <a href="https://github.com/jacobgil/pytorch-grad-cam" target="_blank" rel="noopener">https://github.com/jacobgil/pytorch-grad-cam</a></p><p>code-keras: <a href="https://github.com/jacobgil/keras-grad-cam" target="_blank" rel="noopener">https://github.com/jacobgil/keras-grad-cam</a></p><p>参考链接：</p><ul><li><a href="https://www.jianshu.com/p/b2f7efe10ad8" target="_blank" rel="noopener">https://www.jianshu.com/p/b2f7efe10ad8</a></li><li><a href="https://www.jianshu.com/p/1d7b5c4ecb93" target="_blank" rel="noopener">https://www.jianshu.com/p/1d7b5c4ecb93</a></li><li><a href="http://spytensor.com/index.php/archives/20/" target="_blank" rel="noopener">http://spytensor.com/index.php/archives/20/</a></li></ul><h1 id="1-声明"><a href="#1-声明" class="headerlink" title="1. 声明"></a>1. 声明</h1><p>最近在看到一篇person-reid的文章<a href="http://arxiv.org/abs/1811.07487v1" target="_blank" rel="noopener">Re-Identification with Consistent Attentive Siamese Networks</a>，其中涉及到了Grad-CAM，所以简单学习一下Grad-CAM，但不作为重点。</p><h1 id="2-前言"><a href="#2-前言" class="headerlink" title="2. 前言"></a>2. 前言</h1><p>对于深度模型的可解释性和可视化，现在已经研究出了一些方法，包括不限于<strong>Deconvolution, Guided-Backpropagation, CAM, Grad-CAM</strong>.</p><p>其中 Deconvolution 和 Guided-Backpropagation 得到更偏向于细粒度图， CAM 和 Grad-CAM 得到更偏向于类区分的热力图。</p><p>各种可视化方法及其效果图参见：<a href="https://github.com/utkuozbulak/pytorch-cnn-visualizations" target="_blank" rel="noopener">https://github.com/utkuozbulak/pytorch-cnn-visualizations</a></p><p>参考链接: <a href="https://blog.csdn.net/geek_wh2016/article/details/81060315" target="_blank" rel="noopener">https://blog.csdn.net/geek_wh2016/article/details/81060315</a></p><h2 id="2-1-Deconvolution"><a href="#2-1-Deconvolution" class="headerlink" title="2.1 Deconvolution"></a>2.1 <a href="https://arxiv.org/abs/1311.2901v3" target="_blank" rel="noopener">Deconvolution</a></h2><p><a href="https://arxiv.org/abs/1311.2901v3" target="_blank" rel="noopener">Deconvolution</a></p><p>code: <a href="https://github.com/kvfrans/feature-visualization" target="_blank" rel="noopener">https://github.com/kvfrans/feature-visualization</a></p><p>参考链接：</p><ul><li><a href="http://kvfrans.com/visualizing-features-from-a-convolutional-neural-network/" target="_blank" rel="noopener">http://kvfrans.com/visualizing-features-from-a-convolutional-neural-network/</a></li><li><a href="https://blog.csdn.net/Julialove102123/article/details/78292807" target="_blank" rel="noopener">https://blog.csdn.net/Julialove102123/article/details/78292807</a></li><li><a href="https://blog.csdn.net/gm_margin/article/details/79335140" target="_blank" rel="noopener">https://blog.csdn.net/gm_margin/article/details/79335140</a></li></ul><p>整个过程涉及到三个操作：反池化、反激活、反卷积。其中，反池化是池化的补零逆操作，反激活还是原激活函数，反卷积是卷积核的转置。</p><p><img src="./Grad-CAM/Deconvolution1.png" alt="Deconvolution的结构图"><br><img src="/2018/12/14/Grad-CAM/Deconvolution1.png" title="Deconvolution的结构图"></p><h2 id="2-2-Guided-Backpropagation"><a href="#2-2-Guided-Backpropagation" class="headerlink" title="2.2 Guided-Backpropagation"></a>2.2 Guided-Backpropagation</h2><p><a href="http://arxiv.org/abs/1412.6806" target="_blank" rel="noopener">Guided-Backpropagation</a></p><p>反向传播、反卷积和导向反向传播都是反向传播，区别在于经过 ReLU 层时对梯度的不同处理策略。在这篇<a href="https://arxiv.org/pdf/1412.6806.pdf" target="_blank" rel="noopener">论文</a>中有详细的解释。</p><p>计算公式如下：</p><p><img src="./Grad-CAM/Guided-Backpropagation.jpg" alt="反向传播梯度的选择"><br><img src="/2018/12/14/Grad-CAM/Guided-Backpropagation.jpg" title="反向传播梯度的选择"></p><p>文中提出使用 stride convolution 代替 pooling，研究这种结构的有效性。</p><p>效果显示如下：</p><p><img src="./Grad-CAM/Guided-Backpropagation2.png" alt="三种反向传播的效果"><br><img src="/2018/12/14/Grad-CAM/Guided-Backpropagation2.png" title="三种反向传播的效果"></p><p>可以看出 Guided-Backpropagation 主要提取对分类有效果的特征，但是与是哪类没有关系。</p><h2 id="2-3-CAM"><a href="#2-3-CAM" class="headerlink" title="2.3 CAM"></a>2.3 CAM</h2><p><a href="https://arxiv.org/abs/1512.04150" target="_blank" rel="noopener">CAM</a></p><p>文中主要研究 global average pooling (GAP) 的有效性。</p><p><img src="./Grad-CAM/CAM.png" alt="CAM的网络结构"><br><img src="/2018/12/14/Grad-CAM/CAM.png" title="CAM的网络结构"></p><p><img src="./Grad-CAM/CAM2.png" alt="结果图"><br><img src="/2018/12/14/Grad-CAM/CAM2.png" title="结果图"></p><h2 id="2-4-Grad-CAM"><a href="#2-4-Grad-CAM" class="headerlink" title="2.4 Grad-CAM"></a>2.4 Grad-CAM</h2><p>Grad-CAM 与 CAM 的不同点在于前者的特征加权系数是反向传播得到的，后者的特征加权系数是分类器的权重。</p><h1 id="3-Introduction"><a href="#3-Introduction" class="headerlink" title="3. Introduction"></a>3. Introduction</h1><p>可视化即应该满足高分辨率，也应该满足类别定位能力。</p><p>示例图像</p><p><img src="./Grad-CAM/Grad-CAM.png" alt="示例图像"><br><img src="/2018/12/14/Grad-CAM/Grad-CAM.png" title="示例图像"></p><h1 id="4-Approach"><a href="#4-Approach" class="headerlink" title="4. Approach"></a>4. Approach</h1><p><strong>CAM</strong><br>在CAM中，一个全连接层替换成GAP，参见上面的CAM图，则分类任务可以表示成<br>$$y^c = \sum_k w_k^c \frac{1}{Z} \sum_i \sum_j A_{ij}^k$$<br>其中，$y^c$表示分类结果，$w_k^c$表示第k个特征图(kxhxw)对第c个类别的贡献，即全连接层的系数，$Z$表示特征图的大小，$Z=h\cdot w$，$A_{ij}^k$表示第k个特征图。</p><p>则 CAM 的输出图表示为：<br>$$L_{CAM}^c=\sum_k w_k^c A^k$$</p><p><strong>Grad-CAM</strong><br>在Grad-CAM中，权重系数是通过反向传播得到的。<br>$$\alpha_k^c=\frac{1}{Z}\sum_i \sum_j \frac{\partial y^c}{\partial A_{ij}^k}$$</p><p>则Grad-CAM的输出图表示为：<br>$$L_{Grad-CAM}^c=ReLu(\sum_k \alpha_k^c A^k)$$</p><p>可以证明，Grad-CAM与CAM的公式是同一个公式的变形。</p><p><strong>Guided Grad-CAM</strong><br>Guided Grad-CAM 是将 Grad-CAM 与 Guided Backpropagation 得到的输出图简单地点乘，从而获得类区分定位的高分辨率细节图。</p><p><img src="./Grad-CAM/Guided-Grad-CAM.png" alt="Guided-Grad-CAM的网络架构"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM.png" title="Guided-Grad-CAM的网络架构"></p><p>同时作者还分析了CNN分类错误的样本。<br><img src="./Grad-CAM/Guided-Grad-CAM2.png" alt="Guided-Grad-CAM分类错误的样本"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM2.png" title="Guided-Grad-CAM分类错误的样本"></p><h1 id="5-代码"><a href="#5-代码" class="headerlink" title="5. 代码"></a>5. 代码</h1><p>对于<a href="https://github.com/TJJTJJTJJ/pytorch-grad-cam" target="_blank" rel="noopener">pytorch代码</a>进行分析</p><h2 id="5-1-Grad-CAM"><a href="#5-1-Grad-CAM" class="headerlink" title="5.1 Grad-CAM"></a>5.1 Grad-CAM</h2><p><strong>计算Grad-CAM</strong></p><p>cam: (H,W), ~(0,1)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" Class for extracting activations and</span></span><br><span class="line"><span class="string">    registering gradients from targetted intermediate layers """</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    调用方式:</span></span><br><span class="line"><span class="string">    outputs, x = FeatureExtractor(x)</span></span><br><span class="line"><span class="string">    gradients = FeatureExtractor.gradients</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, target_layers)</span>:</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.target_layers = target_layers</span><br><span class="line">        self.gradients = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_gradient</span><span class="params">(self, grad)</span>:</span></span><br><span class="line">        self.gradients.append(grad)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param x: N*C*H*W, a picture</span></span><br><span class="line"><span class="string">        :return: outputs: list, activations layer output, A in equation</span></span><br><span class="line"><span class="string">                 x : feature map, feature of model output n*c*h*w</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        outputs = []</span><br><span class="line">        self.gradients = []</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.model._modules.items():</span><br><span class="line">            x = module(x)</span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">in</span> self.target_layers:</span><br><span class="line">                x.register_hook(self.save_gradient)</span><br><span class="line">                outputs += [x]</span><br><span class="line">        <span class="keyword">return</span> outputs, x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelOutputs</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" Class for making a forward pass, and getting:</span></span><br><span class="line"><span class="string">    1. The network output.</span></span><br><span class="line"><span class="string">    2. Activations from intermeddiate targetted layers.</span></span><br><span class="line"><span class="string">    3. Gradients from intermeddiate targetted layers. """</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    调用方式：</span></span><br><span class="line"><span class="string">    target_activations, output = ModelOutputs(x)</span></span><br><span class="line"><span class="string">    gradients = ModelOutputs.get_gradients()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, target_layers)</span>:</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.feature_extractor = FeatureExtractor(self.model.features, target_layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_gradients</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.feature_extractor.gradients</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param x: N*C*H*W, a picture</span></span><br><span class="line"><span class="string">        :return: target_activations: list, activations layer output, A in equation</span></span><br><span class="line"><span class="string">                 output : tensor, classification output. N*c. y in equation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        target_activations, output = self.feature_extractor(x)</span><br><span class="line">        output = output.view(output.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        output = self.model.classifier(output)</span><br><span class="line">        <span class="keyword">return</span> target_activations, output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradCam</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Class for making Grad-CAM, and getting:</span></span><br><span class="line"><span class="string">    1. Grad-CAM</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    调用方式：</span></span><br><span class="line"><span class="string">    mask=GradCam(input)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, target_layer_names, use_cuda)</span>:</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.model.eval()</span><br><span class="line">        self.cuda = use_cuda</span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            self.model = model.cuda()</span><br><span class="line"></span><br><span class="line">        self.extractor = ModelOutputs(self.model, target_layer_names)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.model(input)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, input, index=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param input: N*C*H*W, a picture</span></span><br><span class="line"><span class="string">        :param index: int</span></span><br><span class="line"><span class="string">        :return: cam: N*C*H*W ~(0,1) L_&#123;Grad-CAM&#125;^c</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            features, output = self.extractor(input.cuda())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            features, output = self.extractor(input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> index == <span class="keyword">None</span>:</span><br><span class="line">            index = np.argmax(output.cpu().data.numpy())</span><br><span class="line"></span><br><span class="line">        one_hot = np.zeros((<span class="number">1</span>, output.size()[<span class="number">-1</span>]), dtype=np.float32)</span><br><span class="line">        one_hot[<span class="number">0</span>][index] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># After test, requires_grad could be False</span></span><br><span class="line">        one_hot = Variable(torch.from_numpy(one_hot), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            one_hot = torch.sum(one_hot.cuda() * output)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            one_hot = torch.sum(one_hot * output)</span><br><span class="line"></span><br><span class="line">        self.model.features.zero_grad()</span><br><span class="line">        self.model.classifier.zero_grad()</span><br><span class="line">        <span class="comment"># After test, requires_grad could be False</span></span><br><span class="line">        one_hot.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        grads_val = self.extractor.get_gradients()[<span class="number">-1</span>].cpu().data.numpy()</span><br><span class="line"></span><br><span class="line">        target = features[<span class="number">-1</span>]</span><br><span class="line">        target = target.cpu().data.numpy()[<span class="number">0</span>, :]</span><br><span class="line"></span><br><span class="line">        weights = np.mean(grads_val, axis=(<span class="number">2</span>, <span class="number">3</span>))[<span class="number">0</span>, :]</span><br><span class="line">        cam = np.zeros(target.shape[<span class="number">1</span>:], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, w <span class="keyword">in</span> enumerate(weights):</span><br><span class="line">            cam += w * target[i, :, :]</span><br><span class="line"></span><br><span class="line">        cam = np.maximum(cam, <span class="number">0</span>)</span><br><span class="line">        cam = cv2.resize(cam, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">        cam = cam - np.min(cam)</span><br><span class="line">        cam = cam / np.max(cam)</span><br><span class="line">        <span class="keyword">return</span> cam</span><br></pre></td></tr></table></figure><p><strong>显示Grad-CAM</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_cam_on_image</span><span class="params">(img, mask)</span>:</span></span><br><span class="line">    heatmap = cv2.applyColorMap(np.uint8(<span class="number">255</span> * mask), cv2.COLORMAP_JET)</span><br><span class="line">    heatmap = np.float32(heatmap) / <span class="number">255</span></span><br><span class="line">    cam = heatmap + np.float32(img)</span><br><span class="line">    cam = cam / np.max(cam)</span><br><span class="line">    cv2.imwrite(<span class="string">"cam.jpg"</span>, np.uint8(<span class="number">255</span> * cam))</span><br></pre></td></tr></table></figure><h2 id="5-2-GuidedBackpropReLUModel"><a href="#5-2-GuidedBackpropReLUModel" class="headerlink" title="5.2 GuidedBackpropReLUModel"></a>5.2 GuidedBackpropReLUModel</h2><p>gb: (C,H,W) 任意值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GuidedBackpropReLU</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        positive_mask = (input &gt; <span class="number">0</span>).type_as(input)</span><br><span class="line">        output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)</span><br><span class="line">        self.save_for_backward(input, output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        input, output = self.saved_tensors</span><br><span class="line">        grad_input = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        positive_mask_1 = (input &gt; <span class="number">0</span>).type_as(grad_output)</span><br><span class="line">        positive_mask_2 = (grad_output &gt; <span class="number">0</span>).type_as(grad_output)</span><br><span class="line">        grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input),</span><br><span class="line">                                   torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output,</span><br><span class="line">                                                 positive_mask_1), positive_mask_2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GuidedBackpropReLUModel</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, use_cuda)</span>:</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.model.eval()</span><br><span class="line">        self.cuda = use_cuda</span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            self.model = model.cuda()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># replace ReLU with GuidedBackpropReLU</span></span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> self.model.features._modules.items():</span><br><span class="line">            <span class="keyword">if</span> module.__class__.__name__ == <span class="string">'ReLU'</span>:</span><br><span class="line">                self.model.features._modules[idx] = GuidedBackpropReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.model(input)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, input, index=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            output = self.forward(input.cuda())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output = self.forward(input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> index == <span class="keyword">None</span>:</span><br><span class="line">            index = np.argmax(output.cpu().data.numpy())</span><br><span class="line"></span><br><span class="line">        one_hot = np.zeros((<span class="number">1</span>, output.size()[<span class="number">-1</span>]), dtype=np.float32)</span><br><span class="line">        one_hot[<span class="number">0</span>][index] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># After test, requires_grad could be False</span></span><br><span class="line">        one_hot = Variable(torch.from_numpy(one_hot), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            one_hot = torch.sum(one_hot.cuda() * output)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            one_hot = torch.sum(one_hot * output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.model.features.zero_grad()</span></span><br><span class="line">        <span class="comment"># self.model.classifier.zero_grad()</span></span><br><span class="line">        one_hot.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        output = input.grad.cpu().data.numpy()</span><br><span class="line">        output = output[<span class="number">0</span>, :, :, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h2 id="5-3-Guided-Grad-CAM"><a href="#5-3-Guided-Grad-CAM" class="headerlink" title="5.3 Guided Grad-CAM"></a>5.3 Guided Grad-CAM</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cam_mask = np.zeros(gb.shape)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, gb.shape[<span class="number">0</span>]):</span><br><span class="line">    cam_mask[i, :, :] = mask</span><br><span class="line"></span><br><span class="line">cam_gb = np.multiply(cam_mask, gb)</span><br><span class="line">utils.save_image(torch.from_numpy(cam_gb), <span class="string">'cam_gb.jpg'</span>)</span><br></pre></td></tr></table></figure><h1 id="6-效果显示"><a href="#6-效果显示" class="headerlink" title="6. 效果显示"></a>6. 效果显示</h1><p>原图</p><p><img src="./Grad-CAM/Guided-Grad-CAM3.png" alt="原图"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM3.png" title="原图"></p><p>Grad-CAM</p><p><img src="./Grad-CAM/Guided-Grad-CAM4.jpg" alt="Grad-CAM"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM4.jpg" title="Grad-CAM"></p><p>Guided-Backpropagation</p><p><img src="./Grad-CAM/Guided-Grad-CAM5.jpg" alt="Guided-Backpropagation"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM5.jpg" title="Guided-Backpropagation"></p><p>Guided Grad-CAM</p><p><img src="./Grad-CAM/Guided-Grad-CAM6.jpg" alt="Guided Grad-CAM"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM6.jpg" title="Guided Grad-CAM"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1610.02391.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="deep learning" scheme="http://yoursite.com/categories/deep-learning/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="CNN" scheme="http://yoursite.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>RotationNet-paper</title>
    <link href="http://yoursite.com/2018/12/11/RotationNet-paper/"/>
    <id>http://yoursite.com/2018/12/11/RotationNet-paper/</id>
    <published>2018-12-11T08:18:45.000Z</published>
    <updated>2018-12-13T06:26:35.932Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-RotationNet-paper"><a href="#1-RotationNet-paper" class="headerlink" title="1. RotationNet-paper"></a>1. RotationNet-paper</h1><a id="more"></a><p>paper: <a href="https://arxiv.org/pdf/1603.06208.pdf" target="_blank" rel="noopener">RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints(CVPR2018)</a></p><p><a href="https://github.com/kanezaki" target="_blank" rel="noopener">Asako Kanezaki</a>, Yasuyuki Matsushita2, and Yoshifumi Nishida.</p><p>Asako Kanezaki 是日本东京研究所专门研究3D的一个老师。</p><p>code-pytorch: <a href="https://github.com/kanezaki/pytorch-rotationnet" target="_blank" rel="noopener">https://github.com/kanezaki/pytorch-rotationnet</a><br>code-caffe: <a href="https://github.com/kanezaki/rotationnet" target="_blank" rel="noopener">https://github.com/kanezaki/rotationnet</a><br>project: <a href="https://kanezaki.github.io/rotationnet/" target="_blank" rel="noopener">https://kanezaki.github.io/rotationnet/</a></p><p>MIMO data: <a href="https://github.com/kanezaki/MIRO" target="_blank" rel="noopener">https://github.com/kanezaki/MIRO</a></p><p>作者是使用caffe版本提交的论文，我也只是看了看代码，作为理解作者论文的辅助，实际没有跑过代码。</p><p>这篇博客以代码和论文混杂，因为是借助代码理解论文的，又因为不主要做这个方向，所以并没有在意精度什么的。</p><h2 id="1-1-出发点"><a href="#1-1-出发点" class="headerlink" title="1.1 出发点"></a>1.1 出发点</h2><p>作者不仅想要预测出图片的类别label，还想预测出图片的view-points.</p><p>我觉得作者的创新点在于对view的状态顺序编码成view-rotation，限定了view的取值空间，使预测的结果变成了哪种view-rotaion的view准确率高。</p><p>因为在一般情况下，想到的是直接预测view，而不是view-rotation.</p><h2 id="1-2-网络架构"><a href="#1-2-网络架构" class="headerlink" title="1.2 网络架构"></a>1.2 网络架构</h2><p><img src="./RotationNet-paper/RotationNet.png" alt="RotationNet的网络架构"><br><img src="/2018/12/11/RotationNet-paper/RotationNet.png" title="RotationNet的网络架构"></p><h3 id="1-2-1-训练过程："><a href="#1-2-1-训练过程：" class="headerlink" title="1.2.1 训练过程："></a>1.2.1 训练过程：</h3><p>以MIRO数据集、case=3为例，nview=160，vcand=(16, 160),view-rotation=16，num-classes=12.</p><p>这里的view-roration，我的理解是view的排列方式，但是还是不太顺。</p><p>输入的图片个数batch-size必须是nview的倍数，以输入一个样本的160个角度的图片为例，即batch-size=160，nsamp=1，不影响后续的分析，因为每个样本没有任何关系。</p><p>输出是output=batch-size x ((num_classes+1) * nview)= 160 x (13 x 160). 可以理解成对每一个图片，输出网络架构的一行，可以理解成160张图片在160个view下属于13个类的概率。</p><p><img src="./RotationNet-paper/RotationNet1.png" alt="模型的输出"><br><img src="/2018/12/11/RotationNet-paper/RotationNet1.png" title="模型的输出"></p><p>预测view rotation: 利用下面的预测view公式，求log并相减得到output- = (160x160) x 12 x 1，可以理解成一个矩阵： (160x160) x 12，每行表示当前图片当前view下的属于各类的概率，第一个160行表示第一张图片在160个view下的概率分布，第二个160行表示第二张图片在160个view下的概率分布。scores = (16 x 12 x 1)。scores可以理解成当前view-rotation下这160张图片一起属于各类的概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">output = model(input_var)</span><br><span class="line">num_classes = int( output.size( <span class="number">1</span> ) / nview ) - <span class="number">1</span></span><br><span class="line">output = output.view( <span class="number">-1</span>, num_classes + <span class="number">1</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute scores and decide target labels</span></span><br><span class="line">output_ = torch.nn.functional.log_softmax( output )</span><br><span class="line">output_ = output_[ :, :<span class="number">-1</span> ] - torch.t( output_[ :, <span class="number">-1</span> ].repeat( <span class="number">1</span>, output_.size(<span class="number">1</span>)<span class="number">-1</span> ).view( output_.size(<span class="number">1</span>)<span class="number">-1</span>, <span class="number">-1</span> ) )</span><br><span class="line">output_ = output_.view( <span class="number">-1</span>, nview * nview, num_classes )</span><br><span class="line">output_ = output_.data.cpu().numpy()</span><br><span class="line">output_ = output_.transpose( <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span> )</span><br><span class="line"></span><br><span class="line">scores = np.zeros( ( vcand.shape[ <span class="number">0</span> ], num_classes, nsamp ) )</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(vcand.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(vcand.shape[<span class="number">1</span>]):</span><br><span class="line">        scores[ j ] = scores[ j ] + output_[ vcand[ j ][ k ] * nview + k ]</span><br></pre></td></tr></table></figure><p>生成动态真值target-：已知这160张图片的真值target[ n * nview ]，假设是第3类，j-max表示第j-max个view-rotation下，预测为第3类的概率最大，继而生成动态真值target-=(target.size(0) x nview)=160 x 160=25600，可以理解成160张图片在160个view下的真值，在j-max个view-rotation对应的view设置为类别3，其余的设置为13.</p><p><img src="./RotationNet-paper/RotationNet2.png" alt="生成动态真值"><br><img src="/2018/12/11/RotationNet-paper/RotationNet2.png" title="生成动态真值"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">target_ = torch.LongTensor( target.size(<span class="number">0</span>) * nview )</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range( nsamp ):</span><br><span class="line">    j_max = np.argmax( scores[ :, target[ n * nview ], n ] )</span><br><span class="line">    <span class="comment"># assign target labels</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(vcand.shape[<span class="number">1</span>]):</span><br><span class="line">        target_[ n * nview * nview + vcand[ j_max ][ k ] * nview + k ] = target[ n * nview ]</span><br></pre></td></tr></table></figure><h3 id="1-2-2-验证过程"><a href="#1-2-2-验证过程" class="headerlink" title="1.2.2 验证过程"></a>1.2.2 验证过程</h3><p>与训练类似，可以得到output- = (160x160) x 12 x 1，可以理解成一个矩阵： (160x160) x 12，每行表示当前图片当前view下的属于各类的概率，第一个160行表示第一张图片在160个view下的概率分布，第二个160行表示第二张图片在160个view下的概率分布。scores = (16 x 12 x 1)。scores可以理解成每个view-rotation下这160张图片一起属于各类的概率。</p><p>j-max表示在第j-max个view-rotation下，scores可以找到最大概率。</p><p>output[n] 表示每连续的160张图片一起属于某类的概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">output = model(input_var)</span><br><span class="line">num_classes = int( output.size( <span class="number">1</span> ) / nview ) - <span class="number">1</span></span><br><span class="line">output = output.view( <span class="number">-1</span>, num_classes + <span class="number">1</span> )</span><br><span class="line">output = torch.nn.functional.log_softmax( output )</span><br><span class="line">output = output[ :, :<span class="number">-1</span> ] - torch.t( output[ :, <span class="number">-1</span> ].repeat( <span class="number">1</span>, output.size(<span class="number">1</span>)<span class="number">-1</span> ).view( output.size(<span class="number">1</span>)<span class="number">-1</span>, <span class="number">-1</span> ) )</span><br><span class="line">output = output.view( <span class="number">-1</span>, nview * nview, num_classes )</span><br><span class="line"></span><br><span class="line"><span class="comment"># measure accuracy and record loss</span></span><br><span class="line">prec1, prec5 = my_accuracy(output.data, target, topk=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># def my_accuracy</span></span><br><span class="line"></span><br><span class="line">target = target[<span class="number">0</span>:<span class="number">-1</span>:nview]</span><br><span class="line">batch_size = target.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">num_classes = output_.size(<span class="number">2</span>)</span><br><span class="line">output_ = output_.cpu().numpy()</span><br><span class="line">output_ = output_.transpose( <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span> )</span><br><span class="line">scores = np.zeros( ( vcand.shape[ <span class="number">0</span> ], num_classes, batch_size ) )</span><br><span class="line">output = torch.zeros( ( batch_size, num_classes ) )</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(vcand.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(vcand.shape[<span class="number">1</span>]):</span><br><span class="line">        scores[ j ] = scores[ j ] + output_[ vcand[ j ][ k ] * nview + k ]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range( batch_size ):</span><br><span class="line">    j_max = int( np.argmax( scores[ :, :, n ] ) / scores.shape[ <span class="number">1</span> ] )</span><br><span class="line">    output[ n ] = torch.FloatTensor( scores[ j_max, :, n ] )</span><br><span class="line">output = output.cuda()</span><br></pre></td></tr></table></figure><h3 id="1-2-3-测试过程"><a href="#1-2-3-测试过程" class="headerlink" title="1.2.3 测试过程"></a>1.2.3 测试过程</h3><p>因为caffe代码没有看懂，所以根据作者的论文和代码猜一下，当输入的图片没有160张，假设只有100张图片，那么又该怎么做？</p><p>模型的输出是output=batch-size x ((num_classes+1) * nview)= 100 x (13 x 160)，那么怎么求scores？</p><p>求scores是需要全部view的信息的。这里不会了，尽管已经给出了公式，但是公式只能算出output-，没有score，不会了。</p><p>坐等作者回复。</p><blockquote><p>Our method is available only when the relative poses of test images are known. For example, if you captured three images where the second image is 22.5 degrees forward from the first image and the third image is 45 degrees forward from the second image, then the images should be indexed as (0, 1, 3). Then you would get 3x160x12 output values. An easy way to proceed is to create a 160x160x12 “output2” which has zero values, and then insert the output values as “output2[0] = output[0]; output2[1] = output[1]; output2[3] = output[2];”. (In our paper, we used LSD-SLAM to calculate relative poses of test images.)</p></blockquote><p>根据作者的回复，不难理解，给定的测试图片是需要预先知道测试图片序列的相对位置的。</p><h2 id="1-3-预测公式"><a href="#1-3-预测公式" class="headerlink" title="1.3 预测公式"></a>1.3 预测公式</h2><p>$$\max_{(v_i)_{i=1}^M}\prod_{i=1}^M(\log p_{v_{i},y}^{(i)}-\log p_{v_{i},N+1}^{(i)})$$</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-RotationNet-paper&quot;&gt;&lt;a href=&quot;#1-RotationNet-paper&quot; class=&quot;headerlink&quot; title=&quot;1. RotationNet-paper&quot;&gt;&lt;/a&gt;1. RotationNet-paper&lt;/h1&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="RotationNet" scheme="http://yoursite.com/tags/RotationNet/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-eval</title>
    <link href="http://yoursite.com/2018/12/10/pytorch-eval/"/>
    <id>http://yoursite.com/2018/12/10/pytorch-eval/</id>
    <published>2018-12-10T07:18:25.000Z</published>
    <updated>2018-12-10T07:20:22.017Z</updated>
    
    <content type="html"><![CDATA[<h1 id="eval"><a href="#eval" class="headerlink" title="eval"></a>eval</h1><a id="more"></a><p>pytorch 的eval()只是改变一些模块的状态，并不影响backward过程。</p><p><a href="https://blog.csdn.net/u012436149/article/details/78281553" target="_blank" rel="noopener">https://blog.csdn.net/u012436149/article/details/78281553</a></p><p><a href="https://www.jianshu.com/p/6cb1fd785540" target="_blank" rel="noopener">https://www.jianshu.com/p/6cb1fd785540</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;eval&quot;&gt;&lt;a href=&quot;#eval&quot; class=&quot;headerlink&quot; title=&quot;eval&quot;&gt;&lt;/a&gt;eval&lt;/h1&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>SPGAN-tensorflow</title>
    <link href="http://yoursite.com/2018/12/10/SPGAN-tensorflow/"/>
    <id>http://yoursite.com/2018/12/10/SPGAN-tensorflow/</id>
    <published>2018-12-10T03:07:15.000Z</published>
    <updated>2018-12-10T07:20:51.859Z</updated>
    
    <content type="html"><![CDATA[<p>在阅读SPGAN代码源码的过程中，学习到的关于tensorflow的一些知识。<br><a id="more"></a></p><h1 id="1-tf-ConfigProto"><a href="#1-tf-ConfigProto" class="headerlink" title="1. tf.ConfigProto"></a>1. tf.ConfigProto</h1><p>参考链接:<a href="https://blog.csdn.net/dcrmg/article/details/79091941" target="_blank" rel="noopener">https://blog.csdn.net/dcrmg/article/details/79091941</a></p><p>tf.ConfigProto用于对sessison会话的参数配置。</p><ul><li>log_device_placement=True: 可以获取到 operations 和 Tensor 被指派到哪个设备(几号CPU或几号GPU)上运行,会在终端打印出各项操作是在哪个设备上运行的</li><li>allow_soft_placement=True: 允许tf自动选择一个存在并且可用的设备来运行操作.在tf中，通过命令 “with tf.device(‘/cpu:0’):”,允许手动设置操作运行的设备</li><li>config.gpu_options.allow_growth = True: 动态申请显存</li><li>config.gpu_options.per_process_gpu_memory_fraction = 0.4: 占用40%显存,限制GPU使用率.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto(allow_soft_placement=<span class="keyword">True</span>)</span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line">sess = tf.Session(config=config)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 限制GPU使用率</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.per_process_gpu_memory_fraction = <span class="number">0.4</span>  <span class="comment">#占用40%显存</span></span><br><span class="line">session = tf.Session(config=config)</span><br><span class="line">等同于</span><br><span class="line">gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=<span class="number">0.4</span>)</span><br><span class="line">config=tf.ConfigProto(gpu_options=gpu_options)</span><br><span class="line">session = tf.Session(config=config)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置使用哪块GPU</span></span><br><span class="line">方法一： 在python中设置</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">'0'</span> <span class="comment">#使用 GPU 0</span></span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">'0,1'</span> <span class="comment"># 使用 GPU 0，1</span></span><br><span class="line">方法二： 在执行时设置</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span> python yourcode.py</span><br></pre></td></tr></table></figure><h1 id="2-tf读取数据集图片的方式"><a href="#2-tf读取数据集图片的方式" class="headerlink" title="2. tf读取数据集图片的方式"></a>2. tf读取数据集图片的方式</h1><p>参考链接:</p><p><a href="https://www.jb51.net/article/134550.htm" target="_blank" rel="noopener">https://www.jb51.net/article/134550.htm</a></p><p><a href="https://www.jb51.net/article/134547.htm" target="_blank" rel="noopener">https://www.jb51.net/article/134547.htm</a></p><p>tf的流程是文件系统–&gt;文件名队列–&gt;内存队列</p><p>推荐使用方法一</p><h2 id="方法一：使用WholeFileReader输入queue，decode输出是Tensor，eval后是ndarray"><a href="#方法一：使用WholeFileReader输入queue，decode输出是Tensor，eval后是ndarray" class="headerlink" title="方法一：使用WholeFileReader输入queue，decode输出是Tensor，eval后是ndarray"></a>方法一：使用WholeFileReader输入queue，decode输出是Tensor，eval后是ndarray</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file_name</span><span class="params">(file_dir)</span>:</span>  <span class="comment">#来自//www.jb51.net/article/134543.htm</span></span><br><span class="line">  <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(file_dir): <span class="comment">#模块os中的walk()函数遍历文件夹下所有的文件</span></span><br><span class="line">    print(root) <span class="comment">#当前目录路径</span></span><br><span class="line">    print(dirs) <span class="comment">#当前路径下所有子目录</span></span><br><span class="line">    print(files) <span class="comment">#当前路径下所有非目录子文件</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file_name2</span><span class="params">(file_dir)</span>:</span>  <span class="comment">#特定类型的文件</span></span><br><span class="line">  L=[]  </span><br><span class="line">  <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(file_dir):</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files: </span><br><span class="line">      <span class="keyword">if</span> os.path.splitext(file)[<span class="number">1</span>] == <span class="string">'.jpg'</span>:  </span><br><span class="line">        L.append(os.path.join(root, file))</span><br><span class="line">  <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line">path = file_name2(<span class="string">'test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#以下参考//www.jb51.net/article/134547.htm (十图详解TensorFlow数据读取机制)</span></span><br><span class="line"><span class="comment">#path2 = tf.train.match_filenames_once(path)</span></span><br><span class="line">file_queue = tf.train.string_input_producer(paths, shuffle=<span class="keyword">True</span>, num_epochs=<span class="number">2</span>) <span class="comment">#创建输入队列</span></span><br><span class="line">image_reader = tf.WholeFileReader()</span><br><span class="line">key, image = image_reader.read(file_queue)</span><br><span class="line">image = tf.image.decode_jpeg(image, channerls=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="comment">#  coord = tf.train.Coordinator() #协同启动的线程</span></span><br><span class="line"><span class="comment">#  threads = tf.train.start_queue_runners(sess=sess, coord=coord) #启动线程运行队列</span></span><br><span class="line"><span class="comment">#  coord.request_stop() #停止所有的线程</span></span><br><span class="line"><span class="comment">#  coord.join(threads)</span></span><br><span class="line"></span><br><span class="line">  tf.local_variables_initializer().run()</span><br><span class="line">  threads = tf.train.start_queue_runners(sess=sess)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#print (type(image))</span></span><br><span class="line">  <span class="comment">#print (type(image.eval()))</span></span><br><span class="line">  <span class="comment">#print(image.eval().shape)</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> path+path:</span><br><span class="line">    plt.figure</span><br><span class="line">    plt.imshow(image.eval())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="方法二：使用gfile读图片，decode输出是Tensor，eval后是ndarray"><a href="#方法二：使用gfile读图片，decode输出是Tensor，eval后是ndarray" class="headerlink" title="方法二：使用gfile读图片，decode输出是Tensor，eval后是ndarray"></a>方法二：使用gfile读图片，decode输出是Tensor，eval后是ndarray</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">print(tf.__version__)</span><br><span class="line"></span><br><span class="line">image_raw = tf.gfile.FastGFile(<span class="string">'test/a.jpg'</span>,<span class="string">'rb'</span>).read()  <span class="comment">#bytes</span></span><br><span class="line">img = tf.image.decode_jpeg(image_raw) <span class="comment">#Tensor</span></span><br><span class="line"><span class="comment">#img2 = tf.image.convert_image_dtype(img, dtype = tf.uint8)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(type(image_raw)) <span class="comment"># bytes</span></span><br><span class="line">  print(type(img)) <span class="comment"># Tensor</span></span><br><span class="line">  <span class="comment">#print(type(img2))</span></span><br><span class="line"></span><br><span class="line">  print(type(img.eval())) <span class="comment"># ndarray !!!</span></span><br><span class="line">  print(img.eval().shape)</span><br><span class="line">  print(img.eval().dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  print(type(img2.eval()))</span></span><br><span class="line"><span class="comment">#  print(img2.eval().shape)</span></span><br><span class="line"><span class="comment">#  print(img2.eval().dtype)</span></span><br><span class="line">  plt.figure(<span class="number">1</span>)</span><br><span class="line">  plt.imshow(img.eval())</span><br><span class="line">  plt.show()</span><br></pre></td></tr></table></figure><h2 id="方法三：使用read-file，decode输出是Tensor，eval后是ndarray"><a href="#方法三：使用read-file，decode输出是Tensor，eval后是ndarray" class="headerlink" title="方法三：使用read_file，decode输出是Tensor，eval后是ndarray"></a>方法三：使用read_file，decode输出是Tensor，eval后是ndarray</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">print(tf.__version__)</span><br><span class="line"></span><br><span class="line">image_raw = tf.gfile.FastGFile(<span class="string">'test/a.jpg'</span>,<span class="string">'rb'</span>).read()  <span class="comment">#bytes</span></span><br><span class="line">img = tf.image.decode_jpeg(image_raw) <span class="comment">#Tensor</span></span><br><span class="line"><span class="comment">#img2 = tf.image.convert_image_dtype(img, dtype = tf.uint8)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(type(image_raw)) <span class="comment"># bytes</span></span><br><span class="line">  print(type(img)) <span class="comment"># Tensor</span></span><br><span class="line">  <span class="comment">#print(type(img2))</span></span><br><span class="line"></span><br><span class="line">  print(type(img.eval())) <span class="comment"># ndarray !!!</span></span><br><span class="line">  print(img.eval().shape)</span><br><span class="line">  print(img.eval().dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  print(type(img2.eval()))</span></span><br><span class="line"><span class="comment">#  print(img2.eval().shape)</span></span><br><span class="line"><span class="comment">#  print(img2.eval().dtype)</span></span><br><span class="line">  plt.figure(<span class="number">1</span>)</span><br><span class="line">  plt.imshow(img.eval())</span><br><span class="line">  plt.show()</span><br></pre></td></tr></table></figure><h1 id="3-tf-train-shuffle-batch"><a href="#3-tf-train-shuffle-batch" class="headerlink" title="3. tf.train.shuffle_batch"></a>3. tf.train.shuffle_batch</h1><p>参考链接:</p><p><a href="https://www.jianshu.com/p/9cfe9cadde06" target="_blank" rel="noopener">https://www.jianshu.com/p/9cfe9cadde06</a></p><p><a href="https://blog.csdn.net/ying86615791/article/details/73864381" target="_blank" rel="noopener">https://blog.csdn.net/ying86615791/article/details/73864381</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">img_batch = tf.train.shuffle_batch([img],</span><br><span class="line">batch_size=batch_size, capacity=capacity,</span><br><span class="line">min_after_dequeue=min_after_dequeue,num_threads=num_threads,</span><br><span class="line">allow_smaller_final_batch=allow_smaller_final_batch)</span><br></pre></td></tr></table></figure><p>tf不是像pytorch一样全局打乱，而是每一次在较短的队列中打乱。其中，队列的最长长度是capacity，最短长度是min_after_dequeue。</p><h1 id="4-tf-summary"><a href="#4-tf-summary" class="headerlink" title="4. tf.summary"></a>4. tf.summary</h1><p>参考链接：<a href="https://blog.csdn.net/hongxue8888/article/details/78610305" target="_blank" rel="noopener">https://blog.csdn.net/hongxue8888/article/details/78610305</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">'./summaries/'</span> + dataset + <span class="string">'_spgan'</span> , sess.graph)</span><br></pre></td></tr></table></figure><h1 id="5-tf-train-Saver"><a href="#5-tf-train-Saver" class="headerlink" title="5. tf.train.Saver"></a>5. tf.train.Saver</h1><p>参考链接：<a href="http://www.cnblogs.com/denny402/p/6940134.html" target="_blank" rel="noopener">http://www.cnblogs.com/denny402/p/6940134.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(max_to_keep= <span class="number">30</span>)</span><br></pre></td></tr></table></figure><h1 id="6-saver-restore"><a href="#6-saver-restore" class="headerlink" title="6. saver.restore"></a>6. saver.restore</h1><p>参考链接：<a href="https://blog.csdn.net/changeforeve/article/details/80268522" target="_blank" rel="noopener">https://blog.csdn.net/changeforeve/article/details/80268522</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_checkpoint</span><span class="params">(checkpoint_dir, sess, saver)</span>:</span></span><br><span class="line">    print(<span class="string">" [*] Loading checkpoint..."</span>)</span><br><span class="line">    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)</span><br><span class="line">    print(ckpt)</span><br><span class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)</span><br><span class="line">        ckpt_path = os.path.join(checkpoint_dir, ckpt_name)</span><br><span class="line">        saver.restore(sess, ckpt_path)</span><br><span class="line">        print(<span class="string">" [*] Loading successful!"</span>)</span><br><span class="line">        <span class="keyword">return</span> ckpt_path</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">" [*] No suitable checkpoint!"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure><h1 id="7-tf-train-Coordinator"><a href="#7-tf-train-Coordinator" class="headerlink" title="7. tf.train.Coordinator"></a>7. tf.train.Coordinator</h1><p>参考链接:</p><p><a href="https://blog.csdn.net/weixin_42052460/article/details/80714539" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42052460/article/details/80714539</a></p><p><a href="https://www.jianshu.com/p/d063804fb272" target="_blank" rel="noopener">https://www.jianshu.com/p/d063804fb272</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">coord = tf.train.Coordinator()</span><br><span class="line">threads = tf.train.start_queue_runners(sess=sess, coord=coord)</span><br></pre></td></tr></table></figure><h1 id="8-tf-identity"><a href="#8-tf-identity" class="headerlink" title="8. tf.identity"></a>8. tf.identity</h1><p>参考链接:</p><p><a href="https://stackoverflow.com/questions/34877523/in-tensorflow-what-is-tf-identity-used-for" target="_blank" rel="noopener">https://stackoverflow.com/questions/34877523/in-tensorflow-what-is-tf-identity-used-for</a></p><p><a href="https://blog.csdn.net/hu_guan_jie/article/details/78495297" target="_blank" rel="noopener">https://blog.csdn.net/hu_guan_jie/article/details/78495297</a></p><p>tf.idenity的逻辑就是等于号，区别是前者在计算图上加了个节点，使得可以多个设备之间可以通信，但是等于号为什么不行呢？</p><h1 id="9-tf-reuse"><a href="#9-tf-reuse" class="headerlink" title="9. tf.reuse"></a>9. tf.reuse</h1><p>参考链接：<a href="https://blog.csdn.net/UESTC_C2_403/article/details/72329786" target="_blank" rel="noopener">https://blog.csdn.net/UESTC_C2_403/article/details/72329786</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在阅读SPGAN代码源码的过程中，学习到的关于tensorflow的一些知识。&lt;br&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/tensorflow/"/>
    
    
      <category term="SPGAN" scheme="http://yoursite.com/tags/SPGAN/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>github-markdown-mathjax</title>
    <link href="http://yoursite.com/2018/12/03/github-markdown-mathjax/"/>
    <id>http://yoursite.com/2018/12/03/github-markdown-mathjax/</id>
    <published>2018-12-03T02:59:51.000Z</published>
    <updated>2018-12-03T03:22:05.493Z</updated>
    
    <content type="html"><![CDATA[<p>前言：在github上传自己的论文记录后发现，github上的显示和在vscode上的显示不同，主要体现在公式在github上不能正确地显示，并且换行也不能正确地显示。<br><a id="more"></a></p><p>参考链接：<br><a href="https://blog.csdn.net/phdsky/article/details/81431279" target="_blank" rel="noopener">https://blog.csdn.net/phdsky/article/details/81431279</a></p><p>搜索之后发现github的markdown不支持mathjax的渲染。<br><a href="https://github.com/github/markup/issues/897" target="_blank" rel="noopener">Github issue - github’s markdown mathjax rending</a><br><a href="https://stackoverflow.com/questions/11256433/how-to-show-math-equations-in-general-githubs-markdownnot-githubs-blog" target="_blank" rel="noopener">Stackoverflow - How to show math equations in general github’s markdown</a></p><p>解决方案或者是公式转图片，或者是使用github内嵌的公式编辑器，或者是适用于chrome的github with MathJax插件。</p><p>我最后采用的是github with MathJax插件。<br><a href="https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima" target="_blank" rel="noopener">GitHub with MathJax 插件</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前言：在github上传自己的论文记录后发现，github上的显示和在vscode上的显示不同，主要体现在公式在github上不能正确地显示，并且换行也不能正确地显示。&lt;br&gt;
    
    </summary>
    
      <category term="github-markdown" scheme="http://yoursite.com/categories/github-markdown/"/>
    
    
      <category term="github" scheme="http://yoursite.com/tags/github/"/>
    
      <category term="markdown" scheme="http://yoursite.com/tags/markdown/"/>
    
      <category term="mathjax" scheme="http://yoursite.com/tags/mathjax/"/>
    
  </entry>
  
  <entry>
    <title>markdown-math</title>
    <link href="http://yoursite.com/2018/12/03/markdown-math/"/>
    <id>http://yoursite.com/2018/12/03/markdown-math/</id>
    <published>2018-12-03T02:17:02.000Z</published>
    <updated>2018-12-10T07:44:25.722Z</updated>
    
    <content type="html"><![CDATA[<p>前言: 因为最近经常用到markdown写数学公式，每次都查感觉有点啰嗦，所以做个简单小结，把平常用的做个记录。这个博客根据平时自己常用到的进行动态增加。</p><a id="more"></a><p>参考链接：</p><p><a href="https://wangcong.info/article/MarkdownWithMath.html" target="_blank" rel="noopener">https://wangcong.info/article/MarkdownWithMath.html</a></p><p><a href="https://blog.csdn.net/deepinC/article/details/81103326" target="_blank" rel="noopener">https://blog.csdn.net/deepinC/article/details/81103326</a></p><p><a href="https://blog.csdn.net/HaleyPKU/article/details/80341932" target="_blank" rel="noopener">https://blog.csdn.net/HaleyPKU/article/details/80341932</a></p><p><a href="https://blog.csdn.net/qq_39599067/article/details/81184139?utm_source=blogxgwz6" target="_blank" rel="noopener">https://blog.csdn.net/qq_39599067/article/details/81184139?utm_source=blogxgwz6</a></p><p><a href="https://www.zybuluo.com/fyywy520/note/82980" target="_blank" rel="noopener">https://www.zybuluo.com/fyywy520/note/82980</a></p><h1 id="1-公式使用参考"><a href="#1-公式使用参考" class="headerlink" title="1. 公式使用参考"></a>1. 公式使用参考</h1><h2 id="1-1-插入公式"><a href="#1-1-插入公式" class="headerlink" title="1.1 插入公式"></a>1.1 插入公式</h2><p>插入公式分为行中公式，独立公式和自动编号公式</p><p>1.行中公式 $ a=b $</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> 数学公式 $</span></span><br></pre></td></tr></table></figure><p>2.独立公式 $$ a=b $$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$ 数学公式 $$</span></span><br></pre></td></tr></table></figure><p>3.编号公式<br>$$ a=b \tag {1} $$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$ 数学公式 \tag &#123;1&#125; $$</span></span><br></pre></td></tr></table></figure><p>由公式$(1)$可以得出结论</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由公式$(1)$可以得出结论</span><br></pre></td></tr></table></figure><p>4.自动编号公式<br>自动编号公式在github上显示不出来，原则上是可以的，推荐使用手动编号。<br>$$<br>\begin{equation}<br>x^n+y^n=z^n<br>\label{eq:afa}<br>\end{equation}<br>$$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">数学公式</span><br><span class="line">\label&#123;eq:当前公式名&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure><p>5.自动编号公式的引用方法</p><p>在公式 $$\eqref{eq:wwqr}$$ 中，我们看到了这个被自动编号的公式。<br>貌似没有成功</p><p>6.单个公式换行</p><p>单个公式很长的时候需要换行，但仅允许生成一个编号时，可以用split标签包围公式代码，在需要转行的地方使用\，每行需要使用1个&amp;来标识对齐的位置，结束后可使用\tag{…}标签编号。</p><p>$$<br>\begin{split}<br>a &amp;= b \<br>c &amp;= d \<br>e &amp;= f<br>\end{split}\tag{1.2}<br>$$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">a &amp;= b \\</span><br><span class="line">c &amp;= d \\</span><br><span class="line">e &amp;= f </span><br><span class="line">\end&#123;split&#125;\tag&#123;1.3&#125;</span><br><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br></pre></td></tr></table></figure><p>7.多行的独立公式</p><p>有时候需要罗列多个公式，可以用eqnarray*标签包围公式代码，在需要转行的地方使用\，每行需要使用2个&amp;来标识对齐位置，两个&amp;…&amp;号之间的是公式间对齐的位置，每行公式后可使用\tag{…}标签编号：</p><p>github貌似对于多行公式显示不出来。</p><p>$$\begin{eqnarray<em>}<br>x^n+y^n &amp;=&amp; z^n \tag{1.4} \<br>x+y &amp;=&amp; z \tag{1.5}<br>\end{eqnarray</em>}$$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br><span class="line">\begin&#123;eqnarray*&#125;</span><br><span class="line">x^n+y^n &amp;=&amp; z^n \tag&#123;1.4&#125; \\</span><br><span class="line">x+y &amp;=&amp; z \tag&#123;1.5&#125;</span><br><span class="line">\end&#123;eqnarray*&#125;</span><br><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br></pre></td></tr></table></figure><h2 id="1-2-符号"><a href="#1-2-符号" class="headerlink" title="1.2 符号"></a>1.2 符号</h2><table><thead><tr><th style="text-align:center">输入</th><th style="text-align:center">显示</th><th style="text-align:center">输入</th><th style="text-align:center">显示</th></tr></thead><tbody><tr><td style="text-align:center">x^y</td><td style="text-align:center">$x^y$</td><td style="text-align:center">x_y</td><td style="text-align:center">$x_y$</td></tr><tr><td style="text-align:center">\sideset{\^1_2}{\^3_4}\bigotimes</td><td style="text-align:center">$\sideset{^1_2}{^3_4}\bigotimes$</td><td style="text-align:center">\langle</td><td style="text-align:center">&lt;</td></tr><tr><td style="text-align:center">\lceil</td><td style="text-align:center">$\lceil$</td><td style="text-align:center">\rceil</td><td style="text-align:center">$\rceil$</td></tr><tr><td style="text-align:center">\lfloor</td><td style="text-align:center">$\lfloor$</td><td style="text-align:center">\frac{a}{b}</td><td style="text-align:center">$\frac{a}{b}$</td></tr><tr><td style="text-align:center">\sqrt{2}</td><td style="text-align:center">$\sqrt{2}$</td><td style="text-align:center">\alpha,\gamma</td><td style="text-align:center">$\alpha$ $\gamma$</td></tr><tr><td style="text-align:center">\frac{a}{b}</td><td style="text-align:center">$\frac{a}{b}$</td><td style="text-align:center">\sum_{n=1}^N{3n}</td><td style="text-align:center">$\sum_{n=1}^N{3n}$</td></tr><tr><td style="text-align:center">\prod_{n=1}^N{3n}</td><td style="text-align:center">$\prod_{n=1}^N{3n}$</td><td style="text-align:center">\sqrt[2]{5}</td><td style="text-align:center">$\sqrt[2]{5}$</td></tr><tr><td style="text-align:center">\int^5_1{f(x)}{\rm d}x</td><td style="text-align:center">$\int^5_1{f(x)}{\rm d}x$</td><td style="text-align:center">\iint^5_1{f(x)}{\rm d}x</td><td style="text-align:center">$\iint^5_1{f(x)}{\rm d}x$</td></tr><tr><td style="text-align:center">+\infty</td><td style="text-align:center">$+\infty$</td><td style="text-align:center">-\infty</td><td style="text-align:center">$-\infty$</td></tr><tr><td style="text-align:center">\lim_{n\rightarrow+\infty} n</td><td style="text-align:center">$\lim_{n\rightarrow+\infty} n$</td><td style="text-align:center">\in</td><td style="text-align:center">$\in$</td></tr><tr><td style="text-align:center">\geq\,\leq</td><td style="text-align:center">$\geq,\leq$</td><td style="text-align:center">\subset,\supset</td><td style="text-align:center">$\subset,\supset$</td></tr><tr><td style="text-align:center">\pm,\cdot</td><td style="text-align:center">$\pm,\cdot$</td><td style="text-align:center">\times,\div</td><td style="text-align:center">$\times,\div$</td></tr><tr><td style="text-align:center">\not=,\not&lt;</td><td style="text-align:center">$\not=,\not&lt;$</td><td style="text-align:center">\not\supset</td><td style="text-align:center">$\not\supset$</td></tr><tr><td style="text-align:center">\log_2{18}</td><td style="text-align:center">$\log_2{18}$</td><td style="text-align:center"></td></tr></tbody></table><h2 id="1-3-希腊字母"><a href="#1-3-希腊字母" class="headerlink" title="1.3 希腊字母"></a>1.3 希腊字母</h2><table><thead><tr><th style="text-align:center">输入</th><th style="text-align:center">显示</th><th style="text-align:center">输入</th><th style="text-align:center">显示</th></tr></thead><tbody><tr><td style="text-align:center">\alpha,\beta,\gamma</td><td style="text-align:center">$\alpha,\beta,\gamma$</td><td style="text-align:center">\delta,\epsilon, \varepsilon</td><td style="text-align:center">$\delta,\epsilon, \varepsilon$</td></tr><tr><td style="text-align:center">\theta,\lambda,\mu</td><td style="text-align:center">$\theta,\lambda,\mu$</td><td style="text-align:center">\phi,\varphi,\sigma</td><td style="text-align:center">$\phi,\varphi,\sigma$</td></tr></tbody></table><h2 id="1-4-空心字母与Fraktur字母"><a href="#1-4-空心字母与Fraktur字母" class="headerlink" title="1.4 空心字母与Fraktur字母"></a>1.4 空心字母与Fraktur字母</h2><table><thead><tr><th style="text-align:center">输入</th><th style="text-align:center">显示</th><th style="text-align:center">输入</th><th style="text-align:center">显示</th></tr></thead><tbody><tr><td style="text-align:center">\mathbb{A}</td><td style="text-align:center">$\mathbb{A}$</td><td style="text-align:center">\mathfrak{B}</td><td style="text-align:center">$\mathfrak{B}$</td></tr></tbody></table><h2 id="1-5-分段函数"><a href="#1-5-分段函数" class="headerlink" title="1.5 分段函数"></a>1.5 分段函数</h2><p>$$<br>P_{r-j}=<br> \begin{cases}<br>   0 &amp;\mbox{if $r-j$ is odd}\<br>   r!\,(-1)^{(r-j)/2} &amp;\mbox{if $r-j$ is even}<br>   \end{cases}<br>$$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br><span class="line">P_&#123;r-j&#125;=</span><br><span class="line"> \begin&#123;cases&#125;</span><br><span class="line">   0 &amp;\mbox&#123;if $r-j$ is odd&#125;\\</span><br><span class="line">   r!\,(-1)^&#123;(r-j)/2&#125; &amp;\mbox&#123;if $r-j$ is even&#125;</span><br><span class="line">   \end&#123;cases&#125;</span><br><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br></pre></td></tr></table></figure><h2 id="1-6-多行对齐公式"><a href="#1-6-多行对齐公式" class="headerlink" title="1.6 多行对齐公式"></a>1.6 多行对齐公式</h2><p>$$<br>\begin{align}<br>h(x) =&amp; \frac{1}{\int_xt(x)\mathrm{d}x} \tag{1}\<br>f(x) =&amp; \frac{1}{\int_x\eta(x)\mathrm{d}x}g(x)\tag{2}<br>\end{align}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;align&#125;</span><br><span class="line">h(x) =&amp; \frac&#123;<span class="number">1</span>&#125;&#123;\int_xt(x)\mathrm&#123;d&#125;x&#125; \tag&#123;<span class="number">1</span>&#125;\\</span><br><span class="line">f(x) =&amp; \frac&#123;<span class="number">1</span>&#125;&#123;\int_x\eta(x)\mathrm&#123;d&#125;x&#125;g(x)\tag&#123;<span class="number">2</span>&#125;</span><br><span class="line">\end&#123;align&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前言: 因为最近经常用到markdown写数学公式，每次都查感觉有点啰嗦，所以做个简单小结，把平常用的做个记录。这个博客根据平时自己常用到的进行动态增加。&lt;/p&gt;
    
    </summary>
    
      <category term="markdown" scheme="http://yoursite.com/categories/markdown/"/>
    
    
      <category term="markdown" scheme="http://yoursite.com/tags/markdown/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>person-reid-transfer-learning</title>
    <link href="http://yoursite.com/2018/11/29/person-reid-transfer-learning/"/>
    <id>http://yoursite.com/2018/11/29/person-reid-transfer-learning/</id>
    <published>2018-11-29T15:29:52.000Z</published>
    <updated>2018-12-17T07:54:53.982Z</updated>
    
    <content type="html"><![CDATA[<h1 id="transfer-learning"><a href="#transfer-learning" class="headerlink" title="transfer learning"></a>transfer learning</h1><a id="more"></a><p>这个博客主要是因为最近看了几篇关于无监督迁移学习在行人重识别领域的论文，发现隔了几天，自己对论文就忘记得差不多了，所以对论文的关键内容做个简单记录。</p><p>参考链接: <a href="https://github.com/layumi/DukeMTMC-reID_evaluation/blob/master/State-of-the-art/README.md" target="_blank" rel="noopener">Transfer Learning</a></p><p>因为在某些情况下，图片或者公式无法正常显示，所以，我基本会同步到我的博客<br><a href="https://tjjtjjtjj.github.io/2018/11/29/person-reid-transfer-learning/#more" target="_blank" rel="noopener">https://tjjtjjtjj.github.io/2018/11/29/person-reid-transfer-learning/#more</a></p><p>现有方法在transfer learning方向的性能对比</p><p><img src="./pic/transfer/transfer.png" alt="transfer learning"><br><img src="/2018/11/29/person-reid-transfer-learning/transfer.png" title="transfer learning"></p><hr><h2 id="1-ARN"><a href="#1-ARN" class="headerlink" title="1. ARN"></a>1. ARN</h2><p><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w6/Li_Adaptation_and_Re-Identification_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification</a></p><p>Yu-Jhe Li, Fu-En Yang, Yen-Cheng Liu, Yu-Ying Yeh, Xiaofei Du, and Yu-Chiang Frank Wang, CVPR 2018 Workshop</p><p>这篇论文主要分离了数据集的特有特征和行人特征，从而使不同数据集的行人特征投射到统一特征空间中。</p><p>作者是台湾人，没有公布代码。有其他人复现了<a href="https://github.com/huanghoujing/ARN" target="_blank" rel="noopener">代码</a>，但是效果很差。</p><p>我下一步也会尝试复现一下。</p><h3 id="1-1-网络架构"><a href="#1-1-网络架构" class="headerlink" title="1.1 网络架构"></a>1.1 网络架构</h3><p><img src="./pic/ARN/ARN.png" alt="ARN的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/ARN.png" title="ARN的网络架构"></p><p>根据作者的描述，</p><ul><li>$E_I$是resnet50的前四个layer,输入是3X256X256,输出$X^s$是2048X7X7</li><li>$E_T,E_C,E_S$,是相同的网络架构，来自FCN的三层，通过查阅FCN的网络设置，初步猜想是FCN的conv6，conv7，conv8，相应的Decoder暂时按照反卷积来设置。这一部分还需要参考FCN的网络设置。</li><li>$E_T,E_C,E_S$ conv6:7X7X2048,relu6,drop6(0.5),conv7:1X1X2048,relu6,drop6(0.5),conv8:1X1X2048,至于conv6,7的bn和conv8的bn，relu要不要，还需要实验的验证</li><li>在FCN中，逆卷积的使用方式是 deconv(k=64, s=32, p=0)+crop(offset=19)，参考资料:<a href="https://zhuanlan.zhihu.com/p/22976342?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">FCN学习:Semantic Segmentation</a>,<a href="https://blog.csdn.net/zlrai5895/article/details/80473814" target="_blank" rel="noopener">经典网络复现系列（一）：FCN</a></li><li>反卷积的时候一般都是k=2n, s=n,</li><li>参考FCN和pytorch的入门与实践第六章的生成器，我们的Decoder使用deconv(k=1,s=1), deconv(k=1, s=1), deconv(k=7, s=1)</li><li>encoder和decoder都使用bn和relu</li><li>分类层有dropout</li><li>学习率，$E_I=10^{-7}, E_T E_C E_S D_C = 10^{-3}, C_S = 2*10^{-3}  $，并且在前几个epoch只更新$E_I$</li><li>优化器：SGD</li></ul><h3 id="1-2-损失函数"><a href="#1-2-损失函数" class="headerlink" title="1.2 损失函数"></a>1.2 损失函数</h3><p>分类损失<br>$$L_{class}=-\sum_{i=1}^{N_s}y_i^s.log\hat{y}_i^s \tag {1}$$</p><p>对比损失<br>$$L_{ctrs}=\sum_{i,j}{\lambda}(e_{c,i}^s-e_{c,j}^s)^2+ ({1-\lambda}) [max(0, m-(e_{c,i}^s-e_{c,j}^s))]^2 \tag {2}$$</p><p>重构误差<br>$$ L_{rec} = \sum_{i=1}^{N_s} ||X_i^s-\hat{X_i^s}||^2 + \sum_{i=1}^{N_t} ||X_i^t-\hat{X_i^t}||^2 \tag 3 $$</p><p>差别损失<br>$$ L_{diff} = || {H_c^s}^T H_p^s ||_F^2 + || {H_c^t}^T H_p^t ||_F^2 \tag 4 $$</p><p>总损失<br>$$ L_{total} = L_{class} + \alpha L_{ctrs} + \beta L_{rec} + \gamma L_{diff} \tag {5} $$</p><p>其中<br>$$ \alpha=0.01, \beta= 2.0, \gamma=1500 $$</p><h3 id="1-3-模块分析"><a href="#1-3-模块分析" class="headerlink" title="1.3 模块分析"></a>1.3 模块分析</h3><p>三个模块:</p><ol><li><strong>$ L_{rec} $</strong></li><li><strong>$ L_{class} $和$ L_{ctrs} $</strong></li><li><strong>$ E_T$和$E_S$</strong></li></ol><h4 id="1-3-1-半监督-L-rec"><a href="#1-3-1-半监督-L-rec" class="headerlink" title="1.3.1 半监督$ L_{rec} $"></a>1.3.1 半监督$ L_{rec} $</h4><p>这里不是很懂这个重构误差损失函数的作用，下面的这个解释也不行。重构损失是半监督损失函数。暂时理解成重构损失保证在获取特征的过程中尽可能减少信息损失。或者说，类似PCA，保留主成分，这个主成分只能保证尽可能地把样本分开。至于这个主成分是否重要，是否有利于分类，不得而知。</p><p>参考链接：<a href="https://blog.csdn.net/hijack00/article/details/52238549" target="_blank" rel="noopener">深度学习中的“重构”</a></p><p>作者在这里提示，当只有重构损失函数的时候，应该保持$E_I$不更新，只更新$E_C$.</p><p>S: Market, T: Duke; S: Duke, T: Market</p><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th></tr></thead><tbody><tr><td style="text-align:center">$L_{rec}$</td><td style="text-align:center">44.5</td><td style="text-align:center">20.3</td><td style="text-align:center">31.2</td><td style="text-align:center">18.4</td></tr></tbody></table><h4 id="1-3-2-监督-L-rec-L-class-和-L-ctrs"><a href="#1-3-2-监督-L-rec-L-class-和-L-ctrs" class="headerlink" title="1.3.2 监督$ L_{rec} $, $ L_{class} $和$ L_{ctrs} $"></a>1.3.2 监督$ L_{rec} $, $ L_{class} $和$ L_{ctrs} $</h4><p>半监督和监督</p><p>监督损失使得共享空间捕获到行人语义信息。</p><p>S: Market, T: Duke; S: Duke, T: Market</p><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th></tr></thead><tbody><tr><td style="text-align:center">w/o $ L_{class} $, $ L_{ctrs} $</td><td style="text-align:center">52.2</td><td style="text-align:center">23.7</td><td style="text-align:center">36.7</td><td style="text-align:center">19.6</td></tr><tr><td style="text-align:center">w $ L_{class} $, $ L_{ctrs} $</td><td style="text-align:center">70.3</td><td style="text-align:center">39.4</td><td style="text-align:center">60.2</td><td style="text-align:center">33.4</td></tr><tr><td style="text-align:center">$L_{rec}$</td><td style="text-align:center">44.5</td><td style="text-align:center">20.3</td><td style="text-align:center">31.2</td><td style="text-align:center">18.4</td></tr><tr><td style="text-align:center">$L_{rec}$, $ L_{class} $和$ L_{ctrs} $</td><td style="text-align:center">60.5</td><td style="text-align:center">28.7</td><td style="text-align:center">48.4</td><td style="text-align:center">26.8</td></tr></tbody></table><h4 id="1-3-3-无监督-L-rec-E-T-和-E-S"><a href="#1-3-3-无监督-L-rec-E-T-和-E-S" class="headerlink" title="1.3.3 无监督$ L_{rec} $, $ E_T $和$ E_S $"></a>1.3.3 无监督$ L_{rec} $, $ E_T $和$ E_S $</h4><p>特有特征的提取是为了去除共享空间的噪声。</p><p>假设共享空间存在，且特有特征空间存在，如果没有特有特征的提取，那么得到的行人特征或多或少地都会包含特征空间的基向量。</p><p>当然，这里也隐含了一些假设，共享空间和特有空间一定是线性无关的。空间的基向量是2048维。</p><p>S: Market, T: Duke; S: Duke, T: Market</p><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th></tr></thead><tbody><tr><td style="text-align:center">w/o  $ E_T $, $ E_S $</td><td style="text-align:center">60.5</td><td style="text-align:center">28.7</td><td style="text-align:center">48.4</td><td style="text-align:center">26.8</td></tr><tr><td style="text-align:center">w $ L_{class} $, $ L_{ctrs} $</td><td style="text-align:center">70.3</td><td style="text-align:center">39.4</td><td style="text-align:center">60.2</td><td style="text-align:center">33.4</td></tr><tr><td style="text-align:center">$L_{rec}$</td><td style="text-align:center">44.5</td><td style="text-align:center">20.3</td><td style="text-align:center">31.2</td><td style="text-align:center">18.4</td></tr><tr><td style="text-align:center">$ L_{rec} $, $ E_T $和$ E_S $</td><td style="text-align:center">52.2</td><td style="text-align:center">23.7</td><td style="text-align:center">36.7</td><td style="text-align:center">19.6</td></tr></tbody></table><hr><h2 id="2-HHL"><a href="#2-HHL" class="headerlink" title="2. HHL"></a>2. HHL</h2><p><a href="https://github.com/zhunzhong07/zhunzhong07.github.io/blob/master/paper/HHL.pdf" target="_blank" rel="noopener">Generalizing A Person Retrieval Model Hetero- and Homogeneously</a></p><p>Zhun Zhong, Liang Zheng, Shaozi Li, Yi Yang, ECCV 2018</p><p>code: <a href="https://github.com/zhunzhong07/HHL" target="_blank" rel="noopener">https://github.com/zhunzhong07/HHL</a></p><p>web: <a href="http://zhunzhong.site/paper/HHL.pdf" target="_blank" rel="noopener">http://zhunzhong.site/paper/HHL.pdf</a></p><p>中文: <a href="http://www.cnblogs.com/Thinker-pcw/p/9787440.html" target="_blank" rel="noopener">http://www.cnblogs.com/Thinker-pcw/p/9787440.html</a></p><p>preson-reid中主要面临的问题：</p><ol><li>数据集之间的差异</li><li>数据集内部摄像头的差异</li></ol><p>解决方法：</p><ol><li>相机差异：利用StarGAN进行风格转化</li><li>数据集差异：将源域/目标域图片视为负匹配</li></ol><p>数据集之间的三元组损失有把不同数据集的行人特征映射到同一特征空间的效果。</p><p>创新点在于使用straGAN和复杂的三元组损失。</p><h3 id="2-1-网络架构"><a href="#2-1-网络架构" class="headerlink" title="2.1 网络架构"></a>2.1 网络架构</h3><p><img src="./pic/HHL/HHL.png" alt="HHL的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL.png" title="HHL的网络架构"></p><p>网络的简要介绍</p><ul><li>CNN是resnet50，网络包括两个分支，一个计算源数据集的分类损失，一个计算相似度学习的triplet损失。</li><li>FC-2014的组成：linear(2048，1024)–&gt;bn(1024)–&gt;relu–&gt;dropout(0.5),相当于一个embedding。</li><li>FC-#ID是linear(1024,751), FC-128是linear(1024, 128), 两个分支的具体情况是：</li><li><ul><li>x1–&gt;linear(2048, 1024)–&gt;x2–&gt;bn(1024)–&gt;x3–&gt;relu–&gt;x4–&gt;dropout(0.5)–&gt;x5–&gt;linear(102, 751)–&gt;x6</li></ul></li><li><ul><li>x1–&gt;linear(2048, 1024)–&gt;x2–&gt;bn(1024)–&gt;x3–&gt;relu–&gt;x4–&gt;linear(1024, 128)</li></ul></li><li>网络的triplet损失是Batch Hard Triplet Loss</li><li>网络的输入设置：在每一个batch中，对于分类损失，source domain随机选取batchsize=128张图片，对于triplet损失，source domain随机选取8个人的共batchsize=64张图片，其中连续的8张图片属于同一个人，target domain随机选取batchsize=16个人的共16X9=144张图片，假设这16个人都是不同的人。实验发现，当source domain的分类损失的图片比较少的时候，无法实现预期效果，其他情况下没有测试。当batchsize是这样的配比时，可以达到作者的效果。理由未知．</li><li>starGAN是离线训练</li><li>学习率设置：base：$10^{-1}$，其他：$10^{-2}$，并且每过40个epoch，学习率阶梯性地乘以0.1.一共训练60个epoch就可以达到预期效果，这部分设置和PCB很类似。不知道是经验还是怎么。</li><li>关于StarGAN待自己复现之后再做进一步解释，现在只复现过StyleGAN。</li><li>triplet损失的margin=0.3</li></ul><h3 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2 损失函数"></a>2.2 损失函数</h3><p>分类损失<br>$$L_{cross}=-\sum_{i=1}^{N_s}y_i^s.log\hat{y}_i^s$$</p><p>triplet损失<br>$$L_T=\sum_{x_a, x_p, x_n}[D_{x_a, x_p}+m-D_{x_a, x_n}]_+$$</p><p>相机不变性的triplet损失</p><p>目标域中一张原始图片作为anchor，StarGAN图片为positive，其他图片为negative</p><p>$$L_C=L_T((x_t^i)^{n_t}\bigcup(x_{t^*}^i)^{n_t^*})$$</p><p>域不变性的triplet损失</p><p>源域中一张图片为anchor，同一id的其他图片作为positive，目标域的任一图片为negative</p><p>$$L_D=L_T((x_s^i)^{n_s}\bigcup(x_t^i)^{n_t})$$</p><p>相机不变性和域不变性的triplet损失</p><p>是将相机不变性和域不变性合为一体，源域的positive不变，negative为源域的其他图片和目标域的图片，目标域的positive不变，negative为源域的图片和目标域的其他行人图片</p><p>$$L_{CD}=L_T((x_s^i)^{n_s}\bigcup(x_t^i)^{n_t}\bigcup(x_{t<em>}^i)^{n_t^</em>})$$</p><p>总损失：<br>$$L_{HHL}=L_{cross}+\beta*L_{CD}$$</p><p>其中：<br>$$\beta=0.5$$</p><h3 id="2-3-模块分析"><a href="#2-3-模块分析" class="headerlink" title="2.3 模块分析"></a>2.3 模块分析</h3><ol><li><strong>starGAN</strong></li><li><strong>sample方法</strong></li></ol><h4 id="2-3-1-starGAN"><a href="#2-3-1-starGAN" class="headerlink" title="2.3.1 starGAN"></a>2.3.1 starGAN</h4><p>在源数据集上训练，在目标数据集上测试不同图像增强方法下的图片距离，通过表格可以得出，预训练的模型对于目标数据集的随机翻转等等有很好的鲁棒性，但是，对于不同摄像头的同一个人，其距离还是很大。因此，利用StarGAN和相机不变性的triplet损失来减少由于摄像头带来的偏差。</p><table><thead><tr><th style="text-align:center">Source</th><th style="text-align:center">Target</th><th style="text-align:center">Random Crop</th><th style="text-align:center">Random Flip</th><th style="text-align:center">CamStyle Transfer</th></tr></thead><tbody><tr><td style="text-align:center">Duke</td><td style="text-align:center">Market</td><td style="text-align:center">0.049</td><td style="text-align:center">0.034</td><td style="text-align:center">0.485</td></tr><tr><td style="text-align:center">Market</td><td style="text-align:center">Duke</td><td style="text-align:center">0.059</td><td style="text-align:center">0.044</td><td style="text-align:center">0.614</td></tr></tbody></table><h4 id="2-3-2-sample方法"><a href="#2-3-2-sample方法" class="headerlink" title="2.3.2 sample方法"></a>2.3.2 sample方法</h4><p>对于目标域的取样方法，对比了三种方法的性能，分别是随机取样、聚类取样、有监督取样，通过下图可以看出，这三种方法的性能是一样的，最后，作者给的代码是随机取样。</p><p><img src="./pic/HHL/HHL2.png" alt="sample"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL2.png" title="sample"></p><h3 id="2-4-实验设置"><a href="#2-4-实验设置" class="headerlink" title="2.4 实验设置"></a>2.4 实验设置</h3><h4 id="2-4-1-Camera-style-transfer-model：StarGAN"><a href="#2-4-1-Camera-style-transfer-model：StarGAN" class="headerlink" title="2.4.1 Camera style transfer model：StarGAN"></a>2.4.1 Camera style transfer model：StarGAN</h4><p>使用StarGAN进行对于摄像头风格转化。</p><ul><li>2 conv + 6 residual + 2 transposed</li><li>input 128X64</li><li>Adam $\beta_1=0.5, \beta_2=0.999$</li><li>数据初始化:随机翻转和随机裁剪</li><li>学习率：前100个epoch为0.0001，后100个epoch线性衰减到0</li></ul><h4 id="2-4-2-Re-ID-model-training"><a href="#2-4-2-Re-ID-model-training" class="headerlink" title="2.4.2 Re-ID model training"></a>2.4.2 Re-ID model training</h4><ul><li>设置可以参考Zhong, Z., Zheng, L., Zheng, Z., Li, S., Yang, Y.: Camera style adaptation for person re-identification</li><li>input 256*128</li><li>数据初始化：随机裁剪和随机翻转</li><li>dropout=0.5</li><li>学习率：新增的层：0.1，base：0.01，每隔40个epoch乘以0.1</li><li>mini-batch：源域上对于IDE为128，对于tripletloss是64.目标域上对于triplet loss是16.</li><li>epoch=60</li><li>测试：2048-dim计算欧式距离</li></ul><h3 id="2-5-超参数设置"><a href="#2-5-超参数设置" class="headerlink" title="2.5 超参数设置"></a>2.5 超参数设置</h3><ul><li>triplet loss的权重$\beta$</li><li>一个batch中目标域上$n_t$的个数</li></ul><h4 id="2-5-1-参数的设置-beta"><a href="#2-5-1-参数的设置-beta" class="headerlink" title="2.5.1 参数的设置$\beta$"></a>2.5.1 参数的设置$\beta$</h4><p><img src="./pic/HHL/HHL3.png" alt="$\beta$参数的设置"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL3.png" title="$\beta$参数的设置"></p><p>$\beta$应该设置成0.4-0.8</p><h4 id="2-5-2-参数的设置-n-t"><a href="#2-5-2-参数的设置-n-t" class="headerlink" title="2.5.2 参数的设置$n_t$"></a>2.5.2 参数的设置$n_t$</h4><p><img src="./pic/HHL/HHL4.png" alt="$n_t$参数的设置"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL4.png" title="$n_t$参数的设置"></p><p>$n_t$在当前设置(源域上对于IDE为128，对于tripletloss是64)下，应该$n_t&gt;16$</p><p>通过上述参数的设置，结合自己实验时的错误，不妨这么理解，在固定mini-batch=128的情况下</p><ul><li>首先引入源域的triplet_loss，并调整batch和$\beta$，使效果达到最优，,batch的选取2倍数的等间隔，$\beta$可以取等间隔，最后batch=64，即128/2=64，$\beta$则可以先固定成某个值.</li><li>然后引入目标域的triplet_loss，并且要先考虑只有目标域的性能，再考虑结合的性能，每次都需要重新考虑$\beta$和batch的大小</li><li>这么一想，这篇论文做的实验还是很多的。</li></ul><h3 id="2-6-实验结果"><a href="#2-6-实验结果" class="headerlink" title="2.6 实验结果"></a>2.6 实验结果</h3><p><img src="./pic/HHL/HHL5.png" alt="实验结果"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL5.png" title="实验结果"></p><p>通过结果我们看出来，其实提升的效果主要来源于$L_C$，说明预训练的模型对于目标域不同摄像头的图片鲁棒性很差。</p><p>是否说明预训练的模型只学习到了源数据集的跨摄像头的不变行人特征，而对于目标域的摄像头下的不同风格很敏感，而对目标域的同一摄像头下的行人特征很鲁棒。</p><p>$L_T$的提升效果很小是否可以说明目标数据集与源数据集的行人特征空间本身就已经很好地重合了，假如tripl_loss真得具有将不同数据集的行人特征映射到同一特征空间的效果的话。</p><p>通过这篇论文，我们能学到的东西很多，比如对比实验，参数设置实验，想法验证实验等等。</p><h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><h4 id="triplet-loss"><a href="#triplet-loss" class="headerlink" title="triplet_loss"></a>triplet_loss</h4><p>发现triplet_loss很厉害的样子，不妨看看是个什么情况。</p><p>参考链接：<br><a href="https://omoindrot.github.io/triplet-loss" target="_blank" rel="noopener">Triplet Loss and Online Triplet Mining in TensorFlow</a></p><p><a href="http://www.itkeyword.com/doc/2025902251705572502/re-id-with-triplet-loss" target="_blank" rel="noopener">Re-ID with Triplet Loss</a></p><p><a href="https://arxiv.org/pdf/1703.07737.pdf" target="_blank" rel="noopener">In Defense of the Triplet Loss for Person Re-Identification</a></p><p><a href="https://github.com/VisualComputingInstitute/triplet-reid" target="_blank" rel="noopener">code</a></p><p><a href="https://omoindrot.github.io/triplet-loss" target="_blank" rel="noopener">Triplet Loss and Online Triplet Mining in TensorFlow</a>这个博客讲述了triplet_loss的起源、发展和具体使用的几种形式。最后的结论是应该使用在线的batch hard策略。</p><p><a href="http://www.itkeyword.com/doc/2025902251705572502/re-id-with-triplet-loss" target="_blank" rel="noopener">Re-ID with Triplet Loss</a>这篇博客则逻辑性地介绍了各种triplet_loss的变体。最后的结论是batch hard+soft margin效果更好。</p><p>也有提及到，triplet_loss总是不如分类损失强。</p><h4 id="下一步工作"><a href="#下一步工作" class="headerlink" title="下一步工作"></a>下一步工作</h4><p>已经理解源代码</p><hr><h2 id="3-SPGAN"><a href="#3-SPGAN" class="headerlink" title="3. SPGAN"></a>3. SPGAN</h2><p><a href="https://arxiv.org/pdf/1711.07027.pdf" target="_blank" rel="noopener">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</a></p><p>Weijian Deng, Liang Zheng, Guoliang Kang, Yi Yang, Qixiang Ye, Jianbin Jiao, CVPR 2018</p><p>这篇论文主要是构建”Learning via Translation”的框架来进行迁移学习，利用SPGAN(CycleGAN+Simaese net)从源数据集迁移到目标数据集，然后在目标数据集上训练。</p><p>论文的重点是怎么改进CycleGAN。</p><p>web:<a href="http://www.sohu.com/a/208231404_642762" target="_blank" rel="noopener">http://www.sohu.com/a/208231404_642762</a></p><p>code:<a href="https://github.com/Simon4Yan/Learning-via-Translation" target="_blank" rel="noopener">https://github.com/Simon4Yan/Learning-via-Translation</a></p><p>CycleGAN</p><p><a href="https://arxiv.org/pdf/1703.10593.pdf" target="_blank" rel="noopener">Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</a></p><p>code:<a href="https://github.com/zhunzhong07/CamStyle" target="_blank" rel="noopener">https://github.com/zhunzhong07/CamStyle</a></p><p>自己对代码的分析<a href="https://tjjtjjtjj.github.io/2018/11/19/cycleGAN/#more" target="_blank" rel="noopener">https://tjjtjjtjj.github.io/2018/11/19/cycleGAN/#more</a></p><h3 id="3-1-前言"><a href="#3-1-前言" class="headerlink" title="3.1 前言"></a>3.1 前言</h3><p>一般的无监督迁移方法都是假设源域和目标域上有相同ID的图片，不太适用于跨数据集的行人重识别。</p><h3 id="3-2-网络架构"><a href="#3-2-网络架构" class="headerlink" title="3.2 网络架构"></a>3.2 网络架构</h3><p>GAN网络</p><p><img src="./pic/SPGAN/SPGAN1.png" alt="SPGAN"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN1.png" title="SPGAN"></p><p>LMP网络</p><p><img src="./pic/SPGAN/SPGAN3.png" alt="LMP"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN3.png" title="LMP"></p><p>行人重识别整体网络</p><p><img src="./pic/SPGAN/SPGAN2.png" alt="SPGAN"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN2.png" title="SPGAN"></p><p>网络的简要介绍</p><ul><li>整理网络由两部分组成，第一部分是SPGAN，第二部分是常见的行人重识别网络的修改版LMP，重点是第一部分。</li><li>整个网络是用Caffe搭建。</li><li>因为自己没有仔细看caffe的代码，后期有需要的还是要看看超参数设置的。</li><li>SPGAN基本沿用了CycleGAN的设置，epoch=5，更多的epoch没有用。</li><li>SPGAN的正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$,负样本是$G(x_S)$和$x_T$或者$F(x_T)$和$x_S$，作者给的代码中用的正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$，负样本是$x_S$和$x_T$。</li><li>SPGAN的训练分为生成器、判别器、SiaNet。</li><li>$L_{ide}$可以保持转换前后图片颜色保持一致。</li><li>LMP网络直接generated domain上训练。</li><li>在论文的tabel2的注释中，可以看到是分成了7份，与PCB的6份差不多。</li></ul><h3 id="3-3-损失函数"><a href="#3-3-损失函数" class="headerlink" title="3.3 损失函数"></a>3.3 损失函数</h3><h4 id="3-3-1-CycleGAN"><a href="#3-3-1-CycleGAN" class="headerlink" title="3.3.1 CycleGAN"></a>3.3.1 CycleGAN</h4><p>$$L_{T_{adv}}(G,D_T,p_x,p_y)=E_{y\sim p_y}[(D_T(y)-1)^2]+E_{x\sim p_x}[(D_T(G(x))-1)^2]$$<br>$$L_{S_{adv}}(F,D_S,p_x,p_y)=E_{x\sim p_x}[(D_S(x)-1)^2]+E_{y\sim p_y}[(D_S(F(y)))^2]$$<br>$$L_{cyc}(G,F)=E_{x\sim p_x}\parallel F(G(x))-x \parallel_1+E_{y\sim p_y}\parallel G(F(y))-y\parallel_1$$<br>$$L_{ide}(G,F,p_x,p_y)=E_{x\sim p_x}\parallel F(x)-x\parallel_1+E_{y\sim p_y}\parallel G(y)-y\parallel_1$$</p><h4 id="3-3-2-SPGAN"><a href="#3-3-2-SPGAN" class="headerlink" title="3.3.2 SPGAN"></a>3.3.2 SPGAN</h4><p>Siameses Net:<br>$$L_{con}(i,x_1,x_2)=(1-i)(max(0,m-d))^2+id^2$$<br>其中，$m\in [0,2]$，$d=1-cos(\theta)\in [0,2]$表示归一化后的欧式距离.正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$,负样本是$G(x_S)$和$x_T$或者$F(x_T)$和$x_S$。</p><p>Overall objective loss:<br>$$L_{sp}=L_{T_{adv}}+L_{S_{adv}}+\lambda_1 L_{cyc}+\lambda_2 L_{ide}+\lambda_3 L_{con}$$<br>其中，$\lambda_1=10，\lambda_2=5，\lambda_3=2, m=2$</p><h4 id="3-3-3-行人重识别网络"><a href="#3-3-3-行人重识别网络" class="headerlink" title="3.3.3 行人重识别网络"></a>3.3.3 行人重识别网络</h4><p>以resnet50为基础网络，和PCB类似，分割成两块。</p><h3 id="3-4-实验设置"><a href="#3-4-实验设置" class="headerlink" title="3.4 实验设置"></a>3.4 实验设置</h3><h4 id="3-4-1-SPGAN"><a href="#3-4-1-SPGAN" class="headerlink" title="3.4.1 SPGAN"></a>3.4.1 SPGAN</h4><p>SPGAN的整体训练过程与CycleGAN基本是一致的，建议先参考CycleGAN，再学习SPGAN。</p><p>$\lambda_1=10，\lambda_2=5，\lambda_3=2, m=2$，学习率为0.0002，batch=1，total_epoch=5</p><p><strong>SiaNet:</strong> </p><p>4个conv+4个max pool+1个FC。</p><p>x(3,256,256)-&gt;conv(3,64,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)</p><p>-&gt;conv(64,128,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)</p><p>-&gt;conv(128,256,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)</p><p>-&gt;conv(256,512,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)(1,1,512)</p><p>-&gt;FC(512, 128)-&gt;leak_relu(0.2)-&gt;dropout(0.5)-&gt;FC(128,64)</p><p>输入预处理：随机左右翻转、resize(286)、crop(256)、img/127.5-1。</p><p>激活函数全部使用leak_relu(0.2)，没有使用bn</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">metric_net</span><span class="params">(img, scope, df_dim=<span class="number">64</span>, reuse=False, train=True)</span>:</span></span><br><span class="line"></span><br><span class="line">    bn = functools.partial(slim.batch_norm, scale=<span class="keyword">True</span>, is_training=train,</span><br><span class="line">                           decay=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, updates_collections=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope + <span class="string">'_discriminator'</span>, reuse=reuse):</span><br><span class="line">        h0 = lrelu(conv(img, df_dim, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h0_conv'</span>))    <span class="comment"># h0 is (128 x 128 x df_dim)</span></span><br><span class="line">        pool1 = Mpool(h0, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        h1 = lrelu(conv(pool1, df_dim * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h1_conv'</span>))  <span class="comment"># h1 is (32 x 32 x df_dim*2)</span></span><br><span class="line">        pool2 = Mpool(h1, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        h2 = lrelu(conv(pool2, df_dim * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h2_conv'</span>))  <span class="comment"># h2 is (8 x 8 x df_dim*4)</span></span><br><span class="line">        pool3 = Mpool(h2, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        h3 = lrelu(conv(pool3, df_dim * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h3_conv'</span>))  <span class="comment"># h3 is (2 x 2 x df_dim*4)</span></span><br><span class="line">        pool4 = Mpool(h3, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        shape = pool4.get_shape()</span><br><span class="line">        flatten_shape = shape[<span class="number">1</span>].value * shape[<span class="number">2</span>].value * shape[<span class="number">3</span>].value</span><br><span class="line">        h3_reshape = tf.reshape(pool4, [<span class="number">-1</span>, flatten_shape], name = <span class="string">'h3_reshape'</span>)</span><br><span class="line"></span><br><span class="line">        fc1 = lrelu(FC(h3_reshape, df_dim*<span class="number">2</span>, scope=<span class="string">'fc1'</span>))</span><br><span class="line">        dropout_fc1 = slim.dropout(fc1, <span class="number">0.5</span>, scope=<span class="string">'dropout_fc1'</span>)  </span><br><span class="line">        net = FC(dropout_fc1, df_dim, scope=<span class="string">'fc2'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#print_activations(net)</span></span><br><span class="line">        <span class="comment">#print_activations(pool4)</span></span><br><span class="line">        <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><h4 id="3-4-2-LMP"><a href="#3-4-2-LMP" class="headerlink" title="3.4.2 LMP"></a>3.4.2 LMP</h4><p>batch_size=16, total_epoch=50, SGD, momentum=0.9, gamma=0.1, lr_ini=0.001, decay to 0.0001 after 40 epochs.</p><p>这部分的设置和IDE基本类似。</p><h3 id="3-5-对比实验"><a href="#3-5-对比实验" class="headerlink" title="3.5 对比实验"></a>3.5 对比实验</h3><h4 id="模块的对比实验"><a href="#模块的对比实验" class="headerlink" title="模块的对比实验"></a>模块的对比实验</h4><p><img src="./pic/SPGAN/SPGAN4.png" alt="对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN4.png" title="对比实验"></p><p><img src="./pic/SPGAN/SPGAN8.png" alt="生成效果"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN8.png" title="生成效果"></p><p>通过对比实验可以看到，以mAP为指标，CycleGAN增加了3个点，SiaNet(m=2)增加了3个点，LMP增加了4个点。说明作者尝试的3个模块都在一定程度上起到了作用。但是个人感觉还是差点什么。比如，为什么会有效？</p><p>假设目标都是为了使源域与目标域的行人特征映射到同一特征空间。这里的CycleGAN做到了这一点。LMP可以认为是加在哪里都有效的一种方式。那SiaNet其实更像是在保证生成的图片不仅要保留源图片的内容，更要保留源图片的行人特征。这种保留是以一种隐空间的形式在保存，而不是明显的分类损失这样子。</p><p>$\lambda_3 $对比实验</p><p><img src="./pic/SPGAN/SPGAN5.png" alt="$\lambda_3 $对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN5.png" title="$\lambda_3 $对比实验"></p><p>pool 和 part的对比实验</p><p><img src="./pic/SPGAN/SPGAN6.png" alt="pool 和 part的对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN6.png" title="pool 和 part的对比实验"></p><p>也就是说，pool的方式和parts的取法是实验得到的，不是凭空想出来的。</p><p>通过上述实验超参数的设置对比实验，与HHL论文比较，都是固定其他，变化一个参数，然后选取最优的参数，是基于局部最优就是全局最优的思想。感觉到作者的实验做得很足。</p><p>不同base model的对比实验</p><p><img src="./pic/SPGAN/SPGAN7.png" alt="不同base model的对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN7.png" title="不同base model的对比实验"></p><h3 id="附录-1"><a href="#附录-1" class="headerlink" title="附录"></a>附录</h3><h4 id="IDE-and-IDE"><a href="#IDE-and-IDE" class="headerlink" title="IDE and $IDE^+$"></a>IDE and $IDE^+$</h4><p><a href="https://arxiv.org/pdf/1604.02531.pdf" target="_blank" rel="noopener">IDE</a></p><p><a href="https://github.com/zhunzhong07/IDE-baseline-Market-1501" target="_blank" rel="noopener">https://github.com/zhunzhong07/IDE-baseline-Market-1501</a></p><blockquote><p>We name the descriptor as ID-discriminative Embedding (IDE).<br>感觉还是没有很好地理解IDE。</p></blockquote><p>对于IDE+没有找到对应的原文，因为不是重点，暂且跳过。</p><p>IDE的pytorch代码</p><p><a href="https://github.com/Simon4Yan/Person_reID_baseline_pytorch" target="_blank" rel="noopener">https://github.com/Simon4Yan/Person_reID_baseline_pytorch</a></p><p>IDE和$IDE^+$的网络模型是一样的：</p><p>resnet50(layer4)+avgpool+Linear(2048,512)+bn1d(512)+LeakReLU(0.1)+Dropout(0.5)+Linear(512, num_class)</p><p>区别在于训练时bn层是否更新：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model.model = resnet50(layer4)+avgpool+Linear(2048,512)+bn1d(512)+LeakReLU(0.1)+Dropout(0.5)</span></span><br><span class="line"><span class="comment"># model.classifier = Linear(512, num_class)</span></span><br><span class="line"><span class="comment"># IDE</span></span><br><span class="line"><span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        model.train(<span class="keyword">True</span>)  <span class="comment"># Set model to training mode</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.train(<span class="keyword">False</span>)  <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"><span class="comment"># IDE+</span></span><br><span class="line"><span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        model.eval()  <span class="comment"># Fix BN of ResNet50</span></span><br><span class="line">        model.model.fc.train(<span class="keyword">True</span>)</span><br><span class="line">        model.classifier.train(<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.train(<span class="keyword">False</span>)  <span class="comment"># Set model to evaluate mode</span></span><br></pre></td></tr></table></figure><h5 id="新增-2018-12-17"><a href="#新增-2018-12-17" class="headerlink" title="新增 2018-12-17"></a>新增 2018-12-17</h5><p>参考论文: <a href="https://arxiv.org/abs/1811.07487" target="_blank" rel="noopener">Re-Identification with Consistent Attentive Siamese Networks</a></p><p>IDE的网络架构<br><img src="./person-reid-transfer-learning/SPGAN9.png" alt="IDE的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN9.png" title="IDE的网络架构"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一般情况下，cut_at_pooling=False，num_features=256, has_embedding为true</span></span><br><span class="line"><span class="comment"># 一般情况下，新增了feat、feat_bn、relu、drop、classifier</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    __factory = &#123;</span><br><span class="line">        <span class="number">18</span>: torchvision.models.resnet18,</span><br><span class="line">        <span class="number">34</span>: torchvision.models.resnet34,</span><br><span class="line">        <span class="number">50</span>: torchvision.models.resnet50,</span><br><span class="line">        <span class="number">101</span>: torchvision.models.resnet101,</span><br><span class="line">        <span class="number">152</span>: torchvision.models.resnet152,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, depth, pretrained=True, cut_at_pooling=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_features=<span class="number">0</span>, norm=False, dropout=<span class="number">0</span>, num_classes=<span class="number">0</span>, triplet_features=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.depth = depth</span><br><span class="line">        self.pretrained = pretrained</span><br><span class="line">        self.cut_at_pooling = cut_at_pooling</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Construct base (pretrained) resnet</span></span><br><span class="line">        <span class="keyword">if</span> depth <span class="keyword">not</span> <span class="keyword">in</span> ResNet.__factory:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">"Unsupported depth:"</span>, depth)</span><br><span class="line">        self.base = ResNet.__factory[depth](pretrained=pretrained)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.cut_at_pooling:</span><br><span class="line">            self.num_features = num_features</span><br><span class="line">            self.norm = norm</span><br><span class="line">            self.dropout = dropout</span><br><span class="line">            self.has_embedding = num_features &gt; <span class="number">0</span></span><br><span class="line">            self.num_classes = num_classes</span><br><span class="line">            self.triplet_features = triplet_features</span><br><span class="line">            out_planes = self.base.fc.in_features</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Append new layers</span></span><br><span class="line">            <span class="keyword">if</span> self.has_embedding:</span><br><span class="line">                self.feat = nn.Linear(out_planes, self.num_features)</span><br><span class="line">                self.feat_bn = nn.BatchNorm1d(self.num_features)</span><br><span class="line">                init.kaiming_normal_(self.feat.weight, mode=<span class="string">'fan_out'</span>)</span><br><span class="line">                init.constant_(self.feat.bias, <span class="number">0</span>)</span><br><span class="line">                init.constant_(self.feat_bn.weight, <span class="number">1</span>)</span><br><span class="line">                init.constant_(self.feat_bn.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Change the num_features to CNN output channels</span></span><br><span class="line">                self.num_features = out_planes</span><br><span class="line">            <span class="keyword">if</span> self.dropout &gt; <span class="number">0</span>:</span><br><span class="line">                self.drop = nn.Dropout(self.dropout)</span><br><span class="line">            <span class="keyword">if</span> self.num_classes &gt; <span class="number">0</span>:</span><br><span class="line">                self.classifier = nn.Linear(self.num_features, self.num_classes)</span><br><span class="line">                init.normal_(self.classifier.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                init.constant_(self.classifier.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> self.triplet_features &gt; <span class="number">0</span>:</span><br><span class="line">                self.triplet = nn.Linear(self.num_features, self.triplet_features)</span><br><span class="line">                init.normal_(self.triplet.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                init.constant_(self.triplet.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.pretrained:</span><br><span class="line">            self.reset_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, output_feature=None)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.base._modules.items():</span><br><span class="line">            <span class="keyword">if</span> name == <span class="string">'avgpool'</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            x = module(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.cut_at_pooling:</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        x = F.avg_pool2d(x, x.size()[<span class="number">2</span>:])</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_feature == <span class="string">'pool5'</span>:</span><br><span class="line">            x = F.normalize(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line">        <span class="keyword">if</span> self.has_embedding:</span><br><span class="line">            x = self.feat(x)</span><br><span class="line">            x = self.feat_bn(x)</span><br><span class="line">        <span class="keyword">if</span> self.norm:</span><br><span class="line">            x = F.normalize(x)</span><br><span class="line">        <span class="keyword">elif</span> self.has_embedding:</span><br><span class="line">            x = F.relu(x)</span><br><span class="line">        <span class="comment"># triplet feature</span></span><br><span class="line">        <span class="keyword">if</span> self.triplet_features &gt; <span class="number">0</span>:</span><br><span class="line">            x_triplet = self.triplet(x)</span><br><span class="line">        <span class="keyword">if</span> self.dropout &gt; <span class="number">0</span>:</span><br><span class="line">            x = self.drop(x)</span><br><span class="line">        <span class="keyword">if</span> self.num_classes &gt; <span class="number">0</span>:</span><br><span class="line">            x_class = self.classifier(x)</span><br><span class="line">        <span class="comment"># two outputs</span></span><br><span class="line">        <span class="keyword">if</span> self.triplet_features &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> x_class, x_triplet</span><br><span class="line">        <span class="keyword">return</span> x_class</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                init.kaiming_normal(m.weight, mode=<span class="string">'fan_out'</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    init.constant(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                init.constant(m.weight, <span class="number">1</span>)</span><br><span class="line">                init.constant(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</span><br><span class="line">                init.normal(m.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    init.constant(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h4 id="Caffe-and-pytorch"><a href="#Caffe-and-pytorch" class="headerlink" title="Caffe and pytorch"></a>Caffe and pytorch</h4><p><a href="https://github.com/Simon4Yan/Learning-via-Translation/issues/1" target="_blank" rel="noopener">Caffe和pytorch中的bn层的计算方式不一样。</a></p><p>在caffe中，bn层在训练时是eval状态，也是只使用Imagenet的mean和variance</p><blockquote><p>The eval mode for BN layer during training, corresponding to Caffe’s batch_norm_param {use_global_stats: true}, means using ImageNet BN mean and variance during training.</p></blockquote><p>在pytorch中，bn层在训练时如果设置成eval装填，才可以达到caffe的精度。</p><h4 id="疑惑"><a href="#疑惑" class="headerlink" title="疑惑"></a>疑惑</h4><p>IDE和IDE+的效果区别为什么会这么大?</p><h3 id="下一步工作-1"><a href="#下一步工作-1" class="headerlink" title="下一步工作"></a>下一步工作</h3><ul><li style="list-style: none"><input type="checkbox" checked> 已经理解源代码</li></ul><p>尝试在pytorch上复现结果，现在根据作者提供的代码，感觉并不是很难。主要是SPGAN。</p><hr><h2 id="4-基于GAN的类似论文"><a href="#4-基于GAN的类似论文" class="headerlink" title="4. 基于GAN的类似论文"></a>4. 基于GAN的类似论文</h2><p>类似的采取GAN做person-reid方向的论文还有好多，上面两篇是现在最新的，下面就简单地介绍几篇类似的文章，其中涉及到的原理和前文提到的GAN的方法类似。</p><h3 id="4-1-PTGAN"><a href="#4-1-PTGAN" class="headerlink" title="4.1 PTGAN"></a>4.1 PTGAN</h3><p><a href="https://arxiv.org/pdf/1711.08565.pdf" target="_blank" rel="noopener">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</a></p><p>Longhui Wei1, Shiliang Zhang1, Wen Gao1, Qi Tian</p><p>这篇论文对Cycle-GAN进行了改进，保留ID信息的损失函数如下：<br>$$L_{ID}=E_{a \sim p_{data}(a)} [||(G(a)-a) \odot M(a)||<em>2] + E</em>{b \sim p_{data}(b)} [||(F(b)-b) \odot M(b)||_2]$$</p><p>其中，$M(b)$表示使用PSPNet分割后的结果。</p><p>转化效果如下图所示</p><p><img src="./pic/PTGAN/PTGAN.png" alt="PTGAN的转化效果"><br><img src="/2018/11/29/person-reid-transfer-learning/PTGAN.png" title="PTGAN的转化效果"></p><p>这里的Cycle-Gan生成图片的效果和SPGAN生成的效果还是有一些区别的，不是很理解。</p><p>其他的不是本次的重点，不做介绍。</p><h3 id="4-2-DCGAN-CNN"><a href="#4-2-DCGAN-CNN" class="headerlink" title="4.2 DCGAN+CNN"></a>4.2 DCGAN+CNN</h3><p><a href="https://arxiv.org/pdf/1701.07717.pdf" target="_blank" rel="noopener">Unlabeled Samples Generated by GAN Improve the Person Re-Identification Baseline in Vitro</a></p><p>Zhedong Zheng Liang Zheng Yi Yang</p><p>这篇论文主要是利用DCGAN生成新的数据集进行数据集扩充。</p><p>网络架构如图所示：</p><p><img src="./pic/DCGAN+CNN/DCGAN+CNN.png" alt="DCGAN+CNN的网络结构"><br><img src="/2018/11/29/person-reid-transfer-learning/DCGAN+CNN.png" title="DCGAN+CNN的网络结构"></p><p>生成效果图</p><p><img src="./pic/DCGAN+CNN/DCGAN+CNN2.png" alt="生成效果图"><br><img src="/2018/11/29/person-reid-transfer-learning/DCGAN+CNN2.png" title="生成效果图"></p><p>生成图片的标签LSRO<br>$$q_{LSR}=\begin{cases} \frac{\epsilon}{K},&amp;k\neq y\\<br>                        1-\epsilon+\frac{\epsilon}{K},&amp;k=y \end{cases}$$<br>$$l_{LSR}=-(1-\epsilon)log(p(y))-\frac{\epsilon}{K}\sum_{k=1}^{K}log(p(k))$$<br>$$q_{LSRO}=\frac{1}{K}$$<br>$$l_{LSRO}=-(1-Z)log(p(y))-\frac{Z}{K}\sum_{k=1}^Klog(p(k))$$<br>其中，真实图片的Z=0，生成图片的Z=1.</p><h2 id="5-MMFA"><a href="#5-MMFA" class="headerlink" title="5. MMFA"></a>5. MMFA</h2><p><a href="https://arxiv.org/pdf/1807.01440.pdf" target="_blank" rel="noopener">Multi-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification</a></p><p>Shan Lin, Haoliang Li, Chang-Tsun Li, Alex Chichung Kot, BMVC 2018</p><h3 id="5-1-前言"><a href="#5-1-前言" class="headerlink" title="5.1 前言"></a>5.1 前言</h3><p>其想法也是将源域与目标域映射到同一特征空间。创新点是：</p><ul><li>利用MMD缩小源域与目标域的分布差异</li><li>考虑了属性</li></ul><p><a href="https://blog.csdn.net/a529975125/article/details/81176029" target="_blank" rel="noopener">MMD的参考代码</a></p><h3 id="5-2-网络架构"><a href="#5-2-网络架构" class="headerlink" title="5.2 网络架构"></a>5.2 网络架构</h3><p><img src="./pic/MMFA/MMFA.png" alt="MMFA的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/MMFA.png" title="MMFA的网络架构"></p><p>网络架构的说明:</p><ul><li>每一个batch中包括$n_s$张源域图片，$n_t$张目标域图片。batch=32</li><li>backbone是resnet50，并且修改resnet50的avg_pool为max_pool</li><li>$H_S$是pool层的输出向量，$H_S^{id}$是ID-FC层的输出相邻，$H_S^{attr_m}$Attr-FC-m的输出向量。</li><li>input (256,128,3)</li><li>FC=fc+bn+dropout(0.5)+leaky ReLU+fc</li><li>SGD:momentum=0.9,weight decay=5x10e-4</li><li>lr=0.01,每20个epoch乘以0.1</li><li>测试使用max pool的2048维向量的欧式距离</li><li><a href="http://www.liangzheng.org/Project/project_reid.html" target="_blank" rel="noopener">Market</a>有27个<a href="https://github.com/vana77/Market-1501_Attribute" target="_blank" rel="noopener">属性</a>，<a href="http://vision.cs.duke.edu/DukeMTMC/" target="_blank" rel="noopener">Duke</a>有23个<a href="https://github.com/vana77/DukeMTMC-attribute" target="_blank" rel="noopener">属性</a></li></ul><h3 id="5-3-损失函数"><a href="#5-3-损失函数" class="headerlink" title="5.3 损失函数"></a>5.3 损失函数</h3><p>Identity Loss:<br>$$L_{id}=-\frac{1}{n_s}\sum_{i=1}^{n_S}log(p_{id}(h_{S,i}^{id},y_{S,i}))$$</p><p>Attribute Loss:<br>$$L_{attr}=-\frac{1}{M}\frac{1}{n_S}\sum_{m=1}^{M}\sum_{i=1}^{n_S}(a_{S,i}^{m}\cdot log(p_{attr}(h_{S,i}^{attr_m}, m)) -\\<br>(1-a_{S,i}^{m})\cdot log(1-p_{attr}(h_{S,i}^{attr_m}, m)))$$</p><p>Attribute Feature Adaptation<br>$$L_{AAL}=\frac{1}{M}\sum_{m=1}^{M}MMD(H_{S}^{attr_m}, H_{T}^{attr_m})^2\\<br>         =\frac{1}{M}\sum_{m=1}^{M}\parallel \frac{1}{n_S}\sum_{i=1}^{n_S}\phi(h_{S,i}^{attr_m}) - \frac{1}{n_T}\sum_{i=1}^{n_T}\phi(h_{T,j}^{attr_m}) \parallel _{H}^2 \\<br>         =\frac{1}{M}\sum_{m=1}^{M}[ \frac{1}{(n_S)^2}\sum_{i=1}^{n_S}\sum_{i’=1}^{n_S}k(h_{S,i}^{attr_m}, h_{S,i’}^{attr_m})\\<br>         +\frac{1}{(n_T)^2}\sum_{i=1}^{n_T}\sum_{i’=1}^{n_T}k(h_{T,i}^{attr_m}, h_{T,i’}^{attr_m})\\<br>         -\frac{2}{n_S\cdot n_T}\sum_{i=1}^{n_S}\sum_{j=1}^{n_T}k(h_{S,i}^{attr_m}, h_{T,j}^{attr_m})  ] $$</p><p>$$k(h_{S,i}^{attr_m}, h_{T,j}^{attr_m})=exp(-\frac{1}{2\alpha}\parallel  h_{S,i}^{attr_m} - h_{T,j}^{attr_m}\parallel ^2)$$<br>$$\alpha=1,5,10$$</p><p>Mid-level Deep Feature Adaptation<br>$$L_{MDAL}=MMD(H_S,H_T)^2$$</p><p>Overall loss<br>$$L_{all}=L_{id}+\lambda_1 L_{attr}+\lambda_2 L_{AAL}+\lambda_3 L_{MDAL}$$<br>$$\lambda_1=0.1,\lambda_2=1,\lambda_3=1$$</p><h3 id="5-4-实验分析"><a href="#5-4-实验分析" class="headerlink" title="5.4 实验分析"></a>5.4 实验分析</h3><h4 id="5-4-1-实验结果"><a href="#5-4-1-实验结果" class="headerlink" title="5.4.1 实验结果"></a>5.4.1 实验结果</h4><p><img src="./pic/MMFA/MMFA2.png" alt="实验结果"><br><img src="/2018/11/29/person-reid-transfer-learning/MMFA2.png" title="实验结果"></p><h3 id="5-4-2-实验模块"><a href="#5-4-2-实验模块" class="headerlink" title="5.4.2 实验模块"></a>5.4.2 实验模块</h3><p>实验模块对比实验结果<br><img src="./pic/MMFA/MMFA3.png" alt="实验模块对比实验结果"><br><img src="/2018/11/29/person-reid-transfer-learning/MMFA3.png" title="实验模块对比实验结果"></p><h3 id="5-5-附录"><a href="#5-5-附录" class="headerlink" title="5.5 附录"></a>5.5 附录</h3><p>通过实验结果可以看出，在MMFA模型中，ID+Mid-level Deep Feature Adaptation的贡献最大。</p><p>下一步可以尝试考虑Mid-level Deep Feature Adaptation。</p><p>作者把avg pool 换成max pool。</p><h2 id="6-TJ-AIDL"><a href="#6-TJ-AIDL" class="headerlink" title="6. TJ-AIDL"></a>6. TJ-AIDL</h2><p><a href="http://www.eecs.qmul.ac.uk/~xiatian/papers/WangEtAl_CVPR2018.pdf" target="_blank" rel="noopener">Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification</a></p><p>Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li, ECCV 2018</p><h3 id="6-1-前言"><a href="#6-1-前言" class="headerlink" title="6.1 前言"></a>6.1 前言</h3><p>这篇论文的创新点在于：</p><ul><li>根据属性和id的关系，提出了Identity Inferred Attribute Space。</li></ul><h3 id="6-2-网络架构"><a href="#6-2-网络架构" class="headerlink" title="6.2 网络架构"></a>6.2 网络架构</h3><p>Attribute-Identity Transferable Joint Learning</p><p><img src="./pic/TJ-AIDL/TJ-AIDL.png" alt="TJ-AIDL的网络架构 "><br><img src="/2018/11/29/person-reid-transfer-learning/TJ-AIDL.png" title="TJ-AIDL的网络架构"></p><p>Unsupervised Target Domain Adaptation</p><p><img src="./pic/TJ-AIDL/TJ-AIDL2.png" alt="IJ-AIDL的部分网络架构详解"><br><img src="/2018/11/29/person-reid-transfer-learning/TJ-AIDL2.png" title="IJ-AIDL的部分网络架构详解"></p><p>网络架构的简要说明：</p><ul><li>(a) Identity Branch</li><li>(b) Attribute Branch</li><li>(c) Identity Inferred Attribute (IIA) space</li><li>训练过程分为两步:</li><li><ul><li>(I) 源域训练: Attribute-Identity Transferable Joint Learning</li></ul></li><li><ul><li>(II) 目标域微调: Unsupervised Target Domain Adaptation</li></ul></li><li>一般情况下Identity Branch和Attribute Branch是共享网络，但是本论文中特意分成两个非共享网络</li><li>重点在于对$e_{IIA}$的处理</li><li>IIA-encoder 是3个fc层，512/128/m，decoder是encoder的镜像。</li><li>基准网络是MobileNet</li><li>Adam优化器，lr=0.002，mementum$\beta_1=0.5, \beta_2=0.999$</li><li>batch size=8<blockquote><p>We started with training the identity branch by 100,000 iterations on the source identity labels and then the whole model by 20,000 iterations for both transferable joint learning on the labelled source data and unsupervised domain adaptation on the unlabelled target data</p></blockquote></li></ul><h3 id="6-3-损失函数"><a href="#6-3-损失函数" class="headerlink" title="6.3 损失函数"></a>6.3 损失函数</h3><p>Identity Branch (a) softmax<br>$$L_{id}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}log(p_{id}(I_i^S,y_i^S)) \tag{1}$$</p><p>Attribute Branch(b) sigmoid<br>$$L_{att}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}\sum_{j=1}^{m}(a_{i,j}log(p_{att}(I_i,j))+(1-a_{i,j})log(1-p_{att}(I_i,j))) \tag{2}$$</p><p>Identity Inferred Attribute (IIA) space (c)<br>$$L_{rec}=\parallel x_{id}-f_{IIA}(x_{id}) \parallel ^2 \tag{3}$$<br>$$L_{ID-transfer}=\parallel e_{IIA}-\tilde{p}_{att} \parallel ^2 \tag{4}$$<br>$$L_{att,IIA}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}\sum_{j=1}^{m}(a_{i,j}log(p_{IIA}(I_i,j))+(1-a_{i,j})log(1-p_{IIA}(I_i,j))) \tag{5}$$<br>$$L_{IIA}=L_{att,IIA}+\lambda_1 L_{rec}+\lambda_2 L_{ID-transfer} \tag{6}$$<br>$$\lambda_1=10, \lambda_2=10$$</p><p>Impact of IIA on Identity and Attribute Branches<br>$$L_{att-total}=L_{att}+\lambda_2 L_{ID-transfer} \tag{7}$$</p><h3 id="6-4-训练与部署流程"><a href="#6-4-训练与部署流程" class="headerlink" title="6.4 训练与部署流程"></a>6.4 训练与部署流程</h3><p><img src="./pic/TJ-AIDL/TJ-AIDL3.png" alt="IJ-AIDL的训练与部署流程"><br></p><h3 id="6-5-模块分析"><a href="#6-5-模块分析" class="headerlink" title="6.5 模块分析"></a>6.5 模块分析</h3><h4 id="6-5-1-ID和Attribute模块分析"><a href="#6-5-1-ID和Attribute模块分析" class="headerlink" title="6.5.1 ID和Attribute模块分析"></a>6.5.1 ID和Attribute模块分析</h4><p><img src="./pic/TJ-AIDL/TJ-AIDL4.png" alt="IJ-AIDL的ID和Attribute模块分析"><br></p><p>通过ID only的mAP和HHL的baseline，可以看出MobileNet和Resnet50对mAP的影响不受很大。</p><p>另外，可以看出，依然是ID占据了很大比重。</p><h4 id="6-5-2-Adapation的作用"><a href="#6-5-2-Adapation的作用" class="headerlink" title="6.5.2 Adapation的作用"></a>6.5.2 Adapation的作用</h4><p><img src="./pic/TJ-AIDL/TJ-AIDL5.png" alt="IJ-AIDL的Adapation的作用"><br><img src="/2018/11/29/person-reid-transfer-learning/TJ-AIDL5.png" title="IJ-AIDL的Adapation的作用"></p><p>从表格中可以看出，Adaptation的作用很小。说明，预训练的模型已经很好地能保持属性的一致性，即不同角度得到的属性是一样的。</p><h3 id="6-6-补充"><a href="#6-6-补充" class="headerlink" title="6.6 补充"></a>6.6 补充</h3><p>还是难以理解作者这么做的出发点，感觉有点凭空就设计出这么多损失函数，可能是哪里还缺点什么东西。</p><p>训练更新的时候方程(7)的出现原因是什么？更新(6)的时候应该就已经对attr进行了影响吧？</p><p>在step(II)中，是怎么更新方程(6)的。</p><p>Identity Inferred Attribute Space的合理性是怎么体现的？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;transfer-learning&quot;&gt;&lt;a href=&quot;#transfer-learning&quot; class=&quot;headerlink&quot; title=&quot;transfer learning&quot;&gt;&lt;/a&gt;transfer learning&lt;/h1&gt;
    
    </summary>
    
      <category term="ind1" scheme="http://yoursite.com/categories/ind1/"/>
    
    
      <category term="person-reid" scheme="http://yoursite.com/tags/person-reid/"/>
    
      <category term="transfer learning" scheme="http://yoursite.com/tags/transfer-learning/"/>
    
  </entry>
  
  <entry>
    <title>cycleGAN</title>
    <link href="http://yoursite.com/2018/11/19/cycleGAN/"/>
    <id>http://yoursite.com/2018/11/19/cycleGAN/</id>
    <published>2018-11-19T01:11:24.000Z</published>
    <updated>2018-11-19T05:44:34.982Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博客主要记录在跟随cycleGAN作者的代码复现学到的东西。<br>title: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(ICCV2017)</p><p>paper: <a href="https://arxiv.org/abs/1703.10593" target="_blank" rel="noopener">https://arxiv.org/abs/1703.10593</a><br>code: <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank" rel="noopener">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a><br>mycode: <a href="https://github.com/TJJTJJTJJ/pytorch_cycleGAN" target="_blank" rel="noopener">https://github.com/TJJTJJTJJ/pytorch_cycleGAN</a><br>cycle_gan的整体框架写得很漂亮，frame可以参考github的frame<br><a id="more"></a></p><h1 id="1-动态导入模块以及文件内的类"><a href="#1-动态导入模块以及文件内的类" class="headerlink" title="1.动态导入模块以及文件内的类"></a>1.动态导入模块以及文件内的类</h1><p>类似这种文件结构<br>.models<br>|– <strong>init</strong>.py<br>|– base_model.py<br>|– cycle_gan_model.py<br>|– networks.py<br>|– pix2pix_model.py<br>`– test_model.py</p><p>在<strong>init</strong>.py这样写两个函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_model_using_name</span><span class="params">(model_name)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    根据model_name导入具体模型'models/model_name_model.py'</span></span><br><span class="line"><span class="string">    :param model_name: eg. cycle_gan</span></span><br><span class="line"><span class="string">    :return: mdoel class eg.cycle_gan_model.CycleGANModle</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># step1 import 'models/model_name_model'</span></span><br><span class="line">    model_filename = <span class="string">'models.'</span>+model_name+<span class="string">'_model'</span></span><br><span class="line">    modellib = importlib.import_module(model_filename)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># step2 get model_name</span></span><br><span class="line">    model = <span class="keyword">None</span></span><br><span class="line">    target_model_name = model_name.replace(<span class="string">'_'</span>,<span class="string">''</span>)+<span class="string">'model'</span></span><br><span class="line">    <span class="keyword">for</span> name, cls <span class="keyword">in</span> modellib.__dict__.items():</span><br><span class="line">        <span class="keyword">if</span> name.lower() == target_model_name.lower() \</span><br><span class="line">                <span class="keyword">and</span> issubclass(cls, BaseModel):</span><br><span class="line">            model = cls</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> model <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        print_str = <span class="string">"In &#123;model_filename&#125;.py, there should be a subclass of BaseModel with class name "</span> \</span><br><span class="line">              <span class="string">"that matches &#123;target_model_name&#125; in lowercase."</span>.format(model_filename=model_filename, \</span><br><span class="line">                                                                      target_model_name=target_model_name)</span><br><span class="line">        print(print_str)</span><br><span class="line">        exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">modellib.__dict__ == vars(modellib)</span><br><span class="line">vars().keys() == dir()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line">modellib = importlib.import_module(model_filename)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> dir(modellib):</span><br><span class="line">    print(k)</span><br><span class="line">CycleGANModel</span><br><span class="line">__builtins__</span><br><span class="line">__cached__</span><br><span class="line">__doc__</span><br><span class="line">__file__</span><br><span class="line">__loader__</span><br><span class="line">__name__</span><br><span class="line">__package__</span><br><span class="line">__spec__</span><br><span class="line"></span><br><span class="line">print(modellib.__dict__)</span><br><span class="line">&#123;<span class="string">'__name__'</span>: <span class="string">'cycle_gan_model'</span>,</span><br><span class="line">...</span><br><span class="line"><span class="string">'CycleGANModel'</span>: cycle_gan_model.CycleGANModel&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">exit(<span class="number">0</span>)无错误退出</span><br><span class="line">exit(<span class="number">1</span>)有错误退出</span><br></pre></td></tr></table></figure><h1 id="2-学习率直线下降"><a href="#2-学习率直线下降" class="headerlink" title="2.学习率直线下降"></a>2.学习率直线下降</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.epoch_count, opt.niter + opt.niter_decay + <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lambda_rule</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    lr_l = <span class="number">1.0</span> - max(<span class="number">0</span>, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> lr_l</span><br><span class="line">scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)</span><br></pre></td></tr></table></figure><h1 id="3-NotImplemented-amp-amp-NotImplementedError"><a href="#3-NotImplemented-amp-amp-NotImplementedError" class="headerlink" title="3.NotImplemented &amp;&amp; NotImplementedError"></a>3.NotImplemented &amp;&amp; NotImplementedError</h1><p>参考:<br><a href="http://www.php.cn/python-tutorials-160083.html" target="_blank" rel="noopener">http://www.php.cn/python-tutorials-160083.html</a><br><a href="https://stackoverflow.com/questions/1062096/python-notimplemented-constant" target="_blank" rel="noopener">https://stackoverflow.com/questions/1062096/python-notimplemented-constant</a></p><p>return NotImplemented<br>raise NotImplementedError(‘initialization method [%s] is not implemented’ % init_type)</p><h1 id="4-parser的修改"><a href="#4-parser的修改" class="headerlink" title="4.parser的修改"></a>4.parser的修改</h1><p>这里既有外界传入的参数,也有自己的参数isTrain,在主函数里调用的时候调用方式是一致的,只是一个可以通过外界传参,一个不能通过外界传参</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainOptions</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(self)</span>:</span></span><br><span class="line">        parser = argparse.ArgumentParser(</span><br><span class="line">                formatter_class=argparse.ArgumentDefaultsHelpFormatter</span><br><span class="line">            )</span><br><span class="line">        parser.add_argument(<span class="string">'--batch_size'</span>, type=int, default=<span class="number">1</span>, help=<span class="string">'input batch size'</span>)</span><br><span class="line">        parser.set_defaults(dataset_mode=<span class="string">'single'</span>)</span><br><span class="line">        opt, _ = parser.parse_known_args()</span><br><span class="line"></span><br><span class="line">        self.isTrain = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">return</span> opt</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self)</span>:</span></span><br><span class="line">        opt = self.initialize()</span><br><span class="line">        opt.isTrain = self.isTrain</span><br><span class="line"></span><br><span class="line">opt = TrainOptions().parse()</span><br></pre></td></tr></table></figure><h1 id="5-eval-和test-函数的结合"><a href="#5-eval-和test-函数的结合" class="headerlink" title="5.eval()和test()函数的结合"></a>5.eval()和test()函数的结合</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    make models eval mode during test time</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> self.model_names:</span><br><span class="line">        <span class="keyword">if</span> isinstance(name, str):</span><br><span class="line">            net = getattr(self, <span class="string">'net'</span>+name)</span><br><span class="line">            net.eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    don't need backprop during test time</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        self.forward()</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">model.test()</span><br></pre></td></tr></table></figure><h1 id="6-多GPU"><a href="#6-多GPU" class="headerlink" title="6.多GPU"></a>6.多GPU</h1><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">modelb = torch.nn.DataParallel(modela, device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="comment"># save</span></span><br><span class="line">torch.save(modelb.module.cpu().state_dict(),path)</span><br><span class="line">modelb.cuda(gpu_ids[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># load</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_networks</span><span class="params">(self, epoch)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> self.model_names:</span><br><span class="line">        <span class="keyword">if</span> isinstance(name, str):</span><br><span class="line">            load_filename = <span class="string">'%s_net_%s.pth'</span> % (epoch, name)</span><br><span class="line">            load_path = os.path.join(self.save_dir, load_filename)</span><br><span class="line">            net = getattr(self, <span class="string">'net'</span> + name)</span><br><span class="line">            <span class="keyword">if</span> isinstance(net, torch.nn.DataParallel):</span><br><span class="line">                net = net.module</span><br><span class="line">            print(<span class="string">'loading the model from %s'</span> % load_path)</span><br><span class="line">            <span class="comment"># if you are using PyTorch newer than 0.4 (e.g., built from</span></span><br><span class="line">            <span class="comment"># GitHub source), you can remove str() on self.device</span></span><br><span class="line">            state_dict = torch.load(load_path, map_location=str(self.device))</span><br><span class="line">            <span class="keyword">if</span> hasattr(state_dict, <span class="string">'_metadata'</span>):</span><br><span class="line">                <span class="keyword">del</span> state_dict._metadata</span><br><span class="line"></span><br><span class="line">            <span class="comment"># patch InstanceNorm checkpoints prior to 0.4</span></span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> list(state_dict.keys()):  <span class="comment"># need to copy keys here because we mutate in loop</span></span><br><span class="line">                self.__patch_instance_norm_state_dict(state_dict, net, key.split(<span class="string">'.'</span>))</span><br><span class="line">            net.load_state_dict(state_dict)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">torch.nn.DataParallel加载预训练模型</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelA</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ModelA, self).__init__()</span><br><span class="line">        self.base1 = torch.nn.Conv2d(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">aa = ModelA()</span><br><span class="line">bb = torch.nn.DataParallel(aa, device_ids=[<span class="number">0</span>])</span><br><span class="line">bb.module.load_state_dict(torch.load(<span class="string">'aa.pth'</span>))</span><br></pre></td></tr></table></figure><p>对于单gpu和Module<br>对于普通的model.cuda,在保存模型会自动变成cpu,需要再次cuda一次<br>对于DataParallel,在保存模型会自动变成cpu,需要再次cuda一次<br>通过源码可以得知,DataParallel的device_ids初始化就已经确定,所以不用担心cuda到第一个GPU上而导致DataParallel忘记自己可以复制到哪些GPU上,会自动复制的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelA</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ModelA, self).__init__()</span><br><span class="line">        self.base = torch.nn.Conv2d(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">aa = ModelA()</span><br><span class="line">print(aa)</span><br><span class="line">ModelA(</span><br><span class="line">  (base): Conv2d(<span class="number">2</span>, <span class="number">2</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">)</span><br><span class="line">print(aa.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]])), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>]))])</span><br><span class="line">aa.cuda()</span><br><span class="line">print(aa.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]], device=<span class="string">'cuda:0'</span>)), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>], device=<span class="string">'cuda:0'</span>))]</span><br><span class="line"></span><br><span class="line">print(aa.cpu().state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.1570</span>, <span class="number">-0.2992</span>],</span><br><span class="line">                        [<span class="number">-0.2927</span>, <span class="number">-0.2748</span>]],</span><br><span class="line">                       [[<span class="number">-0.0097</span>,  <span class="number">0.0346</span>],</span><br><span class="line">                        [<span class="number">-0.3125</span>,  <span class="number">0.2615</span>]]],</span><br><span class="line">                      [[[<span class="number">-0.2506</span>, <span class="number">-0.2632</span>],</span><br><span class="line">                        [ <span class="number">0.1302</span>, <span class="number">-0.2223</span>]],</span><br><span class="line">                       [[ <span class="number">0.1422</span>,  <span class="number">0.0427</span>],</span><br><span class="line">                        [ <span class="number">0.3453</span>,  <span class="number">0.0219</span>]]]])),</span><br><span class="line">             (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1974</span>, <span class="number">-0.1549</span>]))])</span><br><span class="line"></span><br><span class="line">print(aa.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.1570</span>, <span class="number">-0.2992</span>],</span><br><span class="line">          [<span class="number">-0.2927</span>, <span class="number">-0.2748</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">-0.0097</span>,  <span class="number">0.0346</span>],</span><br><span class="line">          [<span class="number">-0.3125</span>,  <span class="number">0.2615</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2506</span>, <span class="number">-0.2632</span>],</span><br><span class="line">          [ <span class="number">0.1302</span>, <span class="number">-0.2223</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.1422</span>,  <span class="number">0.0427</span>],</span><br><span class="line">          [ <span class="number">0.3453</span>,  <span class="number">0.0219</span>]]]])), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1974</span>, <span class="number">-0.1549</span>]))])</span><br><span class="line"></span><br><span class="line">bb = torch.nn.DataParallel(aa, device_ids=[<span class="number">0</span>])</span><br><span class="line">print(bb)</span><br><span class="line">DataParallel(</span><br><span class="line">  (module): ModelA(</span><br><span class="line">    (base): Conv2d(<span class="number">2</span>, <span class="number">2</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">print(bb.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'module.base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]], device=<span class="string">'cuda:0'</span>)), (<span class="string">'module.base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>], device=<span class="string">'cuda:0'</span>))])</span><br><span class="line">print(bb.module.cpu().state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.1570</span>, <span class="number">-0.2992</span>],</span><br><span class="line">                        [<span class="number">-0.2927</span>, <span class="number">-0.2748</span>]],</span><br><span class="line">                       [[<span class="number">-0.0097</span>,  <span class="number">0.0346</span>],</span><br><span class="line">                        [<span class="number">-0.3125</span>,  <span class="number">0.2615</span>]]],</span><br><span class="line"></span><br><span class="line">                      [[[<span class="number">-0.2506</span>, <span class="number">-0.2632</span>],</span><br><span class="line">                        [ <span class="number">0.1302</span>, <span class="number">-0.2223</span>]],</span><br><span class="line">                       [[ <span class="number">0.1422</span>,  <span class="number">0.0427</span>],</span><br><span class="line">                        [ <span class="number">0.3453</span>,  <span class="number">0.0219</span>]]]])),</span><br><span class="line">             (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1974</span>, <span class="number">-0.1549</span>]))])</span><br><span class="line">print(bb)</span><br><span class="line">DataParallel(</span><br><span class="line">  (module): ModelA(</span><br><span class="line">    (base): Conv2d(<span class="number">2</span>, <span class="number">2</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">print(bb.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]])), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>]))])</span><br><span class="line">bb.cuda(gpu_ids[<span class="number">0</span>])</span><br><span class="line">print(bb.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'module.base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]], device=<span class="string">'cuda:0'</span>)), (<span class="string">'module.base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>], device=<span class="string">'cuda:0'</span>))])</span><br></pre></td></tr></table></figure><h1 id="7-Norm"><a href="#7-Norm" class="headerlink" title="7.Norm"></a>7.Norm</h1><p>参考：<br><a href="https://blog.csdn.net/liuxiao214/article/details/81037416" target="_blank" rel="noopener">https://blog.csdn.net/liuxiao214/article/details/81037416</a></p><p>输入图像：[N,C,H,W]<br>BatchNorm: [1,C,1,1]<br>InstanceNorm: [N,C,1,1]</p><p>经过实验,instanceNorm层的weight, bias, running_mean, running_var总是None<br>代码中加载模型的时候对instanceNorm层进行了删除操作,为什么<br>对于pytorch之前的版本instanceNorm层是有running_mean和running_var的,之后的版本修正了之后,就不再需要了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__patch_instance_norm_state_dict</span><span class="params">(self, state_dict, module, keys, i=<span class="number">0</span>)</span>:</span></span><br><span class="line">    key = keys[i]</span><br><span class="line">    <span class="keyword">if</span> i + <span class="number">1</span> == len(keys):  <span class="comment"># at the end, pointing to a parameter/buffer</span></span><br><span class="line">        <span class="keyword">if</span> module.__class__.__name__.startswith(<span class="string">'InstanceNorm'</span>) <span class="keyword">and</span> \</span><br><span class="line">                (key == <span class="string">'running_mean'</span> <span class="keyword">or</span> key == <span class="string">'running_var'</span>):</span><br><span class="line">            <span class="keyword">if</span> getattr(module, key) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                state_dict.pop(<span class="string">'.'</span>.join(keys))</span><br><span class="line">        <span class="keyword">if</span> module.__class__.__name__.startswith(<span class="string">'InstanceNorm'</span>) <span class="keyword">and</span> \</span><br><span class="line">           (key == <span class="string">'num_batches_tracked'</span>):</span><br><span class="line">            state_dict.pop(<span class="string">'.'</span>.join(keys))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="8-functools"><a href="#8-functools" class="headerlink" title="8.functools"></a>8.functools</h1><p>偏函数：适合为多个调用函数提供一致的函数接口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(m,n,p)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> m*n*p</span><br><span class="line">re=partial(f,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">print(re(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 60</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> type(norm_layer) == functools.partial:</span><br><span class="line">    use_bias = norm_layer.func == nn.InstanceNorm2d</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    use_bias = norm_layer == nn.InstanceNorm2d</span><br></pre></td></tr></table></figure><h1 id="9-论文与代码"><a href="#9-论文与代码" class="headerlink" title="9.论文与代码"></a>9.论文与代码</h1><h2 id="ndf"><a href="#ndf" class="headerlink" title="ndf"></a>ndf</h2><p>模型的定义与论文有一个地方不一致,论文写的第一个conv之后通道数是32,但实现是64.<br>与作者沟通得知,第一层不是32,而是64,剩下的也依次递增.</p><p>下采样的时候没有使用reflect进行补充,而是使用了0填充.<br>与作者沟通后，提出的是都可以尝试一下</p><h2 id="unet-model"><a href="#unet-model" class="headerlink" title="unet model"></a>unet model</h2><p>Unet model<br>与网上的不是很一致<br>3-&gt;<em>1-&gt;</em>2-&gt;<em>4-&gt;</em>8-&gt;<em>8-&gt;</em>8<br>3&lt;-<em>2&lt;-</em>4&lt;-<em>8&lt;-</em>16&lt;-<em>16&lt;-</em>16</p><h2 id="参数-no-lsgan"><a href="#参数-no-lsgan" class="headerlink" title="参数 no_lsgan"></a>参数 no_lsgan</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">self.criterionGAN = networks.GANLoss(use_lsgan=<span class="keyword">not</span> opt.no_lsgan).to(self.device)</span><br><span class="line"><span class="comment"># GAN loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GANLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_lsgan=True, target_real_label=<span class="number">1.0</span>, target_fake_label=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(GANLoss, self).__init__()</span><br><span class="line">        self.register_buffer(<span class="string">'real_label'</span>, torch.tensor(target_real_label))</span><br><span class="line">        self.register_buffer(<span class="string">'fake_label'</span>, torch.tensor(target_fake_label))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_lsgan:</span><br><span class="line">            self.loss = nn.MSELoss()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">use_sigmoid = opt.no_lsgan</span><br><span class="line">self.netD_A = define_D(opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm,</span><br><span class="line">                    use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)</span><br></pre></td></tr></table></figure><p>也就是<br>opt.no_lsgan为True时, netD使用sigmoid, GANloss使用BCELoss()<br>opt.no_lsgan为False时, netD不使用sigmoid, GANloss使用MSELoss()</p><p>MSELoss:均方误差 (x-y)*<em>2<br>BCELoss:二分类的交叉熵:使用前需要使用sigmoid函数,input和target的输入维度是一样的.(N,</em>)</p><p>根据作者提供的运行代码,猜测作者使用的是opt.no_lsgan为False,均方误差</p><p>L1loss: |x-y|</p><h2 id="G-and-D-的反向传播过程"><a href="#G-and-D-的反向传播过程" class="headerlink" title="G and D 的反向传播过程"></a>G and D 的反向传播过程</h2><p>回顾一下G和D的反向传播<br><strong>train G</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set_requires_grad(D, <span class="keyword">False</span>)</span><br><span class="line">fake_A = G(real_A)</span><br><span class="line">loss = criterion(D(fake_A), <span class="keyword">True</span>)</span><br><span class="line">optimizers_G.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizers_G.step()</span><br></pre></td></tr></table></figure><p><strong>train D</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set_requires_grad(D, <span class="keyword">False</span>)</span><br><span class="line"><span class="comment">#fake_A = fake_A.detach() # 取消G的grad</span></span><br><span class="line">loss1 = criterion(fake_A.detach(), <span class="keyword">False</span>)</span><br><span class="line">loss2 = criterion(realA, <span class="keyword">True</span>)</span><br><span class="line">loss = loss1 + loss2</span><br><span class="line">optimizers_D.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizers_D.step()</span><br></pre></td></tr></table></figure><h1 id="10-ConTransposed的计算方法"><a href="#10-ConTransposed的计算方法" class="headerlink" title="10.ConTransposed的计算方法"></a>10.ConTransposed的计算方法</h1><p>逆卷积后的图像大小和之前的能对应上，需要output_padding</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nn.ConvTranspose2d(ngf*mult, int(ngf*mult/<span class="number">2</span>), kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>, bias=use_bias)]</span><br><span class="line">-k+<span class="number">2</span>p+s-out_padding是s的整数</span><br><span class="line">k=<span class="number">3</span>,s=<span class="number">2</span>,p=<span class="number">1</span>,则out_padding=<span class="number">1</span></span><br><span class="line">k=<span class="number">3</span>,s=<span class="number">4</span>,p=<span class="number">1</span>,则out_padding=<span class="number">3</span></span><br></pre></td></tr></table></figure><h1 id="11-初始化参数"><a href="#11-初始化参数" class="headerlink" title="11.初始化参数"></a>11.初始化参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(net, init_type=<span class="string">'normal'</span>, gain=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_func</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="comment"># conv, contranspose ,linear, bn</span></span><br><span class="line">        <span class="comment"># type(m) == nn.Conv2d</span></span><br><span class="line">        classname = m.__class__.__name__</span><br><span class="line">        <span class="keyword">if</span> hasattr(m, <span class="string">'weight'</span>) <span class="keyword">and</span> (classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span> <span class="keyword">or</span> classname.find(<span class="string">'Linear'</span>) != <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> init_type == <span class="string">'normal'</span>:</span><br><span class="line">                init.normal_(m.weight.data, <span class="number">0.0</span>, gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'xavier'</span>:</span><br><span class="line">                init.xavier_normal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'kaiming'</span>:</span><br><span class="line">                init.kaiming_normal_(m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_in'</span>)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'orthogonal'</span>:</span><br><span class="line">                init.orthogonal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> NotImplementedError(<span class="string">'initialization method [%s] is not implemented'</span> % init_type)</span><br><span class="line">            <span class="keyword">if</span> hasattr(m, <span class="string">'bias'</span>) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm2d'</span>) != <span class="number">-1</span>:</span><br><span class="line">            init.normal_(m.weight.data, <span class="number">1.0</span>, gain)</span><br><span class="line">            init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'initialize network with %s'</span> % init_type)</span><br><span class="line">    net.apply(init_func)</span><br></pre></td></tr></table></figure><h1 id="12-disriminator-PatchGAN-and-GANLoss"><a href="#12-disriminator-PatchGAN-and-GANLoss" class="headerlink" title="12.disriminator PatchGAN and GANLoss"></a>12.disriminator PatchGAN and GANLoss</h1><p>PatchGAN的kernel是4.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GANLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_lsgan=True, target_real_label=<span class="number">1.0</span>, target_fake_label=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(GANLoss, self).__init__()</span><br><span class="line">        self.register_buffer(<span class="string">'real_label'</span>, torch.tensor(target_real_label))</span><br><span class="line">        self.register_buffer(<span class="string">'fake_label'</span>, torch.tensor(target_fake_label))</span><br><span class="line">        <span class="keyword">if</span> use_lsgan:</span><br><span class="line">            self.loss = nn.MSELoss()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_target_tensor</span><span class="params">(self, input, target_is_real)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> target_is_real:</span><br><span class="line">            target_tensor = self.real_label</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target_tensor = self.fake_label</span><br><span class="line">        <span class="keyword">return</span> target_tensor.expand_as(input)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, input, target_is_real)</span>:</span></span><br><span class="line">        target_tensor = self.get_target_tensor(input, target_is_real)</span><br><span class="line">        <span class="keyword">return</span> self.loss(input, target_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defines the PatchGAN discriminator with the specified arguments.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NLayerDiscriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_nc, ndf=<span class="number">64</span>, n_layers=<span class="number">3</span>, norm_layer=nn.BatchNorm2d, use_sigmoid=False)</span>:</span></span><br><span class="line">        super(NLayerDiscriminator, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> type(norm_layer) == functools.partial:</span><br><span class="line">            use_bias = norm_layer.func == nn.InstanceNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_bias = norm_layer == nn.InstanceNorm2d</span><br><span class="line"></span><br><span class="line">        kw = <span class="number">4</span></span><br><span class="line">        padw = <span class="number">1</span></span><br><span class="line">        sequence = [</span><br><span class="line">            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=<span class="number">2</span>, padding=padw),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        nf_mult = <span class="number">1</span></span><br><span class="line">        nf_mult_prev = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">            nf_mult_prev = nf_mult</span><br><span class="line">            nf_mult = min(<span class="number">2</span>**n, <span class="number">8</span>)</span><br><span class="line">            sequence += [</span><br><span class="line">                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,</span><br><span class="line">                          kernel_size=kw, stride=<span class="number">2</span>, padding=padw, bias=use_bias),</span><br><span class="line">                norm_layer(ndf * nf_mult),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">            ]</span><br><span class="line"></span><br><span class="line">        nf_mult_prev = nf_mult</span><br><span class="line">        nf_mult = min(<span class="number">2</span>**n_layers, <span class="number">8</span>)</span><br><span class="line">        sequence += [</span><br><span class="line">            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,</span><br><span class="line">                      kernel_size=kw, stride=<span class="number">1</span>, padding=padw, bias=use_bias),</span><br><span class="line">            norm_layer(ndf * nf_mult),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        sequence += [nn.Conv2d(ndf * nf_mult, <span class="number">1</span>, kernel_size=kw, stride=<span class="number">1</span>, padding=padw)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_sigmoid:</span><br><span class="line">            sequence += [nn.Sigmoid()]</span><br><span class="line"></span><br><span class="line">        self.model = nn.Sequential(*sequence)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.model(input)</span><br><span class="line"></span><br><span class="line">pred_real = netD(real)</span><br><span class="line">loss_D_real = self.criterionGAN(pred_real, <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="GANLoss的备注"><a href="#GANLoss的备注" class="headerlink" title="GANLoss的备注"></a>GANLoss的备注</h2><p>使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.<strong>call</strong>(input)，在<strong>call</strong>函数中，主要调用的是 layer.forward(x)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。</p><h1 id="13-PatchGAN的感受野"><a href="#13-PatchGAN的感受野" class="headerlink" title="13.PatchGAN的感受野"></a>13.PatchGAN的感受野</h1><p>论文使用的是70X70 PatchGAN<br>PatchGAN:<br>paper:<br>Image-to-Image Translation with Conditional Adversarial Networks<br><a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="noopener">https://arxiv.org/abs/1611.07004</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">感受野的计算规则</span><br><span class="line">对于第m层,m=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,...,N. hm表示第m层应该有的视野,假设mN=<span class="number">1</span></span><br><span class="line">km, sm, pm,表示第m<span class="number">-1</span>层到第m层的conv的kernel</span><br><span class="line">第一层对于第<span class="number">0</span>层的感受野</span><br><span class="line">h1 = <span class="number">1</span>, </span><br><span class="line">(h0-k1)/s1+<span class="number">1</span>=h1</span><br><span class="line">第二层对于第<span class="number">0</span>层的感受野</span><br><span class="line">h2 = <span class="number">1</span></span><br><span class="line">(h1-k2)/s2+<span class="number">1</span>=h2</span><br><span class="line">(h0-k1)/s1+<span class="number">1</span>=h1</span><br><span class="line">依次类推</span><br><span class="line">反之 https://fomoro.com/tools/receptive-fields/<span class="comment">#</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(output_size, ksize, stride)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (output_size - <span class="number">1</span>) * stride + ksize</span><br><span class="line"></span><br><span class="line">last_layer = f(output_size=<span class="number">1</span>, ksize=<span class="number">4</span>, stride=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Receptive field: 4</span></span><br><span class="line">fourth_layer = f(output_size=last_layer, ksize=<span class="number">4</span>, stride=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Receptive field: 7</span></span><br><span class="line">third_layer = f(output_size=fourth_layer, ksize=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Receptive field: 16</span></span><br><span class="line">second_layer = f(output_size=third_layer, ksize=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Receptive field: 34</span></span><br><span class="line">first_layer = f(output_size=second_layer, ksize=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Receptive field: 70</span></span><br><span class="line"></span><br><span class="line">print(first_layer)</span><br></pre></td></tr></table></figure><h1 id="14-torch-tensor-clone"><a href="#14-torch-tensor-clone" class="headerlink" title="14.torch.tensor.clone()"></a>14.torch.tensor.clone()</h1><p>clone()<br>梯度受影响,clone之后的新的tensor的梯度也会影响到原tensor,但是新tensor本身是没有梯度的.<br>clone之后的新tensor的改变不会影响原有的tensor<br>应该这么理解,clone也是计算图中的一个操作,这样的话就可以解释通了.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input = torch.ones(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">input.requires_grad = <span class="keyword">True</span></span><br><span class="line">input2 = input.clone()</span><br><span class="line">print(input2.requires_grad)</span><br><span class="line">y = input.sum()</span><br><span class="line">y.backward()</span><br><span class="line">print(input.grad)</span><br><span class="line"><span class="comment"># 1,1,1...</span></span><br><span class="line">print(input2.grad)</span><br><span class="line"><span class="comment"># None</span></span><br><span class="line">y = input2.sum()</span><br><span class="line">y.backward()</span><br><span class="line">print(input.grad)</span><br><span class="line"><span class="comment"># 2,2,2...</span></span><br><span class="line">print(input2.grad)</span><br><span class="line"><span class="comment"># None</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input = torch.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">input.requires_grad = <span class="keyword">True</span></span><br><span class="line">input2 = input.clone()</span><br><span class="line">input2[<span class="number">1</span>,<span class="number">1</span>] = <span class="number">6</span></span><br><span class="line">print(input2)</span><br><span class="line">print(input)</span><br><span class="line">tensor([[ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">6.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line">tensor([[ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line">input2[<span class="number">1</span>,<span class="number">1</span>] = <span class="number">6</span></span><br><span class="line">y = (input2*input2).sum()</span><br><span class="line">y.backward()</span><br><span class="line">print(input.grad)</span><br><span class="line">print(input2.grad)</span><br><span class="line">tensor([[ <span class="number">8.</span>,  <span class="number">8.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">8.</span>,  <span class="number">8.</span>]])</span><br><span class="line"><span class="keyword">None</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(input2.grad_fn)</span><br><span class="line">&lt;CopySlices object at <span class="number">0x7fe89dc841d0</span>&gt;</span><br></pre></td></tr></table></figure><p>clone的用法<br>tensor保留梯度的交换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tmp = tensor1.clone()</span><br><span class="line">tensor2 = tmp</span><br><span class="line">tensor1 = tensor3</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tensor1, tensor2 = tensor3, tensor1.clone()</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tmp = self.images[random_id].clone()</span><br><span class="line">self.images[random_id] = image</span><br><span class="line">return_images.append(tmp)</span><br></pre></td></tr></table></figure><h1 id="15-from-XX-import"><a href="#15-from-XX-import" class="headerlink" title="15.from XX import"></a>15.from XX import</h1><p>这里还有一些不太对的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .base_model <span class="keyword">import</span> BaseModel <span class="comment"># 同一个文件夹</span></span><br><span class="line"><span class="keyword">from</span> util.image_pool <span class="keyword">import</span> <span class="comment"># 父级文件夹</span></span><br><span class="line"><span class="comment"># 建议</span></span><br><span class="line"><span class="keyword">from</span> .base_model <span class="keyword">import</span> BaseModel <span class="comment"># 同一个文件夹</span></span><br><span class="line"><span class="keyword">from</span> ..util.image_pool <span class="keyword">import</span> <span class="comment"># 父级文件夹</span></span><br></pre></td></tr></table></figure><h1 id="16-register-buffer"><a href="#16-register-buffer" class="headerlink" title="16.register_buffer"></a>16.register_buffer</h1><p>register_buffer<br>self.register_buffer可以将tensor注册成buffer，在forward中使用self.mybuffer, 而不是self.mybuffer_tmp.<br>定义Parameter和buffer都只需要传入Tensor即可。也不需要将其转成gpu。这是因为，当网络进行.cuda()时候，会自动将里面的层的参数,buffer等转换成相应的GPU上。<br>网络存储时也会将buffer存下，当网络load模型时，会将存储的模型的buffer也进行赋值。<br>buffer的更新在forward中，optim.step只能更新nn.Parameter类型的参数。<br>用法<br>self.register_buffer(‘running_mean’, torch.zeros(num_features))</p><h1 id="17-itertools"><a href="#17-itertools" class="headerlink" title="17. itertools"></a>17. itertools</h1><p>无限迭代器<br>itertools，用于创建高效迭代器的函数,<br>itertools.chain 连接多个列表或者迭代器。<br>将多个网络写在一起,使用一个优化器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),</span><br><span class="line">                                    lr=opt.lr, betas=(opt.beta1, <span class="number">0.999</span>))</span><br><span class="line">self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()),</span><br><span class="line">                                    lr=opt.lr, betas=(opt.beta1, <span class="number">0.999</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自然数无限迭代器</span></span><br><span class="line"><span class="comment"># itertools.count</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> itertools</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>natuals = itertools.count(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> natuals:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 序列无限重复</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> itertools</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cs = itertools.cycle(<span class="string">'ABC'</span>) <span class="comment"># 注意字符串也是序列的一种</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> c <span class="keyword">in</span> cs:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> c</span><br><span class="line">...</span><br><span class="line"><span class="string">'A'</span></span><br><span class="line"><span class="string">'B'</span></span><br><span class="line"><span class="string">'C'</span></span><br><span class="line"><span class="string">'A'</span></span><br><span class="line"><span class="string">'B'</span></span><br><span class="line"><span class="string">'C'</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单元素无限重复</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ns = itertools.repeat(<span class="string">'A'</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> ns:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line">打印<span class="number">10</span>次<span class="string">'A'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 无限迭代器中截取有限序列</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>natuals = itertools.count(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ns = itertools.takewhile(<span class="keyword">lambda</span> x: x &lt;= <span class="number">10</span>, natuals)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> ns:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line">打印出<span class="number">1</span>到<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代对象的串联</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> itertools.chain(<span class="string">'ABC'</span>, <span class="string">'XYZ'</span>):</span><br><span class="line">    <span class="keyword">print</span> c</span><br><span class="line"><span class="comment"># 迭代效果：'A' 'B' 'C' 'X' 'Y' 'Z'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代器中相邻的重复元素挑出来放在一起</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> key, group <span class="keyword">in</span> itertools.groupby(<span class="string">'AAABBBCCAAA'</span>):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> key, list(group) <span class="comment"># 为什么这里要用list()函数呢？</span></span><br><span class="line">...</span><br><span class="line">A [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'A'</span>]</span><br><span class="line">B [<span class="string">'B'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">C [<span class="string">'C'</span>, <span class="string">'C'</span>]</span><br><span class="line">A [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'A'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># imap, ifilter</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> x <span class="keyword">in</span> itertools.imap(<span class="keyword">lambda</span> x, y: x * y, [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>], itertools.count(<span class="number">1</span>)):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> x</span><br><span class="line">...</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">40</span></span><br><span class="line"><span class="number">90</span></span><br></pre></td></tr></table></figure><h1 id="18-visdom"><a href="#18-visdom" class="headerlink" title="18.visdom"></a>18.visdom</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.vis = visdom.Visdom(server=opt.display_server, port=opt.display_port, </span><br><span class="line">                         env=opt.display_env, raise_exceptions=<span class="keyword">True</span>, use_incoming_socket=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># env根据_自动分层。e.g. cycle_gan--&gt;cycle,cycle_gan</span></span><br></pre></td></tr></table></figure><h1 id="19-三引号"><a href="#19-三引号" class="headerlink" title="19.三引号"></a>19.三引号</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">三引号的作用</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>str1 = <span class="string">"""List of name:</span></span><br><span class="line"><span class="string"><span class="meta">... </span>Hua Li</span></span><br><span class="line"><span class="string"><span class="meta">... </span>Chao Deng</span></span><br><span class="line"><span class="string"><span class="meta">... </span>&#123;&#125;</span></span><br><span class="line"><span class="string"><span class="meta">... </span>"""</span>.format(<span class="string">'hhh'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(str1)</span><br><span class="line">List of name:</span><br><span class="line">Hua Li</span><br><span class="line">Chao Deng</span><br></pre></td></tr></table></figure><h1 id="20-异常"><a href="#20-异常" class="headerlink" title="20.异常"></a>20.异常</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.触发异常</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mye</span><span class="params">( level )</span>:</span></span><br><span class="line">    <span class="keyword">if</span> level &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"Invalid level!"</span>)</span><br><span class="line">        <span class="comment"># 触发异常后，后面的代码就不会再执行</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    mye(<span class="number">0</span>)            <span class="comment"># 触发异常</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> err:</span><br><span class="line">    print(<span class="number">1</span>,err)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">1</span> Invalid level!</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.自定义异常</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyException</span><span class="params">(Exception)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,message)</span>:</span></span><br><span class="line">        Exception.__init__(self)</span><br><span class="line">        self.message=message </span><br><span class="line">        print(<span class="string">'This is MyException'</span>)</span><br><span class="line">a=<span class="number">7</span></span><br><span class="line"><span class="keyword">if</span> a&lt;<span class="number">10</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">raise</span> MyException(<span class="string">"my excepition is raised "</span>)</span><br><span class="line">    <span class="keyword">except</span> MyException <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">'*****************'</span>)</span><br><span class="line">        print(e.message)    </span><br><span class="line">This <span class="keyword">is</span> MyException</span><br><span class="line">my excepition <span class="keyword">is</span> raised</span><br></pre></td></tr></table></figure><h1 id="21-自定义类的iter"><a href="#21-自定义类的iter" class="headerlink" title="21.自定义类的iter"></a>21.自定义类的iter</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义类的iter</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cl1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.N = <span class="number">10</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            print(i)</span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="number">5</span>:</span><br><span class="line">                <span class="keyword">yield</span> i</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">cc = cl1()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> cc:</span><br><span class="line">    print(<span class="string">'hhh'</span>,i)</span><br><span class="line"><span class="number">0</span></span><br><span class="line">hhh <span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line">hhh <span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">hhh <span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">hhh <span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line">hhh <span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇博客主要记录在跟随cycleGAN作者的代码复现学到的东西。&lt;br&gt;title: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(ICCV2017)&lt;/p&gt;
&lt;p&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1703.10593&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1703.10593&lt;/a&gt;&lt;br&gt;code: &lt;a href=&quot;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&lt;/a&gt;&lt;br&gt;mycode: &lt;a href=&quot;https://github.com/TJJTJJTJJ/pytorch_cycleGAN&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/TJJTJJTJJ/pytorch_cycleGAN&lt;/a&gt;&lt;br&gt;cycle_gan的整体框架写得很漂亮，frame可以参考github的frame&lt;br&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://yoursite.com/categories/DeepLearning/"/>
    
    
      <category term="cycleGAN" scheme="http://yoursite.com/tags/cycleGAN/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow</title>
    <link href="http://yoursite.com/2018/11/05/tensorflow/"/>
    <id>http://yoursite.com/2018/11/05/tensorflow/</id>
    <published>2018-11-05T14:41:03.000Z</published>
    <updated>2018-11-05T15:10:47.797Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇关于tensorflow的博客，这里面很多东西都是很杂碎的，不在此做处理，等积累的多了，理解才能正确。<br><a id="more"></a></p><p>TensorFlow入门教程</p><p>1.TensorFlow深度学习应用实践<br>评价不好</p><ol start="2"><li><p>TensorFlow：实战Google深度学习框架（第2版）<br>8.6分，可以用来实践</p></li><li><p>Tensorflow：实战Google深度学习框架<br>8.4分</p></li></ol><p>4.莫烦的tensorlfow教程<br><a href="https://github.com/MorvanZhou" target="_blank" rel="noopener">https://github.com/MorvanZhou</a><br>适合实践</p><p>5.某个网友的自己实现的教程<br><a href="https://www.jianshu.com/p/27a2fb320934" target="_blank" rel="noopener">https://www.jianshu.com/p/27a2fb320934</a><br><a href="https://github.com/zhaozhengcoder/Machine-Learning/tree/master/tensorflow_tutorials" target="_blank" rel="noopener">https://github.com/zhaozhengcoder/Machine-Learning/tree/master/tensorflow_tutorials</a></p><p>6.官网API<br><a href="https://tensorflow.google.cn/api_docs/python/tf" target="_blank" rel="noopener">https://tensorflow.google.cn/api_docs/python/tf</a></p><p>7.深度学习之TensorFlow入门、原理与进阶实战<br>7.6分<br>22章，内容更加详实，偏向理论，可以用来只看不实践</p><p>8.TensorFlow实战<br>7.3分<br>适合看看，内容不深，实践性不强，理论也很浅<br>在github上也没有代码</p><p>不应该总是要求全部，所以应该这样的顺序来学习<br>先学：TensorFlow：实战Google深度学习框架（第2版）<br>再学：莫烦：<a href="https://github.com/MorvanZhou+网页的教程" target="_blank" rel="noopener">https://github.com/MorvanZhou+网页的教程</a><br>基本就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">print(a.graph == tf.get_default_graph())</span><br><span class="line">tf.get_variable() name 必须双引号</span><br><span class="line"></span><br><span class="line">name的作用  https://blog.csdn.net/xiaohuihui1994/article/details/<span class="number">81022043</span></span><br><span class="line"></span><br><span class="line">可以理解成sess需要指定，不能自动加入</span><br><span class="line">.run,.eval能执行的两种方式</span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g1) <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">或者</span><br><span class="line">sess = tf.Session()</span><br><span class="line">tf.global_variables_initializer().run(session=sess)</span><br><span class="line">或者</span><br><span class="line">sess = tf.InteractiveSession() <span class="comment"># 会自动注册为默认会话</span></span><br><span class="line">result.eval()</span><br><span class="line">或者</span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    result.eval()</span><br><span class="line"></span><br><span class="line"><span class="comment">####</span></span><br><span class="line">初始化</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">w1.initializer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">『TensorFlow』使用集合collection控制variables</span><br><span class="line">https://www.cnblogs.com/hellcat/p/<span class="number">9006904.</span>html</span><br><span class="line"></span><br><span class="line">collection</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>], initializer = tf.zeros_initializer()) <span class="comment"># 设置初始值为0</span></span><br><span class="line">    gv= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES <span class="comment"># gv = tf.global_variables()</span></span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> gv:</span><br><span class="line">        print(var)</span><br><span class="line"></span><br><span class="line">现在有了几个概念需要理清楚：</span><br><span class="line">计算图： 不同计算图中的变量是独立的</span><br><span class="line">collection： 不同类型的variable放在不同的collection中，主要是tf.GraphKeys.GLOBAL_VARIABLES和tf.GraphKeys.TRAINABLE_VARIABLES</span><br><span class="line">会话： 会话需要与计算图相连接，完成相应计算图的执行，一个会话对应一个计算图及其执行结果</span><br><span class="line"></span><br><span class="line">tf.add_to_collection</span><br><span class="line">https://www.jianshu.com/p/<span class="number">6612</span>f368e8f4</span><br><span class="line"></span><br><span class="line">这样就不需要传入weighs和biases，这里的reuse实现了定义和使用的一体化，不需要专门对weights定义和调用。</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, reuse=False)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1'</span>, reuse=reuse):</span><br><span class="line">        weights = tf.get_variable(<span class="string">"weights"</span>)</span><br><span class="line">        biases = tf.get_variable(<span class="string">"biases"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer2'</span>, reuse=reuse):</span><br><span class="line">        weights = tf.get_variable(<span class="string">"weights"</span>)</span><br><span class="line">        biases = tf.get_variable(<span class="string">"biases"</span>)</span><br><span class="line"></span><br><span class="line">TFRecord数据格式</span><br><span class="line">https://blog.csdn.net/u012759136/article/details/<span class="number">52232266</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一篇关于tensorflow的博客，这里面很多东西都是很杂碎的，不在此做处理，等积累的多了，理解才能正确。&lt;br&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-cuda</title>
    <link href="http://yoursite.com/2018/11/05/pytorch-cuda/"/>
    <id>http://yoursite.com/2018/11/05/pytorch-cuda/</id>
    <published>2018-11-05T13:04:26.000Z</published>
    <updated>2018-11-05T15:12:27.757Z</updated>
    
    <content type="html"><![CDATA[<p>关于pytorch中模型的多GPU<br><a id="more"></a></p><h1 id="1-cudnn-benchmark-True"><a href="#1-cudnn-benchmark-True" class="headerlink" title="1.cudnn.benchmark = True"></a>1.cudnn.benchmark = True</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.backends <span class="keyword">import</span> cudnn</span><br><span class="line">cudnn.benchmark = <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>而且大家都说这样可以增加程序的运行效率。那到底有没有这样的效果，或者什么情况下应该这样做呢？<br>总的来说，大部分情况下，设置这个 flag 可以让内置的 cuDNN 的 auto-tuner 自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。</p><p>一般来讲，应该遵循以下准则：</p><ul><li>如果网络的输入数据维度或类型上变化不大，设置  torch.backends.cudnn.benchmark = true  可以增加运行效率；</li><li>如果网络的输入数据在每次 iteration 都变化的话，会导致 cnDNN 每次都会去寻找一遍最优配置，这样反而会降低运行效率。</li></ul><p>这下就清晰明了很多了。</p><p>其实看完这段还是很蒙蔽，不知道具体什么情况下使用，暂且算加速过程好了。</p><h1 id="2-nn-DataParallel"><a href="#2-nn-DataParallel" class="headerlink" title="2. nn.DataParallel"></a>2. nn.DataParallel</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">model2 = nn.DataParallel(model1)</span><br><span class="line">model2.cuda()</span><br><span class="line"></span><br><span class="line">`</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span> <span class="comment"># Our model </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, output_size)</span>:</span> </span><br><span class="line">        super(Model, self).__init__() </span><br><span class="line">        self.fc = nn.Linear(input_size, output_size) </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span> </span><br><span class="line">        output = self.fc(input) </span><br><span class="line">        print(<span class="string">" In Model: input size"</span>, input.size(), <span class="string">"output size"</span>, output.size()) </span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">----</span><br><span class="line">model1 = Model(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">print(model1)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> model1.parameters():</span><br><span class="line">    print(var)</span><br><span class="line"></span><br><span class="line">Model(</span><br><span class="line">  (fc): Linear(in_features=<span class="number">3</span>, out_features=<span class="number">4</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.1964</span>,  <span class="number">0.4389</span>, <span class="number">-0.2216</span>],</span><br><span class="line">        [<span class="number">-0.1046</span>, <span class="number">-0.2055</span>, <span class="number">-0.5383</span>],</span><br><span class="line">        [ <span class="number">0.0673</span>,  <span class="number">0.0949</span>,  <span class="number">0.5205</span>],</span><br><span class="line">        [ <span class="number">0.5473</span>, <span class="number">-0.3700</span>, <span class="number">-0.4179</span>]])</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2416</span>,  <span class="number">0.4188</span>, <span class="number">-0.0096</span>,  <span class="number">0.1569</span>])</span><br><span class="line">----</span><br><span class="line">model2 = nn.DataParallel(model1)</span><br><span class="line">print(model2)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> model2.parameters():</span><br><span class="line">    print(var)</span><br><span class="line"></span><br><span class="line">DataParallel(</span><br><span class="line">  (module): Model(</span><br><span class="line">    (fc): Linear(in_features=<span class="number">3</span>, out_features=<span class="number">4</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.1964</span>,  <span class="number">0.4389</span>, <span class="number">-0.2216</span>],</span><br><span class="line">        [<span class="number">-0.1046</span>, <span class="number">-0.2055</span>, <span class="number">-0.5383</span>],</span><br><span class="line">        [ <span class="number">0.0673</span>,  <span class="number">0.0949</span>,  <span class="number">0.5205</span>],</span><br><span class="line">        [ <span class="number">0.5473</span>, <span class="number">-0.3700</span>, <span class="number">-0.4179</span>]], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2416</span>,  <span class="number">0.4188</span>, <span class="number">-0.0096</span>,  <span class="number">0.1569</span>], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">----</span><br><span class="line">model2.cuda()</span><br><span class="line">print(model2)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> model2.parameters():</span><br><span class="line">    print(var)</span><br><span class="line"></span><br><span class="line">DataParallel(</span><br><span class="line">  (module): Model(</span><br><span class="line">    (fc): Linear(in_features=<span class="number">3</span>, out_features=<span class="number">4</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.1964</span>,  <span class="number">0.4389</span>, <span class="number">-0.2216</span>],</span><br><span class="line">        [<span class="number">-0.1046</span>, <span class="number">-0.2055</span>, <span class="number">-0.5383</span>],</span><br><span class="line">        [ <span class="number">0.0673</span>,  <span class="number">0.0949</span>,  <span class="number">0.5205</span>],</span><br><span class="line">        [ <span class="number">0.5473</span>, <span class="number">-0.3700</span>, <span class="number">-0.4179</span>]], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2416</span>,  <span class="number">0.4188</span>, <span class="number">-0.0096</span>,  <span class="number">0.1569</span>], device=<span class="string">'cuda:0'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于pytorch中模型的多GPU&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>beyond-part-models</title>
    <link href="http://yoursite.com/2018/11/05/beyond-part-models/"/>
    <id>http://yoursite.com/2018/11/05/beyond-part-models/</id>
    <published>2018-11-05T12:59:51.000Z</published>
    <updated>2018-11-05T15:14:46.007Z</updated>
    
    <content type="html"><![CDATA[<p>PCB:<br><a href="https://github.com/huanghoujing/beyond-part-models" target="_blank" rel="noopener">https://github.com/huanghoujing/beyond-part-models</a><br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── bpm</span><br><span class="line">│   ├── dataset</span><br><span class="line">│   │   ├── Dataset.py</span><br><span class="line">│   │   ├── __init__.py</span><br><span class="line">│   │   ├── Prefetcher.py</span><br><span class="line">│   │   ├── PreProcessImage.py</span><br><span class="line">│   │   ├── TestSet.py</span><br><span class="line">│   │   └── TrainSet.py</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── model</span><br><span class="line">│   │   ├── __init__.py</span><br><span class="line">│   │   ├── PCBModel.py</span><br><span class="line">│   │   └── resnet.py</span><br><span class="line">│   └── utils</span><br><span class="line">│       ├── dataset_utils.py</span><br><span class="line">│       ├── distance.py</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       ├── metric.py</span><br><span class="line">│       ├── re_ranking.py</span><br><span class="line">│       ├── utils.py</span><br><span class="line">│       └── visualization.py</span><br><span class="line">├── example_rank_lists_on_Market1501</span><br><span class="line">│</span><br><span class="line">└── script</span><br><span class="line">    ├── dataset</span><br><span class="line">    │   ├── combine_trainval_sets.py</span><br><span class="line">    │   ├── mapping_im_names_duke.py</span><br><span class="line">    │   ├── mapping_im_names_market1501.py</span><br><span class="line">    │   ├── transform_cuhk03.py</span><br><span class="line">    │   ├── transform_duke.py</span><br><span class="line">    │   └── transform_market1501.py</span><br><span class="line">    └── experiment</span><br><span class="line">        ├── train_pcb.py</span><br><span class="line">        └── visualize_rank_list.py</span><br><span class="line"></span><br><span class="line">bpm:正式的模型训练</span><br><span class="line">script:主要用于数据的预处理和训练的对外借口</span><br></pre></td></tr></table></figure></p><p>以market1501为例</p><h1 id="数据的预处理"><a href="#数据的预处理" class="headerlink" title="数据的预处理"></a>数据的预处理</h1><p>第一个表示id，第二个表示cam，第三个表示同id同cam的第几张图片，对zip_file中的*.jpg移动到save_dir+images中，并且重命名，将所有图片重命名保存到save_dir+images<br>  分为<br>  —-trainval  name+label<br>      |—-train name+label<br>      |—-val   name+mask<br>           |—-query   0<br>           |—-gallery 1<br>  —-test name+mask<br>      |—-query        0<br>      |—-multi-query  2<br>      |—-gallery      1</p><p>  保存到save_dir, ‘partitions.pkl’中</p><p>  trainval提取val的时候，val中的id只提取100个id，并且会自动跳过只在一个cam下的id。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">partitions: dict</span><br><span class="line">            &#123;<span class="string">'trainval_im_names'</span>: train_test_split[<span class="string">'trainval_im_names'</span>],</span><br><span class="line">            <span class="string">'trainval_ids2labels'</span>: trainval_ids2labels,</span><br><span class="line">            <span class="string">'train_im_names'</span>: train_im_names,</span><br><span class="line">            <span class="string">'train_ids2labels'</span>: train_ids2labels,</span><br><span class="line">            <span class="string">'val_im_names'</span>: val_im_names,</span><br><span class="line">            <span class="string">'val_marks'</span>: val_marks,</span><br><span class="line">            <span class="string">'test_im_names'</span>: test_im_names,</span><br><span class="line">            <span class="string">'test_marks'</span>: test_marks&#125;</span><br></pre></td></tr></table></figure></p><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p>对于数据集是怎么加载、转化的，还是没有头绪，写法之前没有遇到过，这一部分有待提高</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>模型是一致的</p><h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><p>test好像是re-ranking了<br>val没有re-ranking</p><p>lr: 0.1 0.01<br>factor: 0.1<br>epochs: 60<br>staircase_decay_at_epochs: 41</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCB:&lt;br&gt;&lt;a href=&quot;https://github.com/huanghoujing/beyond-part-models&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/huanghoujing/beyond-part-models&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>python-2.0</title>
    <link href="http://yoursite.com/2018/11/05/python-2.0/"/>
    <id>http://yoursite.com/2018/11/05/python-2.0/</id>
    <published>2018-11-05T09:27:03.000Z</published>
    <updated>2018-12-03T05:22:46.972Z</updated>
    
    <content type="html"><![CDATA[<p>这也是关于python的一些记录，与其他python不矛盾，也不一定相互独立。<br><a id="more"></a></p><h1 id="1-python-matlab"><a href="#1-python-matlab" class="headerlink" title="1.python+matlab"></a>1.python+matlab</h1><p>python和matlab关于.mat数据的交换<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scipy.io.loadmat(file_name, mdict=<span class="keyword">None</span>, appendmat=<span class="keyword">True</span>, **kwargs)</span><br><span class="line">scipy.io.savemat(file_name, mdict, appendmat=<span class="keyword">True</span>, format=<span class="string">'5'</span>, </span><br><span class="line">                 long_field_names=<span class="keyword">False</span>, do_compression=<span class="keyword">False</span>, oned_as=<span class="string">'row'</span>)</span><br></pre></td></tr></table></figure></p><h1 id="2-python与文件IO"><a href="#2-python与文件IO" class="headerlink" title="2.python与文件IO"></a>2.python与文件IO</h1><p>主要参考：python cookbook</p><h2 id="2-1-文本-txt"><a href="#2-1-文本-txt" class="headerlink" title="2.1 文本.txt"></a>2.1 文本.txt</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t:rt模式下，python在读取文本时会自动把\r\n转换成\n.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># read</span></span><br><span class="line"><span class="comment"># read the entire file as a single string</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.txt'</span>, <span class="string">'rt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># read lines</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.txt'</span>, <span class="string">'rt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line"></span><br><span class="line"><span class="comment">#write</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.txt'</span>, <span class="string">'wt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(text1)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.txt'</span>, <span class="string">'wt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    print(line1, file=f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 换行模式，默认情况下，python会自动识别，或者传入newline,</span></span><br><span class="line"><span class="comment"># newline可以取的值有None, \n, \r, ‘\r\n'，用于区分换行符，但是这个参数只对文本模式有效；</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.txt'</span>, <span class="string">'rt'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码错误问题 errors: replace, ignore</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'sample.txt'</span>, <span class="string">'rt'</span>, encoding=<span class="string">'ascii'</span>, errors=<span class="string">'replace'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="2-2-print的分隔符与行尾符"><a href="#2-2-print的分隔符与行尾符" class="headerlink" title="2.2 print的分隔符与行尾符"></a>2.2 print的分隔符与行尾符</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print seq end</span></span><br><span class="line">print(<span class="string">'ACME'</span>, <span class="number">50</span>, seq=<span class="string">','</span>, end=<span class="string">'!!\n'</span>)</span><br><span class="line">ACME,<span class="number">50</span>!!</span><br><span class="line"></span><br><span class="line">row = (<span class="string">'ACME'</span>, <span class="number">50</span>)</span><br><span class="line">print(*row, seq=<span class="string">','</span>, end=<span class="string">'!!\n'</span>)</span><br><span class="line">ACME,<span class="number">50</span>!!</span><br></pre></td></tr></table></figure><h2 id="2-3-二进制数据"><a href="#2-3-二进制数据" class="headerlink" title="2.3 二进制数据"></a>2.3 二进制数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二进制数据，比如图片、声音等</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.bin'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.bin'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">b'hello'</span>)</span><br></pre></td></tr></table></figure><h2 id="2-4-模拟普通文件"><a href="#2-4-模拟普通文件" class="headerlink" title="2.4 模拟普通文件"></a>2.4 模拟普通文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟文本文件</span></span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line">s = io.StringIO()</span><br><span class="line"><span class="comment"># s = io.StringIO('hello world\n')</span></span><br><span class="line">s.write(<span class="string">'hello world\n'</span>)</span><br><span class="line">print(<span class="string">'this is a test'</span>, file=s)</span><br><span class="line">s.getvalue()</span><br><span class="line"><span class="string">'hello world\nthis is a test'</span></span><br><span class="line">s.read(<span class="number">4</span>)</span><br><span class="line">s.read()</span><br><span class="line"><span class="comment"># 模拟二进制文件</span></span><br><span class="line">s = io.BytesIO()</span><br><span class="line">s.write(<span class="string">b'binary data'</span>)</span><br><span class="line">s.getvalue()</span><br><span class="line"><span class="string">b'binary data'</span></span><br></pre></td></tr></table></figure><h2 id="2-5-压缩文件"><a href="#2-5-压缩文件" class="headerlink" title="2.5 压缩文件"></a>2.5 压缩文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gzip,bz2</span></span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">with</span> gzip.open(<span class="string">'some.gz'</span>, <span class="string">'rt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br><span class="line"><span class="keyword">with</span> gzip.open(<span class="string">'some.gz'</span>, <span class="string">'wt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(text)</span><br><span class="line">-------------------------------------</span><br><span class="line"><span class="keyword">import</span> bz2</span><br><span class="line"><span class="keyword">with</span> bz2.open(<span class="string">'some.bz2'</span>, <span class="string">'rt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br><span class="line"><span class="keyword">with</span> bz2.open(<span class="string">'some.bz2'</span>, <span class="string">'wt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(text)</span><br><span class="line">------------------------------------</span><br><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br><span class="line"><span class="keyword">with</span> ZipFile(zip_file) <span class="keyword">as</span> z:</span><br><span class="line">    z.extractall(path=save_dir)</span><br></pre></td></tr></table></figure><h2 id="2-6-csv"><a href="#2-6-csv" class="headerlink" title="2.6 csv"></a>2.6 csv</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># csv</span></span><br><span class="line"><span class="comment"># 其实namedtuple继承自OrderedDict有序字典</span></span><br><span class="line"><span class="comment"># read</span></span><br><span class="line">------</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stock.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="comment"># 第一种：row是列表，访问：row[0]</span></span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    headers = next(f_csv)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二种：命名元组，访问：Row.Symbol</span></span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    headers = next(f_csv)</span><br><span class="line">    <span class="comment"># headers = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) ]</span></span><br><span class="line">    Row = namedtuple(<span class="string">'Row'</span>, headers)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> f_csv:</span><br><span class="line">        row = Row(*r)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第三种：字典序列，访问：row['Sysbol']</span></span><br><span class="line">    f_csv = csv.DictReader(f)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># write</span></span><br><span class="line">-------</span><br><span class="line"><span class="comment"># 第一种：类表</span></span><br><span class="line">headers = [<span class="string">'Symbol'</span>,<span class="string">'Price'</span>,<span class="string">'Date'</span>,<span class="string">'Time'</span>,<span class="string">'Change'</span>,<span class="string">'Volume'</span>]</span><br><span class="line">rows = [(<span class="string">'AA'</span>, <span class="number">39.48</span>, <span class="string">'6/11/2007'</span>, <span class="string">'9:36am'</span>, <span class="number">-0.18</span>, <span class="number">181800</span>),</span><br><span class="line">        (<span class="string">'AIG'</span>, <span class="number">71.38</span>, <span class="string">'6/11/2007'</span>, <span class="string">'9:36am'</span>, <span class="number">-0.15</span>, <span class="number">195500</span>),</span><br><span class="line">        (<span class="string">'AXP'</span>, <span class="number">62.58</span>, <span class="string">'6/11/2007'</span>, <span class="string">'9:36am'</span>, <span class="number">-0.46</span>, <span class="number">935000</span>),</span><br><span class="line">        ]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stock.csv'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.writer(f)</span><br><span class="line">    f_csv.writerow(headers)</span><br><span class="line">    f_csv.writerows(rows)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种：字典</span></span><br><span class="line">headers = [<span class="string">'Symbol'</span>, <span class="string">'Price'</span>, <span class="string">'Date'</span>, <span class="string">'Time'</span>, <span class="string">'Change'</span>, <span class="string">'Volume'</span>]</span><br><span class="line">rows = [&#123;<span class="string">'Symbol'</span>:<span class="string">'AA'</span>, <span class="string">'Price'</span>:<span class="number">39.48</span>, <span class="string">'Date'</span>:<span class="string">'6/11/2007'</span>,</span><br><span class="line">         <span class="string">'Time'</span>:<span class="string">'9:36am'</span>, <span class="string">'Change'</span>:<span class="number">-0.18</span>, <span class="string">'Volume'</span>:<span class="number">181800</span>&#125;,</span><br><span class="line">        &#123;<span class="string">'Symbol'</span>:<span class="string">'AIG'</span>, <span class="string">'Price'</span>: <span class="number">71.38</span>, <span class="string">'Date'</span>:<span class="string">'6/11/2007'</span>,</span><br><span class="line">         <span class="string">'Time'</span>:<span class="string">'9:36am'</span>, <span class="string">'Change'</span>:<span class="number">-0.15</span>, <span class="string">'Volume'</span>: <span class="number">195500</span>&#125;,</span><br><span class="line">        &#123;<span class="string">'Symbol'</span>:<span class="string">'AXP'</span>, <span class="string">'Price'</span>: <span class="number">62.58</span>, <span class="string">'Date'</span>:<span class="string">'6/11/2007'</span>,</span><br><span class="line">         <span class="string">'Time'</span>:<span class="string">'9:36am'</span>, <span class="string">'Change'</span>:<span class="number">-0.46</span>, <span class="string">'Volume'</span>: <span class="number">935000</span>&#125;,</span><br><span class="line">        ]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stocks.csv'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.DictWriter(f, headers)</span><br><span class="line">    f_csv.writeheader()</span><br><span class="line">    f_csv.writerows(rows)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以tab分割</span></span><br><span class="line">-----</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stock.tsv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_tsv = csv.reader(f, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_tsv:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 类型转换</span></span><br><span class="line">---------</span><br><span class="line"><span class="comment"># 第一种：tuple</span></span><br><span class="line">col_types = [str, float, str, str, float, int]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stocks.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    headers = next(f_csv)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">        <span class="comment"># Apply conversions to the row items</span></span><br><span class="line">        row = tuple(convert(value) <span class="keyword">for</span> convert, value <span class="keyword">in</span> zip(col_types, row))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种：dict</span></span><br><span class="line">print(<span class="string">'Reading as dicts with type conversion'</span>)</span><br><span class="line">field_types = [ (<span class="string">'Price'</span>, float),</span><br><span class="line">                (<span class="string">'Change'</span>, float),</span><br><span class="line">                (<span class="string">'Volume'</span>, int) ]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stocks.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> csv.DictReader(f):</span><br><span class="line">        row.update((key, conversion(row[key]))</span><br><span class="line">            <span class="keyword">for</span> key, conversion <span class="keyword">in</span> field_types)</span><br><span class="line">        print(row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 高级操作</span></span><br><span class="line">--------</span><br><span class="line"><span class="comment"># pandas.read_csv()</span></span><br></pre></td></tr></table></figure><h2 id="2-7-json"><a href="#2-7-json" class="headerlink" title="2.7 json"></a>2.7 json</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># json</span></span><br><span class="line"><span class="comment"># write &amp; read</span></span><br><span class="line">--------------</span><br><span class="line"><span class="comment"># 第一种 dict----str</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">data = &#123;</span><br><span class="line">        <span class="string">'name'</span> : <span class="string">'ACME'</span>,<span class="string">'shares'</span> : <span class="number">100</span>,</span><br><span class="line">        <span class="string">'price'</span> : <span class="number">542.23</span></span><br><span class="line">        &#125;</span><br><span class="line">json_str = json.dumps(data)</span><br><span class="line">daa = json.loads(json_str)</span><br><span class="line"><span class="comment"># 第二种 dict----file</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.json'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(data, f)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.json'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = json.load(f)</span><br></pre></td></tr></table></figure><hr><h1 id="3-sys-path"><a href="#3-sys-path" class="headerlink" title="3. sys.path"></a>3. sys.path</h1><p>sys.path:动态地改变Python搜索路径<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(’引用模块的地址<span class="string">')</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#或者</span></span><br><span class="line"><span class="string">import sys</span></span><br><span class="line"><span class="string">sys.path.insert(0, '</span>引用模块的地址<span class="string">')</span></span><br></pre></td></tr></table></figure></p><h1 id="4-os-path"><a href="#4-os-path" class="headerlink" title="4. os.path"></a>4. os.path</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">os.path.abspath(path) <span class="comment">#返回绝对路径</span></span><br><span class="line">os.path.basename(path) <span class="comment">#返回文件名</span></span><br><span class="line">os.path.exists(path)  <span class="comment">#路径存在则返回True,路径损坏返回False</span></span><br><span class="line">os.path.dirname(path) <span class="comment">#返回文件路径</span></span><br><span class="line">os.path.expanduser(path)  <span class="comment">#把path中包含的"~"和"~user"转换成用户目录</span></span><br><span class="line">os.path.isabs(path)  <span class="comment">#判断是否为绝对路径</span></span><br><span class="line">os.path.isfile(path)  <span class="comment">#判断路径是否为文件</span></span><br><span class="line">os.path.isdir(path)  <span class="comment">#判断路径是否为目录</span></span><br><span class="line">os.path.join(path1[, path2[, ...]])  <span class="comment">#把目录和文件名合成一个路径</span></span><br><span class="line">os.path.samefile(path1, path2)  <span class="comment">#判断目录或文件是否相同</span></span><br><span class="line">os.path.split(path)  <span class="comment">#把路径分割成dirname和basename，返回一个元组</span></span><br><span class="line">os.path.splitext(path)  <span class="comment">#分割路径，返回路径名和文件扩展名的元组</span></span><br><span class="line">os.path.walk(path, visit, arg)</span><br></pre></td></tr></table></figure><hr><h1 id="5-glob-glob"><a href="#5-glob-glob" class="headerlink" title="5. glob.glob"></a>5. glob.glob</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line">listglob = []</span><br><span class="line">listglob = glob.glob(<span class="string">r"/home/xxx/picture/*.png"</span>)</span><br><span class="line">listglob.sort()</span><br><span class="line">print(listglob)</span><br></pre></td></tr></table></figure><h1 id="6-argparse"><a href="#6-argparse" class="headerlink" title="6. argparse"></a>6. argparse</h1><p>参考链接：<a href="http://lib.csdn.net/article/python/49052" target="_blank" rel="noopener">http://lib.csdn.net/article/python/49052</a><br><a href="https://blog.csdn.net/u010895119/article/details/78960740" target="_blank" rel="noopener">https://blog.csdn.net/u010895119/article/details/78960740</a><br><a href="https://www.jianshu.com/p/a50aead61319" target="_blank" rel="noopener">https://www.jianshu.com/p/a50aead61319</a><br><a href="https://blog.csdn.net/guoyajie1990/article/details/76739977" target="_blank" rel="noopener">https://blog.csdn.net/guoyajie1990/article/details/76739977</a></p><p>不是很适合交互式调试 命令行参数<br>分为位置参数和选项参数</p><h2 id="位置参数"><a href="#位置参数" class="headerlink" title="位置参数"></a>位置参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">"say something about this application !!"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'name'</span>, type=int,  help=<span class="string">"i can tell you how to set a name argument"</span>)</span><br><span class="line"></span><br><span class="line">result = parser.parse_args()</span><br><span class="line"></span><br><span class="line">print(result.name)</span><br><span class="line"></span><br><span class="line">$python main.py taylor</span><br><span class="line">taylor</span><br></pre></td></tr></table></figure><h2 id="选项参数"><a href="#选项参数" class="headerlink" title="选项参数"></a>选项参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">"say something about this application !!"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-a"</span>,<span class="string">"--age"</span>, help=<span class="string">"this is an optional argument"</span>)</span><br><span class="line">result = parser.parse_args()</span><br><span class="line">print(result.age)</span><br><span class="line"></span><br><span class="line">$python main.py  --age <span class="number">888</span></span><br><span class="line"><span class="number">888</span></span><br><span class="line">$python main.py  --age=<span class="number">888</span></span><br><span class="line"><span class="number">888</span></span><br></pre></td></tr></table></figure><h2 id="特殊的选项参数"><a href="#特殊的选项参数" class="headerlink" title="特殊的选项参数"></a>特殊的选项参数</h2><p>起着开关的作用<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line">parser = argparse.ArgumentParser(<span class="attribute">description</span>=<span class="string">"say something about this application !!"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-a"</span>, <span class="string">"--age"</span>, <span class="attribute">help</span>=<span class="string">"this is an optional argument"</span>, <span class="attribute">action</span>=<span class="string">"store_true"</span>)</span><br><span class="line">result = parser.parse_args()</span><br><span class="line"><span class="builtin-name">print</span>(result.age)</span><br><span class="line"></span><br><span class="line"><span class="variable">$python</span> main.py  -a</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure></p><p>指定选项<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">"say something about this application !!"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-a"</span>, <span class="string">"--age"</span>, help=<span class="string">"this is an optional argument"</span>, type=int, choices=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">result = parser.parse_args()</span><br><span class="line">print(result.age)</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>nargs<br>nargs=N(N是int类型)，nargs=’*’, nargs=’?’某个参数接受的值，nargs定义了值的个数，加了nargs后，接受的值会变成一个list，’?’代表一个值，’*’代表一个或多个值，举例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'capital'</span>, default=<span class="string">'hello'</span>, nargs=<span class="number">1</span>, help=<span class="string">'将首字母大写'</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">print(args)</span><br><span class="line">print(args.capital)</span><br></pre></td></tr></table></figure></p><p>计数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">"-v"</span>, <span class="string">"--verbosity"</span>,</span><br><span class="line">       action=<span class="string">"count"</span>, default=<span class="number">0</span>, help=<span class="string">"increase output verbosity"</span>)</span><br></pre></td></tr></table></figure></p><h1 id="7-defaultdict"><a href="#7-defaultdict" class="headerlink" title="7. defaultdict"></a>7. defaultdict</h1><p>遍历生成字典<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">counts = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> strings:</span><br><span class="line">    counts[k]=counts.setdault(k, <span class="number">0</span>)+<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">dd = defaultdict(int)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> strings:</span><br><span class="line">    counts[k]=counts[k]+<span class="number">1</span></span><br></pre></td></tr></table></figure></p><h1 id="8-shuti"><a href="#8-shuti" class="headerlink" title="8. shuti"></a>8. shuti</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">shutil.copy 复制文件</span><br><span class="line">shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉</span><br><span class="line">shutil.move( src, dst)  移动文件或重命名</span><br><span class="line">shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的</span><br><span class="line">shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间</span><br><span class="line">shutil.copy( src, dst)  复制一个文件到一个文件或一个目录</span><br><span class="line">shutil.copy2( src, dst)  在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，类似于cp –p的东西</span><br><span class="line">shutil.copy2( src, dst)  如果两个位置的文件系统是一样的话相当于是rename操作，只是改名；如果是不在相同的文件系统的话就是做move操作</span><br><span class="line">shutil.copytree( olddir, newdir, <span class="keyword">True</span>/Flase)</span><br><span class="line">把olddir拷贝一份newdir，如果第<span class="number">3</span>个参数是<span class="keyword">True</span>，则复制目录时将保持文件夹下的符号连接，如果第<span class="number">3</span>个参数是<span class="keyword">False</span>，则将在复制的目录下生成物理副本来替代符号连接</span><br><span class="line">shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容</span><br></pre></td></tr></table></figure><h1 id="9-pickle"><a href="#9-pickle" class="headerlink" title="9. pickle"></a>9. pickle</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pickle</span><br><span class="line">python codebook</span><br></pre></td></tr></table></figure><p>注释：序列化对象，将对象obj保存到文件file中去。参数protocol是序列化模式，默认是0（ASCII协议，表示以文本的形式进行序列化），protocol的值还可以是1和2（1和2表示以二进制的形式进行序列化。其中，1是老式的二进制协议；2是新二进制协议）。file表示保存到的类文件对象，file必须有write()接口，file可以是一个以’w’打开的文件或者是一个StringIO对象，也可以是任何可以实现write()接口的对象。</p><h1 id="10-重定向"><a href="#10-重定向" class="headerlink" title="10.重定向"></a>10.重定向</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Logger</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, fpath=None)</span>:</span></span><br><span class="line">        self.console = sys.stdout</span><br><span class="line">        self.file = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> fpath <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            mkdir_if_missing(os.path.dirname(fpath))</span><br><span class="line">            self.file = open(fpath, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(self, msg)</span>:</span></span><br><span class="line">        self.console.write(msg)</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.write(msg)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">flush</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.console.flush()</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.flush()</span><br><span class="line">            os.fsync(self.file.fileno())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.console.close()</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkdir_if_missing</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">        os.makedirs(path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    sys.stdout = Logger(fpath=<span class="string">'./log.txt'</span>)</span><br><span class="line">    print(<span class="string">'2222222222'</span>)</span><br></pre></td></tr></table></figure><h1 id="其他"><a href="#其他" class="headerlink" title=". 其他"></a>. 其他</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># id2labels</span></span><br><span class="line">trainval_ids2labels = dict(zip(trainval_ids, range(len(trainval_ids))))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这也是关于python的一些记录，与其他python不矛盾，也不一定相互独立。&lt;br&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-optim</title>
    <link href="http://yoursite.com/2018/11/05/pytorch-optim/"/>
    <id>http://yoursite.com/2018/11/05/pytorch-optim/</id>
    <published>2018-11-05T09:13:57.000Z</published>
    <updated>2018-11-05T15:15:49.854Z</updated>
    
    <content type="html"><![CDATA[<p>pytorch-optim<br><a id="more"></a></p><h1 id="optim的学习率设置问题"><a href="#optim的学习率设置问题" class="headerlink" title="optim的学习率设置问题"></a>optim的学习率设置问题</h1><h2 id="1-不同的学习率"><a href="#1-不同的学习率" class="headerlink" title="1.不同的学习率"></a>1.不同的学习率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">optimizer = t.optim.Adam(model.parameters(), lr = <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">optim_group = [&#123;<span class="string">'params'</span>:model.net1.parameters(),<span class="string">'lr'</span>:<span class="number">0.4</span>&#125;,</span><br><span class="line">       &#123;<span class="string">'params'</span>:model.net2.parameters(),<span class="string">'lr'</span>:<span class="number">0.1</span>&#125;]</span><br><span class="line">optimizer = t.optim.Adam(optim_group,lr=<span class="number">0.04</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三种</span></span><br><span class="line">ignored_params = list(map(id, model.model.fc.parameters() )) </span><br><span class="line">               + list(map(id, model.classifier.parameters() ))</span><br><span class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> ignored_params, model.parameters())</span><br><span class="line">optimizer_ft = optim.SGD([</span><br><span class="line">     &#123;<span class="string">'params'</span>: base_params, <span class="string">'lr'</span>: <span class="number">0.01</span>&#125;,</span><br><span class="line">     &#123;<span class="string">'params'</span>: model.model.fc.parameters(), <span class="string">'lr'</span>: <span class="number">0.1</span>&#125;,</span><br><span class="line">     &#123;<span class="string">'params'</span>: model.classifier.parameters(), <span class="string">'lr'</span>: <span class="number">0.1</span>&#125;</span><br><span class="line"> ], weight_decay=<span class="number">5e-4</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><h2 id="2-学习率衰减"><a href="#2-学习率衰减" class="headerlink" title="2. 学习率衰减"></a>2. 学习率衰减</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=<span class="number">40</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, scheduler, num_epochs=<span class="number">25</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">            optimizer.zero_grad()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;pytorch-optim&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-init</title>
    <link href="http://yoursite.com/2018/11/05/pytorch-init/"/>
    <id>http://yoursite.com/2018/11/05/pytorch-init/</id>
    <published>2018-11-05T08:55:36.000Z</published>
    <updated>2018-11-05T15:16:03.014Z</updated>
    
    <content type="html"><![CDATA[<p>pytorch-init<br><a id="more"></a></p><h1 id="pytorch模型的初始化"><a href="#pytorch模型的初始化" class="headerlink" title="pytorch模型的初始化"></a>pytorch模型的初始化</h1><p>pytorch模型的初始化的常用方法。</p><h2 id="1-apply-type"><a href="#1-apply-type" class="headerlink" title="1.apply+type"></a>1.apply+type</h2><p>apply可以理解成从children开始遍历<br>可以用于<strong>init</strong>，可以用于model定义之后，与type配合。</p><blockquote><p>Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init).</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># define model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        self.pre = nn.Sequential(nn.Linear(<span class="number">2</span>,<span class="number">2</span>), nn.Conv2d(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">        self.two = nn.Sequential(nn.Linear(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">        self.apply(init_weights)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    print(m)</span><br><span class="line">    print(type(m))</span><br><span class="line">    print(nn.Linear)</span><br><span class="line">    print(m.__class__)</span><br><span class="line">    print(m.__class__.__name__)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">        m.weight.data.fill_(<span class="number">0.0</span>)</span><br><span class="line">        print(m.weight.data)</span><br><span class="line">    print(<span class="string">"_______________________"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net2 = Net()</span><br><span class="line"></span><br><span class="line">Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Linear</span></span></span><br><span class="line"><span class="class"><span class="title">tensor</span><span class="params">([[ <span class="number">0.</span>,  <span class="number">0.</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">        [ <span class="number">0.</span>,  <span class="number">0.</span>]])</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br><span class="line"><span class="class"><span class="title">Conv2d</span><span class="params">(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="params">(<span class="number">3</span>, <span class="number">3</span>)</span>, stride=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">conv</span>.<span class="title">Conv2d</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">conv</span>.<span class="title">Conv2d</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Conv2d</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br><span class="line"><span class="class"><span class="title">Sequential</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">  <span class="params">(<span class="number">0</span>)</span>: Linear<span class="params">(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=True)</span></span></span></span><br><span class="line"><span class="class"><span class="params">  <span class="params">(<span class="number">1</span>)</span>: Conv2d<span class="params">(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="params">(<span class="number">3</span>, <span class="number">3</span>)</span>, stride=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span></span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Sequential</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br><span class="line"><span class="class"><span class="title">Linear</span><span class="params">(in_features=<span class="number">3</span>, out_features=<span class="number">3</span>, bias=True)</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Linear</span></span></span><br><span class="line"><span class="class"><span class="title">tensor</span><span class="params">([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br><span class="line"><span class="class"><span class="title">Sequential</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">  <span class="params">(<span class="number">0</span>)</span>: Linear<span class="params">(in_features=<span class="number">3</span>, out_features=<span class="number">3</span>, bias=True)</span></span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Sequential</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br><span class="line"><span class="class"><span class="title">Net</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">  <span class="params">(pre)</span>: Sequential<span class="params">(</span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">    <span class="params">(<span class="number">0</span>)</span>: Linear<span class="params">(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=True)</span></span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">    <span class="params">(<span class="number">1</span>)</span>: Conv2d<span class="params">(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="params">(<span class="number">3</span>, <span class="number">3</span>)</span>, stride=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span></span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">  )</span></span></span></span><br><span class="line"><span class="class"><span class="params">  <span class="params">(two)</span>: Sequential<span class="params">(</span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">    <span class="params">(<span class="number">0</span>)</span>: Linear<span class="params">(in_features=<span class="number">3</span>, out_features=<span class="number">3</span>, bias=True)</span></span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">  )</span></span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">__main__</span>.<span class="title">Net</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">__main__</span>.<span class="title">Net</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Net</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br></pre></td></tr></table></figure><h1 id="2-apply-m-class-name"><a href="#2-apply-m-class-name" class="headerlink" title="2.apply+m.class.name"></a>2.apply+m.<strong>class</strong>.<strong>name</strong></h1><p>weights_init_kaiming<br>还要一种初始化函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init_kaiming</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="comment"># print(classname)</span></span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        init.kaiming_normal(m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_in'</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">'Linear'</span>) != <span class="number">-1</span>:</span><br><span class="line">        init.kaiming_normal(m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_out'</span>)</span><br><span class="line">        init.constant(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm1d'</span>) != <span class="number">-1</span>:</span><br><span class="line">        init.normal(m.weight.data, <span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        init.constant(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init_classifier</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Linear'</span>) != <span class="number">-1</span>:</span><br><span class="line">        init.normal(m.weight.data, std=<span class="number">0.001</span>)</span><br><span class="line">        init.constant(m.bias.data, <span class="number">0.0</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;pytorch-init&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>cuda安装教程</title>
    <link href="http://yoursite.com/2018/11/05/cuda/"/>
    <id>http://yoursite.com/2018/11/05/cuda/</id>
    <published>2018-11-05T08:49:29.000Z</published>
    <updated>2018-11-05T08:53:08.588Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>安装教程参考链接<br><a href="https://www.jianshu.com/p/35c7fde85968" target="_blank" rel="noopener">https://www.jianshu.com/p/35c7fde85968</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;安装教程参考链接&lt;br&gt;&lt;a href=&quot;https://www.jianshu.com/p/35c7fde85968&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/3
      
    
    </summary>
    
      <category term="cuda" scheme="http://yoursite.com/categories/cuda/"/>
    
    
      <category term="cuda" scheme="http://yoursite.com/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>python_reptilian</title>
    <link href="http://yoursite.com/2018/10/05/python-reptilian/"/>
    <id>http://yoursite.com/2018/10/05/python-reptilian/</id>
    <published>2018-10-05T03:13:00.000Z</published>
    <updated>2018-10-05T14:39:27.400Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这是第一次爬虫，以理论为主，以实现为辅。因为是到处看的，所以不是很有逻辑性。<br>参考链接：<br><a href="https://www.cnblogs.com/sss4/p/7809821.html" target="_blank" rel="noopener">python爬虫原理</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&amp;mid=2247487461&amp;idx=2&amp;sn=9cf9910f44808e429f32731fb5214380&amp;chksm=ec5ed098db29598e9ce3a99a1ed6005fe552c9fde8533639c561e3ca7c44d71f6f053c68de8e&amp;mpshare=1&amp;scene=1&amp;srcid=0827ZJWXHEkP54zWy7t6FfGV#rd" target="_blank" rel="noopener">Python爬虫的两套解析方法和四种爬虫实现</a><br><a href="http://www.cnblogs.com/linhaifeng/articles/7773496.html" target="_blank" rel="noopener">爬虫基本原理</a><br><a id="more"></a></p><h1 id="一、工具"><a href="#一、工具" class="headerlink" title="一、工具"></a>一、工具</h1><ul><li>两个解析库：BeautifulSoup, lxml</li><li>两个请求库：urllib, requests</li><li>法法</li></ul><hr><h1 id="二、爬虫流程"><a href="#二、爬虫流程" class="headerlink" title="二、爬虫流程"></a>二、爬虫流程</h1><p>用户获取网络数据的方式<br>方式1:浏览器提交请求—-&gt;下载网页代码—-&gt;解析成页面<br>方式2:模拟浏览器发送请求(获取网页代码)—-&gt;提取有用数据—-&gt;存放在数据库或者文件中<br>爬虫就是指方式2.</p><h2 id="1-发起请求"><a href="#1-发起请求" class="headerlink" title="1.发起请求"></a>1.发起请求</h2><p>使用http库向目标站点发送请求，即发送一个Request。<br>Request包含：请求头，请求体等。<br>Request模块缺点：不能执行JS和CSS代码。</p><h2 id="2-获取响应内容"><a href="#2-获取响应内容" class="headerlink" title="2.获取响应内容"></a>2.获取响应内容</h2><p>服务器正常响应，得到一个Response。<br>Response包含：html，json，图片，视频等。</p><h2 id="3-解析内容"><a href="#3-解析内容" class="headerlink" title="3.解析内容"></a>3.解析内容</h2><p>解析html数据：正则表达式(RE模块)，第三方解析库如BeautifulSoup,pyquery等<br>解析json数据：json模块<br>解析二进制数据：以wb形式写入文件</p><h2 id="4-保存数据"><a href="#4-保存数据" class="headerlink" title="4.保存数据"></a>4.保存数据</h2><p>数据库（MySQL, Mongdb, Redis)<br>文件</p><hr><h1 id="三、Request-amp-Response"><a href="#三、Request-amp-Response" class="headerlink" title="三、Request&amp;Response"></a>三、Request&amp;Response</h1><h2 id="1-Request"><a href="#1-Request" class="headerlink" title="1.Request"></a>1.Request</h2><h3 id="1-1-请求方式"><a href="#1-1-请求方式" class="headerlink" title="1.1.请求方式"></a>1.1.请求方式</h3><p>常见的有：GET/POST</p><h3 id="1-2-请求的URL"><a href="#1-2-请求的URL" class="headerlink" title="1.2.请求的URL"></a>1.2.请求的URL</h3><p>url是全球容易资源定位符，用来丁意思互联网上一个唯一的资源，例如：一张图片、一个文件、一段视频。</p><h3 id="1-3-请求头"><a href="#1-3-请求头" class="headerlink" title="1.3.请求头"></a>1.3.请求头</h3><p>User-agen：访问的浏览器请求头没有user-agent客户端配置，会被当成非法用户host<br>cookies：cookie用来保存登录信息<br>Referrer：访问源至哪里来</p><h3 id="1-4-请求体"><a href="#1-4-请求体" class="headerlink" title="1.4.请求体"></a>1.4.请求体</h3><p>get：请求体没有内容<br>post：请求体是format data</p><h2 id="2-Response"><a href="#2-Response" class="headerlink" title="2.Response"></a>2.Response</h2><h3 id="2-1-响应状态码"><a href="#2-1-响应状态码" class="headerlink" title="2.1 响应状态码"></a>2.1 响应状态码</h3><p>200：代表成功<br>301：代表调转<br>404：文件不存在<br>403：无权限访问<br>502：服务器错误</p><h3 id="2-2-响应头"><a href="#2-2-响应头" class="headerlink" title="2.2 响应头"></a>2.2 响应头</h3><p>Set-Cookie:BDSVRTM=0; path=/：可能有多个，是来告诉浏览器，把cookie保存下来<br>Content-Location：服务端响应头中包含Location返回浏览器之后，浏览器就会重新访问另一个页面</p><h3 id="2-3preview"><a href="#2-3preview" class="headerlink" title="2.3preview"></a>2.3preview</h3><p>网页源代码，包括：<br>Json数据、html、图片、二进制数据</p><h1 id="接下来开始尝试写一些基本的爬虫代码，并做记录"><a href="#接下来开始尝试写一些基本的爬虫代码，并做记录" class="headerlink" title="接下来开始尝试写一些基本的爬虫代码，并做记录"></a>接下来开始尝试写一些基本的爬虫代码，并做记录</h1><p><code>`</code>python</p><h1 id="发起请求，并获取请求内容"><a href="#发起请求，并获取请求内容" class="headerlink" title="发起请求，并获取请求内容"></a>发起请求，并获取请求内容</h1><p>from urllib import request<br>resp = request.urlopen(‘<a href="https://movie.douban.com/nowplaying/hangzhou/&#39;" target="_blank" rel="noopener">https://movie.douban.com/nowplaying/hangzhou/&#39;</a>) # http.client.HTTPResponse<br>html_data = resp.read().decode(‘utf-8’) # str 这里的print是最好看的</p><h1 id="解析内容"><a href="#解析内容" class="headerlink" title="解析内容"></a>解析内容</h1><p>from bs4 import BeautifulSoup as bs<br>soup = bs(html_data, ‘html.parser’)  # bs4.BeautifulSoup<br>nowplaying_movie = soup.find_all(‘div’, id=’nowplaying’) # bs4.element.ResultSet list的形式，可以暂时看成是多个组成的list，需要先[0]的进行访问。<br>tmp = nowplaying_movie[0] # bs4.element.Tag<br>nowplaying_movie_list = nowplaying_movie[0].find_all(‘li’, class_=’list-item’) # bs4.element.ResultSet  list形式， bs4.element.Tag<br>nowplaying_list = []  # 此时就是直接获取数据了，find_all是对相应片段的截取<br>for item in nowplaying_movie_list:<br>    nowplaying_dict = {}<br>    nowplaying_dict[‘id’] = item[‘data-subject’]<br>    for tag_img_item in item.find_all(‘img’):<br>        nowplaying_dict[‘name’] = tag_img_item[‘alt’]<br>        nowplaying_list.append(nowplaying_dict)</p><p>requrl = ‘<a href="https://movie.douban.com/subject/&#39;+nowplaying_list[0][&#39;id&#39;]" target="_blank" rel="noopener">https://movie.douban.com/subject/&#39;+nowplaying_list[0][&#39;id&#39;]</a> + ‘/comments’ +’?’ +’start=0’ + ‘&amp;limit=20’</p><h1 id="三句一体"><a href="#三句一体" class="headerlink" title="三句一体"></a>三句一体</h1><p>resp = request.urlopen(requrl)<br>html_data = resp.read().decode(‘utf-8’)<br>soup = bs(html_data, ‘html.parser’)</p><p>comment_div_lists[0].find_all(‘span’, class_=”short”)[0].string # .string 可以暂时理解成中间的字符串</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;这是第一次爬虫，以理论为主，以实现为辅。因为是到处看的，所以不是很有逻辑性。&lt;br&gt;参考链接：&lt;br&gt;&lt;a href=&quot;https://www.cnblogs.com/sss4/p/7809821.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;python爬虫原理&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&amp;amp;mid=2247487461&amp;amp;idx=2&amp;amp;sn=9cf9910f44808e429f32731fb5214380&amp;amp;chksm=ec5ed098db29598e9ce3a99a1ed6005fe552c9fde8533639c561e3ca7c44d71f6f053c68de8e&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0827ZJWXHEkP54zWy7t6FfGV#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Python爬虫的两套解析方法和四种爬虫实现&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.cnblogs.com/linhaifeng/articles/7773496.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;爬虫基本原理&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>git</title>
    <link href="http://yoursite.com/2018/10/04/git/"/>
    <id>http://yoursite.com/2018/10/04/git/</id>
    <published>2018-10-04T08:06:55.000Z</published>
    <updated>2018-11-19T05:03:22.267Z</updated>
    
    <content type="html"><![CDATA[<h1 id="learning-git"><a href="#learning-git" class="headerlink" title="learning git"></a>learning git</h1><a id="more"></a><p>自己之前已经学过一次git了，但是最近在用的时候，仍然感觉不顺手，所以今天趁这个机会，再学一遍，这一次，以命令为主，以原理为辅。</p><h2 id="初始化仓库"><a href="#初始化仓库" class="headerlink" title="初始化仓库"></a>初始化仓库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br></pre></td></tr></table></figure><h2 id="文件到Git仓库"><a href="#文件到Git仓库" class="headerlink" title="文件到Git仓库"></a>文件到Git仓库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git add readme.txt</span><br><span class="line">$ git commit -m <span class="string">"wrote a readme file"</span></span><br></pre></td></tr></table></figure><h2 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><h2 id="查看修改内容"><a href="#查看修改内容" class="headerlink" title="查看修改内容"></a>查看修改内容</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff readme.txt</span><br></pre></td></tr></table></figure><h2 id="查看提交日志"><a href="#查看提交日志" class="headerlink" title="查看提交日志"></a>查看提交日志</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span></span><br></pre></td></tr></table></figure><h2 id="版本回到过去和将来"><a href="#版本回到过去和将来" class="headerlink" title="版本回到过去和将来"></a>版本回到过去和将来</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回到过去</span></span><br><span class="line">git reset --hard HEAD^</span><br><span class="line"><span class="comment"># HEAD~100</span></span><br><span class="line"><span class="comment"># 此时git log已经没有了最新版本的提交信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回到将来</span></span><br><span class="line">git reset --hard 1094a</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">git reflog <span class="comment"># 命令历史</span></span><br><span class="line">git reset --hard 1094a</span><br></pre></td></tr></table></figure><p><img src="https://images2017.cnblogs.com/blog/63651/201709/63651-20170905212837976-775285128.png" alt=""></p><h2 id="撤销修改"><a href="#撤销修改" class="headerlink" title="撤销修改"></a>撤销修改</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时</span></span><br><span class="line">git checkout -- readme.txt</span><br><span class="line"><span class="comment"># 当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，</span></span><br><span class="line">git reset HEAD readme.txt</span><br><span class="line">git checkout -- readme.txt</span><br></pre></td></tr></table></figure><h2 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git rm test.txt</span><br><span class="line">git commit -m <span class="string">"remove"</span></span><br></pre></td></tr></table></figure><h2 id="远程仓库克隆"><a href="#远程仓库克隆" class="headerlink" title="远程仓库克隆"></a>远程仓库克隆</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:michaelliao/gitskills.git</span><br></pre></td></tr></table></figure><h2 id="添加远程仓库"><a href="#添加远程仓库" class="headerlink" title="添加远程仓库"></a>添加远程仓库</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># git://不支持push</span></span><br><span class="line">git remote add origin git://github.com/TJJTJJTJJ/ticgit.git</span><br><span class="line"><span class="comment"># git@只是push</span></span><br><span class="line">git remote add origin git@github.com:TJJTJJTJJ/ticgit.git</span><br><span class="line"><span class="comment"># remove</span></span><br><span class="line">git remote remove origin</span><br></pre></td></tr></table></figure><h2 id="查看远程版本"><a href="#查看远程版本" class="headerlink" title="查看远程版本"></a>查看远程版本</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure><h2 id="提交到远程"><a href="#提交到远程" class="headerlink" title="提交到远程"></a>提交到远程</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><h2 id="获取远程仓库内容"><a href="#获取远程仓库内容" class="headerlink" title="获取远程仓库内容"></a>获取远程仓库内容</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git fetch origin</span><br><span class="line">git merge origin/master</span><br></pre></td></tr></table></figure><h2 id="提交远程仓库"><a href="#提交远程仓库" class="headerlink" title="提交远程仓库"></a>提交远程仓库</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin master</span><br></pre></td></tr></table></figure><p>暂时到这里，剩下的分支，自己暂时还不会用到，等用到了再去学就可以了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;learning-git&quot;&gt;&lt;a href=&quot;#learning-git&quot; class=&quot;headerlink&quot; title=&quot;learning git&quot;&gt;&lt;/a&gt;learning git&lt;/h1&gt;
    
    </summary>
    
      <category term="git" scheme="http://yoursite.com/categories/git/"/>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-chapter10-ImageCaption</title>
    <link href="http://yoursite.com/2018/09/30/pytorch-chapter10-ImageCaption/"/>
    <id>http://yoursite.com/2018/09/30/pytorch-chapter10-ImageCaption/</id>
    <published>2018-09-30T03:00:50.000Z</published>
    <updated>2018-10-04T07:58:45.171Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。<br><a id="more"></a><br>这是我的<a href="https://github.com/TJJTJJTJJ/pytorch__learn" target="_blank" rel="noopener">代码</a><br>大神链接：<a href="https://github.com/anishathalye/neural-style" target="_blank" rel="noopener">https://github.com/anishathalye/neural-style</a><br>这是论文作者写的</p><h1 id="问题以及思考"><a href="#问题以及思考" class="headerlink" title="问题以及思考"></a>问题以及思考</h1><p>这一次感觉写起来很顺利，数据的处理+基本模型的走读基本只用了两天，剩下的两天主要是耗在了beam_searching上，原理的解析和代码的思考。<br>现在记录一下这次走读的过程中学习到的东西，如果是和之前的记录有联系，那么则尽量记在一起。</p><hr><h2 id="局部反向传播管理"><a href="#局部反向传播管理" class="headerlink" title="局部反向传播管理"></a>局部反向传播管理</h2><p>部分参考第八章，基本来自官网文档<br>一共是四种</p><ul><li>@torch.no_grad()</li><li>with torch.no_grad():</li><li>torch.set_grad_enabled(bool)</li><li>with torch.set_grad_enabled(False):</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种：固定上下文管理器，torch.no_grad()和torch.enable_grad()</span></span><br><span class="line">又分为@torch.no_grad()和<span class="keyword">with</span> torch.no_grad()</span><br><span class="line">x = torch.tensor([<span class="number">1</span>], requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print(x.requires_grad)</span><br><span class="line">    y = x*<span class="number">2</span></span><br><span class="line">    print(y.requires_grad)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">y.requires_grad</span><br><span class="line"></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="comment"># 以上说明了上下文管理器内和外是一致的</span></span><br><span class="line"><span class="comment"># 下面说明上下文管理器的作用域只在局部有效</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print(x.requires_grad)</span><br><span class="line">    z = x*<span class="number">2</span></span><br><span class="line">    print(z.requires_grad)</span><br><span class="line">    <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">        print(x.requires_grad)</span><br><span class="line">        y = x*<span class="number">2</span></span><br><span class="line">        print(y.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">    print(x.requires_grad)</span><br><span class="line">    z = x*<span class="number">2</span></span><br><span class="line">    print(z.requires_grad)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        print(x.requires_grad)</span><br><span class="line">        y = x*<span class="number">2</span></span><br><span class="line">        print(y.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>@torch.no_grad()</span><br><span class="line"><span class="meta">... </span><span class="function"><span class="keyword">def</span> <span class="title">dddd</span><span class="params">()</span>:</span></span><br><span class="line"><span class="meta">... </span>    x = torch.tensor([<span class="number">2.2</span>],requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="meta">... </span>    y = <span class="number">2</span>*x</span><br><span class="line"><span class="meta">... </span>    print(y.requires_grad)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dddd()</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种：条件的上下文管理器 torch.set_grad_enabled(bool)</span></span><br><span class="line">又分为<span class="keyword">with</span>  torch.set_grad_enabled(bool)和 torch.set_grad_enabled(bool)</span><br><span class="line"><span class="keyword">with</span> torch.set_grad_enabled(<span class="keyword">False</span>):</span><br><span class="line">    print(x.requires_grad)</span><br><span class="line">    y = x*<span class="number">2</span></span><br><span class="line">    print(y.requires_grad)</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_grad_enabled(<span class="keyword">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试@torch.enable_grad()的时候没有成功，问题应该是版本问题，0.4.0的版本就不行，但是0.4.1的版本就可以了</span></span><br></pre></td></tr></table></figure><hr><h2 id="预训练模型的修改"><a href="#预训练模型的修改" class="headerlink" title="预训练模型的修改"></a>预训练模型的修改</h2><p>备注：感觉这一块应该是很条理才对，但是没有找到类似的说明<br>只能等以后见得多了，再做补充，网上有一些对特定模型的修改，但是都不全面，也没有具体说明各个方法的优劣。<br>应该是这样的，层必须和forward对应，参数的加载可以放在模型定义时，也可以放在模型定义之后。</p><h3 id="不修改原模型的forward流程"><a href="#不修改原模型的forward流程" class="headerlink" title="不修改原模型的forward流程"></a>不修改原模型的forward流程</h3><p>常用于对特定层的修改<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">model = models.resnet50(pretrained=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 只修改最后一层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">fc_features = model.fc.in_features</span><br><span class="line">model.fc = nn.Linear(fc, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">resnet50 = tv.models.resnet50(pretrained=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">del</span> resnet50.fc</span><br><span class="line">resnet50.fc = <span class="keyword">lambda</span> x: x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果直接修改out_features是没有用的</span></span><br><span class="line">model.fc.out_features = <span class="number">9</span></span><br><span class="line">resnet50.fc.weight.shape</span><br><span class="line">torch.Size([<span class="number">1000</span>, <span class="number">2048</span>])</span><br><span class="line">即如果修改某一层，要重新定义这一层</span><br></pre></td></tr></table></figure></p><h3 id="在模型内修改forward流程"><a href="#在模型内修改forward流程" class="headerlink" title="在模型内修改forward流程"></a>在模型内修改forward流程</h3><p>常用于中间层的增加<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要自己先定义类似的网络，注意定义的名字必须一致和方式需要一致，利用state_dict来更新参数</span></span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">resnet50 = models.resnet50(pretrained=<span class="keyword">True</span>)</span><br><span class="line">cnn = CNN(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line">pretrained_dict = resnet50.state_dict()</span><br><span class="line">model_dict = cnn.state_dict()</span><br><span class="line"><span class="comment"># 选取相同名字参数</span></span><br><span class="line">pretrained_dict =  &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items() <span class="keyword">if</span> k <span class="keyword">in</span> model_dict&#125;</span><br><span class="line">model_dict.update(pretrained_dict)</span><br><span class="line">cnn.load_state_dict(model_dict)</span><br><span class="line">print(cnn)</span><br></pre></td></tr></table></figure></p><h3 id="在模型外增加forward流程"><a href="#在模型外增加forward流程" class="headerlink" title="在模型外增加forward流程"></a>在模型外增加forward流程</h3><p>常用与开头或者末尾层的增加<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.add_module(<span class="string">'layer_name'</span>,layer)</span><br><span class="line">可以理解成</span><br><span class="line">self.layer_name = layer</span><br><span class="line">x = model.layer_name(x)</span><br></pre></td></tr></table></figure></p><h3 id="取特定模块，利用children-和nn-Sequential-也可以实现特定层的修改"><a href="#取特定模块，利用children-和nn-Sequential-也可以实现特定层的修改" class="headerlink" title="取特定模块，利用children()和nn.Sequential()也可以实现特定层的修改"></a>取特定模块，利用children()和nn.Sequential()也可以实现特定层的修改</h3><p>这个方法比较啰嗦，不是很推荐，或者不如第一种方法，或者不如最后一种方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model = models.vgg16(pretrained=<span class="keyword">True</span>)</span><br><span class="line">removed = list(model.classifier.children())[:<span class="number">-1</span>]</span><br><span class="line">model.classifier = torch.nn.Sequential(*removed)</span><br><span class="line">model.add_module(<span class="string">'fc'</span>, torch.nn.Linear(<span class="number">4096</span>, out_num)) <span class="comment"># out_num是你希望输出的数量 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接list(model)是不行的，但是list(model.children())就可以</span></span><br><span class="line">list(ResNet34.children())</span><br><span class="line">In [<span class="number">23</span>]: <span class="keyword">for</span> i <span class="keyword">in</span> ResNet34.children():</span><br><span class="line">    ...:     print(type(i))</span><br><span class="line">    ...:     </span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br></pre></td></tr></table></figure></p><h3 id="取特定模块"><a href="#取特定模块" class="headerlink" title="取特定模块"></a>取特定模块</h3><p>利用list和modulelist，可用于对于特定模块的特定操作，可修改forward流程<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第八章的方法 定义新模型， 在模型定义时，加载原模型参数， 修改forward 对于单向的还好，对于有分支的还没有尝试 用了list和modulelist 直接在定义模型的地方取</span></span><br><span class="line">features = list(vgg16(pretrained=<span class="keyword">True</span>).features)[:<span class="number">23</span>]</span><br><span class="line">self.features = nn.ModuleList(features).eval()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ii, model <span class="keyword">in</span> enumerate(self.features):</span><br><span class="line">    x = model(x)</span><br><span class="line">    <span class="keyword">if</span> ii <span class="keyword">in</span> &#123;<span class="number">3</span>,<span class="number">8</span>,<span class="number">15</span>,<span class="number">22</span>&#125;:</span><br><span class="line">        results.append(x)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> resnet34.named_children():</span><br><span class="line">    print(k,v)</span><br><span class="line"></span><br><span class="line">conv1 Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">3</span>, <span class="number">3</span>), bias=<span class="keyword">False</span>)</span><br><span class="line">bn1 BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>)</span><br><span class="line">relu ReLU(inplace)</span><br><span class="line">maxpool MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">layer1 Sequential(</span><br><span class="line">  (<span class="number">0</span>): BasicBlock(</span><br><span class="line">    (conv1): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="keyword">False</span>)</span><br><span class="line">    (bn1): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>)</span><br><span class="line">    (relu): ReLU(inplace)</span><br><span class="line">    (conv2): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="keyword">False</span>)</span><br><span class="line">    (bn2): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>)</span><br><span class="line">  )</span><br><span class="line">...</span><br></pre></td></tr></table></figure><hr><h2 id="tensor-new和fill-和copy"><a href="#tensor-new和fill-和copy" class="headerlink" title="tensor.new和fill_和copy_"></a>tensor.new和fill_和copy_</h2><h3 id="在第九章，创建同类型的tensor用new-保证类型和cuda一致，不保证requires-grad，保证了和源类型一致，不共享内存"><a href="#在第九章，创建同类型的tensor用new-保证类型和cuda一致，不保证requires-grad，保证了和源类型一致，不共享内存" class="headerlink" title="在第九章，创建同类型的tensor用new,保证类型和cuda一致，不保证requires_grad，保证了和源类型一致，不共享内存"></a>在第九章，创建同类型的tensor用new,保证类型和cuda一致，不保证requires_grad，保证了和源类型一致，不共享内存</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.Tensor([<span class="number">2.2</span>],requires_grad=<span class="keyword">True</span>).cuda()</span><br><span class="line">x</span><br><span class="line">tensor([ <span class="number">3.2000</span>], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">y = x.new([<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">y</span><br><span class="line">y.requires_grad</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line">tensor([ <span class="number">4.</span>,  <span class="number">5.</span>], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">z = x.data.new([<span class="number">6</span>,<span class="number">7</span>])</span><br><span class="line">z.requires_grad</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line">z</span><br><span class="line">tensor([ <span class="number">6.</span>,  <span class="number">7.</span>], device=<span class="string">'cuda:0'</span>)</span><br></pre></td></tr></table></figure><h3 id="在第十章，创建同类型同样大小同cuda的tensor，用fill-，fill-也保证了类型和cuda一致，保证了和目标类型一致"><a href="#在第十章，创建同类型同样大小同cuda的tensor，用fill-，fill-也保证了类型和cuda一致，保证了和目标类型一致" class="headerlink" title="在第十章，创建同类型同样大小同cuda的tensor，用fill_，fill_也保证了类型和cuda一致，保证了和目标类型一致"></a>在第十章，创建同类型同样大小同cuda的tensor，用fill_，fill_也保证了类型和cuda一致，保证了和目标类型一致</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">36</span>]: x = t.Tensor(<span class="number">3</span>,<span class="number">4</span>).cuda()</span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: x</span><br><span class="line">Out[<span class="number">37</span>]: </span><br><span class="line">tensor([[ <span class="number">1.1395e-19</span>,  <span class="number">4.5886e-41</span>,  <span class="number">3.4482e+25</span>,  <span class="number">3.0966e-41</span>],</span><br><span class="line">        [ <span class="number">5.7353e-31</span>,  <span class="number">4.5886e-41</span>, <span class="number">-1.2545e+37</span>,  <span class="number">1.3914e+25</span>],</span><br><span class="line">        [ <span class="number">2.9680e-31</span>,  <span class="number">4.5886e-41</span>,  <span class="number">5.7344e-31</span>,  <span class="number">4.5886e-41</span>]],</span><br><span class="line">       device=<span class="string">'cuda:0'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">38</span>]: x.fill_(<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">38</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], device=<span class="string">'cuda:0'</span>)</span><br><span class="line"><span class="comment"># 测试requires_grad提示，不能</span></span><br><span class="line">In [<span class="number">43</span>]: x.requires_grad= <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">44</span>]: x</span><br><span class="line">Out[<span class="number">44</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], device=<span class="string">'cuda:0'</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">45</span>]: x.fill_(<span class="number">1</span>)</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">RuntimeError                              Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-45</span><span class="number">-0</span>c255de765ba&gt; <span class="keyword">in</span> &lt;module&gt;()</span><br><span class="line">----&gt; 1 x.fill_(1)</span><br><span class="line"></span><br><span class="line">RuntimeError: a leaf Variable that requires grad has been used <span class="keyword">in</span> an <span class="keyword">in</span>-place operation.</span><br><span class="line"><span class="comment"># 强行修改值，则grad_fn也发生了变化。</span></span><br><span class="line">In [<span class="number">46</span>]: x[<span class="number">0</span>]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: x</span><br><span class="line">Out[<span class="number">47</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], device=<span class="string">'cuda:0'</span>, grad_fn=&lt;CopySlices&gt;)</span><br></pre></td></tr></table></figure><h3 id="第十章的copy-，类型不变，"><a href="#第十章的copy-，类型不变，" class="headerlink" title="第十章的copy_，类型不变，"></a>第十章的copy_，类型不变，</h3><p>可以作为计算图进行保留<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不共享内存</span></span><br><span class="line">In [<span class="number">49</span>]: x = t.Tensor(<span class="number">2</span>,<span class="number">2</span>).fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">50</span>]: x</span><br><span class="line">Out[<span class="number">50</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">51</span>]: y = t.Tensor(<span class="number">1</span>,<span class="number">2</span>).fill_(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">52</span>]: y</span><br><span class="line">Out[<span class="number">52</span>]: tensor([[<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">53</span>]: x[<span class="number">0</span>].copy_(y[<span class="number">0</span>])</span><br><span class="line">Out[<span class="number">53</span>]: tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">54</span>]: x</span><br><span class="line">Out[<span class="number">54</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">55</span>]: y</span><br><span class="line">Out[<span class="number">55</span>]: tensor([[<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">56</span>]: y[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">57</span>]: y</span><br><span class="line">Out[<span class="number">57</span>]: tensor([[<span class="number">2.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">58</span>]: x</span><br><span class="line">Out[<span class="number">58</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类型不变</span></span><br><span class="line">In [<span class="number">60</span>]: y = t.IntTensor(<span class="number">1</span>,<span class="number">2</span>).fill_(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">61</span>]: y</span><br><span class="line">Out[<span class="number">61</span>]: tensor([[<span class="number">1</span>, <span class="number">1</span>]], dtype=torch.int32)</span><br><span class="line"></span><br><span class="line">In [<span class="number">62</span>]: x = t.Tensor(<span class="number">2</span>,<span class="number">2</span>).fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">63</span>]: x</span><br><span class="line">Out[<span class="number">63</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">64</span>]: x[<span class="number">0</span>].copy_(y[<span class="number">0</span>])</span><br><span class="line">Out[<span class="number">64</span>]: tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">65</span>]: x</span><br><span class="line">Out[<span class="number">65</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># requires_grad，会作为一个计算图保留</span></span><br><span class="line">In [<span class="number">66</span>]: x</span><br><span class="line">Out[<span class="number">66</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">67</span>]: x.requires_grad=<span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">68</span>]: x</span><br><span class="line">Out[<span class="number">68</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]], requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">69</span>]: x[<span class="number">1</span>].copy_(y[<span class="number">0</span>])</span><br><span class="line">Out[<span class="number">69</span>]: tensor([<span class="number">1.</span>, <span class="number">1.</span>], grad_fn=&lt;AsStridedBackward&gt;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cuda，可以保留</span></span><br><span class="line">In [<span class="number">76</span>]: y[<span class="number">0</span>]=<span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">77</span>]: y</span><br><span class="line">Out[<span class="number">77</span>]: tensor([[<span class="number">2</span>, <span class="number">2</span>]], dtype=torch.int32)</span><br><span class="line"></span><br><span class="line">In [<span class="number">78</span>]: x[<span class="number">1</span>].copy_(y[<span class="number">0</span>])</span><br><span class="line">Out[<span class="number">78</span>]: tensor([<span class="number">2.</span>, <span class="number">2.</span>], device=<span class="string">'cuda:0'</span>, grad_fn=&lt;AsStridedBackward&gt;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">79</span>]: x</span><br><span class="line">Out[<span class="number">79</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>]], device=<span class="string">'cuda:0'</span>, grad_fn=&lt;CopySlices&gt;)</span><br></pre></td></tr></table></figure></p><hr><h2 id="tensor赋值操作-只复制值，不共享内存"><a href="#tensor赋值操作-只复制值，不共享内存" class="headerlink" title="tensor赋值操作   只复制值，不共享内存"></a>tensor赋值操作   只复制值，不共享内存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种 利用tensor 只复制值</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: x = t.tensor([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: x</span><br><span class="line">Out[<span class="number">7</span>]: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: y = t.tensor(x)</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: y</span><br><span class="line">Out[<span class="number">9</span>]: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: x[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: y</span><br><span class="line">Out[<span class="number">11</span>]: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种 利用切片， 只复制值</span></span><br><span class="line">In [<span class="number">12</span>]: y = t.Tensor(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: y[<span class="number">0</span>:<span class="number">2</span>]=x</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: y</span><br><span class="line">Out[<span class="number">14</span>]: tensor([<span class="number">1.0000e+00</span>, <span class="number">4.0000e+00</span>, <span class="number">1.1395e-19</span>, <span class="number">4.5886e-41</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: x[<span class="number">0</span>]=<span class="number">6</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: y</span><br><span class="line">Out[<span class="number">16</span>]: tensor([<span class="number">1.0000e+00</span>, <span class="number">4.0000e+00</span>, <span class="number">1.1395e-19</span>, <span class="number">4.5886e-41</span>])</span><br></pre></td></tr></table></figure><hr><h2 id="t-save"><a href="#t-save" class="headerlink" title="t.save"></a>t.save</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单个变量 不保留名字</span></span><br><span class="line">t.save(x, <span class="string">'a.pth'</span>)</span><br><span class="line">y = t.load(<span class="string">'a.pth'</span>) <span class="comment"># 这个时候已经和x没有任何关系了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多个变量 或者保留名字</span></span><br><span class="line">dic = dict(aa=x, bb=y)</span><br><span class="line">t.save(dic, <span class="string">'a.pth'</span>)</span><br><span class="line">y = t.load(<span class="string">'a.pth'</span>) <span class="comment"># 这个时候已经和dic没有任何关系了，但是aa,bb还保留着</span></span><br><span class="line">y</span><br><span class="line">&#123;<span class="string">'aa'</span>: tensor([[ <span class="number">100.0000</span>,  <span class="number">100.0000</span>,  <span class="number">100.0000</span>,  <span class="number">100.0000</span>],</span><br><span class="line">         [  <span class="number">-0.0000</span>,    <span class="number">0.0000</span>,    <span class="number">0.0000</span>,    <span class="number">0.0000</span>],</span><br><span class="line">         [  <span class="number">-0.0000</span>,    <span class="number">0.0000</span>,   <span class="number">-0.0000</span>,    <span class="number">0.0000</span>]]),</span><br><span class="line"> <span class="string">'bb'</span>: tensor(<span class="number">1.00000e-11</span> *</span><br><span class="line">        [[<span class="number">-0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">-3.9650</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>]])&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="第十章的诡异装饰器"><a href="#第十章的诡异装饰器" class="headerlink" title="第十章的诡异装饰器"></a>第十章的诡异装饰器</h2><p>作者在这里实现了batcha_size的拼接的方式。<br>具体的函数闭包可以参考python<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># def create_collate_fn():</span></span><br><span class="line"><span class="comment">#     def collate_fn():</span></span><br><span class="line"><span class="comment">#         pass</span></span><br><span class="line"><span class="comment">#     return collate_fn</span></span><br></pre></td></tr></table></figure></p><p>来，猜一下这里为什么这么写，函数闭包，根据昨天看的，函数闭包和类函数有的一拼，或者说可以用于创建多个类似的函数，暂时先这么理解，因为还没有太多的用到，在这里的函数闭包是为了实现对作为函数的参数进行传递变量，也就是把函数作为变量传递，这种思想要注意一下。<br>设想几种情况。<br>假设函数h的定义是这样的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="function"><span class="keyword">def</span> <span class="title">h</span><span class="params">(x, f)</span>:</span></span><br><span class="line">   ...:     <span class="string">"""</span></span><br><span class="line"><span class="string">   ...:     Args:</span></span><br><span class="line"><span class="string">   ...:       x: int</span></span><br><span class="line"><span class="string">   ...:       f: function</span></span><br><span class="line"><span class="string">   ...:     """</span></span><br><span class="line">   ...:     out = f(x)</span><br><span class="line">   ...:     <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">   ...:     <span class="keyword">return</span> <span class="number">2</span>*x</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: h(<span class="number">2</span>,f)</span><br><span class="line">Out[<span class="number">3</span>]: <span class="number">4</span></span><br></pre></td></tr></table></figure></p><p>第一种情况，函数f的所有输入都是h可以给的，那么这时候如上所示，直接定义一个函数，然后把函数名或者其他等于函数的变量传进去就可以。<br>第二种情况，函数f的有一部分变量，需要是外界给的，即f的定义中，引用到了不属于h的输入的变量。就像这样。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: <span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(i)</span>:</span></span><br><span class="line">   ...:     <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">   ...:         <span class="keyword">return</span> i*x</span><br><span class="line">   ...:     <span class="keyword">return</span> f</span><br><span class="line">   ...:</span><br><span class="line">   ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: ff = g(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: h(<span class="number">4</span>,ff)</span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">12</span></span><br></pre></td></tr></table></figure></p><p>那么这个时候函数闭包就可以很好地实现这种想法。<br>这是暂时对于函数闭包的理解，但我知道这种想法肯定是有问题的。</p><hr><h2 id="rnn的pack和pad"><a href="#rnn的pack和pad" class="headerlink" title="rnn的pack和pad"></a>rnn的pack和pad</h2><p>from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence, pad_packed_sequence</span><br><span class="line">li_ = [[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">0</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>]]</span><br><span class="line">ten = t.Tensor(li_).long()</span><br><span class="line">pad_variable = ten</span><br><span class="line">embedding = nn.Embedding(<span class="number">5</span>,<span class="number">2</span>)</span><br><span class="line">pad_embeddings = embedding(pad_variable)</span><br><span class="line">lengths = [<span class="number">5</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>]</span><br><span class="line">pad_embeddings</span><br><span class="line"></span><br><span class="line">pad_embeddings</span><br><span class="line"></span><br><span class="line">tensor([[[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">         [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">         [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">         [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">         [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">         [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">         [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">         [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">         [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">         [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">         [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">         [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">         [ <span class="number">0.5581</span>,  <span class="number">0.7382</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">         [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">         [ <span class="number">0.5581</span>,  <span class="number">0.7382</span>],</span><br><span class="line">         [ <span class="number">0.5581</span>,  <span class="number">0.7382</span>]]])</span><br><span class="line"></span><br><span class="line">packed_variable = pack_padded_sequence(pad_embeddings, lengths)</span><br><span class="line">PackedSequence(data=tensor([[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">        [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">        [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">        [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>],</span><br><span class="line">        [ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">        [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">        [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">        [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>],</span><br><span class="line">        [ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">        [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">        [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">        [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>],</span><br><span class="line">        [ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">        [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">        [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">        [ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">        [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>]]), batch_sizes=tensor([ <span class="number">4</span>,  <span class="number">4</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">packed_variable.data.shape</span><br><span class="line">torch.Size([<span class="number">17</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">rnn = nn.LSTM(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">output, hn = rnn(packed_variable)</span><br><span class="line"></span><br><span class="line">output</span><br><span class="line">PackedSequence(data=tensor([[<span class="number">-0.1698</span>, <span class="number">-0.1311</span>,  <span class="number">0.2030</span>],</span><br><span class="line">        [<span class="number">-0.0984</span>, <span class="number">-0.0693</span>,  <span class="number">0.1601</span>],</span><br><span class="line">        [<span class="number">-0.0791</span>, <span class="number">-0.1195</span>,  <span class="number">0.2111</span>],</span><br><span class="line">        [<span class="number">-0.0175</span>,  <span class="number">0.0069</span>,  <span class="number">0.0978</span>],</span><br><span class="line">        [<span class="number">-0.2580</span>, <span class="number">-0.1868</span>,  <span class="number">0.3193</span>],</span><br><span class="line">        [<span class="number">-0.1392</span>, <span class="number">-0.0959</span>,  <span class="number">0.2441</span>],</span><br><span class="line">        [<span class="number">-0.1221</span>, <span class="number">-0.1489</span>,  <span class="number">0.3270</span>],</span><br><span class="line">        [<span class="number">-0.0223</span>,  <span class="number">0.0109</span>,  <span class="number">0.1334</span>],</span><br><span class="line">        [<span class="number">-0.3011</span>, <span class="number">-0.2100</span>,  <span class="number">0.3821</span>],</span><br><span class="line">        [<span class="number">-0.1544</span>, <span class="number">-0.1061</span>,  <span class="number">0.2877</span>],</span><br><span class="line">        [<span class="number">-0.1452</span>, <span class="number">-0.1551</span>,  <span class="number">0.3886</span>],</span><br><span class="line">        [<span class="number">-0.0232</span>,  <span class="number">0.0129</span>,  <span class="number">0.1460</span>],</span><br><span class="line">        [<span class="number">-0.3222</span>, <span class="number">-0.2195</span>,  <span class="number">0.4168</span>],</span><br><span class="line">        [<span class="number">-0.1593</span>, <span class="number">-0.1098</span>,  <span class="number">0.3109</span>],</span><br><span class="line">        [<span class="number">-0.1575</span>, <span class="number">-0.1556</span>,  <span class="number">0.4222</span>],</span><br><span class="line">        [<span class="number">-0.3325</span>, <span class="number">-0.2233</span>,  <span class="number">0.4370</span>],</span><br><span class="line">        [<span class="number">-0.1603</span>, <span class="number">-0.1111</span>,  <span class="number">0.3235</span>]]), batch_sizes=tensor([ <span class="number">4</span>,  <span class="number">4</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">hn[<span class="number">1</span>].shape</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">pad_packed_sequence(packed_variable) </span><br><span class="line">(tensor([[[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">          [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">          [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">          [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"> </span><br><span class="line">         [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">          [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">          [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">          [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"> </span><br><span class="line">         [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">          [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">          [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">          [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"> </span><br><span class="line">         [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">          [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">          [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">          [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>]],</span><br><span class="line"> </span><br><span class="line">         [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">          [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">          [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">          [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>]]]), tensor([ <span class="number">5</span>,  <span class="number">5</span>,  <span class="number">4</span>,  <span class="number">3</span>]))</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># embedding_dim=3, seq_len=4,3 batch_size=2 即把两句话a,b作为一个batch,空余补0</span></span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line">a = t.ones(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">b = t.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">pad_sequence([a,b])</span><br><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a,b两句话，分别有3,2个词，batch_size=2, 共有3个batch_size，大小分别是2,2,1</span></span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line">a = t.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = t.tensor([<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">pack_sequence([a,b])</span><br><span class="line">PackedSequence(data=tensor([ <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">2</span>,  <span class="number">5</span>,  <span class="number">3</span>]), batch_sizes=tensor([ <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">1</span>]))</span><br></pre></td></tr></table></figure><hr><h2 id="beam-searching"><a href="#beam-searching" class="headerlink" title="beam_searching"></a>beam_searching</h2><p>[参考链接]<a href="https://blog.csdn.net/xljiulong/article/details/51554780" target="_blank" rel="noopener">https://blog.csdn.net/xljiulong/article/details/51554780</a><br>[参考链接]<a href="http://jhave.org/algorithms/graphs/beamsearch/beamsearch.shtml" target="_blank" rel="noopener">http://jhave.org/algorithms/graphs/beamsearch/beamsearch.shtml</a><br>网上讲的大部分都有各自的问题，不是很清晰，只有那篇英文才是标准的，这哥们应该是翻译的，还不错<br>作者使用的是beam_searching的变种，原理类似，但是条件不一致，具体的在代码注释中，不再陈述。</p><hr><h2 id="第十章和第九章关于生成语句的流程的区别"><a href="#第十章和第九章关于生成语句的流程的区别" class="headerlink" title="第十章和第九章关于生成语句的流程的区别"></a>第十章和第九章关于生成语句的流程的区别</h2><p>第十章和第九章在模型生成的地方有两个点不一样，<br>第九章的模型本身可以进行正常的输入与输出，所以第九章也写成这个样子<br>输入(LongTensor) 1<em>1 输出 tensor 1</em>vocabsize<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">results = list(start_words)</span><br><span class="line">start_word_len = len(start_words)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(opt.max_gen_len):</span><br><span class="line">    output, hidden = model(input, hidden)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i &lt; start_word_len:</span><br><span class="line">        w = results[i]</span><br><span class="line">        input = input.data.new([word2ix[w]]).view(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># output size 1×vocab_size [[1,2,3,...]]</span></span><br><span class="line">        <span class="comment"># 这里应该看一下，输出output是个什么东西</span></span><br><span class="line">        top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()</span><br><span class="line">        w = ix2word[top_index]</span><br><span class="line">        results.append(w)</span><br><span class="line">        input = input.data.new([word2ix[w]]).view(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> w == <span class="string">'&lt;EOP&gt;'</span>:</span><br><span class="line">        <span class="keyword">del</span> results[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简写为</span></span><br><span class="line"><span class="comment"># model: embedder rnn classifier</span></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(opt.max_gen_len):</span><br><span class="line">    output, hidden = model(input, hidden) <span class="comment"># input(tensor) 1*1  output(tensor) 1*vocabsize hidden(tensor) 1*1*hidden_dim</span></span><br><span class="line">    top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()</span><br><span class="line">    w = ix2word(top_index)</span><br><span class="line">    results.append(w)</span><br><span class="line">    </span><br><span class="line">    input = input.data.new(word2ix(w)).view(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> results</span><br></pre></td></tr></table></figure></p><p>第十章因为使用了pack_padded_sequence来加速训练，那么训练的模型就不能直接拿来像第九章进行生成，另外第一个字是图片特征的转化而成的，不需要embedding层，而是需要fc层，其实也可以直接拿来用，把captions设置为空就好了,在这里作者没有直接用，直接用好像比较麻烦。而是采用beam_search中把rnn和classifier层传进去，写了一个标准的beam_search函数，即输入是第一个字，输出是beam_size句话，因为设计到其他选词保留的问题，所以直接传入的的是各个分函数，进行自行拼接。也可能是为了复用logprobs = nn.functional.log_softmax(output, dim=1) ## 暂时不清楚这里为什么用log_softmax，是负数啊，大哥，不过大小好像不变<br>在rnn中有一个问题，就是能不能用t.no_grad,会不会影响其向前传播。</p><hr><h2 id="对数据的预处理"><a href="#对数据的预处理" class="headerlink" title="对数据的预处理"></a>对数据的预处理</h2><p>第九章是把对数据的预处理写在了data里面，但事实上，这个数据预处理应该与主模型分开，是属于前一个过程。有什么需要交互的，也是通过文件进行，包括配置。</p><hr><h2 id="新建立的数据结构的对比大小"><a href="#新建立的数据结构的对比大小" class="headerlink" title="新建立的数据结构的对比大小"></a>新建立的数据结构的对比大小</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Caption</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    现在不太确定这个集合是hash_table还是set，感觉是hash_tale,是因为set不需要专门的存储结构。再看看吧</span></span><br><span class="line"><span class="string">    这里应该不是那三个集合，而是集合中的每一个元素，比如G(i),这种，作者应该是重新创建了一种数据结构来用，来进行存储</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      sentence: list(int)</span></span><br><span class="line"><span class="string">      state: tuple(hn, cn) hn:1*1*hidden_dim</span></span><br><span class="line"><span class="string">      logprob: probability</span></span><br><span class="line"><span class="string">      score: 等于logprb或者logprb/len(sentence)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sentence, state, logprob, score, metadata=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          sentence(list): </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.sentence = sentence</span><br><span class="line">        self.state = state</span><br><span class="line">        self.logprob = logprob</span><br><span class="line">        self.score = score</span><br><span class="line">        self.metadata = metadata</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这里我猜是为了实现堆排序的比较。尽管知道是，但是还是不知道为什么</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__cmp__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="string">"""Compares Captions by score."""</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(other, Caption)</span><br><span class="line">        <span class="keyword">if</span> self.score == other.score:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">elif</span> self.score &lt; other.score:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># For Python 3 compatibility (__cmp__ is deprecated).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(other, Caption)</span><br><span class="line">        <span class="keyword">return</span> self.score &lt; other.score</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Also for Python 3 compatibility.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(other, Caption)</span><br><span class="line">        <span class="keyword">return</span> self.score == other.score</span><br></pre></td></tr></table></figure><hr><h2 id="作者在固定长度的列表中，使用了小顶堆的方式，来保证每次取出去的都是最小的。"><a href="#作者在固定长度的列表中，使用了小顶堆的方式，来保证每次取出去的都是最小的。" class="headerlink" title="作者在固定长度的列表中，使用了小顶堆的方式，来保证每次取出去的都是最小的。"></a>作者在固定长度的列表中，使用了小顶堆的方式，来保证每次取出去的都是最小的。</h2><hr><h2 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h2><p>现在还有一个问题就是当一个.py文件里的函数或者类超过2、3个时，应该以什么的方式注释才能更好地让别人知道这个文件里的函数和怎么干的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>第十章的代码在难度上其实已经感觉下降了好多，当然自己又忘了写requires.txt。但是在调试改bug自己就用了三天。其中的bug有的时候自己已经忘记当初是怎么写的了，尴尬。<br>自己训练出来的模型也没有作者声称的那么好，暂时不知道</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>html</title>
    <link href="http://yoursite.com/2018/09/24/html/"/>
    <id>http://yoursite.com/2018/09/24/html/</id>
    <published>2018-09-24T04:20:24.000Z</published>
    <updated>2018-11-04T14:51:31.429Z</updated>
    
    <content type="html"><![CDATA[<p>HTML 教程<br><a id="more"></a></p><h1 id="html-基础"><a href="#html-基础" class="headerlink" title="html 基础"></a>html 基础</h1><p>因为在visdom中的text支持html标签，所以来简单学学html。<br><a href="http://www.runoob.com/html/html-intro.html" target="_blank" rel="noopener">参考链接:菜鸟教程</a><br>[菜鸟工具在线编辑工具]<a href="https://c.runoob.com/front-end/61" target="_blank" rel="noopener">https://c.runoob.com/front-end/61</a><br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>菜鸟教程(runoob.com)<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>我的第一个标题<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>我的第一个段落。<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>实例解析</p><ul><li>\&lt;!DOCTYPE html&gt; 声明为 HTML5 文档</li><li>&lt;\html&gt; 元素是 HTML 页面的根元素</li><li>&lt;\head&gt; 元素包含了文档的元（meta）数据，如 <meta charset="utf-8"> 定义网页编码格式为 utf-8。</li><li>&lt;\title&gt; 元素描述了文档的标题</li><li><body> 元素包含了可见的页面内容</body></li><li>&lt;\h1&gt; 元素定义一个大标题</li><li>&lt;\p&gt; 元素定义一个段落</li></ul><p>网页结构<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span> 页面标题<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span>这是一个标题<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>这是一个段落。<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>这是另一个段落<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>只有<body>与</body>之间的元素才会显示</p><p>标题 </p><h1>-<h6>  <h1>这是一个标题</h1><br>段落 <p>  </p><p>这是一个段落。</p> 段前段后有空行<br>链接 <a>  <a href="http://www.runoob.com" target="_blank" rel="noopener">这是一个链接</a><br>图片 <img>  <img src="/images/logo.png" width="258" height="39"><br><img src="/images/logo.png" width="258" height="39"><p></p><h1 id="html元素"><a href="#html元素" class="headerlink" title="html元素"></a>html元素</h1><p><br>空元素标签<br>html属性  html设置属性，常以键值对的形式出现  <a href="http://www.runoob.com" target="_blank" rel="noopener">这是一个链接</a> 常用属性： class id  style title<br>html<br>水平线 <hr><br>换行 <br><br>注释 <!-- 这是一个注释 --><br>格式化标签<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">b</span>&gt;</span>加粗文本<span class="tag">&lt;/<span class="name">b</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">i</span>&gt;</span>斜体文本<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">code</span>&gt;</span>电脑自动输出<span class="tag">&lt;/<span class="name">code</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">这是 <span class="tag">&lt;<span class="name">sub</span>&gt;</span> 下标<span class="tag">&lt;/<span class="name">sub</span>&gt;</span> 和 <span class="tag">&lt;<span class="name">sup</span>&gt;</span> 上标<span class="tag">&lt;/<span class="name">sup</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">small</span>&gt;</span>这个文本是缩小的<span class="tag">&lt;/<span class="name">small</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">big</span>&gt;</span>这个文本字体放大<span class="tag">&lt;/<span class="name">big</span>&gt;</span></span><br></pre></td></tr></table></figure></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">&lt;pre&gt;</span></span><br><span class="line">此例演示如何使用 <span class="keyword">pre</span> 标签</span><br><span class="line">对空行和    空格</span><br><span class="line">进行控制</span><br><span class="line">&lt;/<span class="keyword">pre</span>&gt;</span><br><span class="line"></span><br><span class="line">此例演示如何使用 <span class="keyword">pre</span> 标签</span><br><span class="line">对空行和    空格</span><br><span class="line">进行控制</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">code</span>&gt;</span>计算机输出<span class="tag">&lt;/<span class="name">code</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">kbd</span>&gt;</span>键盘输入<span class="tag">&lt;/<span class="name">kbd</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">tt</span>&gt;</span>打字机文本<span class="tag">&lt;/<span class="name">tt</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">samp</span>&gt;</span>计算机代码样本<span class="tag">&lt;/<span class="name">samp</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">var</span>&gt;</span>计算机变量<span class="tag">&lt;/<span class="name">var</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br></pre></td></tr></table></figure><p>地址<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">address</span>&gt;</span></span><br><span class="line">Written by <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"mailto:webmaster@example.com"</span>&gt;</span>Jon Doe<span class="tag">&lt;/<span class="name">a</span>&gt;</span>.<span class="tag">&lt;<span class="name">br</span>&gt;</span> </span><br><span class="line">Visit us at:<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">Example.com<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">Box 564, Disneyland<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">USA</span><br><span class="line"><span class="tag">&lt;/<span class="name">address</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>创建电子邮件标签</p><p>首字母缩写和缩写<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">abbr</span> <span class="attr">title</span>=<span class="string">"etcetera"</span>&gt;</span>etc.<span class="tag">&lt;/<span class="name">abbr</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">acronym</span> <span class="attr">title</span>=<span class="string">"World Wide Web"</span>&gt;</span>WWW<span class="tag">&lt;/<span class="name">acronym</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>文字显示方向<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;p&gt;<span class="xml"><span class="tag">&lt;<span class="name">bdo</span> <span class="attr">dir</span>=<span class="string">"rtl"</span>&gt;</span>该段落文字从右到左显示。<span class="tag">&lt;/<span class="name">bdo</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br></pre></td></tr></table></figure></p><p>块引用<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;p&gt;WWF's goal <span class="keyword">is</span> <span class="keyword">to</span>: </span><br><span class="line">&lt;q&gt;Build a future <span class="keyword">where</span> people live <span class="keyword">in</span> harmony <span class="keyword">with</span> nature.&lt;/q&gt;</span><br><span class="line">We hope they succeed.&lt;/p&gt;</span><br></pre></td></tr></table></figure></p><p>删除字和插入字的效果<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>My favorite color is <span class="tag">&lt;<span class="name">del</span>&gt;</span>blue<span class="tag">&lt;/<span class="name">del</span>&gt;</span> <span class="tag">&lt;<span class="name">ins</span>&gt;</span>red<span class="tag">&lt;/<span class="name">ins</span>&gt;</span>!<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>html 链接属性 target:定义文档在哪个窗口打开 id属性<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;a <span class="attribute">href</span>=<span class="string">"http://www.runoob.com/"</span> <span class="attribute">target</span>=<span class="string">"_blank"</span>&gt;访问菜鸟教程!&lt;/a&gt;</span><br></pre></td></tr></table></figure></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"#C4"</span>&gt;</span>查看章节 4<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h2</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">id</span>=<span class="string">"C4"</span>&gt;</span>章节 4<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>这边显示该章节的内容……<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="html"><a href="#html" class="headerlink" title="html "></a>html <head></head></h1><p><title>, <style>, <meta>, <link>, <script>, <noscript>, and <base>.</p><p><title> </p><ul><li>定义了浏览器工具栏的标题</li><li>当网页添加到收藏夹时，显示在收藏夹中的标题</li><li>显示在搜索引擎结果页面的标题<br><base> 标签描述了基本的链接地址/链接目标，该标签作为HTML文档中所有的链接标签的默认链接 <figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;base <span class="attribute">href</span>=<span class="string">"http://www.runoob.com/images/"</span> <span class="attribute">target</span>=<span class="string">"_blank"</span>&gt;</span><br></pre></td></tr></table></figure></li></ul><p><link> 标签定义了文档与外部资源之间的关系。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;link <span class="attribute">rel</span>=<span class="string">"stylesheet"</span> <span class="attribute">type</span>=<span class="string">"text/css"</span> <span class="attribute">href</span>=<span class="string">"mystyle.css"</span>&gt;</span><br></pre></td></tr></table></figure></p><p><style> 标签定义了HTML文档的样式文件引用地址.<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="css"><span class="selector-tag">body</span> &#123;<span class="attribute">background-color</span>:yellow&#125;</span></span><br><span class="line"><span class="css"><span class="selector-tag">p</span> &#123;<span class="attribute">color</span>:blue&#125;</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p><meta> 元素 META 元素通常用于指定网页的描述，关键词，文件的最后修改时间，作者，和其他元数据。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;meta <span class="attribute">name</span>=<span class="string">"keywords"</span> <span class="attribute">content</span>=<span class="string">"HTML, CSS, XML, XHTML, JavaScript"</span>&gt;</span><br><span class="line">&lt;meta <span class="attribute">name</span>=<span class="string">"description"</span> <span class="attribute">content</span>=<span class="string">"免费 Web &amp; 编程 教程"</span>&gt;</span><br><span class="line">&lt;meta <span class="attribute">http-equiv</span>=<span class="string">"refresh"</span> <span class="attribute">content</span>=<span class="string">"30"</span>&gt;</span><br></pre></td></tr></table></figure></p><h1 id="HTML-样式-CSS"><a href="#HTML-样式-CSS" class="headerlink" title="HTML 样式- CSS"></a>HTML 样式- CSS</h1><p>CSS (Cascading Style Sheets) 用于渲染HTML元素标签的样式.<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 使用添加到 <span class="tag">&lt;<span class="name">head</span>&gt;</span> 部分的样式信息对 HTML 进行格式化 内部样式表 应用于单个文件</span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>菜鸟教程(runoob.com)<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="css"><span class="selector-tag">h1</span> &#123;<span class="attribute">color</span>:red;&#125;</span></span><br><span class="line"><span class="css"><span class="selector-tag">p</span> &#123;<span class="attribute">color</span>:blue;&#125;</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"># 使用 style 属性制作一个没有下划线的链接 内联样式 应用于个别元素</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.runoob.com/"</span> <span class="attr">style</span>=<span class="string">"text-decoration:none;"</span>&gt;</span>访问 runoob.com!<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">#  标签链接到一个外部样式表 外部引用</span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>菜鸟教程(runoob.com)<span class="tag">&lt;/<span class="name">title</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"styles.css"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">font-family（字体），color（颜色），和font-size（字体大小）， text-align（文字对齐）</span><br><span class="line"></span><br><span class="line"># 图像</span><br></pre></td></tr></table></figure></p><p><img src="smiley.gif" alt="Smiley face" style="float:left" width="32" height="32"><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 表格</span></span><br></pre></td></tr></table></figure></p><p><table border="1"><br>    <tr><br>        <th>Header 1</th><br>        <th>Header 2</th><br>    </tr><br>    <tr><br>        <td>row 1, cell 1</td><br>        <td>row 1, cell 2</td><br>    </tr><br>    <tr><br>        <td>row 2, cell 1</td><br>        <td>row 2, cell 2</td><br>    </tr><br></table><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列表</span></span><br></pre></td></tr></table></figure></p><ul><br>  <li>Coffee</li><br>  <li>Tea</li><br>  <li>Milk</li><br></ul><ol><br>  <li>Coffee</li><br>  <li>Tea</li><br>  <li>Milk</li><br></ol><ol start="50"><br>  <li>Coffee</li><br>  <li>Tea</li><br>  <li>Milk</li><br></ol><p><code>`</code></p><h1 id="区块-块级和-内联级"><a href="#区块-块级和-内联级" class="headerlink" title="区块  块级和 内联级"></a>区块 <div> 块级和<span> 内联级</h1><h1 id="表单"><a href="#表单" class="headerlink" title="表单"></a>表单</h1><hr><p>2018-10-05</p><h1 id="前言：主要是为了爬虫做一些补充，此次主要是为了爬虫做补充，不涉及html的理论"><a href="#前言：主要是为了爬虫做一些补充，此次主要是为了爬虫做补充，不涉及html的理论" class="headerlink" title="前言：主要是为了爬虫做一些补充，此次主要是为了爬虫做补充，不涉及html的理论"></a>前言：主要是为了爬虫做一些补充，此次主要是为了爬虫做补充，不涉及html的理论</h1><h2 id=""><a href="#" class="headerlink" title=""></a><script></h2><p>用于加载脚本文件，指一段javascript代码，暂时不影响后续操作。</p><h2 id="amp"><a href="#amp" class="headerlink" title="&amp;"></a><div>&amp;<span></h2><p>区块 <div> 块级和<span> 内联级，本身没有太强的含义，前者以新行显示，后者不以新行显示。主要是作为容器存在，用于布局。</p><h2 id="id-amp-class"><a href="#id-amp-class" class="headerlink" title="id&amp;class"></a>id&amp;class</h2><p>id具有唯一性，在一个网页内同一个命名只能使用一次，定义以#开头<br>class命名的类可以在一个网页使用无数次，定义以.开头<br>但两者都是定义的样式而已。</p></style></title></p></a></h6></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HTML 教程&lt;br&gt;
    
    </summary>
    
      <category term="html" scheme="http://yoursite.com/categories/html/"/>
    
    
      <category term="html" scheme="http://yoursite.com/tags/html/"/>
    
  </entry>
  
</feed>
