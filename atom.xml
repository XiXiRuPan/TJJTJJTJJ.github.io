<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>田佳杰</title>
  <icon>https://www.gravatar.com/avatar/2d54367ed6dc965439f08c9f1b75cea4</icon>
  <subtitle>代码的搬运工</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-28T11:32:38.535Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jiajie Tian</name>
    <email>tianjiajie1881090@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>One_Example_reID</title>
    <link href="http://yoursite.com/2019/03/27/One-Example-reID/"/>
    <id>http://yoursite.com/2019/03/27/One-Example-reID/</id>
    <published>2019-03-27T07:25:37.000Z</published>
    <updated>2019-03-28T11:32:38.535Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这篇文章主要的任务也是为了解决re-ID中需要全部标签的问题，核心思想只对一个摄像头下的全部行人各取一张图片（对于没有出现的行人，从其他摄像头下取一张），然后通过训练模型，聚类给假标签，逐步识别出所有图片。</p><ul><li>paper: <a href="https://yu-wu.net/pdf/TIP2019_One-Example-reID.pdf" target="_blank" rel="noopener">TIP 2019 Progressive Learning for Person Re-Identification with One Example</a></li><li>author: Yu Wu, Yutian Lin, Yi Yang (University of Technology Sydney (UTS))</li><li>code: <a href="https://github.com/Yu-Wu/One-Example-Person-ReID" target="_blank" rel="noopener">https://github.com/Yu-Wu/One-Example-Person-ReID</a></li><li>project: <a href="https://zhuanlan.zhihu.com/p/54576174" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/54576174</a></li></ul><a id="more"></a><p>关于这篇文章的作者也多提几句。</p><p>关于行人重识别(person re-ID)，我看到的多是<strong>悉尼科技大学的郑良团队</strong>，所以简单介绍一下郑良团队的成员。</p><ul><li>郑良：<a href="http://www.liangzheng.com.cn/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/liangzheng06" target="_blank" rel="noopener">github</a>，2015年清华博士毕业，有学生郑哲东，钟准，孙奕帆。</li><li>郑哲东：<a href="http://www.zdzheng.xyz/" target="_blank" rel="noopener">主页</a>, <a href="https://github.com/layumi?tab=repositories" target="_blank" rel="noopener">github</a>，悉尼科技大学博三。</li><li>钟准：<a href="http://zhunzhong.site/" target="_blank" rel="noopener">主页</a>, <a href="https://github.com/zhunzhong07" target="_blank" rel="noopener">github</a>（鸣人），厦门大学交换生博四。</li><li>孙奕帆：<a href="https://syfafterzy.github.io/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/syfafterzy" target="_blank" rel="noopener">github</a>，清华博二。</li><li>林雨恬：<a href="https://vana77.github.io/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/vana77" target="_blank" rel="noopener">github</a>，很漂亮也很厉害的一个小姐姐.</li><li>Wu Yu：<a href="https://yu-wu.net/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/Yu-Wu" target="_blank" rel="noopener">github</a>，<a href="https://www.zhihu.com/people/wu2008yu/activities" target="_blank" rel="noopener">知乎</a>，悉尼科技大学博一。</li></ul><p>人家牛逼，虚心学习。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>person re-ID 有监督模型，半监督模型。作者采用的是 one-shot image-based setting，也就是每个行人只有一个样例。</p><p>具体来说，作者将数据集分为三部分：labeled data, selected pseudo-labeled data and index-labeled data，其中 labeled data 和 selected pseudo-labeled data 用分类损失，index-labeled data 用 exclusive loss。exclusive loss 的目标是尽可能地使图片之间都离得比较远。</p><img src="/2019/03/27/One-Example-reID/join_training_procedure.png" title="join training procedure"><p>作者主要的<strong>改进点</strong>在于提出了 <strong>exclusive loss</strong> 和 label estimation</p><p>作者的另一篇相关文章是在视频上做的</p><ul><li>paper: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf" target="_blank" rel="noopener">CVPR 2018 Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning</a></li><li>知乎: <a href="https://www.leiphone.com/news/201806/o8a3H5um1H2zXrof.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201806/o8a3H5um1H2zXrof.html</a></li><li>code: <a href="https://github.com/Yu-Wu/Exploit-Unknown-Gradually" target="_blank" rel="noopener">https://github.com/Yu-Wu/Exploit-Unknown-Gradually</a></li></ul><h1 id="2-The-progressive-model"><a href="#2-The-progressive-model" class="headerlink" title="2. The progressive model"></a>2. The progressive model</h1><h2 id="2-1-Framework-overview"><a href="#2-1-Framework-overview" class="headerlink" title="2.1 Framework overview"></a>2.1 Framework overview</h2><p>训练模型分为两步：在三个数据集上训练CNN模型—&gt;在 unlabeled data 上选择一些数据放到 pseudo-labeled data 上。</p><h2 id="2-2-Preliminaries"><a href="#2-2-Preliminaries" class="headerlink" title="2.2 Preliminaries"></a>2.2 Preliminaries</h2><ul><li>labeled data set $\mathcal{L}$</li><li>unlabeled data set $\mathcal{U}$</li><li><ul><li>pseudo-labeled data set $\mathcal{S}^t$</li></ul></li><li><ul><li>index-labeled data set $\mathcal{M}^t$</li></ul></li></ul><h2 id="2-3-The-joint-learning-method"><a href="#2-3-The-joint-learning-method" class="headerlink" title="2.3 The joint learning method"></a>2.3 The joint learning method</h2><p><strong>The Exclusive Loss</strong>: index-labeled data set $\mathcal{M}^t$,</p><script type="math/tex; mode=display">\max \limits_{\theta}\parallel \phi(\theta;x_i) - \phi(\theta;x_j)\parallel</script><p>令$||v_i||=\tilde{\phi}(\theta;x_i)$是$x_i$的归一化后的特征，因此可以将最大化欧氏距离近似为最小化cos距离。</p><ul><li><strong>这个公式没有推导出来，还在询问作者的答案</strong>.</li><li>在<a href="https://arxiv.org/pdf/1604.01850.pdf" target="_blank" rel="noopener">Joint Detection and Identification Feature Learning for Person Search</a>看到了类似的公式，并且有了新理解，在下面补充。</li></ul><script type="math/tex; mode=display">l(V)=-log \frac{exp(v_i^T\tilde{\phi}(x_i)/\tau)}{\sum_{j=1}^{|M^t|}exp(v_j^T\tilde{\phi}(x_i)/\tau)}</script><p>其中，V是所有数据的归一化特征，更高的$\tau$导致softer probability distribution.在更新时，先计算当前数据与所有数据的cos距离，在反向传播时，$v_i=1\2*(v_i+\tilde{\phi}(x_i)$并且归一化。</p><p><strong>The cross-entropy loss</strong>:labeled data set $\mathcal{L}$ and pseudo-labeled data set $\mathcal{S}^t$</p><p>不再陈述</p><p><strong>The total loss</strong>:</p><script type="math/tex; mode=display">\min \lambda l(\mathcal{L}) + \lambda l(\mathcal{S}^t) + (1-\lambda) l(V)(\mathcal{M}^t)</script><h2 id="2-4-The-effective-sampling-criterion"><a href="#2-4-The-effective-sampling-criterion" class="headerlink" title="2.4 The effective sampling criterion"></a>2.4 The effective sampling criterion</h2><p>没有采用分类损失，而是利用最近邻赋予假标签并且假标签的真实性为与最近邻真值的距离，每次取$m_t = m_{t-1}+p\cdot n_u$</p><p>验证集是另外一个re-ID的训练集。</p><p>整体算法如下：</p><img src="/2019/03/27/One-Example-reID/Algorithm.png" title="algorithm"><h2 id="2-5-补充公式"><a href="#2-5-补充公式" class="headerlink" title="2.5 补充公式"></a>2.5 补充公式</h2><p><a href="https://arxiv.org/pdf/1604.01850.pdf" target="_blank" rel="noopener">Joint Detection and Identification Feature Learning for Person Search</a></p><p>定义：n个数据，每个数据表示成向量，即L-2归一化向量$V={v_i}_{i=1}^{n}$，则新向量$x$属于$v_i$类的概率定义为:</p><script type="math/tex; mode=display">p_i=\frac{exp(v_i^Tx/\tau)}{\sum_{j=1}^{n}exp(v_i^Tx/\tau)}</script><p>其中，$\tau$更高，概率分布更平缓，设为0.1.</p><p>这里有两种含义。</p><p>第一种，$v_i$有标签，但$V$的类别多，属于同一类的数据较少，$x$不属于$V$，$x$有标签t，那么n个数据的概率最大值为x的标签，目标应该是最大化x属于t类的概率，其目的是为了分类更加准确。</p><p>第二种，$v_i$没有标签，$x=v_k$属于$V$，目标是最大化x属于第k个元素的概率，最大也就是1，此时$x=v_k$与其他向量$v_j$都正交，其目的是为了令各个向量都离得比较远，也就是作者的目的。作者这里没有用正交来做损失函数，而是用了softmax，很厉害。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这篇文章主要的任务也是为了解决re-ID中需要全部标签的问题，核心思想只对一个摄像头下的全部行人各取一张图片（对于没有出现的行人，从其他摄像头下取一张），然后通过训练模型，聚类给假标签，逐步识别出所有图片。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://yu-wu.net/pdf/TIP2019_One-Example-reID.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TIP 2019 Progressive Learning for Person Re-Identification with One Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;author: Yu Wu, Yutian Lin, Yi Yang (University of Technology Sydney (UTS))&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/Yu-Wu/One-Example-Person-ReID&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Yu-Wu/One-Example-Person-ReID&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;project: &lt;a href=&quot;https://zhuanlan.zhihu.com/p/54576174&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/54576174&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="re-ID" scheme="http://yoursite.com/categories/re-ID/"/>
    
    
      <category term="one_example" scheme="http://yoursite.com/tags/one-example/"/>
    
      <category term="re-ID" scheme="http://yoursite.com/tags/re-ID/"/>
    
  </entry>
  
  <entry>
    <title>GANimation</title>
    <link href="http://yoursite.com/2019/01/24/GANimation/"/>
    <id>http://yoursite.com/2019/01/24/GANimation/</id>
    <published>2019-01-24T07:53:06.000Z</published>
    <updated>2019-03-28T08:15:53.709Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这篇文章是根据GANnotation的一个公式查过来的，感觉还挺厉害。</p><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><ul><li>paper: <a href="https://www.albertpumarola.com/publications/files/pumarola2018ganimation.pdf" target="_blank" rel="noopener">GANimation Anatomically-aware Facial Animation from a Single Image</a></li><li>github: <a href="https://github.com/albertpumarola/GANimation" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation</a></li><li>project: <a href="https://www.albertpumarola.com/research/GANimation/index.html" target="_blank" rel="noopener">https://www.albertpumarola.com/research/GANimation/index.html</a></li></ul><p>关键词：starGAN的改进、连续的表情变换、贴回去能够一致</p><a id="more"></a><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>在人脸转换中，StarGAN是最成功的GAN，但是只能生成离散的人脸。作者要做的就是生成连续的表情变化。</p><h1 id="2-Problem-Formulation"><a href="#2-Problem-Formulation" class="headerlink" title="2. Problem Formulation"></a>2. Problem Formulation</h1><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">$\mathrm{I}_{y_r}\in \mathbb{R}^{H×W×3}$</td><td style="text-align:center">输入图片</td></tr><tr><td style="text-align:center">$\mathrm{y}_r=(y_1,…,y_N)^T$</td><td style="text-align:center">其中，每一个$y_i$表示第i个action unit的程度，在0~1之间</td></tr><tr><td style="text-align:center">$\mathrm{I}_{y_g}$</td><td style="text-align:center">输出图片</td></tr><tr><td style="text-align:center">$\mathcal{M}$</td><td style="text-align:center">映射函数M: $(\mathrm{I}_{y_r},\mathrm{y}_g)$—&gt;$\mathrm{I}_{y_g}$</td></tr></tbody></table></div><p>非成对图片</p><h1 id="3-Our-Approach"><a href="#3-Our-Approach" class="headerlink" title="3. Our Approach"></a>3. Our Approach</h1><p><img src="./GANimation/GANimation1.png" alt="网络结构"><br><img src="/2019/01/24/GANimation/GANimation1.png" title="网络结构"></p><h2 id="3-1-Network-Architechture"><a href="#3-1-Network-Architechture" class="headerlink" title="3.1 Network Architechture"></a>3.1 Network Architechture</h2><h3 id="3-1-1-Generator"><a href="#3-1-1-Generator" class="headerlink" title="3.1.1 Generator"></a>3.1.1 Generator</h3><p>对于G的改进，为了能够使G只聚焦于对于新表情的生成，而保留其他元素，引入attention机制，也就是G生成的不是一整张图片，而是两个mask，color mask C 和 attention mask A.即：</p><script type="math/tex; mode=display">\mathrm{I}_{\mathrm{y}_f}=(1-A)\cdot C + A\cdot \mathrm{I}_{\mathrm{y}_o}</script><p>其中，$\mathrm{A}=G_A(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f)\in {0,…,1}^{H×W}$，$\mathrm{C}=G_C(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f)\in {0,…,1}^{H×W×3}$</p><p><img src="./GANimation/GANimation2.png" alt="生成器结构"><br><img src="/2019/01/24/GANimation/GANimation2.png" title="生成器结构"></p><h3 id="3-1-2-Conditional-Critic"><a href="#3-1-2-Conditional-Critic" class="headerlink" title="3.1.2 Conditional Critic"></a>3.1.2 Conditional Critic</h3><p>PatchGAN: 输入图像 $\mathrm{I}\dashrightarrow \mathrm{Y}_{\mathrm{I}}\in \mathbb{R}^{H/2^6×W/2^6}$</p><p>并且对判别器进行改进，加入额外的回归判别类别。</p><h2 id="3-2-Learning-the-model"><a href="#3-2-Learning-the-model" class="headerlink" title="3.2 Learning the model"></a>3.2 Learning the model</h2><p>损失函数</p><h3 id="3-2-1-Image-Adversarial-Loss"><a href="#3-2-1-Image-Adversarial-Loss" class="headerlink" title="3.2.1 Image Adversarial Loss"></a>3.2.1 Image Adversarial Loss</h3><p>判断图片是生成的还是真实的。</p><p>和StarGAN的损失一样。</p><script type="math/tex; mode=display">L_I(G,D_I,I_{y_o},y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[D_\mathrm{I}(G(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f))]-\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[D_\mathrm{I}(\mathrm{I}_{\mathrm{y}_o})]+\lambda_{gp} \mathbb{E}_{\tilde{I}\thicksim \mathbb{P}_{\tilde{I}}}[(\parallel \nabla_{\tilde{I}}D_I(\tilde{I}) \parallel _2-1)^2]</script><h3 id="3-2-2-Attention-Loss"><a href="#3-2-2-Attention-Loss" class="headerlink" title="3.2.2 Attention Loss"></a>3.2.2 Attention Loss</h3><p>这个损失是针对attention mask A 和 color mask C.</p><p>Total Variation Regularization</p><script type="math/tex; mode=display">L_A(G,I_{y_o},y_f)=\lambda_{TV}\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\sum_{i,j}^{H,W}[(A_{i+1,j}-A_{i,j})^2+(A_{i,j+1}-A_{i,j})^2]]+\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel A \parallel_2]</script><p>这个公式的初步感觉是A要尽可能平缓，并且A中的元素尽可能小。</p><p>根据作者的说法，是为了保证A不变成全是1的矩阵，并且为了保证更加平滑的空间结合。以代码为准。</p><h3 id="3-2-3-Conditional-Expression-Loss"><a href="#3-2-3-Conditional-Expression-Loss" class="headerlink" title="3.2.3 Conditional Expression Loss"></a>3.2.3 Conditional Expression Loss</h3><p>这个应该和starGAN的判断图片属性分类正确损失是一样的。</p><script type="math/tex; mode=display">L_y(G,D_y,I_{y_o},y_o,y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel D_y(G(I_{y_o}|y_f))-y_f \parallel]+\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel D_y(I_{y_o})-y_o \parallel]</script><h3 id="3-2-4-Identity-Loss"><a href="#3-2-4-Identity-Loss" class="headerlink" title="3.2.4 Identity Loss"></a>3.2.4 Identity Loss</h3><p>这个应该就是starGAN的重构损失</p><script type="math/tex; mode=display">L_{idg}(G,I_{y_o},y_o,y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel G(G(I_{y_o}|y_f)|y_o)-I_{y_o} \parallel _1]</script><p>这个损失是为了保证生成前后图片的id是一样的。</p><h3 id="3-2-5-Full-Loss"><a href="#3-2-5-Full-Loss" class="headerlink" title="3.2.5 Full Loss"></a>3.2.5 Full Loss</h3><script type="math/tex; mode=display">L=L_I+\lambda_y L_y+\lambda_A (L_A(G,I_{y_g},y_r)+L_A(G,I_{y_r},y_g))+\lambda_{idt}L_{idt}</script><script type="math/tex; mode=display">\lambda_{gp}=10, \lambda_A=0.1, \lambda_{TV}=0.0001, \lambda_y=4000, \lambda_{idt}=10</script><h1 id="4-Implementation-Details"><a href="#4-Implementation-Details" class="headerlink" title="4. Implementation Details"></a>4. Implementation Details</h1><p><strong>The attention mechanism guaranties a smooth transition between the morphed cropped face and the original image.</strong></p><p>也就是说 attention mechanism 能够保证生成的图片很好地再贴回去。</p><h2 id="4-1-Single-Action-Units-Edition"><a href="#4-1-Single-Action-Units-Edition" class="headerlink" title="4.1 Single Action Units Edition"></a>4.1 Single Action Units Edition</h2><p><img src="./GANimation/GANimation3.png" alt="Single Action Units Edition"><br><img src="/2019/01/24/GANimation/GANimation3.png" title="Single Action Units Edition"></p><ul><li>[x] 这里的AU是什么？ intensity怎么理解？</li></ul><p>AU:<a href="https://www.cs.cmu.edu/~face/facs.htm" target="_blank" rel="noopener">https://www.cs.cmu.edu/~face/facs.htm</a></p><p>intensity: <a href="https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units" target="_blank" rel="noopener">https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units</a></p><p><img src="./GANimation/GANimation4.png" alt="Attention Model"><br><img src="/2019/01/24/GANimation/GANimation4.png" title="Attention Model"></p><h2 id="4-2-Simultaneous-Edition-of-Multiple-AUs"><a href="#4-2-Simultaneous-Edition-of-Multiple-AUs" class="headerlink" title="4.2 Simultaneous Edition of Multiple AUs"></a>4.2 Simultaneous Edition of Multiple AUs</h2><p><img src="./GANimation/GANimation5.png" alt="Facial animation from a single image"><br><img src="/2019/01/24/GANimation/GANimation5.png" title="Facial animation from a single image"></p><script type="math/tex; mode=display">\alpha y_g+(1-\alpha)y_r</script><h2 id="4-3-Discrete-Emotions-Editing"><a href="#4-3-Discrete-Emotions-Editing" class="headerlink" title="4.3 Discrete Emotions Editing"></a>4.3 Discrete Emotions Editing</h2><p><img src="./GANimation/GANimation6.png" alt="Qualitative comparison"><br><img src="/2019/01/24/GANimation/GANimation6.png" title="Qualitative comparison"></p><p>作者生成的图片比StarGAN更清晰。</p><h2 id="4-4-High-Expressions-Variability"><a href="#4-4-High-Expressions-Variability" class="headerlink" title="4.4 High Expressions Variability"></a>4.4 High Expressions Variability</h2><h2 id="4-5-Images-in-the-Wild"><a href="#4-5-Images-in-the-Wild" class="headerlink" title="4.5 Images in the Wild"></a>4.5 Images in the Wild</h2><p><img src="./GANimation/GANimation7.png" alt="Qualitative evaluation on images in the wild"><br><img src="/2019/01/24/GANimation/GANimation7.png" title="Qualitative evaluation on images in the wild"></p><p>作者先检测到人脸，然后扣下来，做训练测试，然后再贴回去，与原图保持了一样的清晰度，个人猜测是因为表情的变化只在人脸的中央就可以完成，不涉及到背景的变换，如果涉及到背景的变换，那么是否还能保证贴回去与原图保持一致性。</p><h2 id="4-6-Pushing-the-Limits-of-the-Model"><a href="#4-6-Pushing-the-Limits-of-the-Model" class="headerlink" title="4.6 Pushing the Limits of the Model"></a>4.6 Pushing the Limits of the Model</h2><p><img src="./GANimation/GANimation8.png" alt="Success and Failure Cases"><br><img src="/2019/01/24/GANimation/GANimation8.png" title="Success and Failure Cases."></p><h1 id="5-code"><a href="#5-code" class="headerlink" title="5. code"></a>5. code</h1><h2 id="5-1-生成器Generator"><a href="#5-1-生成器Generator" class="headerlink" title="5.1 生成器Generator"></a>5.1 生成器Generator</h2><p>GANimation的Generator的主体网络和starGAN的Generator的主体网络一致，只是多加了一个conv</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(NetworkBase)</span>:</span></span><br><span class="line">    <span class="string">"""Generator. Encoder-Decoder Architecture."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conv_dim=<span class="number">64</span>, c_dim=<span class="number">5</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self._name = <span class="string">'generator_wgan'</span></span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">3</span>+c_dim, conv_dim, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.InstanceNorm2d(conv_dim, affine=<span class="keyword">True</span>))</span><br><span class="line">        layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Down-Sampling</span></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            layers.append(nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">            layers.append(nn.InstanceNorm2d(curr_dim*<span class="number">2</span>, affine=<span class="keyword">True</span>))</span><br><span class="line">            layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Bottleneck</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(repeat_num):</span><br><span class="line">            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Up-Sampling</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">            layers.append(nn.InstanceNorm2d(curr_dim//<span class="number">2</span>, affine=<span class="keyword">True</span>))</span><br><span class="line">            layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.main = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(curr_dim, <span class="number">3</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.Tanh())</span><br><span class="line">        self.img_reg = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.Sigmoid())</span><br><span class="line">        self.attetion_reg = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, c)</span>:</span></span><br><span class="line">        <span class="comment"># replicate spatially and concatenate domain information</span></span><br><span class="line">        c = c.unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">        c = c.expand(c.size(<span class="number">0</span>), c.size(<span class="number">1</span>), x.size(<span class="number">2</span>), x.size(<span class="number">3</span>))</span><br><span class="line">        x = torch.cat([x, c], dim=<span class="number">1</span>)</span><br><span class="line">        features = self.main(x)</span><br><span class="line">        <span class="keyword">return</span> self.img_reg(features), self.attetion_reg(features)</span><br></pre></td></tr></table></figure><h2 id="5-2-Discriminator"><a href="#5-2-Discriminator" class="headerlink" title="5.2 Discriminator"></a>5.2 Discriminator</h2><p>Discriminator和StarGAN 的Discriminator完全一样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(NetworkBase)</span>:</span></span><br><span class="line">    <span class="string">"""Discriminator. PatchGAN."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_size=<span class="number">128</span>, conv_dim=<span class="number">64</span>, c_dim=<span class="number">5</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self._name = <span class="string">'discriminator_wgan'</span></span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">3</span>, conv_dim, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.LeakyReLU(<span class="number">0.01</span>, inplace=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, repeat_num):</span><br><span class="line">            layers.append(nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">            layers.append(nn.LeakyReLU(<span class="number">0.01</span>, inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        k_size = int(image_size / np.power(<span class="number">2</span>, repeat_num))</span><br><span class="line">        self.main = nn.Sequential(*layers)</span><br><span class="line">        self.conv1 = nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=k_size, bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        h = self.main(x)</span><br><span class="line">        out_real = self.conv1(h)</span><br><span class="line">        out_aux = self.conv2(h)</span><br><span class="line">        <span class="keyword">return</span> out_real.squeeze(), out_aux.squeeze()</span><br></pre></td></tr></table></figure><h2 id="5-3-train-D"><a href="#5-3-train-D" class="headerlink" title="5.3 train D"></a>5.3 train D</h2><p>这里训练D的过程和starGAN有所不同，并且超参数也有所不同。</p><script type="math/tex; mode=display">\lambda_{D-cond}=4000, \lambda_{gp}=10</script><p>starGAN:</p><script type="math/tex; mode=display">\lambda_{cls}=1, \lambda_{gp}=10</script><ul><li>[ ] 为什么和怎么使用的MSELoss</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># starGAN是把Image Adversarial Loss的三项一起反向传播，但GANimation是分开反向传播的，不确定这么做是否有影响。</span></span><br><span class="line">loss_D, fake_imgs_masked = self._forward_D()</span><br><span class="line">self._optimizer_D.zero_grad()</span><br><span class="line">loss_D.backward()</span><br><span class="line">self._optimizer_D.step()</span><br><span class="line"></span><br><span class="line">loss_D_gp= self._gradinet_penalty_D(fake_imgs_masked)</span><br><span class="line">self._optimizer_D.zero_grad()</span><br><span class="line">loss_D_gp.backward()</span><br><span class="line">self._optimizer_D.step()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_forward_D</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># generate fake images</span></span><br><span class="line">    fake_imgs, fake_img_mask = self._G.forward(self._real_img, self._desired_cond)</span><br><span class="line">    fake_img_mask = self._do_if_necessary_saturate_mask(fake_img_mask, saturate=self._opt.do_saturate_mask)</span><br><span class="line">    fake_imgs_masked = fake_img_mask * self._real_img + (<span class="number">1</span> - fake_img_mask) * fake_imgs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># D(real_I)</span></span><br><span class="line">    <span class="comment"># 识别真图片为真，(Image Adversarial Loss)</span></span><br><span class="line">    <span class="comment"># 图片类别分类准确，这里的分类用的不是交叉熵，而是MSELoss，(Conditional Expression Loss)</span></span><br><span class="line">    <span class="comment"># 刚刚发现一个问题，如果是分类损失，MSELoss的输入必须是同样大小的，按照starGAN，D的输出是类别大小(batch*classification)，G的输入是(batch*1)，那这个样子肯定是没法进行MSELoss的，所以还需要看了数据的处理之后才能明白怎么回事。</span></span><br><span class="line">    <span class="comment"># self._criterion_D_cond = torch.nn.MSELoss().cuda()</span></span><br><span class="line">    d_real_img_prob, d_real_img_cond = self._D.forward(self._real_img)</span><br><span class="line">    self._loss_d_real = self._compute_loss_D(d_real_img_prob, <span class="keyword">True</span>) * self._opt.lambda_D_prob</span><br><span class="line">    self._loss_d_cond = self._criterion_D_cond(d_real_img_cond, self._real_cond) / self._B * self._opt.lambda_D_cond</span><br><span class="line"></span><br><span class="line">    <span class="comment"># D(fake_I)</span></span><br><span class="line">    <span class="comment"># 识别假图片为假，(Image Adversarial Loss)</span></span><br><span class="line">    d_fake_desired_img_prob, _ = self._D.forward(fake_imgs_masked.detach())</span><br><span class="line">    self._loss_d_fake = self._compute_loss_D(d_fake_desired_img_prob, <span class="keyword">False</span>) * self._opt.lambda_D_prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># combine losses</span></span><br><span class="line">    <span class="keyword">return</span> self._loss_d_real + self._loss_d_cond + self._loss_d_fake, fake_imgs_masked</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_loss_D</span><span class="params">(self, estim, is_real)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -torch.mean(estim) <span class="keyword">if</span> is_real <span class="keyword">else</span> torch.mean(estim)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gradinet_penalty_D</span><span class="params">(self, fake_imgs_masked)</span>:</span></span><br><span class="line">    <span class="comment"># (Image Adversarial Loss)的第三项</span></span><br><span class="line">    <span class="comment"># interpolate sample</span></span><br><span class="line">    alpha = torch.rand(self._B, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).cuda().expand_as(self._real_img)</span><br><span class="line">    interpolated = Variable(alpha * self._real_img.data + (<span class="number">1</span> - alpha) * fake_imgs_masked.data, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">    interpolated_prob, _ = self._D(interpolated)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute gradients</span></span><br><span class="line">    grad = torch.autograd.grad(outputs=interpolated_prob,</span><br><span class="line">                                inputs=interpolated,</span><br><span class="line">                                grad_outputs=torch.ones(interpolated_prob.size()).cuda(),</span><br><span class="line">                                retain_graph=<span class="keyword">True</span>,</span><br><span class="line">                                create_graph=<span class="keyword">True</span>,</span><br><span class="line">                                only_inputs=<span class="keyword">True</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># penalize gradients</span></span><br><span class="line">    grad = grad.view(grad.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    grad_l2norm = torch.sqrt(torch.sum(grad ** <span class="number">2</span>, dim=<span class="number">1</span>))</span><br><span class="line">    self._loss_d_gp = torch.mean((grad_l2norm - <span class="number">1</span>) ** <span class="number">2</span>) * self._opt.lambda_D_gp</span><br></pre></td></tr></table></figure><h2 id="5-4-train-G"><a href="#5-4-train-G" class="headerlink" title="5.4 train G"></a>5.4 train G</h2><p>这一部分和starGAN的训练类似，比starGAN多一个mask的平滑loss。</p><script type="math/tex; mode=display">\lambda_{D-cond}=4000, \lambda_{cyc}=10, \lambda_{mask}=0.1, \lambda_{mask-smooth}=1*e^{-5}</script><p>starGAN:</p><script type="math/tex; mode=display">\lambda_{cls}=1, \lambda_{cyc}=10</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_forward_G</span><span class="params">(self, keep_data_for_visuals)</span>:</span></span><br><span class="line">    <span class="comment"># generate fake images</span></span><br><span class="line">    fake_imgs, fake_img_mask = self._G.forward(self._real_img, self._desired_cond)</span><br><span class="line">    fake_img_mask = self._do_if_necessary_saturate_mask(fake_img_mask, saturate=self._opt.do_saturate_mask)</span><br><span class="line">    fake_imgs_masked = fake_img_mask * self._real_img + (<span class="number">1</span> - fake_img_mask) * fake_imgs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># D(G(Ic1, c2)*M) masked</span></span><br><span class="line">    <span class="comment"># 生成图片为真 (Image Adversarial Loss)</span></span><br><span class="line">    <span class="comment"># 生成图片的属性为真 (Conditional Expression Loss)</span></span><br><span class="line">    d_fake_desired_img_masked_prob, d_fake_desired_img_masked_cond = self._D.forward(fake_imgs_masked)</span><br><span class="line">    self._loss_g_masked_fake = self._compute_loss_D(d_fake_desired_img_masked_prob, <span class="keyword">True</span>) * self._opt.lambda_D_prob</span><br><span class="line">    self._loss_g_masked_cond = self._criterion_D_cond(d_fake_desired_img_masked_cond, self._desired_cond) / self._B * self._opt.lambda_D_cond</span><br><span class="line"></span><br><span class="line">    <span class="comment"># G(G(Ic1,c2), c1)</span></span><br><span class="line">    <span class="comment"># 重构损失 (Identity Loss)</span></span><br><span class="line">    rec_real_img_rgb, rec_real_img_mask = self._G.forward(fake_imgs_masked, self._real_cond)</span><br><span class="line">    rec_real_img_mask = self._do_if_necessary_saturate_mask(rec_real_img_mask, saturate=self._opt.do_saturate_mask)</span><br><span class="line">    rec_real_imgs = rec_real_img_mask * fake_imgs_masked + (<span class="number">1</span> - rec_real_img_mask) * rec_real_img_rgb</span><br><span class="line"></span><br><span class="line">    <span class="comment"># l_cyc(G(G(Ic1,c2), c1)*M)</span></span><br><span class="line">    self._loss_g_cyc = self._criterion_cycle(rec_real_imgs, self._real_img) * self._opt.lambda_cyc</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss mask</span></span><br><span class="line">    <span class="comment"># (Attention Loss) 不仅对生成的mask进行了平滑，也对重构生成的mask进行了平滑损失</span></span><br><span class="line">    self._loss_g_mask_1 = torch.mean(fake_img_mask) * self._opt.lambda_mask</span><br><span class="line">    self._loss_g_mask_2 = torch.mean(rec_real_img_mask) * self._opt.lambda_mask</span><br><span class="line">    self._loss_g_mask_1_smooth = self._compute_loss_smooth(fake_img_mask) * self._opt.lambda_mask_smooth</span><br><span class="line">    self._loss_g_mask_2_smooth = self._compute_loss_smooth(rec_real_img_mask) * self._opt.lambda_mask_smooth</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_loss_smooth</span><span class="params">(self, mat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.sum(torch.abs(mat[:, :, :, :<span class="number">-1</span>] - mat[:, :, :, <span class="number">1</span>:])) + \</span><br><span class="line">            torch.sum(torch.abs(mat[:, :, :<span class="number">-1</span>, :] - mat[:, :, <span class="number">1</span>:, :]))</span><br></pre></td></tr></table></figure><h2 id="5-5-保存图片"><a href="#5-5-保存图片" class="headerlink" title="5.5 保存图片"></a>5.5 保存图片</h2><p>这个保存图片在starGAN就没有太理解，在这里又看到了类似的，才理解这是对输入图片归一化的反向操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># starGAN</span></span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">denorm</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="string">"""Convert the range from [-1, 1] to [0, 1]."""</span></span><br><span class="line">    out = (x + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> out.clamp_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GANimation</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">mean = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line">std = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line"><span class="keyword">for</span> i, m, s <span class="keyword">in</span> zip(img, mean, std):</span><br><span class="line">  i.mul_(s).add_(m)</span><br><span class="line">image_numpy = img.numpy()</span><br><span class="line">image_numpy_t = np.transpose(image_numpy, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">image_numpy_t = image_numpy_t*<span class="number">254.0</span></span><br><span class="line">image_numpy_t.astype(np.uint8)</span><br></pre></td></tr></table></figure><h2 id="5-6-其他"><a href="#5-6-其他" class="headerlink" title="5.6 其他"></a>5.6 其他</h2><p>没有实际跑这个代码，所以对于一些细节不是很清晰，尤其是数据处理那里，暂时根据查到的AU资料理解成17个AU(但1, 2, 4, 5, 6, 7, 9, 10, 12, 14, 15, 17, 20, 23, 25, 26, 28, and 45是18个AU)，每个AU是一个0~5的数字。</p><p>但是对于作者所说的能够生成连续的表情变换，这一点只能在测试代码中看出，但是在训练的时候并没有特意去表示连续的变化，暂时对于连续的变化存疑。</p><p>主要是openface这个库有点晕，等数据集下载之后试试。</p><p><a href="https://github.com/albertpumarola/GANimation/issues/45" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/45</a><br><a href="https://github.com/albertpumarola/GANimation/issues/62" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/62</a><br><a href="https://github.com/albertpumarola/GANimation/issues/43" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/43</a><br><a href="https://github.com/albertpumarola/GANimation/issues/32" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/32</a><br><a href="https://github.com/albertpumarola/GANimation/issues/25" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/25</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这篇文章是根据GANnotation的一个公式查过来的，感觉还挺厉害。&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{I}=(1-M) \circ C + M \circ I&lt;/script&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://www.albertpumarola.com/publications/files/pumarola2018ganimation.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GANimation Anatomically-aware Facial Animation from a Single Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;github: &lt;a href=&quot;https://github.com/albertpumarola/GANimation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/albertpumarola/GANimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;project: &lt;a href=&quot;https://www.albertpumarola.com/research/GANimation/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.albertpumarola.com/research/GANimation/index.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关键词：starGAN的改进、连续的表情变换、贴回去能够一致&lt;/p&gt;
    
    </summary>
    
      <category term="GAN" scheme="http://yoursite.com/categories/GAN/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="mask and colour" scheme="http://yoursite.com/tags/mask-and-colour/"/>
    
  </entry>
  
  <entry>
    <title>GANnotation</title>
    <link href="http://yoursite.com/2019/01/17/GANnotation/"/>
    <id>http://yoursite.com/2019/01/17/GANnotation/</id>
    <published>2019-01-17T09:00:01.000Z</published>
    <updated>2019-01-24T07:25:49.642Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这是对GAN应用到人脸合成的改进，只是在arxiv上，先看看再说。</p><a id="more"></a><ul><li>paper: <a href="https://arxiv.org/abs/1811.03492" target="_blank" rel="noopener">https://arxiv.org/abs/1811.03492</a></li><li>github: <a href="https://github.com/ESanchezLozano/GANnotation" target="_blank" rel="noopener">https://github.com/ESanchezLozano/GANnotation</a></li><li>youtube: <a href="https://youtu.be/-8r7zexg4yg" target="_blank" rel="noopener">https://youtu.be/-8r7zexg4yg</a></li></ul><p>key word: self-consistency loss, triple consistency loss, progressive image translation</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>作者对 <strong>self-consistency loss</strong> 的有效性进行了论证， self-consistency loss 可以有效地使图片转换前后 <strong>preserve identity</strong>，并且提出了新的loss <strong>triple consistency loss</strong>.</p><p><img src="./GANnotation/GANnotation2.png" alt="triple consistency loss"><br><img src="/2019/01/17/GANnotation/GANnotation2.png" title="triple consistency loss"></p><p>作者观察到当生成的图片再次经过生成另一种属性的图片时，生成的效果很差。记生成的图片再次经过网络生成新的图片的过程为 <strong>“progressive image translation”</strong>。比如输入一张图片a，先通过网络生成图片b，再将图片b送入网络，得到生成图片c。</p><p><img src="./GANnotation/GANnotation1.png" alt="progressive image translation"><br><img src="/2019/01/17/GANnotation/GANnotation1.png" title="progressive image translation"></p><p>并且作者提出了<strong>GAN-notation</strong>，即 <strong>unconstrained landmark guided face-to-face synthesis</strong>, 同时改变人脸的姿势和表情(simultaneous change in pose and expression)， 论证了 triple consistency loss 的有效性。</p><h1 id="2-Proposed-approach"><a href="#2-Proposed-approach" class="headerlink" title="2. Proposed approach"></a>2. Proposed approach</h1><p><img src="./GANnotation/GANnotation3.png" alt="Proposed approach"><br><img src="/2019/01/17/GANnotation/GANnotation3.png" title="Proposed approach"></p><h2 id="2-1-Notation"><a href="#2-1-Notation" class="headerlink" title="2.1 Notation"></a>2.1 Notation</h2><p>符号说明：</p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">$I\in \mathcal{I}, w × h$</td><td style="text-align:center">人脸图片$I$</td></tr><tr><td style="text-align:center">$s_i \in \mathbb{R}^{2n}$</td><td style="text-align:center">$n$个有序点的集合</td></tr><tr><td style="text-align:center">$H(s_t) \in \mathcal{H}, H(s_t) \in \mathbb{R}^{n×w×h}$</td><td style="text-align:center">$s_i$编码成 heatmap，以均值为点的高斯分布的形式呈现</td></tr><tr><td style="text-align:center">$G: \mathcal{I}×\mathcal{H} \to \hat{\mathcal{I}}$</td><td style="text-align:center">$\hat{\mathcal{I}}$是生成图片的集合</td></tr><tr><td style="text-align:center">$\hat{I} = G(I;H(s_t))$</td><td style="text-align:center">其中$I$和$H(s_t)$是在通道维拼接而成的</td></tr><tr><td style="text-align:center">$\mathbb{P}_{\mathcal{I}}$</td><td style="text-align:center">图片$I$的分布</td></tr><tr><td style="text-align:center">$\mathbb{P}_{\hat{\mathcal{I}}}$</td><td style="text-align:center">图片$\hat{I}$的分布</td></tr></tbody></table></div><h2 id="2-2-Architecture"><a href="#2-2-Architecture" class="headerlink" title="2.2 Architecture"></a>2.2 Architecture</h2><p><strong>G</strong>:</p><ul><li>two spatial downsampling convolutions</li><li>followed by a set of residual blocks</li><li>two spatial upsampling blocks with 1=2 strided convolutions</li><li>output: colour image $C$ and a mask $M$.</li></ul><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><p><strong>D</strong>: PatchGAN, 128×128—&gt;4×4×512</p><h2 id="2-3-Training"><a href="#2-3-Training" class="headerlink" title="2.3 Training"></a>2.3 Training</h2><h3 id="2-3-1-Adversarial-loss"><a href="#2-3-1-Adversarial-loss" class="headerlink" title="2.3.1 Adversarial loss"></a>2.3.1 Adversarial loss</h3><p>作者使用的是 hinge adversarial loss。</p><p>我在参考<a href="https://arxiv.org/pdf/1705.02894.pdf" target="_blank" rel="noopener">文献19</a>中没有找到类似的loss，自己理解的是，对于判别器D而言，希望真图片的真值是$y \leqslant -1$，生成图片的真值是$y \geqslant 1$， 对于生成器G而言，希望生成的图片的真值是$y \geqslant 0$</p><script type="math/tex; mode=display">\begin{align}L_{adv}(D) = &-\mathbb{E}_{\hat{I}\backsim \mathbb{P}_{\hat{\mathcal{I}}}}[min(0, -1+D(\hat{I}))]+ \\&-\mathbb{E}_{I\backsim \mathbb{P}_{\mathcal{I}}}[min(0, -1+D(\hat{I}))]\end{align}</script><script type="math/tex; mode=display">L_{adv}(G)=-\mathbb{E}_{I \backsim \mathbb{P}_I } [D(\hat{I})]</script><h3 id="2-3-2-Pixel-loss"><a href="#2-3-2-Pixel-loss" class="headerlink" title="2.3.2 Pixel loss"></a>2.3.2 Pixel loss</h3><p>这个公式应该需要成对图片</p><script type="math/tex; mode=display">L_{pix} = \parallel G(I;H(s_t)) - I_t \parallel</script><h3 id="2-3-3-Consistency-loss"><a href="#2-3-3-Consistency-loss" class="headerlink" title="2.3.3 Consistency loss"></a>2.3.3 Consistency loss</h3><script type="math/tex; mode=display">L_{self}=\parallel G(G(I;H(s_t)); H(s_i)) - I \parallel</script><h3 id="2-3-4-Triple-Consistency-loss"><a href="#2-3-4-Triple-Consistency-loss" class="headerlink" title="2.3.4 Triple Consistency loss"></a>2.3.4 Triple Consistency loss</h3><script type="math/tex; mode=display">L_{triple}=\parallel G(\hat{I};H(s_n)) - G(I;H(s_n)) \parallel ^2</script><p>其中，$ \hat{I} = G(I;H(s_t)) $.</p><h3 id="2-3-5-Identity-preserving-loss"><a href="#2-3-5-Identity-preserving-loss" class="headerlink" title="2.3.5 Identity preserving loss"></a>2.3.5 Identity preserving loss</h3><script type="math/tex; mode=display">L_{id}=\sum_{l=fc,p} \parallel \Phi_{CNN}^l(I) - \Phi_{CNN}^l(\hat{I})  \parallel</script><p>其中，这个公式的目的是为了preserve the identity(???), 使用 Light CNN 的 fully connected layer 和 last pooling layer 提取出的特征。</p><p>不懂，为啥子呢？之前看StarGAN已经不使用这个公式了。</p><h3 id="2-3-6-Perceptual-loss"><a href="#2-3-6-Perceptual-loss" class="headerlink" title="2.3.6 Perceptual loss"></a>2.3.6 Perceptual loss</h3><p>这个应该就是之前见过的Vgg提取特征做损失。</p><script type="math/tex; mode=display">\begin{align}L_{pp} =& \sum_{l} \parallel \Phi_{VGG}^l(I) \Phi_{VGG}^l(\hat{I}) \parallel \\&+\parallel  \Gamma(\Phi_{VGG}^{relu3_3}(I)) - \Phi_{VGG}^{relu3_3}(\hat{I}) \parallel_F\end{align}</script><p>其中，$l=\lbrace relu1_2, relu2_2, relu3_3, relu4_3\rbrace$</p><p>感觉用到的损失太多了。</p><h3 id="2-3-7-Full-loss"><a href="#2-3-7-Full-loss" class="headerlink" title="2.3.7 Full loss"></a>2.3.7 Full loss</h3><script type="math/tex; mode=display">\begin{align}L(G) =& \lambda_{adv}L_{adv} + \lambda_{pix}L_{pix}+\lambda_{self}L_{self}+\\&\lambda_{triple}L_{triple}+\lambda_{id}L_{id}+\lambda_{pp}L_{pp}+\lambda_{tv}L_{tv}\end{align}</script><p>其中，$\lambda_{adv}=1, \lambda_{pix}=10, \lambda_{self}=100, \lambda_{triple}=100, \lambda_{id}=1, \lambda_{pp}=10, \lambda_{tv}=10^{-4} $</p><p>在作者给的<a href="https://arxiv.org/pdf/1603.08155.pdf" target="_blank" rel="noopener">参考文献14</a>中，也没有明确找到$L_{tv}$的表达式。之后看代码再确定一下吧。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h1><ul><li>Adam, $\beta_1=0.5, \beta_2=0.999$</li></ul><h2 id="3-1-On-the-use-of-a-triple-consistency-loss"><a href="#3-1-On-the-use-of-a-triple-consistency-loss" class="headerlink" title="3.1 On the use of a triple consistency loss"></a>3.1 On the use of a triple consistency loss</h2><p><img src="./GANnotation/GANnotation4.png" alt="StarGAN with/without triple consistency loss"><br><img src="/2019/01/17/GANnotation/GANnotation4.png" title="StarGAN with/without triple consistency loss"></p><p>可以看到即使是 one-to-one ， with triple consistency loss 的效果也要更好一些。</p><h2 id="3-2-GANnotation"><a href="#3-2-GANnotation" class="headerlink" title="3.2 GANnotation"></a>3.2 GANnotation</h2><p>StarGAN只能对很粗的概念进行转换，但是GANnotation可以直接对比较细的属性pose进行转换，而且还可以连续变换，更厉害一些。</p><p><img src="./GANnotation/GANnotation5.png" alt="GANnotation"><br><img src="/2019/01/17/GANnotation/GANnotation5.png" title="GANnotation"></p><p><img src="./GANnotation/GANnotation6.png" alt="GANnotation.vs.CR-GAN"><br><img src="/2019/01/17/GANnotation/GANnotation6.png" title="GANnotation.vs.CR-GAN"></p><h1 id="4-code"><a href="#4-code" class="headerlink" title="4. code"></a>4. code</h1><h2 id="4-1-生成器G"><a href="#4-1-生成器G" class="headerlink" title="4.1 生成器G"></a>4.1 生成器G</h2><p>生成器G输入是一张3通道的人脸图片和66通道的目标人脸关键点热力图，输出是3通道的colour image 和1通道的mask图。</p><p>生成器G共分为5部分：down_conv、bottleneck、feature_layer、colour_layer、mask_layer。</p><ul><li>[x] colour_image 和 mask 怎么理解？</li></ul><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conv_dim=<span class="number">64</span>, c_dim=<span class="number">66</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        initial_layer = [nn.Conv2d(<span class="number">3</span>+c_dim, conv_dim, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>,padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">        initial_layer += [nn.InstanceNorm2d(conv_dim, affine=<span class="keyword">True</span>)]</span><br><span class="line">        initial_layer += [nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>)]</span><br><span class="line"></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            initial_layer += [nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">            initial_layer += [nn.InstanceNorm2d(curr_dim*<span class="number">2</span>, affine=<span class="keyword">True</span>)]</span><br><span class="line">            initial_layer += [nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>)]</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.down_conv = nn.Sequential(*initial_layer)</span><br><span class="line"></span><br><span class="line">        bottleneck = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(repeat_num):</span><br><span class="line">            bottleneck += [ResidualBlock(curr_dim)]</span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(*bottleneck)</span><br><span class="line"></span><br><span class="line">        features = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            features += [nn.ConvTranspose2d(curr_dim, curr_dim//<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">            features += [nn.InstanceNorm2d(curr_dim//<span class="number">2</span>, affine=<span class="keyword">True</span>)]</span><br><span class="line">            features += [nn.LeakyReLU(<span class="number">0.2</span>,inplace=<span class="keyword">True</span>)]</span><br><span class="line">            curr_dim = curr_dim // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.feature_layer = nn.Sequential(*features)</span><br><span class="line"></span><br><span class="line">        colour = [nn.Conv2d(curr_dim, <span class="number">3</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">        colour += [nn.Tanh()]</span><br><span class="line">        self.colour_layer = nn.Sequential(*colour)</span><br><span class="line"></span><br><span class="line">        mask = [nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">        mask += [nn.Sigmoid()]</span><br><span class="line">        self.mask_layer = nn.Sequential(*mask)</span><br><span class="line">        init_weights(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">( self, x )</span>:</span></span><br><span class="line">        down = self.down_conv(x)</span><br><span class="line">        bottle = self.bottleneck(down)</span><br><span class="line">        features = self.feature_layer(bottle)</span><br><span class="line">        col = self.colour_layer(features)</span><br><span class="line">        mask = self.mask_layer(features)</span><br><span class="line">        output = mask * ( x[:,<span class="number">0</span>:<span class="number">3</span>,:,:] - col ) + col</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(net, init_type=<span class="string">'normal'</span>, gain=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_func</span><span class="params">(m)</span>:</span></span><br><span class="line">        classname = m.__class__.__name__</span><br><span class="line">        <span class="keyword">if</span> hasattr(m, <span class="string">'weight'</span>) <span class="keyword">and</span> (classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span> <span class="keyword">or</span> classname.find(<span class="string">'Linear'</span>) != <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> init_type == <span class="string">'normal'</span>:</span><br><span class="line">                weight_init.normal_(m.weight.data, <span class="number">0.0</span>, gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'xavier'</span>:</span><br><span class="line">                weight_init.xavier_normal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'kaiming'</span>:</span><br><span class="line">                weight_init.kaiming_normal_(m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_in'</span>)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'orthogonal'</span>:</span><br><span class="line">                weight_init.orthogonal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> NotImplementedError(<span class="string">'initialization method [%s] is not implemented'</span> % init_type)</span><br><span class="line">            <span class="keyword">if</span> hasattr(m, <span class="string">'bias'</span>) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                weight_init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm2d'</span>) != <span class="number">-1</span>:</span><br><span class="line">            weight_init.normal_(m.weight.data, <span class="number">1.0</span>, gain)</span><br><span class="line">            weight_init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'initialize network with %s'</span> % init_type)</span><br><span class="line">    net.apply(init_func)</span><br></pre></td></tr></table></figure><p>np.loadtxt</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">points = np.loadtxt(<span class="string">'test_images/test_1.txt'</span>).transpose().reshape(<span class="number">66</span>,<span class="number">2</span>,<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="4-2-其他"><a href="#4-2-其他" class="headerlink" title="4.2 其他"></a>4.2 其他</h2><p>作者现在只公布了demo代码。但不久就会公布训练代码，搓搓小手手期待中。</p><h2 id="4-3-StarGAN-with-Triple-Consistency-Loss"><a href="#4-3-StarGAN-with-Triple-Consistency-Loss" class="headerlink" title="4.3 StarGAN-with-Triple-Consistency-Loss"></a>4.3 StarGAN-with-Triple-Consistency-Loss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate target domain labels randomly.</span></span><br><span class="line">rand_idx = torch.randperm(label_org.size(<span class="number">0</span>))</span><br><span class="line">label_trg = label_org[rand_idx]</span><br><span class="line"></span><br><span class="line">rand_idx = torch.randperm(label_org.size(<span class="number">0</span>))</span><br><span class="line">label_third = label_org[rand_idx]</span><br><span class="line"><span class="comment"># ================================================================================#</span></span><br><span class="line"><span class="comment">#                               Triple consistency loss.                          #</span></span><br><span class="line"><span class="comment"># ================================================================================#</span></span><br><span class="line">x_third = self.G(x_fake, c_third)</span><br><span class="line">x_third_real = self.G(x_real, c_third)</span><br><span class="line">g_loss_triple = torch.mean(torch.abs(x_third - x_third_real))</span><br></pre></td></tr></table></figure><h1 id="5-答疑解惑"><a href="#5-答疑解惑" class="headerlink" title="5. 答疑解惑"></a>5. 答疑解惑</h1><h2 id="5-1-colour-image-和-mask-怎么理解？"><a href="#5-1-colour-image-和-mask-怎么理解？" class="headerlink" title="5.1 colour_image 和 mask 怎么理解？"></a>5.1 colour_image 和 mask 怎么理解？</h2><p>这个公式来源于</p><p>paper: <a href="http://arxiv.org/abs/1807.09251" target="_blank" rel="noopener">Ganimation: Anatomically-aware facial animation from a single image</a></p><p>github: <a href="https://github.com/albertpumarola/GANimatio" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimatio</a></p><p>project: <a href="http://www.albertpumarola.com/#projects" target="_blank" rel="noopener">http://www.albertpumarola.com/#projects</a></p><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><p>网络生成器没有直接回归整个图像，而是输出两个掩码，一个着色掩码C和一个注意力掩码A，其中，掩码A表示C的每个像素在多大程度上对输出图像有贡献，这样生成器就无需渲染与表情无关的元素，仅聚焦于定义了人脸表情的像素上。</p><p>这个公式貌似比StarGAN还牛逼。能不能放在StarGAN上呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这是对GAN应用到人脸合成的改进，只是在arxiv上，先看看再说。&lt;/p&gt;
    
    </summary>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="face synthesis" scheme="http://yoursite.com/tags/face-synthesis/"/>
    
  </entry>
  
  <entry>
    <title>tensorboard</title>
    <link href="http://yoursite.com/2019/01/15/tensorboard/"/>
    <id>http://yoursite.com/2019/01/15/tensorboard/</id>
    <published>2019-01-15T09:15:48.000Z</published>
    <updated>2019-01-17T08:20:10.969Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>新装pytorch之后，可视化工具visdom就不能用了，所以改用tensorboard，以下是tensorboard及其变体的使用命令。</p><a id="more"></a><h1 id="1-原理"><a href="#1-原理" class="headerlink" title="1. 原理"></a>1. 原理</h1><p>tensorboard与visdom不同，前者是直接读取文件进行展示，需要程序先把要展示的内容保存成文件，然后tensorboard再读取文件进行展示，后者是代码直接展示，在程序中就直接传递给visdom了。</p><p>相同点是两者都需要在程序外面启动。</p><h1 id="2-API以及使用流程"><a href="#2-API以及使用流程" class="headerlink" title="2. API以及使用流程"></a>2. API以及使用流程</h1><h2 id="2-1-API"><a href="#2-1-API" class="headerlink" title="2.1 API"></a>2.1 API</h2><ul><li>tf.summary.FileWriter——用于将汇总数据写入磁盘 </li><li>tf.summary.scalar——对标量数据汇总和记录 </li><li>tf.summary.histogram——记录数据的直方图 </li><li>tf.summary.image——将图像写入summary </li><li>tf.summary.merge——对各类的汇总进行一次合并 </li><li>tf.summary.merge_all——合并默认图像中的所有汇总</li></ul><h2 id="2-2-使用流程"><a href="#2-2-使用流程" class="headerlink" title="2.2 使用流程"></a>2.2 使用流程</h2><ol><li>添加记录节点：tf.summary.scalar/image/histogram()等</li><li>汇总记录节点：merged = tf.summary.merge_all()</li><li>运行汇总节点：summary = sess.run(merged)，得到汇总结果</li><li>日志书写器实例化：summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph)，实例化的同时传入 graph 将当前计算图写入日志</li><li>调用日志书写器实例对象summary_writer的add_summary(summary, global_step=i)方法将所有汇总日志写入文件</li><li>调用日志书写器实例对象summary_writer的close()方法写入内存，否则它每隔120s写入一次， close() 之后就无法再次写入了，需要重新打开reopen()，这里可以替代为summary_writer.flush()。</li></ol><p><img src="./tensorboard/tensorboard.jpg" alt="tensorboard使用流程图"><br><img src="/2019/01/15/tensorboard/tensorboard.jpg" title="tensorboard使用流程图"></p><h1 id="3-启动"><a href="#3-启动" class="headerlink" title="3. 启动"></a>3. 启动</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir /path/to/log</span><br></pre></td></tr></table></figure><h1 id="4-初始化"><a href="#4-初始化" class="headerlink" title="4. 初始化"></a>4. 初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.summary.FileWriter(logdir, graph=None, flush_secs=120, max_queue=10)</span></span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>, sess.graph(), flush_secs = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">writer.add_graph(sess.graph())</span><br></pre></td></tr></table></figure><p>其他常用API</p><ul><li>add_event(event)：Adds an event to the event file</li><li>add_graph(graph, global_step=None)：Adds a Graph to the event file，Most users pass a graph in the constructor instead</li><li>add_summary(summary, global_step=None)：Adds a Summary protocol buffer to the event file，一定注意要传入 global_step</li><li>close()：Flushes the event file to disk and close the file</li><li>flush()：Flushes the event file to disk</li><li>add_meta_graph(meta_graph_def,global_step=None)</li><li>add_run_metadata(run_metadata, tag, global_step=None)</li></ul><p>备注：这个event还是没有看懂怎么使用。</p><h1 id="5-scalar"><a href="#5-scalar" class="headerlink" title="5. scalar"></a>5. scalar</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 三个工具：writer用于指定路径，sess用于运行，summary_op用于执行。</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy) <span class="comment"># tag, value</span></span><br><span class="line"></span><br><span class="line">summary_op = tf.summary.merge_all()</span><br><span class="line">summary_str = sess.run(summary_op)</span><br><span class="line">summary_writer.add_summary(summary_str, global_step) <span class="comment"># y, x</span></span><br><span class="line"><span class="comment"># 备注：global_step总是int型，即使输入0.4，在图像显示也是0.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or family</span></span><br><span class="line">tf.summary.scalar(<span class="string">'loss1'</span>, <span class="number">1</span>, family=<span class="string">'loss'</span>)</span><br><span class="line">tf.summary.scalar(<span class="string">'loss2'</span>, <span class="number">2</span>, family=<span class="string">'loss'</span>)</span><br><span class="line">tf.summary.scalar(<span class="string">'loss3'</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 示意如下图</span></span><br></pre></td></tr></table></figure><p><img src="./tensorboard/tensorboard3.png" alt="family"><br><img src="/2019/01/15/tensorboard/tensorboard3.png" title="family"></p><p>或者自定义数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 两个工具 summary_writer: 用于连接路径， summary: 用于指定y值</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">summary = tf.Summary(value=[</span><br><span class="line">  tf.Summary.Value(tag=<span class="string">'test2'</span>, simple_value=<span class="number">0</span>),</span><br><span class="line">  tf.Summary.Value(tag=<span class="string">'test3'</span>, simple_value=<span class="number">1</span>),</span><br><span class="line">])</span><br><span class="line">summary_writer.add_summary(summary, global_step) <span class="comment"># y, x</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">summary = tf.Summary()</span><br><span class="line">summary.value.add(tag=<span class="string">'test4'</span>, simple_value=<span class="number">0</span>)</span><br><span class="line">summary.value.add(tag=<span class="string">'test5'</span>, simple_value=<span class="number">1</span>)</span><br><span class="line">sumamry_writer.add_summary(summary, global_step)</span><br></pre></td></tr></table></figure><h1 id="6-histogram"><a href="#6-histogram" class="headerlink" title="6. histogram"></a>6. histogram</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.histogram(<span class="string">'layer'</span>+str(i+<span class="number">1</span>)+<span class="string">'weights'</span>,weights)</span><br></pre></td></tr></table></figure><p>这里的weights可以是list型，也可以是pytorch的tensor型，其他类型没试，但可以推断，一般情况下的都可以，猜测会统一转换为numpy型。</p><h1 id="7-image"><a href="#7-image" class="headerlink" title="7. image"></a>7. image</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.image(<span class="string">'input'</span>, x_image, max_outputs=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tf.summary.image(<span class="string">'test'</span>, tf.reshape(images, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]), <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tf.summary.image(<span class="string">'test1'</span>, torch.rand(<span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>备注：x_image必须是uint8或者float32型的4-D Tensor[batch_size, height, width, channels]，其中channels为1， 3 或者4</p><p>对于numpy</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line">image_ = Image.open(<span class="string">'path'</span>)</span><br><span class="line">image_numpy = numpy.array(image_) <span class="comment"># 271, 108, 3</span></span><br><span class="line">tf.summary.image(<span class="string">'test1'</span>, image_numpy.reshape(<span class="number">1</span>, <span class="number">271</span>, <span class="number">108</span>,<span class="number">3</span>))</span><br><span class="line">summary_op = tf.summary.merge_all()</span><br><span class="line">summary_str = sess.run(summary_op)</span><br><span class="line">summary_writer.add_summary(summary_str, global_step)</span><br></pre></td></tr></table></figure><p>对于pytorch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image_ = Image.open(<span class="string">'path'</span>) <span class="comment"># 3, 271, 108</span></span><br><span class="line">image_tensor = T.ToTensor()(image_)</span><br><span class="line">tf.summary.image(<span class="string">'test9'</span>, image_tensor.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">271</span>, <span class="number">108</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 备注：将permute改为reshape之后不知道为什么会展示9空格的相同的黑白图片</span></span><br><span class="line">summary_op = tf.summary.merge_all()</span><br><span class="line">summary_str = sess.run(summary_op)</span><br><span class="line">summary_writer.add_summary(summary_str, global_step)</span><br></pre></td></tr></table></figure><p>uint8或者float32型的4-D Tensor[batch_size, height, width, channels]，其中channels为1， 3 或者4</p><h1 id="8-多个event的可视化"><a href="#8-多个event的可视化" class="headerlink" title="8. 多个event的可视化"></a>8. 多个event的可视化</h1><p>如果 logdir 目录的子目录中包含另一次运行时的数据(多个 event)，那么 TensorBoard 会展示所有运行的数据(主要是scalar)，这样可以用于比较不同参数下模型的效果，调节模型的参数，让其达到最好的效果！</p><p>这个还有待测试</p><h1 id="9-tensorboard-logger"><a href="#9-tensorboard-logger" class="headerlink" title="9. tensorboard_logger"></a>9. tensorboard_logger</h1><p>tensorboard_logger可以暂时理解成简化版的tensorboard，或者是tensorboard的高级API。</p><p>tensorboard_logger依赖于tensorboard。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">pip install tensorboard_logger</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">tensorboard --logdir /path/to/log --port /port <span class="comment"># 实际还是通过tensorboard来使用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">from</span> tensorboard_logger <span class="keyword">import</span> Logger</span><br><span class="line">logger = Logger(logdir=<span class="string">'./logs'</span>, flush_secs=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># logger.logdir = './logs2'  # 可以修改，尝试着修改了一下，发现没用，暂时用不着。</span></span><br><span class="line"><span class="comment"># logger.flush_secs = 3   # 可以修改</span></span><br><span class="line"><span class="comment"># API</span></span><br><span class="line">logger.log_value(<span class="string">'loss'</span>, <span class="number">10</span>, step=<span class="number">3</span>) <span class="comment"># step 可以不写，推荐写</span></span><br><span class="line"><span class="comment"># image_path = 'test.jpg'</span></span><br><span class="line"><span class="comment"># image_ = Image.open(image_path)</span></span><br><span class="line"><span class="comment"># image_numpy = np.array(image_)</span></span><br><span class="line">logger.log_images(<span class="string">'images'</span>, image_numpy.reshape(<span class="number">1</span>,<span class="number">271</span>,<span class="number">168</span>,<span class="number">3</span>), step=<span class="number">2</span>) <span class="comment"># 规则同上</span></span><br><span class="line">logger.log_histogram(<span class="string">'weights'</span>, torch.rand(<span class="number">2</span>,<span class="number">3</span>)+<span class="number">10</span>, step=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数tensorboard_logger.configure, tensorboard_logger.log_value与tensorboard_logger.Logger可以达到一样的效果</span></span><br><span class="line"><span class="keyword">from</span> tensorboard_logger <span class="keyword">import</span> configure, log_value</span><br><span class="line">configure(<span class="string">"runs/run-1234"</span>, flush_secs=<span class="number">5</span>)</span><br><span class="line">log_value(<span class="string">'v1'</span>, v1, step)</span><br></pre></td></tr></table></figure><h1 id="10-tensorboardX"><a href="#10-tensorboardX" class="headerlink" title="10. tensorboardX"></a>10. tensorboardX</h1><p><a href="https://github.com/lanpa/tensorboardX" target="_blank" rel="noopener">https://github.com/lanpa/tensorboardX</a></p><p>tensorboardX也依赖于tensorflow和tensorboard。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">pip install tensorboardX</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">tensorboard --logdir /path/to/log --port /port</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(logdir = <span class="string">'tensorboard7'</span>, comment = <span class="string">'test1'</span>, flush_secs = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 当有logdir时，comment被忽略，</span></span><br><span class="line"><span class="comment"># 当没有logdir时，只有comment(writer = SummaryWriter(comment = 'tensorboard7_test1', flush_secs = 1))会作为补充自动生成这样的文件目录：</span></span><br><span class="line"><span class="comment"># ./runs/Jan17_11-24-12_zbp-PowerEdge-T630tensorboard7_test1/events....</span></span><br><span class="line"><span class="comment"># 此外，任意tf.summary.FileWriter的参数都可以加上去。</span></span><br><span class="line">writer.close()</span><br><span class="line"><span class="comment"># 没有writer.flush()</span></span><br></pre></td></tr></table></figure><p>另一种写法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> SummaryWriter(logdir = <span class="string">'tensorboard7'</span>) <span class="keyword">as</span> w:</span><br><span class="line">    w.add_something()</span><br></pre></td></tr></table></figure><p>不推荐with的写法，会自动重新创建一个文件，乱。</p><h2 id="10-1-add-scalar"><a href="#10-1-add-scalar" class="headerlink" title="10.1 add_scalar"></a>10.1 add_scalar</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># API</span></span><br><span class="line">writer.add_scalar(<span class="string">'loss1'</span>, <span class="number">2</span>, global_step=<span class="number">2</span>)</span><br><span class="line">writer.add_scalars(<span class="string">'loss'</span>, &#123;<span class="string">'loss1'</span>:<span class="number">2</span>, <span class="string">'loss2'</span>:<span class="number">3</span>&#125;, global_step=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 示例如下图，add_scalars可以将多个loss显示同一个图上，实现类似visdom的功能。</span></span><br><span class="line"><span class="comment"># add_scalars(main_tag,tag_scalar_dict,global_step=None)</span></span><br></pre></td></tr></table></figure><p><img src="./tensorboard/tensorboard1.png" alt="add_scalar&amp;add_scalars"><br><img src="/2019/01/15/tensorboard/tensorboard1.png" title="add_scalar&add_scalars"></p><p><strong>备注：</strong>:</p><ol><li><strong>SummaryWriter以及之前的类似Writer都可以自动创建文件夹</strong>。</li><li><strong>tensorboard —logdir /path/to/log会自动迭代该文件下的所有文件夹和文件，将event展示出来，并且将同名字的scalar放在一起，可以用于对比修改前后的结果</strong>。</li></ol><h2 id="10-2-add-image"><a href="#10-2-add-image" class="headerlink" title="10.2 add_image"></a>10.2 add_image</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">writer.add_image(tag, img_tensor, global_step=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># img_tensor: 图像数据，shape（3，H，W) 配合torchvision.utils.make_grid使用</span></span><br><span class="line"><span class="comment"># writer.add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW'),</span></span><br><span class="line"><span class="comment">#  dataformats: 'CHW', 'HWC', 'HW'</span></span><br><span class="line"></span><br><span class="line">image_path = <span class="string">'test.jpg'</span></span><br><span class="line">image_ = Image.open(image_path)</span><br><span class="line">image_tensor = T.ToTensor()(image_)</span><br><span class="line">writer.add_image(<span class="string">'test1'</span>, image_tensor, global_step=<span class="number">2</span>)</span><br><span class="line">writer.add_image(<span class="string">'test2'</span>, torchvision.utils.make_grid(torch.rand(<span class="number">16</span>, <span class="number">3</span>, <span class="number">256</span>,<span class="number">256</span>), nrow=<span class="number">8</span>, padding=<span class="number">20</span>), global_step=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="10-3-add-histogram"><a href="#10-3-add-histogram" class="headerlink" title="10.3 add_histogram"></a>10.3 add_histogram</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">writer.add_histogram(tag,values,global_step=<span class="keyword">None</span>,bins=<span class="string">'tensorflow'</span>)</span><br><span class="line"><span class="comment"># bins:  (string): one of &#123;'tensorflow','auto', 'fd', ...&#125;, this determines</span></span><br><span class="line"><span class="comment"># how the bins are made. You can find other options in:</span></span><br><span class="line"><span class="comment"># https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html</span></span><br><span class="line">writer.add_histogram(<span class="string">'test1'</span>, torch.rand(<span class="number">16</span>, <span class="number">3</span>, <span class="number">256</span>,<span class="number">256</span>), global_step=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 感觉 bins 这个参数也差不多么</span></span><br></pre></td></tr></table></figure><h2 id="10-4-add-graph"><a href="#10-4-add-graph" class="headerlink" title="10.4 add_graph"></a>10.4 add_graph</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net1</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net1, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x )</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">10</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = torch.nn.functional.softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">input = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">model = Net1()</span><br><span class="line">writer.add_graph(model, (input,))</span><br><span class="line"></span><br><span class="line">add_graph(model, input_to_model, verbose=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># verbose 用于是否print</span></span><br></pre></td></tr></table></figure><p><img src="./tensorboard/tensorboard4.png" alt="add_graph"><br><img src="/2019/01/15/tensorboard/tensorboard4.png" title="add_graph"></p><p>终于pytorch也能显示graph了，不容易，应该提前熟悉这些API的。</p><h1 id="11-结论"><a href="#11-结论" class="headerlink" title="11. 结论"></a>11. 结论</h1><p>综上所述，我觉得：</p><ul><li>tensorboardX最适合非tensorflow的深度学习框架</li><li>tensorboard适合tensoflow</li><li>tensorboard_logger也适合非tensorflow的深度学习框架，可以看成简化版的tensorboardX</li><li>tensorboardX和tensorboard_logger可以互相替换</li><li>开始使用tensorboardX作为我的新的可视化工具。</li></ul><p>不太懂tensorboardX和tensorboard_logger之间的区别，难道是开发的人不一样？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;新装pytorch之后，可视化工具visdom就不能用了，所以改用tensorboard，以下是tensorboard及其变体的使用命令。&lt;/p&gt;
    
    </summary>
    
      <category term="tensorboard" scheme="http://yoursite.com/categories/tensorboard/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
      <category term="tensorboard" scheme="http://yoursite.com/tags/tensorboard/"/>
    
  </entry>
  
  <entry>
    <title>SyRI</title>
    <link href="http://yoursite.com/2019/01/14/SyRI/"/>
    <id>http://yoursite.com/2019/01/14/SyRI/</id>
    <published>2019-01-14T03:32:20.000Z</published>
    <updated>2019-01-14T07:07:45.946Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这也是一篇合成数据集的论文。</p><a id="more"></a><p>paper: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Slawomir_Bak_Domain_Adaptation_through_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Domain Adaptation through Synthesis for Unsupervised Person Re-identification</a></p><p>github: 无</p><p>项目地址: 无</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>针对的问题是行人重识别数据缺乏在不同光照条件下的多样性，所以作者进行了数据合成。</p><p><strong>HDR maps</strong>: high dynamic range environment maps.</p><p><strong>three-step domain adaption technique</strong>:</p><p><img src="./SyRI/SyRI1.png" alt="three-step domain adaption technique"><br><img src="/2019/01/14/SyRI/SyRI1.png" title="three-step domain adaption technique"></p><ol><li>Illumination inference</li><li>Domain translation</li><li>Fine-tuning</li></ol><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>这篇文章看得我有点迷迷糊糊的，感觉其中用到了cycleGAN，优化了cycleGAN的一个损失函数，但是怎么进行的illuminaion inference、fine-tuning，没有充分理解。</p><h1 id="罗浩对这篇文章的解读"><a href="#罗浩对这篇文章的解读" class="headerlink" title="罗浩对这篇文章的解读"></a>罗浩对这篇文章的解读</h1><p><a href="https://zhuanlan.zhihu.com/p/44212707" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44212707</a></p><blockquote><p>由于训练数据的缺乏，以及3D建模的技术增强，利用游戏等合成的逼真数据来进行视觉研究已经逐渐打入各个领域。这一篇就是利用电脑合成数据在进行ReID的论文。论文提出了新的合成数据集SyRI dataset，该数据集通过改变HDR参数等，一个行人ID可以拥有一百多个环境下图像数据。此外，为了在未见过的真实场景上实现更好的域适应效果，论文基于这个合成数据集提出了一种新的方法。</p><p>整个Pipeline包括三个环节。第一步是拿到了target domain $R_{M+1}$ 的一些未标注图片之后，要对光照进行一个推理，在合成数据集 $S_{k^*}$ 里面找到最接近样本 。然后利用CycleGAN将合成数据生成target domain style的数据。最后利用生成的数据对ReID网络进行fune-tuning。整体来说pipeline比较简单.</p><p>例如，在CycleGAN生成图环节，为了得到更高质量的图像，论文使用了和PTGAN一样的前景mask的思想。不同点在于PTGAN是借助于语义分割网络得到一个行人前景的mask，本文是直接使用了一个2D高斯核作为mask</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这也是一篇合成数据集的论文。&lt;/p&gt;
    
    </summary>
    
      <category term="person re-id" scheme="http://yoursite.com/categories/person-re-id/"/>
    
    
      <category term="person re-identification" scheme="http://yoursite.com/tags/person-re-identification/"/>
    
      <category term="domain adaption" scheme="http://yoursite.com/tags/domain-adaption/"/>
    
  </entry>
  
  <entry>
    <title>InstaGAN</title>
    <link href="http://yoursite.com/2019/01/04/InstaGAN/"/>
    <id>http://yoursite.com/2019/01/04/InstaGAN/</id>
    <published>2019-01-04T03:25:47.000Z</published>
    <updated>2019-01-10T14:57:05.795Z</updated>
    
    <content type="html"><![CDATA[<p>实例转换</p><a id="more"></a><h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p><a href="https://arxiv.org/pdf/1812.10889.pdf" target="_blank" rel="noopener">InstaGAN Instance-aware image-to-image translation</a></p><p>Sangwoo Mo, Minsu Cho, Jinwoo Shin</p><p>github: <a href="https://github.com/sangwoomo/instagan" target="_blank" rel="noopener">https://github.com/sangwoomo/instagan</a></p><p>project: <a href="https://openreview.net/forum?id=ryxwJhC9YX" target="_blank" rel="noopener">https://openreview.net/forum?id=ryxwJhC9YX</a></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p><img src="./InstaGAN/InstaGAN1.png" alt="Tranlation results"><br><img src="/2019/01/04/InstaGAN/InstaGAN1.png" title="Tranlation results"></p><p>整体分为三部分:</p><ol><li>an instance-augmented neural architecture</li><li>a context preserving loss</li><li>a sequential mini-batch inference/training technique</li></ol><ul><li>an instance-augmented neural architecture: an image and the corresponding set of instance attributes.</li><li>a context preserving loss: target instances and an identity function</li><li>a sequential mini-batch inference/training technique: translating the mini-batches of instance attributes sequentially</li></ul><h1 id="2-InstaGAN"><a href="#2-InstaGAN" class="headerlink" title="2. InstaGAN"></a>2. InstaGAN</h1><p>符号说明：</p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">$\mathcal{X}$, $\mathcal{Y}$</td><td style="text-align:center">image domain</td></tr><tr><td style="text-align:center">$\mathcal{A}, \mathcal{B}$</td><td style="text-align:center">a space of set of instance attributes</td></tr><tr><td style="text-align:center">$\boldsymbol{a} = \lbrace a_i \rbrace _{i=1}^N $</td><td style="text-align:center">set of instance attributes</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">instance segmentation mask</td></tr><tr><td style="text-align:center">$G_{XY}:\mathcal{X}-&gt;\mathcal{Y}, G_{YX}:\mathcal{Y}-&gt;\mathcal{X}$</td><td style="text-align:center">tranlation function</td></tr></tbody></table></div><h2 id="2-1-InstaGAN-architecture"><a href="#2-1-InstaGAN-architecture" class="headerlink" title="2.1 InstaGAN architecture"></a>2.1 InstaGAN architecture</h2><p><img src="./InstaGAN/InstaGAN2.png" alt="InstaGAN architecture"><br><img src="/2019/01/04/InstaGAN/InstaGAN2.png" title="InstaGAN architecture"></p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">$f_{GX}$</td><td style="text-align:center">image feature extractor</td></tr><tr><td style="text-align:center">$f_{GA}$</td><td style="text-align:center">attribute feature extractor</td></tr><tr><td style="text-align:center">$H_{GX}(x,a)=[f_{GX}(x);\sum_{i=1}^Nf_{GA}(a_i)]$</td><td style="text-align:center">image representation</td></tr><tr><td style="text-align:center">$H_{GA}^n(x,a)=[f_{GX}(x);\sum_{i=1}^Nf_{GA}(a_i);f_{GA}(a_n)]$</td><td style="text-align:center">image representation</td></tr><tr><td style="text-align:center">$h_{DX}(x,a)=[f_{DX}(x);\sum_{i=1}^Nf_{DA}(a_i)]$</td><td style="text-align:center">image representation for discriminator</td></tr><tr><td style="text-align:center">$f_{GX},f_{GA},f_{DX},f_{DA},g_{GX},G_{GA},G_{DX}$</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">$(x,a)-&gt;(y’,b’)$</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">$(y,b)-&gt;(x’,a’)$</td></tr></tbody></table></div><p><strong>作者为了能实现mask顺序不变性，采用相加的方式</strong>。</p><h2 id="2-2-Training-loss"><a href="#2-2-Training-loss" class="headerlink" title="2.2 Training loss"></a>2.2 Training loss</h2><ol><li><strong>domain loss</strong>: GAN loss</li><li><strong>content loss</strong>: cycle-consistency loss and identity mapping loss and context preserving loss</li></ol><p><strong>LSGAN</strong>: 判断图片是原始的还是生成的</p><script type="math/tex; mode=display">L_{LSGAN}=(D_X(x,a)-1)^2+(D_X(G_{YX}(y,b)))^2+(D_Y(y,b)-1)^2+(D_Y(G_{XY}(y,b)))^2</script><p><strong>cycle-consistency loss</strong>: 循环一致性</p><script type="math/tex; mode=display">L_{cyc}=\parallel G_{YX}(G_{XY}(x,a))-(x,a) \parallel \_1+\parallel G_{XY}(G_{YX}(y,b))-(y,b) \parallel _1</script><p><strong>identity mapping loss</strong>: 恒等映射</p><script type="math/tex; mode=display">L_{idt}=\parallel G_{XY}(y,b)-(y,b) \parallel \_1 + \parallel G_{YX}(x,a)-(x,a) \parallel _1</script><p><strong>context preserving loss</strong>: 保留背景</p><script type="math/tex; mode=display">L_{ctx}=\parallel w(a,b')\odot(x-y') \parallel _1 + \parallel w(b,a')\odot(y-x') \parallel _1</script><p>其中，$w(a,b’), w(b,a’)$表示在原图片和生成图片都是背景的位置的权重是1.</p><p><strong>Total loss</strong>:</p><script type="math/tex; mode=display">\begin{align}L_{InstaGAN}&=L_{LSGAN} + \lambda_{cyc} L_{cyc}+ \lambda_{idt} L_{idt}+ \lambda_{ctx} L_{ctx} \\&=L_{LSGAN}+L_{content}\end{align}</script><h2 id="2-3-sequential-mini-batch-translation"><a href="#2-3-sequential-mini-batch-translation" class="headerlink" title="2.3 sequential mini-batch translation"></a>2.3 sequential mini-batch translation</h2><p>考虑到在图片上的实例可能很多，而GPU的所需空间随之线性增长，可能不符合现实情况，所以需要考虑在图片上可以转化一小部分实例。</p><p><img src="./InstaGAN/InstaGAN3.png" alt="sequential mini-batch translation"><br><img src="/2019/01/04/InstaGAN/InstaGAN3.png" title="sequential mini-batch translation"></p><p>符号说明：</p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">$a=\cup_{i=1}^Ma_i$</td><td style="text-align:center">divide the set of instance masks a into mini-batch $a_1,a_2,…,a_M$</td></tr><tr><td style="text-align:center">$(x_m, a_m)-&gt;(y’_m, b’_m) or (x_{m+1}, a_{m+1})$</td><td style="text-align:center">mini-batch translation</td></tr><tr><td style="text-align:center">$(y’_m, b’_{1:m})=(y’_m, \cup_{i=1}^m b’_i)$</td><td style="text-align:center">用于判断真假</td></tr></tbody></table></div><p>在这种情况下，不同的损失函数作用的范围发生改变，第m次时，content loss作用在$(x_m, a_m), (y’_m, b’_m)$，domain loss 作用在$(x,a), (y’_m, b’_{1:m})$，即</p><script type="math/tex; mode=display">L_{InstaGAN-SM}=\sum_{m=1}^M \lbrace L_{LSGAN}((x,a),(y'_m, b'_{1:m}))+L_{content}((x_m, a_m),(y'_m, b'_m)) \rbrace</script><script type="math/tex; mode=display">L_{content}=\lambda_{cyc} L_{cyc}+\lambda_{idt} L_{idt}+\lambda_{ctx} L_{ctx}</script><ul><li>每m个迭代detach一次，来使用固定大小的GPU。</li><li>划分mini-batch的原则：size of instances, 由大到小</li></ul><h1 id="3-experimental-results"><a href="#3-experimental-results" class="headerlink" title="3. experimental results"></a>3. experimental results</h1><h2 id="3-1-image-to-image-translation-results"><a href="#3-1-image-to-image-translation-results" class="headerlink" title="3.1 image-to-image translation results"></a>3.1 image-to-image translation results</h2><p><img src="./InstaGAN/InstaGAN4.png" alt="translation results"><br><img src="/2019/01/04/InstaGAN/InstaGAN4.png" title="translation results"></p><p>通过上述结果的展示，我可以认为在这方面InstaGAN要比CycleGAN的效果更好，更能得到想要的指定的结果。</p><p><img src="./InstaGAN/InstaGAN5.png" alt="results of translation "><br><img src="/2019/01/04/InstaGAN/InstaGAN5.png" title="results of translation"></p><p>第一个结果表明可以通过控制掩码来控制生成的图片。</p><p>第二个结果表明可以使用预测的掩码进行转换图片，从而减少获取掩码的成本。</p><h2 id="3-2-ablation-study"><a href="#3-2-ablation-study" class="headerlink" title="3.2 ablation study"></a>3.2 ablation study</h2><p><img src="./InstaGAN/InstaGAN6.png" alt="ablation study"><br><img src="/2019/01/04/InstaGAN/InstaGAN6.png" title="ablation study"></p><p>Fig.9 主要使研究作者提出的三部分功能的作用，instance mask，损失函数，mini-batch的影响，从效果上看，还是最后一张图片效果更好一些。</p><p><img src="./InstaGAN/InstaGAN7.png" alt="ablation  study on the effects of the sequential mini-batch inference/training technique "><br><img src="/2019/01/04/InstaGAN/InstaGAN7.png" title="ablation study on the effects of the sequential mini-batch inference/training technique"></p><p>Fig.10分别表示在training和inference中使用one-step还是sequential方法，我觉得都差不多，但是对于有限的GPU是个很好的方法。</p><h1 id="4-Appendix"><a href="#4-Appendix" class="headerlink" title="4. Appendix"></a>4. Appendix</h1><h2 id="4-1-architecture-details"><a href="#4-1-architecture-details" class="headerlink" title="4.1 architecture details"></a>4.1 architecture details</h2><blockquote><p>PatchGAN discriminator is composed of 5 convolutional layers, including normalization and non-linearity layers. We used the first 3 convolution layers for feature extractors, and the last 2 convolution layers for classifier.</p></blockquote><h2 id="4-2-traning-details"><a href="#4-2-traning-details" class="headerlink" title="4.2 traning details"></a>4.2 traning details</h2><ul><li>$\lambda_{cyc}=10, \lambda_{idt}=10, \lambda_{ctx}=10$</li><li>Adam: $\beta_1=0.5, \beta_2=0.999$</li><li>batch_size=4</li><li>GPU = 4</li><li>learning rate: 0.0002 for G, 0.0001 for D, 前m个epoch保持不变，后n个epoch线性衰减为0.不同的数据集的m和n不同</li><li>size对于不同的数据集也不同。</li></ul><h2 id="4-3-trend-of-translation-results"><a href="#4-3-trend-of-translation-results" class="headerlink" title="4.3 trend of translation results"></a>4.3 trend of translation results</h2><p><img src="./InstaGAN/InstaGAN8.png" alt="trend of translation results"><br><img src="/2019/01/04/InstaGAN/InstaGAN8.png" title="trend of translation results"></p><h2 id="4-4-其他"><a href="#4-4-其他" class="headerlink" title="4.4 其他"></a>4.4 其他</h2><p>我觉得这是相当于对于CycleGAN，加上了指向性生成，不再是单独地生成目标域风格的图片，而是对指定区域生成目标域风格的图片。</p><p>刚刚想到一个问题，InstaGAN可以生成指定形状的图片，但是对于同一形状的不同物体，比如生成红色的裙子和黑色的裙子这样子的任务，可能不行。</p><h2 id="4-5-video-translation-results"><a href="#4-5-video-translation-results" class="headerlink" title="4.5 video translation results"></a>4.5 video translation results</h2><p><img src="./InstaGAN/InstaGAN9.png" alt="video translation results"><br><img src="/2019/01/04/InstaGAN/InstaGAN9.png" title="video translation results"></p><p>作者使用pix2pix作为分割。</p><p>感觉在视频上，裤子换成裙子后，能保持所有帧的裙子都是一样的，说明转换的稳定性很好。</p><h1 id="5-code"><a href="#5-code" class="headerlink" title="5. code"></a>5. code</h1><p>看细节还是需要看代码的实现过程。</p><h2 id="5-1-文件目录"><a href="#5-1-文件目录" class="headerlink" title="5.1 文件目录"></a>5.1 文件目录</h2><p><img src="./InstaGAN/InstaGAN10.png" alt="文件目录"><br><img src="/2019/01/04/InstaGAN/InstaGAN10.png" title="文件目录"></p><p>通过文件组织，可以发现cycleGAN尽可能地考虑了可扩展性。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">|-- LICENSE</span><br><span class="line">|-- README.md</span><br><span class="line">|-- data</span><br><span class="line">|   |-- __init__.py</span><br><span class="line">|   |-- aligned_dataset.py</span><br><span class="line">|   |-- base_data_loader.py</span><br><span class="line">|   |-- base_dataset.py</span><br><span class="line">|   |-- image_folder.py</span><br><span class="line">|   |-- single_dataset.py</span><br><span class="line">|   |-- unaligned_dataset.py</span><br><span class="line">|   `-- unaligned_seg_dataset.py</span><br><span class="line">|-- datasets</span><br><span class="line">|   |-- combine_A_and_B.py</span><br><span class="line">|   |-- download_coco.sh</span><br><span class="line">|   |-- download_cyclegan_dataset.sh</span><br><span class="line">|   |-- download_pix2pix_dataset.sh</span><br><span class="line">|   |-- generate_ccp_dataset.py</span><br><span class="line">|   |-- generate_coco_dataset.py</span><br><span class="line">|   |-- generate_mhp_dataset.py</span><br><span class="line">|   |-- make_dataset_aligned.py</span><br><span class="line">|   |-- pants2skirt_mhp</span><br><span class="line">|-- docs</span><br><span class="line">|   `-- more_results.md</span><br><span class="line">|-- environment.yml</span><br><span class="line">|-- models</span><br><span class="line">|   |-- __init__.py</span><br><span class="line">|   |-- base_model.py</span><br><span class="line">|   |-- cycle_gan_model.py</span><br><span class="line">|   |-- insta_gan_model.py</span><br><span class="line">|   |-- networks.py</span><br><span class="line">|   |-- pix2pix_model.py</span><br><span class="line">|   `-- test_model.py</span><br><span class="line">|-- options</span><br><span class="line">|   |-- __init__.py</span><br><span class="line">|   |-- base_options.py</span><br><span class="line">|   |-- test_options.py</span><br><span class="line">|   `-- train_options.py</span><br><span class="line">|-- requirements.txt</span><br><span class="line">|-- scripts</span><br><span class="line">|   |-- conda_deps.sh</span><br><span class="line">|   |-- download_cyclegan_model.sh</span><br><span class="line">|   |-- download_pix2pix_model.sh</span><br><span class="line">|   |-- install_deps.sh</span><br><span class="line">|   |-- test_before_push.py</span><br><span class="line">|   |-- test_cyclegan.sh</span><br><span class="line">|   |-- test_pix2pix.sh</span><br><span class="line">|   |-- test_single.sh</span><br><span class="line">|   |-- train_cyclegan.sh</span><br><span class="line">|   `-- train_pix2pix.sh</span><br><span class="line">|-- test.py</span><br><span class="line">|-- train.py</span><br><span class="line">`-- util</span><br><span class="line">    |-- __init__.py</span><br><span class="line">    |-- get_data.py</span><br><span class="line">    |-- html.py</span><br><span class="line">    |-- image_pool.py</span><br><span class="line">    |-- util.py</span><br><span class="line">    `-- visualizer.py</span><br></pre></td></tr></table></figure><h2 id="5-2-seg"><a href="#5-2-seg" class="headerlink" title="5.2 seg"></a>5.2 seg</h2><p>从下面的代码可以看出，需要读取固定数量的instance的segmentation。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># self.max_instances = 20</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_segs</span><span class="params">(self, seg_path, seed)</span>:</span></span><br><span class="line">  segs = list()</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(self.max_instances):</span><br><span class="line">    path = seg_path.replace(<span class="string">'.png'</span>, <span class="string">'_&#123;&#125;.png'</span>.format(i))</span><br><span class="line">    <span class="keyword">if</span> os.path.isfile(path):</span><br><span class="line">      seg = Image.open(path).convert(<span class="string">'L'</span>)</span><br><span class="line">      seg = self.fixed_transform(seg, seed)</span><br><span class="line">      segs.append(seg)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      segs.append(-torch.ones(segs[<span class="number">0</span>].size()))</span><br><span class="line">  <span class="keyword">return</span> torch.cat(segs)</span><br></pre></td></tr></table></figure><p><strong>备注</strong>: 原始图片transforms之后,0~1变成了-1~1; 分割图片transforms之后-1表示背景,取值-1~1,这也是为什么补充的时候用-1补充的原因.</p><h2 id="5-3-generator"><a href="#5-3-generator" class="headerlink" title="5.3 generator"></a>5.3 generator</h2><blockquote><p>ResNet generator is composed of downsampling blocks, residual blocks, and upsampling blocks. We used downsampling blocks and residual blocks for encoders, and used upsampling blocks for generators.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResnetSetGenerator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_nc=<span class="number">3</span>, output_nc=<span class="number">3</span>, ngf=<span class="number">64</span>, norm_layer=nn.InstanceNorm2d, use_dropout=False, n_blocks=<span class="number">9</span>, padding_type=<span class="string">'reflect'</span>)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> (n_blocks &gt;= <span class="number">0</span>)</span><br><span class="line">        super(ResnetSetGenerator, self).__init__()</span><br><span class="line">        self.input_nc = input_nc</span><br><span class="line">        self.output_nc = output_nc</span><br><span class="line">        self.ngf = ngf</span><br><span class="line">        <span class="keyword">if</span> type(norm_layer) == functools.partial:</span><br><span class="line">            use_bias = norm_layer.func == nn.InstanceNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_bias = norm_layer == nn.InstanceNorm2d</span><br><span class="line"></span><br><span class="line">        n_downsampling = <span class="number">2</span></span><br><span class="line">        self.encoder_img = self.get_encoder(input_nc, n_downsampling, ngf, norm_layer, use_dropout, n_blocks, padding_type, use_bias)</span><br><span class="line">        self.encoder_seg = self.get_encoder(<span class="number">1</span>, n_downsampling, ngf, norm_layer, use_dropout, n_blocks, padding_type, use_bias)</span><br><span class="line">        self.decoder_img = self.get_decoder(output_nc, n_downsampling, <span class="number">2</span> * ngf, norm_layer, use_bias)  <span class="comment"># 2*ngf</span></span><br><span class="line">        self.decoder_seg = self.get_decoder(<span class="number">1</span>, n_downsampling, <span class="number">3</span> * ngf, norm_layer, use_bias)  <span class="comment"># 3*ngf</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_encoder</span><span class="params">(self, input_nc, n_downsampling, ngf, norm_layer, use_dropout, n_blocks, padding_type, use_bias)</span>:</span></span><br><span class="line">        model = [nn.ReflectionPad2d(<span class="number">3</span>),</span><br><span class="line">                 nn.Conv2d(input_nc, ngf, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>, bias=use_bias),</span><br><span class="line">                 norm_layer(ngf),</span><br><span class="line">                 nn.ReLU(<span class="keyword">True</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_downsampling):</span><br><span class="line">            mult = <span class="number">2</span> ** i</span><br><span class="line">            model += [nn.Conv2d(ngf * mult, ngf * mult * <span class="number">2</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=use_bias),</span><br><span class="line">                      norm_layer(ngf * mult * <span class="number">2</span>),</span><br><span class="line">                      nn.ReLU(<span class="keyword">True</span>)]</span><br><span class="line"></span><br><span class="line">        mult = <span class="number">2</span> ** n_downsampling</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_blocks):</span><br><span class="line">            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_decoder</span><span class="params">(self, output_nc, n_downsampling, ngf, norm_layer, use_bias)</span>:</span></span><br><span class="line">        model = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_downsampling):</span><br><span class="line">            mult = <span class="number">2</span>**(n_downsampling - i)</span><br><span class="line">            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / <span class="number">2</span>), kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>, bias=use_bias),</span><br><span class="line">                      norm_layer(int(ngf * mult / <span class="number">2</span>)),</span><br><span class="line">                      nn.ReLU(<span class="keyword">True</span>)]</span><br><span class="line">        model += [nn.ReflectionPad2d(<span class="number">3</span>)]</span><br><span class="line">        model += [nn.Conv2d(ngf, output_nc, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>)]</span><br><span class="line">        model += [nn.Tanh()]</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inp)</span>:</span></span><br><span class="line">        <span class="comment"># split data</span></span><br><span class="line">        img = inp[:, :self.input_nc, :, :]  <span class="comment"># (B, CX, W, H)</span></span><br><span class="line">        segs = inp[:, self.input_nc:, :, :]  <span class="comment"># (B, CA, W, H)</span></span><br><span class="line">        mean = (segs + <span class="number">1</span>).mean(<span class="number">0</span>).mean(<span class="number">-1</span>).mean(<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">if</span> mean.sum() == <span class="number">0</span>:</span><br><span class="line">            mean[<span class="number">0</span>] = <span class="number">1</span>  <span class="comment"># forward at least one segmentation</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># run encoder</span></span><br><span class="line">        enc_img = self.encoder_img(img)</span><br><span class="line">        enc_segs = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(segs.size(<span class="number">1</span>)): <span class="comment"># 第i个instance</span></span><br><span class="line">            <span class="keyword">if</span> mean[i] &gt; <span class="number">0</span>:  <span class="comment"># skip empty segmentation</span></span><br><span class="line">                seg = segs[:, i, :, :].unsqueeze(<span class="number">1</span>)</span><br><span class="line">                enc_segs.append(self.encoder_seg(seg))</span><br><span class="line">        enc_segs = torch.cat(enc_segs)</span><br><span class="line">        enc_segs_sum = torch.sum(enc_segs, dim=<span class="number">0</span>, keepdim=<span class="keyword">True</span>)  <span class="comment"># aggregated set feature</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># run decoder</span></span><br><span class="line">        feat = torch.cat([enc_img, enc_segs_sum], dim=<span class="number">1</span>)</span><br><span class="line">        out = [self.decoder_img(feat)]</span><br><span class="line">        idx = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(segs.size(<span class="number">1</span>)):</span><br><span class="line">            <span class="keyword">if</span> mean[i] &gt; <span class="number">0</span>:</span><br><span class="line">                enc_seg = enc_segs[idx].unsqueeze(<span class="number">0</span>)  <span class="comment"># (1, ngf, w, h)</span></span><br><span class="line">                idx += <span class="number">1</span>  <span class="comment"># move to next index</span></span><br><span class="line">                feat = torch.cat([enc_seg, enc_img, enc_segs_sum], dim=<span class="number">1</span>)</span><br><span class="line">                out += [self.decoder_seg(feat)]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out += [segs[:, i, :, :].unsqueeze(<span class="number">1</span>)]  <span class="comment"># skip empty segmentation</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(out, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="5-4-Discriminator"><a href="#5-4-Discriminator" class="headerlink" title="5.4 Discriminator"></a>5.4 Discriminator</h2><blockquote><p>On the other hand, PatchGAN discriminator is composed of 5 convolutional layers, including normalization and non-linearity layers. We used the first 3 convolution layers for feature extractors, and the last 2 convolution layers for classifier.</p><p>In addition, we observed that applying Spectral Normalization (SN) (Miyato et al., 2018) for discriminators significantly improve the performance, although we used LSGAN (Mao et al., 2017), while the original motivation of SN was to enforce Lipschitz condition to match with the theory of WGAN (Arjovsky et al., 2017; Gulrajani et al., 2017).</p></blockquote><ul><li>[x] SpectralNorm 这个是怎么运行的？ <a href="http://www.twistedwg.com/2018/10/13/SNGAN.html" target="_blank" rel="noopener">http://www.twistedwg.com/2018/10/13/SNGAN.html</a></li></ul><p>虽然还是没有太搞懂其原理，但大致清楚了，是求矩阵的谱范数，因为难以求解，便用迭代的方式计算u、v。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define spectral normalization layer</span></span><br><span class="line"><span class="comment"># Code from Christian Cosgrove's repository</span></span><br><span class="line"><span class="comment"># https://github.com/christiancosgrove/pytorch-spectral-normalization-gan/blob/master/spectral_normalization.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2normalize</span><span class="params">(v, eps=<span class="number">1e-12</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> v / (v.norm() + eps)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpectralNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, module, name=<span class="string">'weight'</span>, power_iterations=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(SpectralNorm, self).__init__()</span><br><span class="line">        self.module = module</span><br><span class="line">        self.name = name</span><br><span class="line">        self.power_iterations = power_iterations</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._made_params():</span><br><span class="line">            self._make_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_u_v</span><span class="params">(self)</span>:</span></span><br><span class="line">        u = getattr(self.module, self.name + <span class="string">"_u"</span>)</span><br><span class="line">        v = getattr(self.module, self.name + <span class="string">"_v"</span>)</span><br><span class="line">        w = getattr(self.module, self.name + <span class="string">"_bar"</span>)</span><br><span class="line"></span><br><span class="line">        height = w.data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.power_iterations):</span><br><span class="line">            v.data = l2normalize(torch.mv(torch.t(w.view(height, <span class="number">-1</span>).data), u.data))</span><br><span class="line">            u.data = l2normalize(torch.mv(w.view(height, <span class="number">-1</span>).data, v.data))</span><br><span class="line"></span><br><span class="line">        sigma = u.dot(w.view(height, <span class="number">-1</span>).mv(v))</span><br><span class="line">        setattr(self.module, self.name, w / sigma.expand_as(w))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_made_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            u = getattr(self.module, self.name + <span class="string">"_u"</span>)</span><br><span class="line">            v = getattr(self.module, self.name + <span class="string">"_v"</span>)</span><br><span class="line">            w = getattr(self.module, self.name + <span class="string">"_bar"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">except</span> AttributeError:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        w = getattr(self.module, self.name) <span class="comment"># shape: (64,3,4,4)</span></span><br><span class="line"></span><br><span class="line">        height = w.data.shape[<span class="number">0</span>]  <span class="comment"># int 64</span></span><br><span class="line">        width = w.view(height, <span class="number">-1</span>).data.shape[<span class="number">1</span>] <span class="comment"># int 48</span></span><br><span class="line"></span><br><span class="line">        u = nn.Parameter(w.data.new(height).normal_(<span class="number">0</span>, <span class="number">1</span>), requires_grad=<span class="keyword">False</span>) <span class="comment"># shape (64)</span></span><br><span class="line">        v = nn.Parameter(w.data.new(width).normal_(<span class="number">0</span>, <span class="number">1</span>), requires_grad=<span class="keyword">False</span>) <span class="comment"># shape (48)</span></span><br><span class="line">        u.data = l2normalize(u.data) </span><br><span class="line">        v.data = l2normalize(v.data)</span><br><span class="line">        w_bar = nn.Parameter(w.data)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">del</span> self.module._parameters[self.name]</span><br><span class="line"></span><br><span class="line">        self.module.register_parameter(self.name + <span class="string">"_u"</span>, u)</span><br><span class="line">        self.module.register_parameter(self.name + <span class="string">"_v"</span>, v)</span><br><span class="line">        self.module.register_parameter(self.name + <span class="string">"_bar"</span>, w_bar)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        self._update_u_v()</span><br><span class="line">        <span class="keyword">return</span> self.module.forward(*args)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NLayerSetDiscriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_nc, ndf=<span class="number">64</span>, n_layers=<span class="number">3</span>, norm_layer=nn.BatchNorm2d, use_sigmoid=False)</span>:</span></span><br><span class="line">        super(NLayerSetDiscriminator, self).__init__()</span><br><span class="line">        self.input_nc = input_nc</span><br><span class="line">        <span class="keyword">if</span> type(norm_layer) == functools.partial:</span><br><span class="line">            use_bias = norm_layer.func == nn.InstanceNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_bias = norm_layer == nn.InstanceNorm2d</span><br><span class="line"></span><br><span class="line">        kw = <span class="number">4</span></span><br><span class="line">        padw = <span class="number">1</span></span><br><span class="line">        self.feature_img = self.get_feature_extractor(input_nc, ndf, n_layers, kw, padw, norm_layer, use_bias)</span><br><span class="line">        self.feature_seg = self.get_feature_extractor(<span class="number">1</span>, ndf, n_layers, kw, padw, norm_layer, use_bias)</span><br><span class="line">        self.classifier = self.get_classifier(<span class="number">2</span> * ndf, n_layers, kw, padw, norm_layer, use_sigmoid)  <span class="comment"># 2*ndf</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_feature_extractor</span><span class="params">(self, input_nc, ndf, n_layers, kw, padw, norm_layer, use_bias)</span>:</span></span><br><span class="line">        model = [</span><br><span class="line">            <span class="comment"># Use spectral normalization</span></span><br><span class="line">            SpectralNorm(nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=<span class="number">2</span>, padding=padw)),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">        ]</span><br><span class="line">        nf_mult = <span class="number">1</span></span><br><span class="line">        nf_mult_prev = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">            nf_mult_prev = nf_mult</span><br><span class="line">            nf_mult = min(<span class="number">2</span> ** n, <span class="number">8</span>)</span><br><span class="line">            model += [</span><br><span class="line">                <span class="comment"># Use spectral normalization</span></span><br><span class="line">                SpectralNorm(nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=<span class="number">2</span>, padding=padw, bias=use_bias)),</span><br><span class="line">                norm_layer(ndf * nf_mult),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">            ]</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_classifier</span><span class="params">(self, ndf, n_layers, kw, padw, norm_layer, use_sigmoid)</span>:</span></span><br><span class="line">        nf_mult_prev = min(<span class="number">2</span> ** (n_layers<span class="number">-1</span>), <span class="number">8</span>)</span><br><span class="line">        nf_mult = min(<span class="number">2</span> ** n_layers, <span class="number">8</span>)</span><br><span class="line">        model = [</span><br><span class="line">            <span class="comment"># Use spectral normalization</span></span><br><span class="line">            SpectralNorm(nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=<span class="number">1</span>, padding=padw)),</span><br><span class="line">            norm_layer(ndf * nf_mult),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">        ]</span><br><span class="line">        <span class="comment"># Use spectral normalization</span></span><br><span class="line">        model += [SpectralNorm(nn.Conv2d(ndf * nf_mult, <span class="number">1</span>, kernel_size=kw, stride=<span class="number">1</span>, padding=padw))]</span><br><span class="line">        <span class="keyword">if</span> use_sigmoid:</span><br><span class="line">            model += [nn.Sigmoid()]</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inp)</span>:</span></span><br><span class="line">        <span class="comment"># split data</span></span><br><span class="line">        img = inp[:, :self.input_nc, :, :]  <span class="comment"># (B, CX, W, H)</span></span><br><span class="line">        segs = inp[:, self.input_nc:, :, :]  <span class="comment"># (B, CA, W, H)</span></span><br><span class="line">        mean = (segs + <span class="number">1</span>).mean(<span class="number">0</span>).mean(<span class="number">-1</span>).mean(<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">if</span> mean.sum() == <span class="number">0</span>:</span><br><span class="line">            mean[<span class="number">0</span>] = <span class="number">1</span>  <span class="comment"># forward at least one segmentation</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># run feature extractor</span></span><br><span class="line">        feat_img = self.feature_img(img)</span><br><span class="line">        feat_segs = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(segs.size(<span class="number">1</span>)):  <span class="comment"># 第i个instance</span></span><br><span class="line">            <span class="keyword">if</span> mean[i] &gt; <span class="number">0</span>:  <span class="comment"># skip empty segmentation</span></span><br><span class="line">                seg = segs[:, i, :, :].unsqueeze(<span class="number">1</span>)</span><br><span class="line">                feat_segs.append(self.feature_seg(seg))</span><br><span class="line">        feat_segs_sum = torch.sum(torch.stack(feat_segs), dim=<span class="number">0</span>)  <span class="comment"># aggregated set feature</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># run classifier</span></span><br><span class="line">        feat = torch.cat([feat_img, feat_segs_sum], dim=<span class="number">1</span>)</span><br><span class="line">        out = self.classifier(feat)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="5-5-model-的输入"><a href="#5-5-model-的输入" class="headerlink" title="5.5 model 的输入"></a>5.5 model 的输入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_input</span><span class="params">(self, input)</span>:</span></span><br><span class="line">    AtoB = self.opt.direction == <span class="string">'AtoB'</span></span><br><span class="line">    self.real_A_img = input[<span class="string">'A'</span> <span class="keyword">if</span> AtoB <span class="keyword">else</span> <span class="string">'B'</span>].to(self.device)</span><br><span class="line">    self.real_B_img = input[<span class="string">'B'</span> <span class="keyword">if</span> AtoB <span class="keyword">else</span> <span class="string">'A'</span>].to(self.device)</span><br><span class="line">    real_A_segs = input[<span class="string">'A_segs'</span> <span class="keyword">if</span> AtoB <span class="keyword">else</span> <span class="string">'B_segs'</span>]</span><br><span class="line">    real_B_segs = input[<span class="string">'B_segs'</span> <span class="keyword">if</span> AtoB <span class="keyword">else</span> <span class="string">'A_segs'</span>]</span><br><span class="line">    self.real_A_segs = self.select_masks(real_A_segs).to(self.device)  <span class="comment"># shape:(1,4,240,160)</span></span><br><span class="line">    self.real_B_segs = self.select_masks(real_B_segs).to(self.device)</span><br><span class="line">    self.real_A = torch.cat([self.real_A_img, self.real_A_segs], dim=<span class="number">1</span>) <span class="comment"># shape:(1,7,240,160)</span></span><br><span class="line">    self.real_B = torch.cat([self.real_B_img, self.real_B_segs], dim=<span class="number">1</span>)</span><br><span class="line">    self.real_A_seg = self.merge_masks(self.real_A_segs)  <span class="comment"># merged mask</span></span><br><span class="line">    self.real_B_seg = self.merge_masks(self.real_B_segs)  <span class="comment"># merged mask</span></span><br><span class="line">    self.image_paths = input[<span class="string">'A_paths'</span> <span class="keyword">if</span> AtoB <span class="keyword">else</span> <span class="string">'B_paths'</span>]</span><br></pre></td></tr></table></figure><p>前面说过，每次都生成20个mask，不足用-1补充，在输入网络时，只取面积最大的4个mask，然后对这4个进行或者从高到低排序或者随机排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ins_max = 4</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_masks_random</span><span class="params">(self, segs_batch)</span>:</span></span><br><span class="line">  <span class="string">"""Select masks in random order"""</span></span><br><span class="line">  ret = list()</span><br><span class="line">  <span class="keyword">for</span> segs <span class="keyword">in</span> segs_batch:</span><br><span class="line">    mean = (segs + <span class="number">1</span>).mean(<span class="number">-1</span>).mean(<span class="number">-1</span>)</span><br><span class="line">    m, i = mean.topk(self.opt.ins_max)</span><br><span class="line">    num = min(len(mean.nonzero()), self.opt.ins_max)</span><br><span class="line">    reorder = np.concatenate((np.random.permutation(num), np.arange(num, self.opt.ins_max)))</span><br><span class="line">    ret.append(segs[i[reorder], :, :])</span><br><span class="line">  <span class="keyword">return</span> torch.stack(ret)</span><br></pre></td></tr></table></figure><p>这里的mask的合并没有太看懂，是为了去除(-1,1)之外的数字吗？</p><p>跑了代码,觉得是的,或许是担心有其他干扰因素吧,反正剩下的都是-1~1之间的数字.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_masks</span><span class="params">(self, segs)</span>:</span></span><br><span class="line">  <span class="string">"""Merge masks (B, N, W, H) -&gt; (B, 1, W, H)"""</span></span><br><span class="line">  ret = torch.sum((segs + <span class="number">1</span>)/<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)  <span class="comment"># (B, 1, W, H)</span></span><br><span class="line">  <span class="keyword">return</span> ret.clamp(max=<span class="number">1</span>, min=<span class="number">0</span>) * <span class="number">2</span> - <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li>[ ] 这一步的意义是什么？？</li></ul><p>理解了，如果图片中没有instance，那么就不用进行下一步的转换了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.forward_A = (self.real_A_seg_sng + <span class="number">1</span>).sum() &gt; <span class="number">0</span>  <span class="comment"># check if there are remaining instances</span></span><br><span class="line">self.forward_B = (self.real_B_seg_sng + <span class="number">1</span>).sum() &gt; <span class="number">0</span>  <span class="comment"># check if there are remaining instances</span></span><br></pre></td></tr></table></figure><ul><li>[x] fake_B_mul的意义是什么？</li></ul><p>因为在sequential mini-batch translation中，GAN_loss是全局的，所以每次需要把之前的fake_B_seg_sng保存起来一起计算，因此每次的临时的self.fake_B_mul，而self.fake_B_seg_list保存是mini-batch计算得到的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.forward_A:</span><br><span class="line">    self.real_A_sng = torch.cat([self.real_A_img_sng, self.real_A_seg_sng], dim=<span class="number">1</span>)</span><br><span class="line">    self.fake_B_sng = self.netG_A(self.real_A_sng)</span><br><span class="line">    self.rec_A_sng = self.netG_B(self.fake_B_sng)</span><br><span class="line"></span><br><span class="line">    self.fake_B_img_sng, self.fake_B_seg_sng = self.split(self.fake_B_sng)</span><br><span class="line">    self.rec_A_img_sng, self.rec_A_seg_sng = self.split(self.rec_A_sng)</span><br><span class="line">    fake_B_seg_list = self.fake_B_seg_list + [self.fake_B_seg_sng]  <span class="comment"># not detach</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.ins_iter - idx - <span class="number">1</span>):</span><br><span class="line">        fake_B_seg_list.append(empty)</span><br><span class="line"></span><br><span class="line">    self.fake_B_seg_mul = torch.cat(fake_B_seg_list, dim=<span class="number">1</span>)</span><br><span class="line">    self.fake_B_mul = torch.cat([self.fake_B_img_sng, self.fake_B_seg_mul], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>[x] 怎么选取的背景</li></ul><p>只要在A中且在B中都是背景的则都算是背景，否则只要有instance的区域不为背景。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_masks</span><span class="params">(self, segs)</span>:</span></span><br><span class="line">    <span class="string">"""Merge masks (B, N, W, H) -&gt; (B, 1, W, H)"""</span></span><br><span class="line">    <span class="comment"># segs: shape(1,4,240,260)， 取值(-1~1) 训练集A中有两个instance，训练集B中有两个instance，</span></span><br><span class="line">    ret = torch.sum((segs + <span class="number">1</span>)/<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)  <span class="comment"># (B, 1, W, H)</span></span><br><span class="line">    <span class="keyword">return</span> ret.clamp(max=<span class="number">1</span>, min=<span class="number">0</span>) * <span class="number">2</span> - <span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_masks</span><span class="params">(self, segs)</span>:</span></span><br><span class="line">    <span class="string">"""Merge masks (B, N, W, H) -&gt; (B, 1, W, H)"""</span></span><br><span class="line">    ret = torch.sum((segs + <span class="number">1</span>)/<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)  <span class="comment"># (B, 1, W, H)</span></span><br><span class="line">    <span class="keyword">return</span> ret.clamp(max=<span class="number">1</span>, min=<span class="number">0</span>) * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight_for_ctx</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    <span class="string">"""Get weight for context preserving loss"""</span></span><br><span class="line">    z = self.merge_masks(torch.cat([x, y], dim=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> - z) / <span class="number">2</span>  <span class="comment"># [-1,1] -&gt; [1,0]</span></span><br></pre></td></tr></table></figure><ul><li>[ ] 这里的empty的作用是什么</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">empty = -torch.ones(self.real_A_seg_sng.size()).to(self.device)</span><br></pre></td></tr></table></figure><ul><li><p>[ ] pix2pix 是怎么预测mask的，需要提前训练吗，数据集怎么提供？如果可以直接用，那么是否可以直接实现行人重识别的换人？</p></li><li><p>[x] 论文+代码，共4天</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;实例转换&lt;/p&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>deep-learning-network</title>
    <link href="http://yoursite.com/2019/01/03/deep-learning-network/"/>
    <id>http://yoursite.com/2019/01/03/deep-learning-network/</id>
    <published>2019-01-03T02:43:38.000Z</published>
    <updated>2019-01-04T01:57:14.995Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>最近看了一些代码，发现大家的代码风格互相不一样，且没有一个统一的风格，所以抽象一个大致流程，用于更快地熟悉代码。</p><a id="more"></a><h1 id="1-神经网络"><a href="#1-神经网络" class="headerlink" title="1. 神经网络"></a>1. 神经网络</h1><p>从pytorch的代码中看，可以发现发现这么几个阶段。</p><h2 id="1-1-数据dataset"><a href="#1-1-数据dataset" class="headerlink" title="1.1 数据dataset"></a>1.1 数据dataset</h2><p>这一阶段根据具体需求可以分为离线预处理、在线预处理，主要是对数据预处理，包括不限于图片的处理，名字的处理等等。</p><p>代表代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">xxx</span><span class="params">()</span>:</span></span><br><span class="line">    __getitem__(index):</span><br><span class="line"></span><br><span class="line">data = data.Dataloader()</span><br></pre></td></tr></table></figure><h2 id="1-2-神经网络"><a href="#1-2-神经网络" class="headerlink" title="1.2 神经网络"></a>1.2 神经网络</h2><p>主要分为：</p><ul><li>搭建网络</li><li>输入</li><li>损失函数</li><li>反向传播</li></ul><p>其中关键的是搭建网络和损失函数。</p><p>有些代码会对神经网络的输入再次处理，有些代码会将输入、损失函数、反向传播重新写个方法或者类，类似trainer。</p><h2 id="1-3-可视化"><a href="#1-3-可视化" class="headerlink" title="1.3 可视化"></a>1.3 可视化</h2><p>可视化方法包括不限于visdom、tensorboard、html、输出重定向。</p><p>loss的获取，用一个dict的loss记录。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;最近看了一些代码，发现大家的代码风格互相不一样，且没有一个统一的风格，所以抽象一个大致流程，用于更快地熟悉代码。&lt;/p&gt;
    
    </summary>
    
      <category term="deep-learning" scheme="http://yoursite.com/categories/deep-learning/"/>
    
    
      <category term="deep-learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="network" scheme="http://yoursite.com/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>SEIGAN</title>
    <link href="http://yoursite.com/2018/12/27/SEIGAN/"/>
    <id>http://yoursite.com/2018/12/27/SEIGAN/</id>
    <published>2018-12-27T08:57:16.000Z</published>
    <updated>2018-12-28T09:11:52.292Z</updated>
    
    <content type="html"><![CDATA[<p>论文分享</p><a id="more"></a><h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p><a href="https://arxiv.org/abs/1811.07630" target="_blank" rel="noopener">SEIGAN: Towards Compositional Image Generation by Simultaneously Learning to Segment, Enhance, and Inpaint.</a></p><p>Pavel Ostyakov, Roman Suvorov, Elizaveta Logacheva1 Oleg Khomenko, Sergey I. Nikolenko</p><p>一作Pavel Ostyakov是莫斯科三星AI中心的人员，在 Kaggle Cdiscount’s Image Classification Challenge 比赛中获得第一名，<a href="https://www.youtube.com/watch?v=Mw2vdYv4ups" target="_blank" rel="noopener">YouTube 视频</a>，github:<a href="https://github.com/PavelOstyakov?tab=repositories" target="_blank" rel="noopener">https://github.com/PavelOstyakov?tab=repositories</a>，他参加的 Kaggle 比赛多一点。</p><p>论文代码还没有公布。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p><img src="./SEIGAN/SEIGAN1.png" alt="实验效果图"><br><img src="/2018/12/27/SEIGAN/SEIGAN1.png" title="实验效果图"></p><p>要解决的问题是图像合成，一共三件事：<strong>剪切、贴图、修复</strong>。即如何把一张图片的物体剪切下来，然后贴到另一张背景图上，并且补全被剪掉的区域。</p><p>并且实验证明，结果不仅合成的新图片逼真，而且分割也做得很好。</p><blockquote><p>This process consists of three basic operations: (1) cut, extracting an object from image; (2) paste and enhance, making the pasted object appear natural in the new context; (3) inpaint, restoring the background after cutting out an object</p></blockquote><p><strong>关键词</strong>：语义分割、贴图、去目标、补全、修复、GAN、image generation、图像合成</p><p>备注：训练过程不需要成对图片。</p><p>相似工作：<a href="https://arxiv.org/abs/1803.06414" target="_blank" rel="noopener">Learning to segment via cut-and-paste</a></p><p>通过作者的论述，思想相同的地方很多。</p><h1 id="2-Methods"><a href="#2-Methods" class="headerlink" title="2. Methods"></a>2. Methods</h1><h2 id="2-1-Problem-Setting-and-General-Pipeline"><a href="#2-1-Problem-Setting-and-General-Pipeline" class="headerlink" title="2.1 Problem Setting and General Pipeline"></a>2.1 Problem Setting and General Pipeline</h2><p><strong>字符表示</strong>：</p><div class="table-container"><table><thead><tr><th style="text-align:center">字符</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">$x=<o,b_x>$</o,b_x></td><td style="text-align:center">图片$x$，由目标$O$和背景$B_x$组成</td></tr><tr><td style="text-align:center">$y=&lt;\emptyset,B_y&gt;$</td><td style="text-align:center">图片$y$，包含背景$B_y$，没有目标</td></tr><tr><td style="text-align:center">$Y={&lt;\emptyset,B_y&gt;}_{y\in Y}$</td><td style="text-align:center">背景数据集</td></tr><tr><td style="text-align:center">$X={<o_x,b_x>}_{x\in X}$</o_x,b_x></td><td style="text-align:center">不同物体在不同背景的图片</td></tr><tr><td style="text-align:center">$m$</td><td style="text-align:center">目标$O$在图片$x$上的mask $m$</td></tr><tr><td style="text-align:center">$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$</td><td style="text-align:center">去除目标只剩背景$B_x$并修复后的图片</td></tr><tr><td style="text-align:center">$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$</td><td style="text-align:center">将目标粘贴到背景$B_y$并增强后的图片</td></tr></tbody></table></div><p><strong>问题定义</strong>：</p><p>对于任意一组图片$x=<o,b_x>$和$y=&lt;\emptyset,B_y&gt;$，利用模型将其变成$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$和$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$，其中$\hat{B}_x\approx B_x$，$\hat{B}_y\approx B_y$和$\hat{O}\approx O$.</o,b_x></p><p><strong>步骤分解</strong>：</p><ul><li>分割(segmentation)：预测图片$x=<o,b_x>$的分割掩码$m=Mask(x)$，并且做简单的粘贴$z=m\odot x+(1-m)\odot y$，$\odot$我的理解是对应元素的乘法。</o,b_x></li></ul><blockquote><p>$\odot$ denotes componentwise multiplication;</p></blockquote><ul><li><p>增强(enhancement)：对于$z$，做进一步图片增强，使其更自然，得到$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$</p></li><li><p>修复(inpainting)：对于去除目标后的图片$(1-m)\odot x$，修复成$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$</p></li></ul><p><img src="./SEIGAN/SEIGAN2.png" alt="网络架构"><br><img src="/2018/12/27/SEIGAN/SEIGAN2.png" title="网络架构"></p><p>其中 swap network 就是 segmentation network 和 enhancement network 的组合。</p><ul><li>类似CycleGAN，网络架构应用两次，第一次得到$\hat{x}$和$\hat{y}$，第二次得到$\hat{\hat{x}}$和$\hat{\hat{y}}$</li><li>$G_{seg}$：Fig.3.right，输入$x=<o,b_x>$，输出$m=Mask(x)$</o,b_x></li><li>$G_{inp}$：Fig.3.left，输入$(1-m)\odot x$，输出$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$</li><li>$G_{enh}$：Fig.3.left，输入$z=m\odot x+(1-m)\odot y$和噪声，输出$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$</li><li>$D_{bg}$：背景判别器，判断真背景($y$)和假背景($\hat{x}$、$\hat{\hat{y}}$)，真为1，假为0</li><li>$D_{obj}$：目标判别器，判断背景和目标是一体的($x$)和背景和目标是合成的($z$,$\hat{y}$)，一体的为1，合成的为0.</li></ul><h2 id="2-2-The-inpainting-Network-G-inp"><a href="#2-2-The-inpainting-Network-G-inp" class="headerlink" title="2.2 The inpainting Network $G_{inp}$"></a>2.2 The inpainting Network $G_{inp}$</h2><p><strong>背景对抗损失函数</strong>:</p><script type="math/tex; mode=display">l^{GAN}\_{inp}=(1-D_{bg}(\hat{x})),l^{GAN}\_{inp2}=(1-D_{bg}(\hat{\hat{y}}))</script><script type="math/tex; mode=display">l^{GAN}_{bg}=\lambda_1 l^{GAN}\_{inp} + \lambda_2 l^{GAN}\_{inp2}</script><p><strong>背景重构损失函数</strong>：</p><script type="math/tex; mode=display">l^{texture}_{bg}=|Gram(VGG_1(y))-Gram(VGG_1(\hat{\hat{y}}))|</script><script type="math/tex; mode=display">l^{perc}_{bg}=|VGG_2(y)-VGG_2(\hat{\hat{y}})|</script><script type="math/tex; mode=display">l^{MAE}\_{bg}=|y-\hat{\hat{y}}|</script><script type="math/tex; mode=display">l^{rec}_{bg}=\lambda_3 l^{texture}_{bg}+\lambda_4 l^{perc}_{bg}+\lambda_5l^{MAE}_{bg}</script><p>其中，$VGG_1(y)$表示VGG19的前五层，$VGG_2(y)$表示VGG19的后五层</p><p><strong>$G_{inp}$ 的网络架构</strong>:</p><p><img src="./SEIGAN/SEIGAN3.png" alt="$G_{inp}$ 的网络架构"><br><img src="/2018/12/27/SEIGAN/SEIGAN3.png" title="$G_{inp}$ 的网络架构"></p><p>其实没太看懂这个网络，可能还是需要有代码。</p><h2 id="2-3-The-Swap-Network"><a href="#2-3-The-Swap-Network" class="headerlink" title="2.3 The Swap Network"></a>2.3 The Swap Network</h2><p>The swap network = the segmentation network + the enhancement network</p><p><strong>目标重构损失函数</strong>：</p><script type="math/tex; mode=display">l^{perc}_{obj}=|VGG_2(x)-VGG_2(\hat{\hat{x}})|</script><script type="math/tex; mode=display">l^{MAE}\_{obj}=|x-\hat{\hat{x}}|</script><script type="math/tex; mode=display">l^{rec}_{obj}=\lambda_6 l^{perc}_{obj}+\lambda_7 l^{MAE}_{obj}</script><p><strong>目标对抗损失函数</strong>：</p><script type="math/tex; mode=display">l^{GAN}\_{coarse}=(1-D_{obj}(z))^2</script><script type="math/tex; mode=display">l^{GAN}\_{enh}=(1-D_{obj}(\hat{y}))^2</script><script type="math/tex; mode=display">l^{GAN}_{obj}=\lambda_8 l^{GAN}_{coarse} + \lambda_9 l^{GAN}_{enh}</script><p><strong>identity loss</strong>：</p><script type="math/tex; mode=display">l^{id}\_{bg}=|G_{inp}((1-G_{seg}(y))\odot y)-y|</script><script type="math/tex; mode=display">l^{id}\_{obj}=|G_{enh}(x)-x|</script><script type="math/tex; mode=display">l^{id}=\lambda_{10}l^{id}\_{obj} + \lambda_{11}l^{id}_{bg}</script><h2 id="2-4-Total-Loss-Funcion-Remarks-and-Network-Architectures"><a href="#2-4-Total-Loss-Funcion-Remarks-and-Network-Architectures" class="headerlink" title="2.4 Total Loss Funcion, Remarks, and Network Architectures"></a>2.4 Total Loss Funcion, Remarks, and Network Architectures</h2><p><strong>The Generator Loss</strong>:</p><script type="math/tex; mode=display">\begin{split}l=\lambda_1 l^{GAN}\_{inp} + \lambda_2 l^{GAN}\_{inp2} + \lambda_4 l^{perc}_{bg} + \lambda_5l^{MAE}_{bg} + \lambda_6 l^{perc}_{obj} + \\\lambda_7 l^{MAE}_{obj} + \lambda_8 l^{GAN}_{coarse} + \lambda_9 l^{GAN}\_{enh} + \lambda_{10}l^{id}\_{obj} + \lambda_{11}l^{id}_{bg}\end{split}</script><p><strong>The Discriminator Loss</strong>:</p><script type="math/tex; mode=display">l^{disc}\_{bg}=(1-D_{bg}(y))^2+\frac{1}{2}D_{bg}(\hat{x})^2+\frac{1}{2}D_{bg}(\hat{\hat{y}})^2</script><script type="math/tex; mode=display">l^{disc}\_{obj}=(1-D_{obj}(x))^2+\frac{1}{2}D_{obj}(\hat{y})^2+\frac{1}{2}D_{obj}(z)^2</script><p><strong>Remarks</strong>：</p><ol><li>a pool of fake images，类似Cycle GAN也有。</li><li>对于不同大小和比例的图片$x,y$，作者使用了增强网络</li><li>texture loss $l^{rec}_{bg}$ 比 threshold 作用在 mask m 上的效果更好</li></ol><blockquote><p>In our setup this problem is addressed by a separate enhancement network, so we have fewer limitations when looking for appropriate training data.</p></blockquote><h1 id="3-Experimental-evaluation"><a href="#3-Experimental-evaluation" class="headerlink" title="3. Experimental evaluation"></a>3. Experimental evaluation</h1><p>实验性能主要分为两个：</p><ol><li>生成图片的主观真实性</li><li>生成分割掩码的准确性</li></ol><p><strong>生成图片的主观真实性</strong>：<br><img src="./SEIGAN/SEIGAN4.png" alt="生成图片的主观真实性"><br><img src="/2018/12/27/SEIGAN/SEIGAN4.png" title="生成图片的主观真实性"></p><p>从Fig.4.left可以简单地看出，Full的效果还是很明显的。</p><p><strong>生成分割掩码的准确性</strong>：<br><img src="./SEIGAN/SEIGAN5.png" alt="生成分割掩码的准确性"><br><img src="/2018/12/27/SEIGAN/SEIGAN5.png" title="生成分割掩码的准确性"></p><p><strong>实验效果</strong>：<br><img src="./SEIGAN/SEIGAN6.png" alt="实验效果"><br><img src="/2018/12/27/SEIGAN/SEIGAN6.png" title="实验效果"></p><h1 id="4-Future"><a href="#4-Future" class="headerlink" title="4. Future"></a>4. Future</h1><p>这个方法是否可以用在两个图片的目标对换呢？如果用在跨数据集的行人重识别上，那么是否可以将源数据集的行人粘贴在目标数据集上呢？好像有一个GAN就是类似的.</p><p><a href="https://arxiv.org/abs/1711.08565" target="_blank" rel="noopener">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文分享&lt;/p&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>GPU</title>
    <link href="http://yoursite.com/2018/12/26/GPU/"/>
    <id>http://yoursite.com/2018/12/26/GPU/</id>
    <published>2018-12-26T07:14:08.000Z</published>
    <updated>2019-03-02T03:49:26.703Z</updated>
    
    <content type="html"><![CDATA[<p>主要记录从安装显卡驱动到cudnn的过程。</p><a id="more"></a><h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这篇博客主要记录自己安装显卡驱动到cudnn的过程，只能当作工作记录，不可以作为参考手册。</p><p>整个过程主要分为：</p><ul><li>显卡驱动</li><li>cuda</li><li>cudnn</li><li>ananconda</li><li>pytorch</li><li>tensorflow</li><li>matlab</li><li>分辨率</li><li>cuda、cudnn卸载升级</li></ul><h1 id="1-显卡驱动"><a href="#1-显卡驱动" class="headerlink" title="1. 显卡驱动"></a>1. 显卡驱动</h1><p>显卡驱动下载:<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">https://www.nvidia.cn/Download/index.aspx?lang=cn</a></p><p>官方参考手册:</p><p><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4RD7GVh1d" target="_blank" rel="noopener">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4RD7GVh1d</a></p><h2 id="1-1-删除已经安装的nvidia驱动-对应官方参考手册2-7"><a href="#1-1-删除已经安装的nvidia驱动-对应官方参考手册2-7" class="headerlink" title="1.1 删除已经安装的nvidia驱动(对应官方参考手册2.7)"></a>1.1 删除已经安装的nvidia驱动(对应官方参考手册2.7)</h2><p>如果已经安装过nvidia驱动没有成功或者接下来的过程中发生驱动安装错误的情况，则删除已经安装的nvidia驱动(对应官方参考手册2.7)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get --purge remove nvidia-*</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">sudo apt-get autoremove --purge nvidia-*</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><h2 id="1-2-禁止自带的-nouveau-nvidia-驱动-官方参考手册4-3"><a href="#1-2-禁止自带的-nouveau-nvidia-驱动-官方参考手册4-3" class="headerlink" title="1.2 禁止自带的 nouveau nvidia 驱动(官方参考手册4.3)"></a>1.2 禁止自带的 nouveau nvidia 驱动(官方参考手册4.3)</h2><h3 id="1-2-1-判断-nouveau-nvidia-驱动是否被禁止"><a href="#1-2-1-判断-nouveau-nvidia-驱动是否被禁止" class="headerlink" title="1.2.1 判断 nouveau nvidia 驱动是否被禁止"></a>1.2.1 判断 nouveau nvidia 驱动是否被禁止</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure><p>有输出表示没有禁止，需要执行1.2.2；没有输出表示禁止成功，可以跳过1.2.2，执行1.3。</p><h3 id="1-2-2-禁止-nouveau-nvidia-驱动"><a href="#1-2-2-禁止-nouveau-nvidia-驱动" class="headerlink" title="1.2.2 禁止 nouveau nvidia 驱动"></a>1.2.2 禁止 nouveau nvidia 驱动</h3><p>创建文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/modprobe.d/blacklist-nouveau.conf</span><br></pre></td></tr></table></figure><p>在文件中写入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=<span class="number">0</span></span><br></pre></td></tr></table></figure><p>更新</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-initramfs -u</span><br></pre></td></tr></table></figure><p>判断</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure><p>有输出表示没有禁止，需要重新执行1.2.2；没有输出表示禁止成功，重启电脑。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><h2 id="1-3-安装-nvidia-驱动"><a href="#1-3-安装-nvidia-驱动" class="headerlink" title="1.3 安装 nvidia 驱动"></a>1.3 安装 nvidia 驱动</h2><p>根据机器型号下载相应的驱动，名称如：NVIDIA-Linux-x86_64-410.78.run。</p><p>重启电脑后或者在桌面环境后，切换到tty1文本模式:ctrl+alt+F1</p><h3 id="1-3-1-关闭-x-windows-服务"><a href="#1-3-1-关闭-x-windows-服务" class="headerlink" title="1.3.1 关闭 x-windows 服务"></a>1.3.1 关闭 x-windows 服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm stop</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">sudo /etc/init.d/lightdm stop</span><br></pre></td></tr></table></figure><h3 id="1-3-2-安装驱动"><a href="#1-3-2-安装驱动" class="headerlink" title="1.3.2 安装驱动"></a>1.3.2 安装驱动</h3><p>先按照 1.3.2.a 安装驱动，在安装驱动的过程中，如果出现循环登陆的情况，按照 1.3.2.b 安装驱动。</p><h4 id="1-3-2-a-正常安装驱动"><a href="#1-3-2-a-正常安装驱动" class="headerlink" title="1.3.2.a 正常安装驱动"></a>1.3.2.a 正常安装驱动</h4><p>1.3.2.a.1 安装并测试</p><p><strong>安装</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo sh ./NVIDIA-Linux-x86_64-410.78.run</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">sudo bash ./NVIDIA-Linux-x86_64-410.78.run</span><br></pre></td></tr></table></figure><p>在弹出来的选项中，选择默认的即可。<br>好像是32-bit那个选否，core为否。</p><p>执行 1.3.3。</p><h4 id="1-3-2-b-循环登陆时安装驱动"><a href="#1-3-2-b-循环登陆时安装驱动" class="headerlink" title="1.3.2.b 循环登陆时安装驱动"></a>1.3.2.b 循环登陆时安装驱动</h4><p>一般而言我们安装的ubuntu 的显示器并没有接到nvidia的显卡上，而是使用了intel的集显。我们安装驱动其实只是想将我们运算的显卡的驱动更新，结果都给搞了，所以产生了冲突。当然，也可能是opengl产生的冲突。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get --purge remove nvidia-*</span><br><span class="line"><span class="meta">#</span> ctrl+alt+F1</span><br><span class="line"><span class="meta">#</span> 可以考虑适当关机</span><br><span class="line">sudo service lightdm stop</span><br><span class="line">sudo sh ./NVIDIA-Linux-x86_64-410.78.run –no-x-check –no-nouveau-check –no-opengl-files</span><br></pre></td></tr></table></figure><ul><li>–no-x-check 安装驱动时关闭X服务</li><li>–no-nouveau-check 安装驱动时禁用nouveau</li><li>–no-opengl-files 只安装驱动文件，不安装OpenGL文件</li></ul><p>在弹出来的选项中，选择默认的即可。</p><p>执行1.3.3</p><h3 id="1-3-3-测试"><a href="#1-3-3-测试" class="headerlink" title="1.3.3 测试"></a>1.3.3 测试</h3><p><strong>测试:</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-smi # 有输出表示成功，没有输出表示不成功，重新下载安装。</span><br></pre></td></tr></table></figure><p>有输出表示成功，执行 启动 <strong>x-windows 服务</strong> 及其之后的命令。 </p><p>没有输出表示<strong>失败</strong>，重新执行 1.1 、1.3.1 和 1.3.2.a.1.即：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get --purge remove nvidia-*</span><br><span class="line"><span class="meta">#</span> ctrl+alt+F1</span><br><span class="line">sudo service lightdm stop</span><br><span class="line">sudo sh ./NVIDIA-Linux-x86_64-410.78.run</span><br></pre></td></tr></table></figure><p>如果几次都不成功，则验证下载的文件是否完整</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo md5sum sudo service lightdm stop</span><br><span class="line"><span class="meta">#</span> 有输出表示完整，没有输出表示不完整</span><br></pre></td></tr></table></figure><p><strong>启动 x-windows 服务</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm start</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">sudo /etc/init.d/lightdm start</span><br></pre></td></tr></table></figure><p>进入图形桌面: ctrl+alt+F7</p><p>如果正常登陆，继续下一个命令，如果出现重复登陆，则按照 1.3.2.b 安装驱动。</p><p><strong>二次测试驱动(可以不需要)</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-smi # 有输出表示成功，没有输出表示不成功</span><br><span class="line">sudo nvidia-settings # 有输出或者没有输出但是smi有输出都表示成功，反之表示不成功</span><br></pre></td></tr></table></figure><h1 id="2-CUDA"><a href="#2-CUDA" class="headerlink" title="2. CUDA"></a>2. CUDA</h1><h2 id="2-1-安装文件"><a href="#2-1-安装文件" class="headerlink" title="2.1 安装文件"></a>2.1 安装文件</h2><p><strong>提前安装某些文件</strong>:</p><p>gcc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 验证gcc</span><br><span class="line">gcc --version</span><br><span class="line"><span class="meta">#</span> 如果有输出，表示gcc安装成功，进行下一步，如果没有输出，需要安装gcc</span><br><span class="line"><span class="meta">#</span> 安装gcc命令</span><br><span class="line">sudo apt-get install gcc</span><br></pre></td></tr></table></figure><p>内核(官方文档2.4)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install linux-headers-$(uname -r)</span><br></pre></td></tr></table></figure><p><strong>安装CUDA</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sh cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure><p>协议一路回车到底，然后会出现几个选项。</p><ul><li>EULA: accept</li><li>NVIDIA Acceerated Graphics Driver: n</li><li>CUDA Toolkit: y</li><li>Tookkit: 回车</li><li>symbolic link: y</li><li>Samples: y</li></ul><p>安装后，如果出现提示信息，表示缺少几个库，安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev</span><br></pre></td></tr></table></figure><p>重新运行安装命令。</p><p>如果有补丁，则依次安装补丁，选择默认即可。</p><h2 id="2-2-设置环境变量-官方文档7-1-1"><a href="#2-2-设置环境变量-官方文档7-1-1" class="headerlink" title="2.2 设置环境变量(官方文档7.1.1)"></a>2.2 设置环境变量(官方文档7.1.1)</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit ~/.bashrc</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">sudo gedit /etc/profile</span><br></pre></td></tr></table></figure><p>写入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> CUDA-9.0</span><br><span class="line">export CUDA_HOME=/usr/local/cuda-9.0</span><br><span class="line">export PATH=$PATH:$CUDA_HOME/bin</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 如果有多个cuda，那么可以写成</span><br><span class="line">export CUDA_HOME=/usr/local/cuda-9.0:$CUDA_HOME</span><br><span class="line">export PATH=/usr/local/cuda-9.0/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64$:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><p>刷新</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="2-3-验证"><a href="#2-3-验证" class="headerlink" title="2.3 验证"></a>2.3 验证</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/cuda/samples/1_Utilities/deviceQuery</span><br><span class="line">sudo make</span><br><span class="line">./deviceQuery # 有输出表示成功，反之表示失败，重新安装cuda</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -V # 有输出表示成功，反之表示失败，重新安装cuda</span><br></pre></td></tr></table></figure><h1 id="3-cuDNN"><a href="#3-cuDNN" class="headerlink" title="3. cuDNN"></a>3. cuDNN</h1><p><strong>注册下载对应版本的cudnn</strong>:</p><p><strong>解压</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cudnn-9.0-linux-x86-v7.1.tgz</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">cp  cudnn-9.0-linux-x86-v7.1.solitairetheme8 cudnn-9.0-linux-x86-v7.1.tgz</span><br><span class="line">tar -zxvf cudnn-9.0-linux-x86-v7.1.tgz</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line"><span class="meta">#</span> 直接右键解压缩</span><br></pre></td></tr></table></figure><p><strong>移动</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda-9.0/include/</span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda-9.0/lib64/</span><br><span class="line">sudo chmod a+r /usr/local/cuda-9.0/include/cudnn.h</span><br><span class="line">sudo chmod a+r /usr/local/cuda-9.0/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p><strong>建立软连接</strong>(可选)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod +r libcudnn.so.7.0.5</span><br><span class="line">sudo ln -sf libcudnn.so.7.0.5 libcudnn.so.7  </span><br><span class="line">sudo ln -sf libcudnn.so.7 libcudnn.so</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><p><strong>查看cudnn版本</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure><h1 id="4-ananconda"><a href="#4-ananconda" class="headerlink" title="4. ananconda"></a>4. ananconda</h1><p><strong>下载</strong>: Anaconda3-5.2.0-Linux-x86_64.sh</p><p><strong>安装</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sh ./Anaconda3-5.2.0-Linux-x86_64.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 路径</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p>默认或者同意</p><p><strong>清华源</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure><p><strong>测试</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">which python</span><br><span class="line">conda install numpy</span><br></pre></td></tr></table></figure><h1 id="5-pytorch"><a href="#5-pytorch" class="headerlink" title="5. pytorch"></a>5. pytorch</h1><p><strong>安装</strong>：在官网找到安装命令即可，如果速度太慢，可以 ctrl+c 之后复制链接到其他电脑上手动下载并安装离线包。</p><p>安装离线包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install xxx</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">pip install xxx</span><br></pre></td></tr></table></figure><p><strong>GPU测试</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipython</span><br><span class="line">import torch as t</span><br><span class="line">t.cuda.is_available()</span><br><span class="line"><span class="meta">#</span> True</span><br></pre></td></tr></table></figure><h1 id="6-Tensorflow"><a href="#6-Tensorflow" class="headerlink" title="6. Tensorflow"></a>6. Tensorflow</h1><p><strong>安装</strong>：在官网找到安装命令即可，如果速度太慢，可以 ctrl+c 之后复制链接到其他电脑上手动下载并安装离线包。</p><p>安装离线包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install xxx</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">pip install xxx</span><br></pre></td></tr></table></figure><p><strong>测试</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipython</span><br><span class="line">import tensorflow as tf</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span><br><span class="line"><span class="meta">#</span> 如果输出有GPU，成功，没有的话重新开始装吧，我帮不了你，因为我成功了。</span><br></pre></td></tr></table></figure><p><strong>备注</strong>：<br>tensorflow与cuda、cudnn的版本需要严格一致。可以通过建立虚拟环境的形式或者tensorflow会自动安装对应版本的cuda、cudnn来解决。参见官网:<br><a href="https://tensorflow.google.cn/install/source" target="_blank" rel="noopener">https://tensorflow.google.cn/install/source</a></p><h1 id="7-matlab"><a href="#7-matlab" class="headerlink" title="7. matlab"></a>7. matlab</h1><p>我使用的是分成两部分的matlab，参考<br><a href="https://blog.csdn.net/qq_36982160/article/details/78397514" target="_blank" rel="noopener">https://blog.csdn.net/qq_36982160/article/details/78397514</a></p><p>matlab要比前面复杂一些，需要要有耐心。</p><p><strong>挂载</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /media/matlab</span><br><span class="line">sudo mount -t auto -o loop R2016b_glnxa64_dvd1.iso matlab/</span><br></pre></td></tr></table></figure><p><strong>安装</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /media/matlab</span><br><span class="line">sudo ./matlab/install</span><br></pre></td></tr></table></figure><ul><li>选项：第二个，使用文件安装密钥，不需要Internet连接</li><li>选项：是(y)</li><li>密钥：crack的readme的第一个数字串</li><li>文件路径：默认</li><li>下一步</li><li>安装</li></ul><p>安装进行到80%左右的时候，会弹出一个提示框，说请插入dvd2，这时候我们需要重新开一个终端，把dvd2挂载到matlab文件夹中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t auto -o loop R2016b_glnxa64_dvd2.iso /media/matlab/</span><br></pre></td></tr></table></figure><p>点击OK继续</p><p><strong>激活</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/MATLAB/R2016b/bin</span><br><span class="line">./matlab</span><br></pre></td></tr></table></figure><ul><li>在弹出的界面中选择: Activate manually without the internet，点击next</li><li>文件路径选择：./license_standalone.lic</li></ul><p>把Crack文件夹中R2016b/Linux/R2016b/bin/glnxa64四个文件，复制到/usr/local/MATLAB/R2016b/bin/glnxa64目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp Crack/R2016b/bin/glnxa64/lib* /usr/local/MATLAB/R2016b/bin/glnxa64</span><br></pre></td></tr></table></figure><p>至此，Matlab已经安装并激活。</p><p><strong>环境变量</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit ~./bashrc</span><br><span class="line"><span class="meta">#</span> 输入</span><br><span class="line"><span class="meta">#</span> matlab</span><br><span class="line">export PATH=/usr/local/MATLAB/R2016b/bin:$PATH</span><br><span class="line"><span class="meta">#</span> 以后直接在命令框中输入matlab即可启动</span><br></pre></td></tr></table></figure><p><strong>创建快捷方式</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /usr/share/applications/Matlab.desktop</span><br><span class="line"><span class="meta">#</span> 写入</span><br><span class="line">[Desktop Entry]</span><br><span class="line">Type=Application</span><br><span class="line">Name=Matlab</span><br><span class="line">GenericName=Matlab 2016b</span><br><span class="line">Comment=Matlab:The Language of Technical Computing</span><br><span class="line">Exec=sh /usr/local/MATLAB/R2016b/bin/matlab -desktop</span><br><span class="line">Icon=/usr/local/MATLAB/Matlab.png</span><br><span class="line">Terminal=false</span><br><span class="line">Categories=Development;</span><br></pre></td></tr></table></figure><ul><li>Exec代表应用程序的位置</li><li>Icon代表应用程序图标的位置</li><li>Terminal为false表示启动时不启动命令行窗口，为true表示启动命令行窗口</li></ul><p>此时会在/usr/share/applications中看到matlab（和文件Name对应）的快捷方式</p><h1 id="8-分辨率"><a href="#8-分辨率" class="headerlink" title="8. 分辨率"></a>8. 分辨率</h1><p><strong>查看分辨率</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xrandr</span><br><span class="line"><span class="meta">#</span> VGA1</span><br></pre></td></tr></table></figure><p><strong>自定义分辨率</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cvt 1920 1080</span><br><span class="line"><span class="meta">#</span> "1920x1080_60.00"  173.00  1920 2048 2248 2576  1080 1083 1088 1120 -hsync +vsync</span><br></pre></td></tr></table></figure><p><strong>设置分辨率</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/profile</span><br><span class="line"><span class="meta">#</span> 末尾加入</span><br><span class="line">xrandr --newmode "1920x1080_60.00" 173.00 1920 2048 2248 2576 1080 1083 1088 1120 -hsync +vsync</span><br><span class="line">xrandr --addmode VGA1 "1920x1080_60.00"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 刷新</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>以后在分辨率选项里就有1920x1080，同时开机显示率也自动变成1920x1080.</p><h1 id="9-cuda、cudnn的卸载"><a href="#9-cuda、cudnn的卸载" class="headerlink" title="9. cuda、cudnn的卸载"></a>9. cuda、cudnn的卸载</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 卸载cuda</span><br><span class="line">sudo /usr/local/cuda-9.0/bin/uninstall_cuda_9.0.pl</span><br><span class="line"><span class="meta">#</span> 删除cuda</span><br><span class="line">sudo rm -rf /usr/local/cuda-9.0</span><br><span class="line"><span class="meta">#</span> 卸载cudnn</span><br><span class="line">sudo rm -rf /usr/local/cuda/include/cudnn.h</span><br><span class="line">sudo rm -rf /usr/local/cuda/lib64/libcudnn</span><br></pre></td></tr></table></figure><h1 id="10-teamviewer-for-Linux-的安装与卸载"><a href="#10-teamviewer-for-Linux-的安装与卸载" class="headerlink" title="10. teamviewer for Linux 的安装与卸载"></a>10. teamviewer for Linux 的安装与卸载</h1><p>主要针对teamviewer出现商业用途时使用。</p><p>参考: <a href="https://www.cnblogs.com/fxust/p/8040706.html" target="_blank" rel="noopener">https://www.cnblogs.com/fxust/p/8040706.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 1. 卸载</span></span><br><span class="line">dpkg -l |grep xxx</span><br><span class="line"><span class="comment">#xxx就是安装的软件名</span></span><br><span class="line">sudo dpkg --purge xxxx</span><br><span class="line">rm -rf ~/.local/share/xxxx</span><br><span class="line"><span class="comment"># 然后把所有teamviewer的文件，尤其是log文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 安装</span></span><br><span class="line">sudo dpkg -i teamviewer_13<span class="number">.0</span><span class="number">.5693</span>_amd64.deb</span><br><span class="line"><span class="comment"># 2.1 如果出现错误</span></span><br><span class="line"><span class="comment">#添加架构依赖</span></span><br><span class="line">sudo dpkg --add-architecture i386</span><br><span class="line"><span class="comment">#更新软件库</span></span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment">#执行强制安装</span></span><br><span class="line">sudo apt-get -f install</span><br><span class="line"><span class="comment">#再安装</span></span><br><span class="line">sudo dpkg -i teamviewer_13<span class="number">.0</span><span class="number">.5693</span>_amd64.deb</span><br><span class="line"><span class="comment"># 配置文件可以修改，可以不改</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;主要记录从安装显卡驱动到cudnn的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="GPU" scheme="http://yoursite.com/categories/GPU/"/>
    
    
      <category term="driver" scheme="http://yoursite.com/tags/driver/"/>
    
      <category term="cuda" scheme="http://yoursite.com/tags/cuda/"/>
    
      <category term="cudnn" scheme="http://yoursite.com/tags/cudnn/"/>
    
  </entry>
  
  <entry>
    <title>starGAN</title>
    <link href="http://yoursite.com/2018/12/19/starGAN/"/>
    <id>http://yoursite.com/2018/12/19/starGAN/</id>
    <published>2018-12-19T01:44:59.000Z</published>
    <updated>2018-12-23T15:11:34.619Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>因为在person-reid论文<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.pdf" target="_blank" rel="noopener">HHL</a>中涉及到了starGAN，所以做一个StarGAN的阅读记录，并比较与CycleGAN的区别。</p><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf" target="_blank" rel="noopener">StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</a></p><p>Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo</p><a id="more"></a><p>code-pytorch-official: <a href="https://github.com/yunjey/stargan" target="_blank" rel="noopener">https://github.com/yunjey/stargan</a><br>code-tensorflow: &lt;<a href="https://github.com/taki0112/StarGAN-Tensorflow" target="_blank" rel="noopener">https://github.com/taki0112/StarGAN-Tensorflow</a> &gt;</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>解决多域之间图像转换一对多的问题，本文主要针对人脸进行改变。</p><p><strong>关键词</strong>：multi-domain image-image translation</p><p><strong>效果</strong>：转换效果如图所示</p><p><img src="./starGAN/starGAN1.png" alt="转换效果"><br><img src="/2018/12/19/starGAN/starGAN1.png" title="转换效果"></p><p><strong>网络模型</strong>：CycleGAN和StarGAN模型对比</p><p>starGAN有一个生成器G，两个判别器。</p><p><img src="./starGAN/starGAN2.png" alt="CycleGAN和StarGAN模型对比"><br><img src="/2018/12/19/starGAN/starGAN2.png" title="CycleGAN和StarGAN模型对比"></p><p><strong>备注</strong>：</p><p>multi-domain：单数据集的不同属性作为了一个domain</p><p>multi-datasets：不同数据集的不同属性</p><p>starGAN 分为multi-domain和multi-dataset两种。</p><h1 id="2-Star-Generative-Adversarial-Networks"><a href="#2-Star-Generative-Adversarial-Networks" class="headerlink" title="2. Star Generative Adversarial Networks"></a>2. Star Generative Adversarial Networks</h1><h2 id="2-1-Multi-Domain-Image-to-Image-Translation"><a href="#2-1-Multi-Domain-Image-to-Image-Translation" class="headerlink" title="2.1  Multi-Domain Image-to-Image Translation"></a>2.1  Multi-Domain Image-to-Image Translation</h2><p><strong>starGAN</strong>: starGAN的训练模型</p><p><img src="./starGAN/starGAN3.png" alt="starGAN in mutli domain"><br><img src="/2018/12/19/starGAN/starGAN3.png" title="starGAN in mutli domain"></p><blockquote><p>To achieve this, we train G to translate an input image x into an output image y conditioned on the target domain label c, <strong>G(x; c) -&gt; y</strong>. We randomly generate the target domain label c so that G learns to flexibly translate the input image. We also introduce an auxiliary classifier [22] that allows a single discriminator to control multiple domains. That is, our discriminator produces probability distributions over both sources and domain labels, <strong>D:x-&gt;{$D_{src}$(x); $D_{cls}$(x)}</strong></p></blockquote><p><strong>符号说明</strong>：符号表</p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">x</td><td style="text-align:center">input image</td></tr><tr><td style="text-align:center">c</td><td style="text-align:center">target domain label</td></tr><tr><td style="text-align:center">c’</td><td style="text-align:center">source domain label</td></tr><tr><td style="text-align:center">y</td><td style="text-align:center">generate image</td></tr></tbody></table></div><p><strong>Loss</strong>: training loss</p><p><strong>Adversarial Loss</strong>:(CycleGAN也有)对抗损失</p><script type="math/tex; mode=display">L_{adv}=E_x[log D_{src}(x)]+E_{x,c}[log (1-D_{src}(G(x,c)))] \tag{1}</script><p><strong>Domain Classification Loss</strong>:(特有)分类损失</p><blockquote><p>That is, we decompose the objective into two terms: a domain classification loss of real images used to optimize D, and a domain classification loss of fake images used to optimize G.</p></blockquote><p><em>优化 D</em>:</p><script type="math/tex; mode=display">L_{cls}^r=E_{x,c'}[-log D_{cls}(c'|x)] \tag{2}</script><blockquote><p>By minimizing this objective, D learns to classify a real image x to its corresponding original domain c’.</p></blockquote><p><em>优化 G</em>:</p><script type="math/tex; mode=display">L_{cls}^f=E_{x,c}[-log D_{cls}(c|G(x,c))] \tag{3}</script><blockquote><p>G tries to minimize this objective to generate images that can be classified as the target domain c.</p></blockquote><p><strong>Reconstruction Loss</strong>: (共有)重构损失</p><script type="math/tex; mode=display">L_{rec}=E_{x,c,c'}[\parallel x-G(G(x,c),c') \parallel _1]  \tag{4}</script><p><strong>Full Objective</strong>: 共有</p><script type="math/tex; mode=display">L_D=-L_{adv}+\lambda_{cls} L_{cls}^r</script><script type="math/tex; mode=display">L_G=L_{adv}+\lambda_{cls} L_{cls}^f+\lambda_{rec}L_{rec} \tag{5}</script><script type="math/tex; mode=display">\lambda_{cls}=1, \lambda_{rec}=10</script><h2 id="2-2-Training-with-Multiple-Datasets"><a href="#2-2-Training-with-Multiple-Datasets" class="headerlink" title="2.2. Training with Multiple Datasets"></a>2.2. Training with Multiple Datasets</h2><p><img src="./starGAN/starGAN5.png" alt="starGAN in multi datasets"><br><img src="/2018/12/19/starGAN/starGAN5.png" title="starGAN in multi datasets"></p><p>StarGAN也适用于多数据集间的转换，上述过程中的重构损失要求数据集之间的标签一致(？？？)。针对这个问题，作者引入Mask Vector.</p><p><strong>Mask Vector</strong>: 修改真值。</p><script type="math/tex; mode=display">\tilde{c} = [c_1, ..., c_n, m]</script><blockquote><p>$c_i$ represents a vector for the labels of the i-th dataset. The vector of the known label $c_i$ can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n−1 unknown labels we simply assign zero values.</p></blockquote><p>这样的话，所有的c都需要变成$\tilde{c}$</p><h1 id="3-Implementation"><a href="#3-Implementation" class="headerlink" title="3. Implementation"></a>3. Implementation</h1><p><strong>Improved GAN training</strong>: 为了稳定训练过程，替代方程1.</p><script type="math/tex; mode=display">L_{adv}=E_x[log D_{src}(x)]-E_{x,c}[log (D_{src}(G(x,c)))]-\lambda_{gp}E_{\hat{x}}[(\parallel \nabla_{\hat{x}}  D_{src}(\hat{x}) \parallel_2-1)^2] \tag{6}</script><script type="math/tex; mode=display">\lambda_{gp}=10</script><p><strong>Network Architecture</strong>: 类似CycleGAN。</p><p>G: Leaky ReLU: 0.01</p><p><img src="./starGAN/starGAN6.png" alt="G"><br><img src="/2018/12/19/starGAN/starGAN6.png" title="G"></p><p>D: PatchGAN</p><p>现在网络架构可以看到的是作者使用的不是70x70的patchGAN，通过patchGAN的论文，也没有看到这种结构。</p><p><img src="./starGAN/starGAN7.png" alt="D"><br><img src="/2018/12/19/starGAN/starGAN7.png"></p><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><h2 id="4-1-Baseline-Models"><a href="#4-1-Baseline-Models" class="headerlink" title="4.1 Baseline Models"></a>4.1 Baseline Models</h2><p><img src="./starGAN/starGAN4.png" alt="baseline models"><br><img src="/2018/12/19/starGAN/starGAN4.png" title="baseline models"></p><p>通过结果可以看出，在Gender这个属性，ICGAN的转换效果要更好一些，但是损失了ID信息。</p><h2 id="4-2-Training"><a href="#4-2-Training" class="headerlink" title="4.2 Training"></a>4.2 Training</h2><ul><li>Adam: $\beta_1=0.5, \beta_2=0.999$</li><li>Updates: one generator update after five discriminator updates</li><li>lr: For CelebA, 0.0001 for the first 100000 epochs, and linearly decay the lr to 0 over the next 100000 epochs. For the RaFD, 0.0001 for the first 100000 epochs, and linearly decay the lr to 0 over the next 100000 epochs.作者在论文写的是10和100，但是代码显示的是100000</li><li>batch: 16</li><li>input: For CelebA, crop: 178, resize: 128; For RaFD, </li></ul><h2 id="4-3-Results"><a href="#4-3-Results" class="headerlink" title="4.3 Results"></a>4.3 Results</h2><p>作者通过人脸的转换实验，不仅说明了StarGAN在单数据集的不同domian中效果好，而且在多数据集的不同domian中效果也好。</p><h1 id="5-代码"><a href="#5-代码" class="headerlink" title="5. 代码"></a>5. 代码</h1><p>在这里分析pytorch的代码，并对其中关键的代码进行解读。</p><p>如果不说明，则假设讨论单数据集的多域。</p><h2 id="5-1-Model-G-and-D"><a href="#5-1-Model-G-and-D" class="headerlink" title="5.1 Model: G and D"></a>5.1 Model: G and D</h2><p><strong>Generator</strong>:<br>生成器Generator，结构与前面提到的网络架构一致，这里需要注意两点：</p><ul><li>当训练集是单数据集的多domain时，label需要扩充成图片大小，一起输入网络(这里有个疑问：网络真得能知道后面的通道是label吗)</li><li>当训练集是多数据集的多domain时，label的维度是c+c2+2，因为有mask，同样需要广播成图片大小，一起输入网络</li></ul><p><strong>Discriminator</strong>:<br>判别器Discriminator，有个疑问是关于是感受野和计算损失的。</p><p><img src="./starGAN/starGAN8.png" alt="感受野"><br><img src="/2018/12/19/starGAN/starGAN8.png" title="感受野"></p><p>下面会提及到计算损失的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Residual Block with instance normalization."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim_in, dim_out)</span>:</span></span><br><span class="line">        super(ResidualBlock, self).__init__()</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            nn.Conv2d(dim_in, dim_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.InstanceNorm2d(dim_out, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Conv2d(dim_out, dim_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.InstanceNorm2d(dim_out, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x + self.main(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Generator network."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conv_dim=<span class="number">64</span>, c_dim=<span class="number">5</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">3</span>+c_dim, conv_dim, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.InstanceNorm2d(conv_dim, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>))</span><br><span class="line">        layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Down-sampling layers.</span></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            layers.append(nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">            layers.append(nn.InstanceNorm2d(curr_dim*<span class="number">2</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>))</span><br><span class="line">            layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Bottleneck layers.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(repeat_num):</span><br><span class="line">            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Up-sampling layers.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">            layers.append(nn.InstanceNorm2d(curr_dim//<span class="number">2</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>))</span><br><span class="line">            layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        layers.append(nn.Conv2d(curr_dim, <span class="number">3</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.Tanh())</span><br><span class="line">        self.main = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, c)</span>:</span></span><br><span class="line">        <span class="comment"># Replicate spatially and concatenate domain information.</span></span><br><span class="line">        <span class="comment"># c: N*c_dim</span></span><br><span class="line">        <span class="comment"># 生成器直接将目标域c在通道维度进行拼接</span></span><br><span class="line">        c = c.view(c.size(<span class="number">0</span>), c.size(<span class="number">1</span>), <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        c = c.repeat(<span class="number">1</span>, <span class="number">1</span>, x.size(<span class="number">2</span>), x.size(<span class="number">3</span>))</span><br><span class="line">        x = torch.cat([x, c], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.main(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Discriminator network with PatchGAN."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_size=<span class="number">128</span>, conv_dim=<span class="number">64</span>, c_dim=<span class="number">5</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">3</span>, conv_dim, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.LeakyReLU(<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, repeat_num):</span><br><span class="line">            layers.append(nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">            layers.append(nn.LeakyReLU(<span class="number">0.01</span>))</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        kernel_size = int(image_size / np.power(<span class="number">2</span>, repeat_num))</span><br><span class="line">        self.main = nn.Sequential(*layers)</span><br><span class="line">        self.conv1 = nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        h = self.main(x)</span><br><span class="line">        <span class="comment"># True or False</span></span><br><span class="line">        out_src = self.conv1(h)</span><br><span class="line">        <span class="comment"># classes onehot</span></span><br><span class="line">        out_cls = self.conv2(h)</span><br><span class="line">        <span class="keyword">return</span> out_src, out_cls.view(out_cls.size(<span class="number">0</span>), out_cls.size(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="5-2-input"><a href="#5-2-input" class="headerlink" title="5.2 input"></a>5.2 input</h2><p>对于任一张图片，其target label是随机取其他图片的label，而没有刻意去指定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># label_org和label_trg可以认为是单个图片的真实label，形式可以是[0,1,1,0]或者4，根据不同的数据集形式进行处理，前者是多分类label，后者是单分类label，用于计算损失</span></span><br><span class="line"><span class="comment"># c_org，c_trg是与图片一起输入网络的&#123;0,1&#125;向量，形式是[0,1,1,0]或者是[0,0,0,1]的形式，用于网络的输入</span></span><br><span class="line">x_real, label_org = next(data_iter)</span><br><span class="line">rand_idx = torch.randperm(label_org.size(<span class="number">0</span>))</span><br><span class="line">label_trg = label_org[rand_idx]</span><br><span class="line"><span class="keyword">if</span> self.dataset == <span class="string">'CelebA'</span>:</span><br><span class="line">    c_org = label_org.clone()</span><br><span class="line">    c_trg = label_trg.clone()</span><br><span class="line"><span class="keyword">elif</span> self.dataset == <span class="string">'RaFD'</span>:</span><br><span class="line">    c_org = self.label2onehot(label_org, self.c_dim)</span><br><span class="line">    c_trg = self.label2onehot(label_trg, self.c_dim)</span><br></pre></td></tr></table></figure><h2 id="5-3-train-G-and-D"><a href="#5-3-train-G-and-D" class="headerlink" title="5.3 train G and D"></a>5.3 train G and D</h2><h3 id="5-3-1-train-D"><a href="#5-3-1-train-D" class="headerlink" title="5.3.1 train D"></a>5.3.1 train D</h3><p>这里需要对上述提到的损失函数做进一步处理。</p><p><strong>判断图片真假损失</strong>:由方程6得：</p><script type="math/tex; mode=display">L_{adv}=-D_{src}(x)+D_{src}(G(x,c))+\lambda_{gp}(\parallel \nabla_{\hat{x}}  D_{src}(\hat{x}) \parallel_2-1)^2 \tag{7}</script><script type="math/tex; mode=display">\lambda_{gp}=10</script><p><strong>判断原图片属性正确</strong>:由方程2得：</p><script type="math/tex; mode=display">L_{cls}^r=D_{cls}(c'|x) \tag{8}</script><p><strong>总损失</strong>：</p><script type="math/tex; mode=display">L_D=-L_{adv}+\lambda_{cls} L_{cls}^r</script><script type="math/tex; mode=display">\lambda_{cls}=1</script><p><strong>备注</strong>：</p><ul><li>在计算真假损失的时候，是直接求输出的均值，这一点不是很理解。</li><li>方程7的第三项的计算见gradient_penalty，对整个图片的梯度求和。</li><li>方程8的的求解见classification_loss，就是一个简单的分类损失。</li><li>不理解方程2为什么要加个符号？方程7也是符号正好相反？</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"><span class="comment">#                             2. Train the discriminator                              #</span></span><br><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss with real images.</span></span><br><span class="line">out_src, out_cls = self.D(x_real) <span class="comment"># out_src：N,1,2,2; out_cls: N,c_dim</span></span><br><span class="line">d_loss_real = - torch.mean(out_src) <span class="comment"># 方程7的第一项</span></span><br><span class="line">d_loss_cls = self.classification_loss(out_cls, label_org, self.dataset) <span class="comment"># 方程8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss with fake images.</span></span><br><span class="line">x_fake = self.G(x_real, c_trg)</span><br><span class="line">out_src, out_cls = self.D(x_fake.detach())</span><br><span class="line">d_loss_fake = torch.mean(out_src) <span class="comment"># 方程7的第二项</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss for gradient penalty.</span></span><br><span class="line">alpha = torch.rand(x_real.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).to(self.device)</span><br><span class="line">x_hat = (alpha * x_real.data + (<span class="number">1</span> - alpha) * x_fake.data).requires_grad_(<span class="keyword">True</span>)</span><br><span class="line">out_src, _ = self.D(x_hat)</span><br><span class="line">d_loss_gp = self.gradient_penalty(out_src, x_hat) <span class="comment"># 方程7的第三项</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Backward and optimize.</span></span><br><span class="line">d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp <span class="comment"># 总损失</span></span><br><span class="line">self.reset_grad()</span><br><span class="line">d_loss.backward()</span><br><span class="line">self.d_optimizer.step()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_penalty</span><span class="params">(self, y, x)</span>:</span></span><br><span class="line">    <span class="string">"""Compute gradient penalty: (L2_norm(dy/dx) - 1)**2."""</span></span><br><span class="line">    weight = torch.ones(y.size()).to(self.device)</span><br><span class="line">    dydx = torch.autograd.grad(outputs=y,</span><br><span class="line">                                inputs=x,</span><br><span class="line">                                grad_outputs=weight,</span><br><span class="line">                                retain_graph=<span class="keyword">True</span>,</span><br><span class="line">                                create_graph=<span class="keyword">True</span>,</span><br><span class="line">                                only_inputs=<span class="keyword">True</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    dydx = dydx.view(dydx.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    dydx_l2norm = torch.sqrt(torch.sum(dydx**<span class="number">2</span>, dim=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> torch.mean((dydx_l2norm<span class="number">-1</span>)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classification_loss</span><span class="params">(self, logit, target, dataset=<span class="string">'CelebA'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Compute binary or softmax cross entropy loss."""</span></span><br><span class="line">    <span class="keyword">if</span> dataset == <span class="string">'CelebA'</span>:</span><br><span class="line">        <span class="keyword">return</span> F.binary_cross_entropy_with_logits(logit, target, size_average=<span class="keyword">False</span>) / logit.size(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> dataset == <span class="string">'RaFD'</span>:</span><br><span class="line">        <span class="keyword">return</span> F.cross_entropy(logit, target)</span><br></pre></td></tr></table></figure><h3 id="5-3-2-train-G"><a href="#5-3-2-train-G" class="headerlink" title="5.3.2 train G"></a>5.3.2 train G</h3><p>与以往训练几个G之后才训练D不同，这里是训练几个D之后才训练G。</p><p>同样对上述提到的损失做进一步处理。</p><p><strong>生成图片为真</strong>：由方程6得，与方程7正好相反：</p><script type="math/tex; mode=display">L_{adv}=-D_{src}(G(x,c)) \tag{9}</script><p><strong>生成图片的属性正确</strong>：由方程3得：</p><script type="math/tex; mode=display">L_{cls}^f=D_{cls}(c|G(x,c)) \tag{10}</script><p><strong>Reconstruction Loss</strong>: 重构损失</p><script type="math/tex; mode=display">L_{rec}=\parallel x-G(G(x,c),c') \parallel _1  \tag{4}</script><p><strong>总损失</strong></p><script type="math/tex; mode=display">L_G=L_{adv}+\lambda_{cls} L_{cls}^f+\lambda_{rec}L_{rec} \tag{5}</script><script type="math/tex; mode=display">\lambda_{cls}=1, \lambda_{rec}=10</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"><span class="comment">#                               3. Train the generator                                #</span></span><br><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (i+<span class="number">1</span>) % self.n_critic == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Original-to-target domain.</span></span><br><span class="line">    x_fake = self.G(x_real, c_trg)</span><br><span class="line">    out_src, out_cls = self.D(x_fake)</span><br><span class="line">    g_loss_fake = - torch.mean(out_src) <span class="comment"># 方程9</span></span><br><span class="line">    g_loss_cls = self.classification_loss(out_cls, label_trg, self.dataset) <span class="comment"># 方程10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Target-to-original domain.</span></span><br><span class="line">    x_reconst = self.G(x_fake, c_org)</span><br><span class="line">    g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward and optimize.</span></span><br><span class="line">    g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls</span><br><span class="line">    self.reset_grad()</span><br><span class="line">    g_loss.backward()</span><br><span class="line">    self.g_optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="5-4-val"><a href="#5-4-val" class="headerlink" title="5.4 val"></a>5.4 val</h2><p><strong>CelebA数据集：</strong>这里制作target domain label的方法分为头发属性(互相排斥)和其他属性(不排斥):对于选中的头发属性’Black_Hair’, ‘Blond_Hair’, ‘Brown_Hair’,则把5张图片的’Black_Hair’全部设为1,’Blond_Hair’, ‘Brown_Hair’设为0,作为第一个target domain label, 再把5张图片的’Blond_Hair’全部设为1,’Black_Hair’, ‘Brown_Hair’设为0,作为第二个target domain label, 再把5张图片的’Brown_Hair’全部设为1,’Black_Hair’,’Blond_Hair’ 设为0,作为第三个target domain label,对于其他属性,则直接取相反数做为第三个target domain label和第四个target domain label.</p><p><strong>RaFD数据集</strong>:属于排斥属性，也和头发类似，对某一列全部设为1，其余设为0.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">c_org</span><br><span class="line">Out[<span class="number">24</span>]: </span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line"></span><br><span class="line">c_trg_list</span><br><span class="line">Out[<span class="number">25</span>]: </span><br><span class="line">[tensor([[ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]], device=<span class="string">'cuda:0'</span>),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]], device=<span class="string">'cuda:0'</span>),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]], device=<span class="string">'cuda:0'</span>),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]], device=<span class="string">'cuda:0'</span>),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]], device=<span class="string">'cuda:0'</span>)]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_labels</span><span class="params">(self, c_org, c_dim=<span class="number">5</span>, dataset=<span class="string">'CelebA'</span>, selected_attrs=None)</span>:</span></span><br><span class="line">    <span class="string">"""Generate target domain labels for debugging and testing."""</span></span><br><span class="line">    <span class="comment"># Get hair color indices.</span></span><br><span class="line">    <span class="keyword">if</span> dataset == <span class="string">'CelebA'</span>:</span><br><span class="line">        hair_color_indices = []</span><br><span class="line">        <span class="keyword">for</span> i, attr_name <span class="keyword">in</span> enumerate(selected_attrs):</span><br><span class="line">            <span class="keyword">if</span> attr_name <span class="keyword">in</span> [<span class="string">'Black_Hair'</span>, <span class="string">'Blond_Hair'</span>, <span class="string">'Brown_Hair'</span>, <span class="string">'Gray_Hair'</span>]:</span><br><span class="line">                hair_color_indices.append(i)</span><br><span class="line"></span><br><span class="line">    c_trg_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(c_dim):</span><br><span class="line">        <span class="keyword">if</span> dataset == <span class="string">'CelebA'</span>:</span><br><span class="line">            c_trg = c_org.clone()</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> hair_color_indices:  <span class="comment"># Set one hair color to 1 and the rest to 0.</span></span><br><span class="line">                c_trg[:, i] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> hair_color_indices:</span><br><span class="line">                    <span class="keyword">if</span> j != i:</span><br><span class="line">                        c_trg[:, j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                c_trg[:, i] = (c_trg[:, i] == <span class="number">0</span>)  <span class="comment"># Reverse attribute value.</span></span><br><span class="line">        <span class="keyword">elif</span> dataset == <span class="string">'RaFD'</span>:</span><br><span class="line">            c_trg = self.label2onehot(torch.ones(c_org.size(<span class="number">0</span>))*i, c_dim)</span><br><span class="line"></span><br><span class="line">        c_trg_list.append(c_trg.to(self.device))</span><br><span class="line">    <span class="keyword">return</span> c_trg_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fetch fixed inputs for debugging.</span></span><br><span class="line">data_iter = iter(data_loader)</span><br><span class="line">x_fixed, c_org = next(data_iter)</span><br><span class="line">x_fixed = x_fixed.to(self.device)</span><br><span class="line">c_fixed_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs)</span><br></pre></td></tr></table></figure><h2 id="5-5-多数据集"><a href="#5-5-多数据集" class="headerlink" title="5.5 多数据集"></a>5.5 多数据集</h2><p>在多数据集的情况下，损失函数大体不变，略微不同。</p><h3 id="5-5-1-input"><a href="#5-5-1-input" class="headerlink" title="5.5.1 input"></a>5.5.1 input</h3><p>多数据集顺序输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> [<span class="string">'CelebA'</span>, <span class="string">'RaFD'</span>]:</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">celeba_iter = iter(self.celeba_loader)</span><br><span class="line">x_real, label_org = next(celeba_iter)</span><br><span class="line">rand_idx = torch.randperm(label_org.size(<span class="number">0</span>))</span><br><span class="line">label_trg = label_org[rand_idx]</span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">'CelebA'</span>:</span><br><span class="line">    c_org = label_org.clone()</span><br><span class="line">    c_trg = label_trg.clone()</span><br><span class="line">    zero = torch.zeros(x_real.size(<span class="number">0</span>), self.c2_dim)</span><br><span class="line">    mask = self.label2onehot(torch.zeros(x_real.size(<span class="number">0</span>)), <span class="number">2</span>)</span><br><span class="line">    c_org = torch.cat([c_org, zero, mask], dim=<span class="number">1</span>)</span><br><span class="line">    c_trg = torch.cat([c_trg, zero, mask], dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">elif</span> dataset == <span class="string">'RaFD'</span>:</span><br><span class="line">    c_org = self.label2onehot(label_org, self.c2_dim)</span><br><span class="line">    c_trg = self.label2onehot(label_trg, self.c2_dim)</span><br><span class="line">    zero = torch.zeros(x_real.size(<span class="number">0</span>), self.c_dim)</span><br><span class="line">    mask = self.label2onehot(torch.ones(x_real.size(<span class="number">0</span>)), <span class="number">2</span>)</span><br><span class="line">    c_org = torch.cat([zero, c_org, mask], dim=<span class="number">1</span>)</span><br><span class="line">    c_trg = torch.cat([zero, c_trg, mask], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="5-5-2-train-D-and-G"><a href="#5-5-2-train-D-and-G" class="headerlink" title="5.5.2 train D and G"></a>5.5.2 train D and G</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"><span class="comment">#                             2. Train the discriminator                              #</span></span><br><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss with real images.</span></span><br><span class="line">out_src, out_cls = self.D(x_real) </span><br><span class="line">out_cls = out_cls[:, :self.c_dim] <span class="keyword">if</span> dataset == <span class="string">'CelebA'</span> <span class="keyword">else</span> out_cls[:, self.c_dim:] <span class="comment"># 属性损失只考虑一半</span></span><br><span class="line">d_loss_real = - torch.mean(out_src) <span class="comment"># 方程7的第一项</span></span><br><span class="line">d_loss_cls = self.classification_loss(out_cls, label_org, dataset) <span class="comment"># 方程8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss with fake images.</span></span><br><span class="line">x_fake = self.G(x_real, c_trg)</span><br><span class="line">out_src, _ = self.D(x_fake.detach())</span><br><span class="line">d_loss_fake = torch.mean(out_src) <span class="comment"># 方程7的第二项</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss for gradient penalty.</span></span><br><span class="line">alpha = torch.rand(x_real.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).to(self.device)</span><br><span class="line">x_hat = (alpha * x_real.data + (<span class="number">1</span> - alpha) * x_fake.data).requires_grad_(<span class="keyword">True</span>)</span><br><span class="line">out_src, _ = self.D(x_hat)</span><br><span class="line">d_loss_gp = self.gradient_penalty(out_src, x_hat) <span class="comment"># 方程7的第三项</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Backward and optimize.</span></span><br><span class="line">d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp</span><br><span class="line">self.reset_grad()</span><br><span class="line">d_loss.backward()</span><br><span class="line">self.d_optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Logging.</span></span><br><span class="line">loss = &#123;&#125;</span><br><span class="line">loss[<span class="string">'D/loss_real'</span>] = d_loss_real.item()</span><br><span class="line">loss[<span class="string">'D/loss_fake'</span>] = d_loss_fake.item()</span><br><span class="line">loss[<span class="string">'D/loss_cls'</span>] = d_loss_cls.item()</span><br><span class="line">loss[<span class="string">'D/loss_gp'</span>] = d_loss_gp.item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"><span class="comment">#                               3. Train the generator                                #</span></span><br><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (i+<span class="number">1</span>) % self.n_critic == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Original-to-target domain.</span></span><br><span class="line">    x_fake = self.G(x_real, c_trg)</span><br><span class="line">    out_src, out_cls = self.D(x_fake)</span><br><span class="line">    out_cls = out_cls[:, :self.c_dim] <span class="keyword">if</span> dataset == <span class="string">'CelebA'</span> <span class="keyword">else</span> out_cls[:, self.c_dim:] <span class="comment"># 生成图片的属性只考虑一半</span></span><br><span class="line">    g_loss_fake = - torch.mean(out_src)</span><br><span class="line">    g_loss_cls = self.classification_loss(out_cls, label_trg, dataset)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Target-to-original domain.</span></span><br><span class="line">    x_reconst = self.G(x_fake, c_org)</span><br><span class="line">    g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward and optimize.</span></span><br><span class="line">    g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls</span><br><span class="line">    self.reset_grad()</span><br><span class="line">    g_loss.backward()</span><br><span class="line">    self.g_optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Logging.</span></span><br><span class="line">    loss[<span class="string">'G/loss_fake'</span>] = g_loss_fake.item()</span><br><span class="line">    loss[<span class="string">'G/loss_rec'</span>] = g_loss_rec.item()</span><br><span class="line">    loss[<span class="string">'G/loss_cls'</span>] = g_loss_cls.item()</span><br></pre></td></tr></table></figure><h3 id="5-5-3-val-and-test"><a href="#5-5-3-val-and-test" class="headerlink" title="5.5.3 val and test"></a>5.5.3 val and test</h3><p>对当前图片生成两个数据集下不同属性的图片，也就是说，具有跨数据集生成图片的能力。</p><p><strong>val</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (i+<span class="number">1</span>) % self.sample_step == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        x_fake_list = [x_fixed]</span><br><span class="line">        <span class="keyword">for</span> c_fixed <span class="keyword">in</span> c_celeba_list:</span><br><span class="line">            c_trg = torch.cat([c_fixed, zero_rafd, mask_celeba], dim=<span class="number">1</span>)</span><br><span class="line">            x_fake_list.append(self.G(x_fixed, c_trg))</span><br><span class="line">        <span class="keyword">for</span> c_fixed <span class="keyword">in</span> c_rafd_list:</span><br><span class="line">            c_trg = torch.cat([zero_celeba, c_fixed, mask_rafd], dim=<span class="number">1</span>)</span><br><span class="line">            x_fake_list.append(self.G(x_fixed, c_trg))</span><br></pre></td></tr></table></figure><p><strong>test</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (x_real, c_org) <span class="keyword">in</span> enumerate(self.celeba_loader):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prepare input images and target domain labels.</span></span><br><span class="line">    x_real = x_real.to(self.device)</span><br><span class="line">    c_celeba_list = self.create_labels(c_org, self.c_dim, <span class="string">'CelebA'</span>, self.selected_attrs)</span><br><span class="line">    c_rafd_list = self.create_labels(c_org, self.c2_dim, <span class="string">'RaFD'</span>)</span><br><span class="line">    zero_celeba = torch.zeros(x_real.size(<span class="number">0</span>), self.c_dim).to(self.device)            <span class="comment"># Zero vector for CelebA.</span></span><br><span class="line">    zero_rafd = torch.zeros(x_real.size(<span class="number">0</span>), self.c2_dim).to(self.device)             <span class="comment"># Zero vector for RaFD.</span></span><br><span class="line">    mask_celeba = self.label2onehot(torch.zeros(x_real.size(<span class="number">0</span>)), <span class="number">2</span>).to(self.device)  <span class="comment"># Mask vector: [1, 0].</span></span><br><span class="line">    mask_rafd = self.label2onehot(torch.ones(x_real.size(<span class="number">0</span>)), <span class="number">2</span>).to(self.device)     <span class="comment"># Mask vector: [0, 1].</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Translate images.</span></span><br><span class="line">    x_fake_list = [x_real]</span><br><span class="line">    <span class="keyword">for</span> c_celeba <span class="keyword">in</span> c_celeba_list:</span><br><span class="line">        c_trg = torch.cat([c_celeba, zero_rafd, mask_celeba], dim=<span class="number">1</span>)</span><br><span class="line">        x_fake_list.append(self.G(x_real, c_trg))</span><br><span class="line">    <span class="keyword">for</span> c_rafd <span class="keyword">in</span> c_rafd_list:</span><br><span class="line">        c_trg = torch.cat([zero_celeba, c_rafd, mask_rafd], dim=<span class="number">1</span>)</span><br><span class="line">        x_fake_list.append(self.G(x_real, c_trg))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save the translated images.</span></span><br><span class="line">    x_concat = torch.cat(x_fake_list, dim=<span class="number">3</span>)</span><br><span class="line">    result_path = os.path.join(self.result_dir, <span class="string">'&#123;&#125;-images.jpg'</span>.format(i+<span class="number">1</span>))</span><br><span class="line">    save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">'Saved real and fake images into &#123;&#125;...'</span>.format(result_path))</span><br></pre></td></tr></table></figure><h1 id="6-其他"><a href="#6-其他" class="headerlink" title="6. 其他"></a>6. 其他</h1><p>通过代码,我们可以猜出,对于starGAN,每一个domain都是一个二值属性,这些属性可以是互相排斥的,例如头发颜色,可以是不互相排斥的,并且这里和CycleGAN还是有一些区别的,CycleGAN的domain是数据集,source domain 和 target domain是风马牛不相及的,source domain和target domain有自己的风格,例如map数据集,是没有真值的,有的只是深度网络提取出的特征和70*70patchGAN.但是starGAN中,生成的图片和原始图片是一个数据集的,并且这两张图片不是要求风格一样,感觉这能应用到person-reid中也是神奇.</p><p>在图片真假的分类损失中，之前的GAN都是使用True和False来表示，这次换了一个新公式直接mean，还有点难理解。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;因为在person-reid论文&lt;a href=&quot;http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HHL&lt;/a&gt;中涉及到了starGAN，所以做一个StarGAN的阅读记录，并比较与CycleGAN的区别。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo&lt;/p&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="starGAN" scheme="http://yoursite.com/tags/starGAN/"/>
    
  </entry>
  
  <entry>
    <title>CASN</title>
    <link href="http://yoursite.com/2018/12/17/CASN/"/>
    <id>http://yoursite.com/2018/12/17/CASN/</id>
    <published>2018-12-17T03:42:25.000Z</published>
    <updated>2018-12-18T07:59:48.469Z</updated>
    
    <content type="html"><![CDATA[<p>CASN: <a href="https://arxiv.org/abs/1811.07487" target="_blank" rel="noopener">Re-Identification with Consistent Attentive Siamese Networks</a></p><p>Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J. Radke</p><a id="more"></a><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>这篇论文；</p><ul><li>采用了Grad-CAM作为attention机制</li><li>attention consistency: 采用Siamese-Net来使同一个人的attention位置是一样的</li></ul><h1 id="2-The-Consistent-Attentive-Siamese-Network"><a href="#2-The-Consistent-Attentive-Siamese-Network" class="headerlink" title="2. The Consistent Attentive Siamese Network"></a>2. The Consistent Attentive Siamese Network</h1><p>整体网络架构如图所示：</p><p><img src="./CASN/CASN1.png" alt="CASN的整体网络架构"><br><img src="/2018/12/17/CASN/CASN1.png" title="CASN的整体网络架构"></p><ul><li>整体网络架构以IDE为基准网络，分为两部分:</li><li><ul><li>Identification Module</li></ul></li><li><ul><li>Siamese Module</li></ul></li><li>Identification 和 Siamese 的特征提取网络共享，不同的只是fc层</li></ul><h2 id="2-1-The-Identification-Module"><a href="#2-1-The-Identification-Module" class="headerlink" title="2.1 The Identification Module"></a>2.1 The Identification Module</h2><p>通过<a href="https://tjjtjjtjj.github.io/2018/12/14/Grad-CAM/#more" target="_blank" rel="noopener">Grad-CAM</a>的学习，已经知道了Grad-CAM的作用。</p><p><strong>Identification loss:</strong></p><script type="math/tex; mode=display">L_{ide}=-\sum_{n=1}^N log \frac{exp(y_{c_n})}{\sum_j exp(y_j)}</script><p>Identification loss 更偏向于不同行人之间的判别信息。</p><p><strong>Identification attention loss:</strong></p><script type="math/tex; mode=display">L_{ia}=\overline{y_{c_n}}</script><p>其中，给定一张图片$I_n$和类别$c_n$，Grad-CAM得到attention map $M_n$，做归一化操作，令$\Sigma(M_n)=sigmoid(\alpha(M_n-\beta))$，从而得到去掉attention区域的新图片$\overline{I_n}=I_n*(1-\Sigma(M_n))$，$\overline{y_{c_n}}$是$\overline{I_n}$的预测值。</p><p>Identification attention loss 更偏向于行人的全部信息。</p><p>我的理解是$\overline{I_n}$中尽可能包含少的ID信息，所以预测出是$c_n$的概率更小，得到的attention区域尽可能地包括全部信息。</p><p><strong>两种loss的效果对比图</strong><br><img src="./CASN/CASN2.png" alt="两种loss的效果对比图"><br><img src="/2018/12/17/CASN/CASN2.png" title="两种loss的效果对比图"></p><h2 id="2-2-The-Siamese-Module"><a href="#2-2-The-Siamese-Module" class="headerlink" title="2.2 The Siamese Module"></a>2.2 The Siamese Module</h2><p>Siamese Module 结构图<br><img src="./CASN/CASN3.png" alt="Siamese Module 结构图"><br><img src="/2018/12/17/CASN/CASN3.png" title="Siamese Module 结构图"></p><p><strong>Siamese loss</strong></p><script type="math/tex; mode=display">L_{bce}=-\sum_p log(\frac{exp(z_{c_p})}{exp(z_0)+exp(z_1)})</script><p><strong>Siamese attention loss</strong></p><script type="math/tex; mode=display">\alpha_i=\begin{cases}1, \mbox{if} f_i^->0 \\0, otherwise\end{cases}</script><script type="math/tex; mode=display">s_1=<\alpha, f_1>, s_2=<\alpha, f_2>(dot products)</script><script type="math/tex; mode=display">\alpha_1^k=GAP(\frac{\partial s_1}{\partial A_1}), \alpha_2^k=GAP(\frac{\partial s_2}{\partial A_2}) \tag{5}</script><script type="math/tex; mode=display">M_1=ReLU(\sum_k \alpha_1^k A_1^k),M_2=ReLU(\sum_k \alpha_2^k A_2^k)</script><script type="math/tex; mode=display">L_{sa}=L_{bce}+\alpha \parallel M_{m1}^{resize}-M_{m2}^{resize} \parallel _2</script><script type="math/tex; mode=display">\alpha=0.2</script><p>其中，$M_{m1}$是$M_1$中超过阈值t的元素，$M_{m1}^{resize}$是$M_{m1}$resize成相同大小的元素，主要是为了解决对齐问题。</p><p>通过与作者的沟通，作者认为$s_1$表示了$f_1$中对 BCE prediction 有用的信息。但是我没有见过这么表示对预测有用的信息的方法，之前只见过通过类别进行反向传播的(Grad-CAM)，但是作者这么坚持，说明应该是有效的。</p><blockquote><ol><li>Sorry I didn’t get your first question. By finding neurons in fi which are larger than zero, we find features in fi which have positive influence on BCE prediction.</li><li>s1 is not I1’s class. We here call s1 the importance score, which collect scores for every neuron which contributes to BCE prediction.</li></ol></blockquote><h2 id="2-3-Overall-Design-of-the-CASN"><a href="#2-3-Overall-Design-of-the-CASN" class="headerlink" title="2.3 Overall Design of the CASN"></a>2.3 Overall Design of the CASN</h2><p>CASN的整体架构<br><img src="./CASN/CASN4.png" alt="CASN的整体架构"><br><img src="/2018/12/17/CASN/CASN4.png" title="CASN的整体架构"></p><p><strong>The overall loss</strong></p><script type="math/tex; mode=display">L=L_{ide}+\lambda_1 L_{ia}+\lambda_2 L_{sa}</script><h3 id="3-Experiments-and-Results"><a href="#3-Experiments-and-Results" class="headerlink" title="3. Experiments and Results"></a>3. Experiments and Results</h3><h4 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h4><ul><li>input: 288x144</li><li>SGD: momentum=0.9</li><li>lr=0.03</li><li>epoch=40</li><li>lr decay=0.1 after 30</li><li>baseline: IDE and PCB( input: 384x128)</li><li>batch=16</li><li>test:  we send the query and gallery as pair inputs to obtain attention maps $\parallel M_{m1}^{resize}-M_{m2}^{resize} \parallel _2$</li></ul><h4 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h4><p><img src="./CASN/CASN5.png" alt="Results"><br><img src="/2018/12/17/CASN/CASN5.png" title="Results"></p><p><img src="./CASN/CASN6.png" alt="Ablation Study"><br><img src="/2018/12/17/CASN/CASN6.png" title="Ablation Study"></p><p>通过 Ablation Study , 对比CASN(IDE)、PCB，可以看出IA或者SA的作用和简单地分成6块达到的效果是类似的，这是不是说明了这种attention机制没有很大的作用，或者说分成6块就已经是一种很好的attention机制。</p><p>另外，+IA、+SA、CASN的对比，感觉IA或者SA一种机制就已经足够了，两者达到的效果是一样的，只使用一种就可以了。</p><h3 id="4-Others"><a href="#4-Others" class="headerlink" title="4. Others"></a>4. Others</h3><p>这篇论文不懂的地方：</p><ul><li>$L_{ia}$为什么可以直接这么写，不需要经过softmax之类的，或者不应该是每类的概率差不多么，</li><li>$L_{ia}$还是经过相同的网络得到的吗？反向求导要怎么写？</li><li>根据Grad-CAM的以类别反向求导，方程5给我的感觉更像是$f$的特征和作为输入图片的分类预测值，合理性站不住脚。</li><li>在测试时，需要每次输入一对图片，是不是太慢了。</li><li>如果实验结果可以复现的话，那么IA我觉得还是很有用的，解释性也强。</li></ul><p>参考：</p><ul><li>对于 Identification attention loss 的流程，需要参考<a href="https://tjjtjjtjj.github.io/2018/12/14/Grad-CAM/#more" target="_blank" rel="noopener">Grad-CAM</a>和<a href="https://arxiv.org/pdf/1802.10171.pdf" target="_blank" rel="noopener">GAIN</a>, GAIN也可以在<a href="https://tjjtjjtjj.github.io/2018/12/14/Grad-CAM/#more" target="_blank" rel="noopener">Grad-CAM</a>中找到详解。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CASN: &lt;a href=&quot;https://arxiv.org/abs/1811.07487&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Re-Identification with Consistent Attentive Siamese Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J. Radke&lt;/p&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="person-reid" scheme="http://yoursite.com/tags/person-reid/"/>
    
      <category term="Grad-CAM" scheme="http://yoursite.com/tags/Grad-CAM/"/>
    
  </entry>
  
  <entry>
    <title>Grad-CAM</title>
    <link href="http://yoursite.com/2018/12/14/Grad-CAM/"/>
    <id>http://yoursite.com/2018/12/14/Grad-CAM/</id>
    <published>2018-12-14T01:50:53.000Z</published>
    <updated>2018-12-20T14:37:49.738Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1610.02391.pdf" target="_blank" rel="noopener">Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization</a></p><a id="more"></a><p>Gradient-weighted Class Activation Mapping</p><p>Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam</p><p>code-torch: <a href="https://github.com/ramprs/grad-cam/" target="_blank" rel="noopener">https://github.com/ramprs/grad-cam/</a></p><p>code-pytorch: <a href="https://github.com/jacobgil/pytorch-grad-cam" target="_blank" rel="noopener">https://github.com/jacobgil/pytorch-grad-cam</a></p><p>code-keras: <a href="https://github.com/jacobgil/keras-grad-cam" target="_blank" rel="noopener">https://github.com/jacobgil/keras-grad-cam</a></p><p>参考链接：</p><ul><li><a href="https://www.jianshu.com/p/b2f7efe10ad8" target="_blank" rel="noopener">https://www.jianshu.com/p/b2f7efe10ad8</a></li><li><a href="https://www.jianshu.com/p/1d7b5c4ecb93" target="_blank" rel="noopener">https://www.jianshu.com/p/1d7b5c4ecb93</a></li><li><a href="http://spytensor.com/index.php/archives/20/" target="_blank" rel="noopener">http://spytensor.com/index.php/archives/20/</a></li><li><a href="https://www.jianshu.com/p/e4fa1348e5bc" target="_blank" rel="noopener">https://www.jianshu.com/p/e4fa1348e5bc</a></li></ul><h1 id="1-声明"><a href="#1-声明" class="headerlink" title="1. 声明"></a>1. 声明</h1><p>最近在看到一篇person-reid的文章<a href="http://arxiv.org/abs/1811.07487v1" target="_blank" rel="noopener">Re-Identification with Consistent Attentive Siamese Networks</a>，其中涉及到了Grad-CAM，所以简单学习一下Grad-CAM，但不作为重点。</p><p>2018-12-18</p><p>在使用的过程中，发现自己写的这篇博客不太容易让自己一目了然，所以根据<a href="https://www.jianshu.com/p/e4fa1348e5bc" target="_blank" rel="noopener">链接</a>来进行更新一次。</p><p>2018-12-20</p><p>在重新看论文person-reid的过程中，发现其中涉及到的网络架构师GAIN，所以补充GAIN的说明及其代码。此时，这篇的重点变成了Grad-CAM和GAIN。</p><h1 id="2-前言"><a href="#2-前言" class="headerlink" title="2. 前言"></a>2. 前言</h1><p>对于深度模型的可解释性和可视化，现在已经研究出了一些方法，包括不限于<strong>Deconvolution, Guided-Backpropagation, CAM, Grad-CAM</strong>.</p><p>其中 Deconvolution 和 Guided-Backpropagation 得到更偏向于细粒度图， CAM 和 Grad-CAM 得到更偏向于类区分的热力图。</p><p>各种可视化方法及其效果图参见：<a href="https://github.com/utkuozbulak/pytorch-cnn-visualizations" target="_blank" rel="noopener">https://github.com/utkuozbulak/pytorch-cnn-visualizations</a></p><p>参考链接: <a href="https://blog.csdn.net/geek_wh2016/article/details/81060315" target="_blank" rel="noopener">https://blog.csdn.net/geek_wh2016/article/details/81060315</a></p><h2 id="2-1-Deconvolution"><a href="#2-1-Deconvolution" class="headerlink" title="2.1 Deconvolution"></a>2.1 <a href="https://arxiv.org/abs/1311.2901v3" target="_blank" rel="noopener">Deconvolution</a></h2><p><a href="https://arxiv.org/abs/1311.2901v3" target="_blank" rel="noopener">Deconvolution: Visualizing and Understanding Convolutional Networks</a></p><p>code: <a href="https://github.com/kvfrans/feature-visualization" target="_blank" rel="noopener">https://github.com/kvfrans/feature-visualization</a></p><p><strong>综述</strong>: 这篇paper是CNN可视化的开山之作(由 Lecun 得意门生 Matthew Zeiler 发表于2013年)，主要解决了两个问题:</p><ol><li>why CNN perform so well?</li><li>how CNN might be improved?</li></ol><p><strong>实现</strong>: 对于CNN，可视化就是整个过程的逆过程，即Unpooling+ReLU+Deconv.</p><ol><li>Unpooling: 记录max-pool的位置，即Switches表格，unpooling时，最大值放回该位置，其他位置放0.</li><li>ReLU: 继续使用ReLU.</li><li>Deconv: 使用相同卷积核的转置作为新的卷积核，对特征进行卷积.</li></ol><p>参考链接：</p><ul><li><a href="http://kvfrans.com/visualizing-features-from-a-convolutional-neural-network/" target="_blank" rel="noopener">http://kvfrans.com/visualizing-features-from-a-convolutional-neural-network/</a></li><li><a href="https://blog.csdn.net/Julialove102123/article/details/78292807" target="_blank" rel="noopener">https://blog.csdn.net/Julialove102123/article/details/78292807</a></li><li><a href="https://blog.csdn.net/gm_margin/article/details/79335140" target="_blank" rel="noopener">https://blog.csdn.net/gm_margin/article/details/79335140</a></li></ul><p><img src="./Grad-CAM/Deconvolution1.png" alt="Deconvolution的结构图"><br><img src="/2018/12/14/Grad-CAM/Deconvolution1.png" title="Deconvolution的结构图"></p><h2 id="2-2-Guided-Backpropagation"><a href="#2-2-Guided-Backpropagation" class="headerlink" title="2.2 Guided-Backpropagation"></a>2.2 Guided-Backpropagation</h2><p><a href="http://arxiv.org/abs/1412.6806" target="_blank" rel="noopener">Guided-Backpropagation: Striving for Simplicity: The All Convolutional Net</a></p><p>反向传播、反卷积和导向反向传播都是反向传播，区别在于经过 ReLU 层时对梯度的不同处理策略。在这篇<a href="https://arxiv.org/pdf/1412.6806.pdf" target="_blank" rel="noopener">论文</a>中有详细的解释。</p><p>计算公式如下：</p><p><img src="./Grad-CAM/Guided-Backpropagation.jpg" alt="反向传播梯度的选择"><br><img src="/2018/12/14/Grad-CAM/Guided-Backpropagation.jpg" title="反向传播梯度的选择"></p><p>文中提出使用 stride convolution 代替 pooling，研究这种结构的有效性。</p><p>效果显示如下：</p><p><img src="./Grad-CAM/Guided-Backpropagation2.png" alt="三种反向传播的效果"><br><img src="/2018/12/14/Grad-CAM/Guided-Backpropagation2.png" title="三种反向传播的效果"></p><p>可以看出 Guided-Backpropagation 主要提取对分类有效果的特征，但是与是哪类没有关系。</p><h2 id="2-3-CAM"><a href="#2-3-CAM" class="headerlink" title="2.3 CAM"></a>2.3 CAM</h2><p><a href="https://arxiv.org/abs/1512.04150" target="_blank" rel="noopener">CAM: Learning Deep Features for Discriminative Localization</a></p><p><strong>综述</strong>：论文重新审视了global average pooling (GAP) 的有效性，并详细阐述了GAP如何使得CNN有优异的目标定位能力。<br><strong>介绍</strong>：摒弃FC，使用GAP。<br><strong>实现</strong>：</p><p><img src="./Grad-CAM/CAM.png" alt="CAM的网络结构"><br><img src="/2018/12/14/Grad-CAM/CAM.png" title="CAM的网络结构"></p><p><img src="./Grad-CAM/CAM2.png" alt="结果图"><br><img src="/2018/12/14/Grad-CAM/CAM2.png" title="结果图"></p><h2 id="2-4-Grad-CAM"><a href="#2-4-Grad-CAM" class="headerlink" title="2.4 Grad-CAM"></a>2.4 Grad-CAM</h2><p>Grad-CAM 是CAM的改进版， 与 CAM 的不同点在于前者的特征加权系数是反向传播得到的，后者的特征加权系数是分类器的权重。</p><p>Grad-CAM 可以加载到任意网络架构上，而不需要修改网络架构，而CAM必须使用GAP。</p><p>下面会详细介绍。</p><h2 id="2-5-GAIN"><a href="#2-5-GAIN" class="headerlink" title="2.5 GAIN"></a>2.5 GAIN</h2><p><a href="https://arxiv.org/pdf/1802.10171.pdf" target="_blank" rel="noopener">GAIN: Tell Me Where to Look: Guided Attention Inference Network</a></p><p>code: <a href="https://github.com/alokwhitewolf/Guided-Attention-Inference-Network" target="_blank" rel="noopener">https://github.com/alokwhitewolf/Guided-Attention-Inference-Network</a></p><p>GAIN 是 Grad-CAM 的改进版，Grad-CAM只能可视化解释现有的网络结构的结果，却不能指导网络架构，GAIN可以指导网络修正错误，关注更正确的位置。</p><p><strong>问题</strong>：在船识别的过程中，网络的关注点是水面而不是船。</p><p><img src="./Grad-CAM/GAIN1.png" alt="注意力错误"><br><img src="/2018/12/14/Grad-CAM/GAIN1.png" title="注意力错误"></p><p><strong>实现</strong>：通过最小化遮挡图像的物体来训练。</p><p><img src="./Grad-CAM/GAIN2.png" alt="GAIN的网络架构"><br><img src="/2018/12/14/Grad-CAM/GAIN2.png" title="GAIN的网络架构"></p><p>整体网络架构中，只有一个网络，两个处理流都是共享同一个网络。</p><p><strong>公式</strong>：损失函数</p><script type="math/tex; mode=display">w_{l,k}^c=GAP(\frac{\partial s^c}{\partial f_{l,k}})</script><script type="math/tex; mode=display">A^c=ReLU(conv(f_l, w^c))</script><script type="math/tex; mode=display">T(A^c)=\frac{1}{1+exp(-\omega(A^c-\sigma))}</script><script type="math/tex; mode=display">I^{*c}=I-(T(A^c)\odot I)</script><script type="math/tex; mode=display">L_{am}=\frac{1}{n}\sum_cs^c(I^{*c})</script><script type="math/tex; mode=display">L_{self}=L_{cl}+\alpha L_{am}</script><script type="math/tex; mode=display">\alpha=1</script><p><strong>扩展</strong>：如果有额外的监督真值，比如分割，那么可以进行扩充网络</p><p><img src="./Grad-CAM/GAIN3.png" alt="扩充的网络架构"><br><img src="/2018/12/14/Grad-CAM/GAIN3.png" title="扩充的网络架构"></p><script type="math/tex; mode=display">L_e=\frac{1}{n}\sum_c (A^c-H^c)^2</script><script type="math/tex; mode=display">L_{ext}=L_{cl}+\alpha L_{am}+\omega L_e</script><script type="math/tex; mode=display">\alpha=1, \omega=10</script><h3 id="2-5-1-GAIN-code"><a href="#2-5-1-GAIN-code" class="headerlink" title="2.5.1 GAIN-code"></a>2.5.1 GAIN-code</h3><p><strong>第一步</strong>：训练分类网络</p><p><strong>FCN</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line">self.conv1_1 = L.Convolution2D(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv1_2 = L.Convolution2D(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">self.conv2_1 = L.Convolution2D(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv2_2 = L.Convolution2D(<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">self.conv3_1 = L.Convolution2D(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv3_2 = L.Convolution2D(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv3_3 = L.Convolution2D(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">self.conv4_1 = L.Convolution2D(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv4_2 = L.Convolution2D(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv4_3 = L.Convolution2D(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">self.conv5_1 = L.Convolution2D(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv5_2 = L.Convolution2D(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv5_3 = L.Convolution2D(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">self.fc6 = L.Convolution2D(<span class="number">512</span>, <span class="number">4096</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">self.fc7 = L.Convolution2D(<span class="number">4096</span>, <span class="number">4096</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">self.score_fr = L.Convolution2D(<span class="number">4096</span>, n_class, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segment</span><span class="params">(self, x, t=None)</span>:</span></span><br><span class="line">    <span class="comment"># conv1</span></span><br><span class="line">    self.conv1_1.pad = (<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line">    h = F.relu(self.conv1_1(x))</span><br><span class="line">    conv1_1 = h</span><br><span class="line">    h = F.relu(self.conv1_2(conv1_1))</span><br><span class="line">    conv1_2 = h</span><br><span class="line">    h = _max_pooling_2d(conv1_2)</span><br><span class="line">    pool1 = h  <span class="comment"># 1/2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># conv2</span></span><br><span class="line">    h = F.relu(self.conv2_1(pool1))</span><br><span class="line">    conv2_1 = h</span><br><span class="line">    h = F.relu(self.conv2_2(conv2_1))</span><br><span class="line">    conv2_2 = h</span><br><span class="line">    h = _max_pooling_2d(conv2_2)</span><br><span class="line">    pool2 = h  <span class="comment"># 1/4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># conv3</span></span><br><span class="line">    h = F.relu(self.conv3_1(pool2))</span><br><span class="line">    conv3_1 = h</span><br><span class="line">    h = F.relu(self.conv3_2(conv3_1))</span><br><span class="line">    conv3_2 = h</span><br><span class="line">    h = F.relu(self.conv3_3(conv3_2))</span><br><span class="line">    conv3_3 = h</span><br><span class="line">    h = _max_pooling_2d(conv3_3)</span><br><span class="line">    pool3 = h  <span class="comment"># 1/8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># conv4</span></span><br><span class="line">    h = F.relu(self.conv4_1(pool3))</span><br><span class="line">    h = F.relu(self.conv4_2(h))</span><br><span class="line">    h = F.relu(self.conv4_3(h))</span><br><span class="line">    h = _max_pooling_2d(h)</span><br><span class="line">    pool4 = h  <span class="comment"># 1/16</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># conv5</span></span><br><span class="line">    h = F.relu(self.conv5_1(pool4))</span><br><span class="line">    h = F.relu(self.conv5_2(h))</span><br><span class="line">    h = F.relu(self.conv5_3(h))</span><br><span class="line">    h = _max_pooling_2d(h)</span><br><span class="line">    pool5 = h  <span class="comment"># 1/32</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># fc6</span></span><br><span class="line">    h = F.relu(self.fc6(pool5))</span><br><span class="line">    h = F.dropout(h, ratio=<span class="number">.5</span>)</span><br><span class="line">    fc6 = h  <span class="comment"># 1/32</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># fc7</span></span><br><span class="line">    h = F.relu(self.fc7(fc6))</span><br><span class="line">    h = F.dropout(h, ratio=<span class="number">.5</span>)</span><br><span class="line">    fc7 = h  <span class="comment"># 1/32</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># score_fr</span></span><br><span class="line">    h = self.score_fr(fc7)</span><br><span class="line">    score_fr = h  <span class="comment"># 1/32</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># score_pool3</span></span><br><span class="line">    h = self.score_pool3(pool3)</span><br><span class="line">    score_pool3 = h  <span class="comment"># 1/8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># score_pool4</span></span><br><span class="line">    h = self.score_pool4(pool4)</span><br><span class="line">    score_pool4 = h  <span class="comment"># 1/16</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># upscore2</span></span><br><span class="line">    h = self.upscore2(score_fr)</span><br><span class="line">    upscore2 = h  <span class="comment"># 1/16</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># score_pool4c</span></span><br><span class="line">    h = score_pool4[:, :,</span><br><span class="line">                    <span class="number">5</span>:<span class="number">5</span> + upscore2.shape[<span class="number">2</span>],</span><br><span class="line">                    <span class="number">5</span>:<span class="number">5</span> + upscore2.shape[<span class="number">3</span>]]</span><br><span class="line">    score_pool4c = h  <span class="comment"># 1/16</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># fuse_pool4</span></span><br><span class="line">    h = upscore2 + score_pool4c</span><br><span class="line">    fuse_pool4 = h  <span class="comment"># 1/16</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># upscore_pool4</span></span><br><span class="line">    h = self.upscore_pool4(fuse_pool4)</span><br><span class="line">    upscore_pool4 = h  <span class="comment"># 1/8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># score_pool4c</span></span><br><span class="line">    h = score_pool3[:, :,</span><br><span class="line">                    <span class="number">9</span>:<span class="number">9</span> + upscore_pool4.shape[<span class="number">2</span>],</span><br><span class="line">                    <span class="number">9</span>:<span class="number">9</span> + upscore_pool4.shape[<span class="number">3</span>]]</span><br><span class="line">    score_pool3c = h  <span class="comment"># 1/8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># fuse_pool3</span></span><br><span class="line">    h = upscore_pool4 + score_pool3c</span><br><span class="line">    fuse_pool3 = h  <span class="comment"># 1/8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># upscore8</span></span><br><span class="line">    h = self.upscore8(fuse_pool3)</span><br><span class="line">    upscore8 = h  <span class="comment"># 1/1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># score</span></span><br><span class="line">    h = upscore8[:, :, <span class="number">31</span>:<span class="number">31</span> + x.shape[<span class="number">2</span>], <span class="number">31</span>:<span class="number">31</span> + x.shape[<span class="number">3</span>]]</span><br><span class="line">    score = h  <span class="comment"># 1/1</span></span><br><span class="line">    self.score = score</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> t <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> chainer.config.train</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    loss = F.softmax_cross_entropy(score, t, normalize=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">if</span> np.isnan(float(loss.data)):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Loss is nan.'</span>)</span><br><span class="line">    chainer.report(&#123;<span class="string">'loss'</span>: loss&#125;, self)</span><br><span class="line">    self.conv1_1.pad = (<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p><strong>FCN-v1.0</strong>: 普通的分类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">self.conv1_1 = L.Convolution2D(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv1_2 = L.Convolution2D(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">self.conv2_1 = L.Convolution2D(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv2_2 = L.Convolution2D(<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">self.conv3_1 = L.Convolution2D(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv3_2 = L.Convolution2D(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv3_3 = L.Convolution2D(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">self.conv4_1 = L.Convolution2D(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv4_2 = L.Convolution2D(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv4_3 = L.Convolution2D(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">self.conv5_1 = L.Convolution2D(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv5_2 = L.Convolution2D(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">self.conv5_3 = L.Convolution2D(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">self.fc6_cl = L.Linear(<span class="number">512</span>, <span class="number">4096</span>)</span><br><span class="line">self.fc7_cl = L.Linear(<span class="number">4096</span>, <span class="number">4096</span>)</span><br><span class="line">self.score_cl = L.Linear(<span class="number">4096</span>, n_class<span class="number">-1</span>) <span class="comment"># Disregard 0 class for classification</span></span><br><span class="line"></span><br><span class="line">self.final_conv_layer = <span class="string">'conv5_3'</span></span><br><span class="line">self.grad_target_layer = <span class="string">'prob'</span></span><br><span class="line">self.freezed_layers = [<span class="string">'fc6_cl'</span>, <span class="string">'fc7_cl'</span>, <span class="string">'score_cl'</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(self, x, is_training=True)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> chainer.using_config(<span class="string">'train'</span>,<span class="keyword">False</span>):</span><br><span class="line">        <span class="comment"># conv1</span></span><br><span class="line">        h = F.relu(self.conv1_1(x))</span><br><span class="line">        h = F.relu(self.conv1_2(h))</span><br><span class="line">        h = _max_pooling_2d(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv2</span></span><br><span class="line">        h = F.relu(self.conv2_1(h))</span><br><span class="line">        h = F.relu(self.conv2_2(h))</span><br><span class="line">        h = _max_pooling_2d(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv3</span></span><br><span class="line">        h = F.relu(self.conv3_1(h))</span><br><span class="line">        h = F.relu(self.conv3_2(h))</span><br><span class="line">        h = F.relu(self.conv3_3(h))</span><br><span class="line">        h = _max_pooling_2d(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv4</span></span><br><span class="line">        h = F.relu(self.conv4_1(h))</span><br><span class="line">        h = F.relu(self.conv4_2(h))</span><br><span class="line">        h = F.relu(self.conv4_3(h))</span><br><span class="line">        h = _max_pooling_2d(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv5</span></span><br><span class="line">        h = F.relu(self.conv5_1(h))</span><br><span class="line">        h = F.relu(self.conv5_2(h))</span><br><span class="line">        h = F.relu(self.conv5_3(h))</span><br><span class="line">        h = _max_pooling_2d(h)</span><br><span class="line">        h = _average_pooling_2d(h)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> chainer.using_config(<span class="string">'train'</span>,is_training):</span><br><span class="line">        h = F.relu(F.dropout(self.fc6_cl(h), <span class="number">.5</span>))</span><br><span class="line">        h = F.relu(F.dropout(self.fc7_cl(h), <span class="number">.5</span>))</span><br><span class="line">        h = self.score_cl(h)</span><br><span class="line">        <span class="comment"># 1*20</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure><p><strong>loss</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cl_output=classify(image)</span></span><br><span class="line"><span class="comment"># cl_output: 1*20</span></span><br><span class="line"><span class="comment"># target: 1*20 ~ [0,1] 1表示有这个label，0表示没有这个label，用的是多分类损失函数，且类别之间不排斥，类似对每个类别做二元分类。</span></span><br><span class="line">loss = F.sigmoid_cross_entropy(cl_output, target, normalize=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><strong>第二步</strong>：训练GAIN</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">self.GAIN_functions = collections.OrderedDict([</span><br><span class="line">    (<span class="string">'conv1_1'</span>, [self.conv1_1, F.relu]),</span><br><span class="line">    (<span class="string">'conv1_2'</span>, [self.conv1_2, F.relu]),</span><br><span class="line">    (<span class="string">'pool1'</span>, [_max_pooling_2d]),</span><br><span class="line"></span><br><span class="line">    (<span class="string">'conv2_1'</span>, [self.conv2_1, F.relu]),</span><br><span class="line">    (<span class="string">'conv2_2'</span>, [self.conv2_2, F.relu]),</span><br><span class="line">    (<span class="string">'pool2'</span>, [_max_pooling_2d]),</span><br><span class="line"></span><br><span class="line">    (<span class="string">'conv3_1'</span>, [self.conv3_1, F.relu]),</span><br><span class="line">    (<span class="string">'conv3_2'</span>, [self.conv3_2, F.relu]),</span><br><span class="line">    (<span class="string">'conv3_3'</span>, [self.conv3_3, F.relu]),</span><br><span class="line">    (<span class="string">'pool3'</span>, [_max_pooling_2d]),</span><br><span class="line"></span><br><span class="line">    (<span class="string">'conv4_1'</span>, [self.conv4_1, F.relu]),</span><br><span class="line">    (<span class="string">'conv4_2'</span>, [self.conv4_2, F.relu]),</span><br><span class="line">    (<span class="string">'conv4_3'</span>, [self.conv4_3, F.relu]),</span><br><span class="line">    (<span class="string">'pool4'</span>, [_max_pooling_2d]),</span><br><span class="line"></span><br><span class="line">    (<span class="string">'conv5_1'</span>, [self.conv5_1, F.relu]),</span><br><span class="line">    (<span class="string">'conv5_2'</span>, [self.conv5_2, F.relu]),</span><br><span class="line">    (<span class="string">'conv5_3'</span>, [self.conv5_3, F.relu]),</span><br><span class="line">    (<span class="string">'pool5'</span>, [_max_pooling_2d]),</span><br><span class="line"></span><br><span class="line">    (<span class="string">'avg_pool'</span>, [_average_pooling_2d]),</span><br><span class="line"></span><br><span class="line">    (<span class="string">'fc6_cl'</span>, [self.fc6_cl, F.relu]),</span><br><span class="line">    (<span class="string">'fc7_cl'</span>, [self.fc7_cl, F.relu]),</span><br><span class="line">    (<span class="string">'prob'</span>, [self.score_cl, F.sigmoid])</span><br><span class="line"></span><br><span class="line">])</span><br><span class="line">self.final_conv_layer = <span class="string">'conv5_3'</span></span><br><span class="line">self.grad_target_layer = <span class="string">'prob'</span></span><br><span class="line">self.freezed_layers = [<span class="string">'fc6_cl'</span>, <span class="string">'fc7_cl'</span>, <span class="string">'score_cl'</span>]</span><br></pre></td></tr></table></figure><p>通过分类结果获取mask：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stream_cl</span><span class="params">(self, inp, label=None)</span>:</span></span><br><span class="line">    <span class="comment"># h: 1*3*281*500</span></span><br><span class="line">    <span class="comment"># label: 真值 array([0, 14])</span></span><br><span class="line">    <span class="comment"># return: gcam: mask,size(1,3,281,500); h: size(1,20), class_id: 一个数字</span></span><br><span class="line">    h = inp</span><br><span class="line">    <span class="keyword">for</span> key, funcs <span class="keyword">in</span> self.GAIN_functions.items():</span><br><span class="line">        <span class="keyword">for</span> func <span class="keyword">in</span> funcs:</span><br><span class="line">            h = func(h)</span><br><span class="line">        <span class="keyword">if</span> key == self.final_conv_layer:</span><br><span class="line">            activation = h</span><br><span class="line">        <span class="keyword">if</span> key == self.grad_target_layer:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    gcam, class_id = self.get_gcam(h, activation, (inp.shape[<span class="number">-2</span>], inp.shape[<span class="number">-1</span>]), label=label)</span><br><span class="line">    <span class="keyword">return</span> gcam, h, class_id</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gcam</span><span class="params">(self, end_output, activations, shape, label)</span>:</span></span><br><span class="line">    <span class="comment"># end_output: size: 1,20</span></span><br><span class="line">    <span class="comment"># activations: size: 1*512*18*32</span></span><br><span class="line">    <span class="comment"># shape: (281, 500)</span></span><br><span class="line">    <span class="comment"># label: 真值</span></span><br><span class="line">    self.cleargrads()</span><br><span class="line">    class_id = self.set_init_grad(end_output, label)</span><br><span class="line">    end_output.backward(retain_grad=<span class="keyword">True</span>)</span><br><span class="line">    grad = activations.grad_var</span><br><span class="line">    grad = F.average_pooling_2d(grad, (grad.shape[<span class="number">-2</span>], grad.shape[<span class="number">-1</span>]), <span class="number">1</span>)</span><br><span class="line">    grad = F.expand_dims(F.reshape(grad, (grad.shape[<span class="number">0</span>]*grad.shape[<span class="number">1</span>], grad.shape[<span class="number">2</span>], grad.shape[<span class="number">3</span>])), <span class="number">0</span>)</span><br><span class="line">    weights = activations</span><br><span class="line">    weights = F.expand_dims(F.reshape(weights, (weights.shape[<span class="number">0</span>]*weights.shape[<span class="number">1</span>], weights.shape[<span class="number">2</span>], weights.shape[<span class="number">3</span>])), <span class="number">0</span>)</span><br><span class="line">    gcam = F.resize_images(F.relu(F.convolution_2d(weights, grad, <span class="keyword">None</span>, <span class="number">1</span>, <span class="number">0</span>)), shape)</span><br><span class="line">    <span class="keyword">return</span> gcam, class_id</span><br></pre></td></tr></table></figure><p>$L_{cl}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcam, cl_scores, class_id = self._optimizers[<span class="string">'main'</span>].target.stream_cl(image, gt_labels)</span><br><span class="line">cl_loss = F.sigmoid_cross_entropy(cl_scores, target, normalize=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>$L_{am}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">masked_output = self._optimizers[<span class="string">'main'</span>].target.stream_am(masked_image)</span><br><span class="line">masked_output = F.sigmoid(masked_output)</span><br><span class="line">am_loss = masked_output[<span class="number">0</span>][class_id][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>备注: $L_{cl}$和$L_{am}$完全共享网络。</p><h1 id="3-Introduction"><a href="#3-Introduction" class="headerlink" title="3. Introduction"></a>3. Introduction</h1><p>可视化即应该满足高分辨率，也应该满足类别定位能力。</p><p>示例图像</p><p><img src="./Grad-CAM/Grad-CAM.png" alt="示例图像"><br><img src="/2018/12/14/Grad-CAM/Grad-CAM.png" title="示例图像"></p><h1 id="4-Approach"><a href="#4-Approach" class="headerlink" title="4. Approach"></a>4. Approach</h1><p><strong>CAM</strong><br>在CAM中，一个全连接层替换成GAP，参见上面的CAM图，则分类任务可以表示成</p><script type="math/tex; mode=display">y^c = \sum_k w_k^c \frac{1}{Z} \sum_i \sum_j A_{ij}^k</script><p>其中，$y^c$表示分类结果，$w_k^c$表示第k个特征图(kxhxw)对第c个类别的贡献，即全连接层的系数，$Z$表示特征图的大小，$Z=h\cdot w$，$A_{ij}^k$表示第k个特征图。</p><p>则 CAM 的输出图表示为：</p><script type="math/tex; mode=display">L_{CAM}^c=\sum_k w_k^c A^k</script><p><strong>Grad-CAM</strong><br>在Grad-CAM中，权重系数是通过反向传播得到的。</p><script type="math/tex; mode=display">\alpha_k^c=\frac{1}{Z}\sum_i \sum_j \frac{\partial y^c}{\partial A_{ij}^k}</script><p>则Grad-CAM的输出图表示为：</p><script type="math/tex; mode=display">L_{Grad-CAM}^c=ReLU(\sum_k \alpha_k^c A^k)</script><p>可以证明，Grad-CAM与CAM的公式是同一个公式的变形。</p><p><strong>Guided Grad-CAM</strong><br>Guided Grad-CAM 是将 Grad-CAM 与 Guided Backpropagation 得到的输出图简单地点乘，从而获得类区分定位的高分辨率细节图。</p><p><img src="./Grad-CAM/Guided-Grad-CAM.png" alt="Guided-Grad-CAM的网络架构"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM.png" title="Guided-Grad-CAM的网络架构"></p><p>同时作者还分析了CNN分类错误的样本。<br><img src="./Grad-CAM/Guided-Grad-CAM2.png" alt="Guided-Grad-CAM分类错误的样本"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM2.png" title="Guided-Grad-CAM分类错误的样本"></p><h1 id="5-代码"><a href="#5-代码" class="headerlink" title="5. 代码"></a>5. 代码</h1><p>对于<a href="https://github.com/TJJTJJTJJ/pytorch-grad-cam" target="_blank" rel="noopener">pytorch代码</a>进行分析</p><h2 id="5-1-Grad-CAM"><a href="#5-1-Grad-CAM" class="headerlink" title="5.1 Grad-CAM"></a>5.1 Grad-CAM</h2><p><strong>计算Grad-CAM</strong>:</p><p>反向传播：先计算出当前图片的分类结果output(size:1*5)(假设共5类)，选出最优分类结果，假设是第2类，然后令one-hot=[0,1,0,0,0]，求得sum-one-hot=<one-hot,output>得到一个数字，然后反向传播。</one-hot,output></p><p>cam: (H,W), ~(0,1)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" Class for extracting activations and</span></span><br><span class="line"><span class="string">    registering gradients from targetted intermediate layers """</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    调用方式:</span></span><br><span class="line"><span class="string">    outputs, x = FeatureExtractor(x)</span></span><br><span class="line"><span class="string">    gradients = FeatureExtractor.gradients</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, target_layers)</span>:</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.target_layers = target_layers</span><br><span class="line">        self.gradients = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_gradient</span><span class="params">(self, grad)</span>:</span></span><br><span class="line">        self.gradients.append(grad)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param x: N*C*H*W, a picture</span></span><br><span class="line"><span class="string">        :return: outputs: list, activations layer output, A in equation</span></span><br><span class="line"><span class="string">                 x : feature map, feature of model output n*c*h*w</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        outputs = []</span><br><span class="line">        self.gradients = []</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.model._modules.items():</span><br><span class="line">            x = module(x)</span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">in</span> self.target_layers:</span><br><span class="line">                x.register_hook(self.save_gradient)</span><br><span class="line">                outputs += [x]</span><br><span class="line">        <span class="keyword">return</span> outputs, x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelOutputs</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" Class for making a forward pass, and getting:</span></span><br><span class="line"><span class="string">    1. The network output.</span></span><br><span class="line"><span class="string">    2. Activations from intermeddiate targetted layers.</span></span><br><span class="line"><span class="string">    3. Gradients from intermeddiate targetted layers. """</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    调用方式：</span></span><br><span class="line"><span class="string">    target_activations, output = ModelOutputs(x)</span></span><br><span class="line"><span class="string">    gradients = ModelOutputs.get_gradients()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, target_layers)</span>:</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.feature_extractor = FeatureExtractor(self.model.features, target_layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_gradients</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.feature_extractor.gradients</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param x: N*C*H*W, a picture</span></span><br><span class="line"><span class="string">        :return: target_activations: list, activations layer output, A in equation</span></span><br><span class="line"><span class="string">                 output : tensor, classification output. N*c. y in equation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        target_activations, output = self.feature_extractor(x)</span><br><span class="line">        output = output.view(output.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        output = self.model.classifier(output)</span><br><span class="line">        <span class="keyword">return</span> target_activations, output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradCam</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Class for making Grad-CAM, and getting:</span></span><br><span class="line"><span class="string">    1. Grad-CAM</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    调用方式：</span></span><br><span class="line"><span class="string">    mask=GradCam(input)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, target_layer_names, use_cuda)</span>:</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.model.eval()</span><br><span class="line">        self.cuda = use_cuda</span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            self.model = model.cuda()</span><br><span class="line"></span><br><span class="line">        self.extractor = ModelOutputs(self.model, target_layer_names)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.model(input)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, input, index=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param input: N*C*H*W, a picture</span></span><br><span class="line"><span class="string">        :param index: int</span></span><br><span class="line"><span class="string">        :return: cam: N*C*H*W ~(0,1) L_&#123;Grad-CAM&#125;^c</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            features, output = self.extractor(input.cuda())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            features, output = self.extractor(input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> index == <span class="keyword">None</span>:</span><br><span class="line">            index = np.argmax(output.cpu().data.numpy())</span><br><span class="line"></span><br><span class="line">        one_hot = np.zeros((<span class="number">1</span>, output.size()[<span class="number">-1</span>]), dtype=np.float32)</span><br><span class="line">        one_hot[<span class="number">0</span>][index] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># After test, requires_grad could be False</span></span><br><span class="line">        one_hot = Variable(torch.from_numpy(one_hot), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            one_hot = torch.sum(one_hot.cuda() * output)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            one_hot = torch.sum(one_hot * output)</span><br><span class="line"></span><br><span class="line">        self.model.features.zero_grad()</span><br><span class="line">        self.model.classifier.zero_grad()</span><br><span class="line">        <span class="comment"># After test, requires_grad could be False</span></span><br><span class="line">        one_hot.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        grads_val = self.extractor.get_gradients()[<span class="number">-1</span>].cpu().data.numpy()</span><br><span class="line"></span><br><span class="line">        target = features[<span class="number">-1</span>]</span><br><span class="line">        target = target.cpu().data.numpy()[<span class="number">0</span>, :]</span><br><span class="line"></span><br><span class="line">        weights = np.mean(grads_val, axis=(<span class="number">2</span>, <span class="number">3</span>))[<span class="number">0</span>, :]</span><br><span class="line">        cam = np.zeros(target.shape[<span class="number">1</span>:], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, w <span class="keyword">in</span> enumerate(weights):</span><br><span class="line">            cam += w * target[i, :, :]</span><br><span class="line"></span><br><span class="line">        cam = np.maximum(cam, <span class="number">0</span>)</span><br><span class="line">        cam = cv2.resize(cam, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">        cam = cam - np.min(cam)</span><br><span class="line">        cam = cam / np.max(cam)</span><br><span class="line">        <span class="keyword">return</span> cam</span><br></pre></td></tr></table></figure><p><strong>显示Grad-CAM</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_cam_on_image</span><span class="params">(img, mask)</span>:</span></span><br><span class="line">    heatmap = cv2.applyColorMap(np.uint8(<span class="number">255</span> * mask), cv2.COLORMAP_JET)</span><br><span class="line">    heatmap = np.float32(heatmap) / <span class="number">255</span></span><br><span class="line">    cam = heatmap + np.float32(img)</span><br><span class="line">    cam = cam / np.max(cam)</span><br><span class="line">    cv2.imwrite(<span class="string">"cam.jpg"</span>, np.uint8(<span class="number">255</span> * cam))</span><br></pre></td></tr></table></figure><h2 id="5-2-GuidedBackpropReLUModel"><a href="#5-2-GuidedBackpropReLUModel" class="headerlink" title="5.2 GuidedBackpropReLUModel"></a>5.2 GuidedBackpropReLUModel</h2><p>gb: (C,H,W) 任意值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GuidedBackpropReLU</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        positive_mask = (input &gt; <span class="number">0</span>).type_as(input)</span><br><span class="line">        output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)</span><br><span class="line">        self.save_for_backward(input, output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        input, output = self.saved_tensors</span><br><span class="line">        grad_input = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        positive_mask_1 = (input &gt; <span class="number">0</span>).type_as(grad_output)</span><br><span class="line">        positive_mask_2 = (grad_output &gt; <span class="number">0</span>).type_as(grad_output)</span><br><span class="line">        grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input),</span><br><span class="line">                                   torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output,</span><br><span class="line">                                                 positive_mask_1), positive_mask_2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GuidedBackpropReLUModel</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model, use_cuda)</span>:</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.model.eval()</span><br><span class="line">        self.cuda = use_cuda</span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            self.model = model.cuda()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># replace ReLU with GuidedBackpropReLU</span></span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> self.model.features._modules.items():</span><br><span class="line">            <span class="keyword">if</span> module.__class__.__name__ == <span class="string">'ReLU'</span>:</span><br><span class="line">                self.model.features._modules[idx] = GuidedBackpropReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.model(input)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, input, index=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            output = self.forward(input.cuda())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output = self.forward(input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> index == <span class="keyword">None</span>:</span><br><span class="line">            index = np.argmax(output.cpu().data.numpy())</span><br><span class="line"></span><br><span class="line">        one_hot = np.zeros((<span class="number">1</span>, output.size()[<span class="number">-1</span>]), dtype=np.float32)</span><br><span class="line">        one_hot[<span class="number">0</span>][index] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># After test, requires_grad could be False</span></span><br><span class="line">        one_hot = Variable(torch.from_numpy(one_hot), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            one_hot = torch.sum(one_hot.cuda() * output)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            one_hot = torch.sum(one_hot * output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.model.features.zero_grad()</span></span><br><span class="line">        <span class="comment"># self.model.classifier.zero_grad()</span></span><br><span class="line">        one_hot.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        output = input.grad.cpu().data.numpy()</span><br><span class="line">        output = output[<span class="number">0</span>, :, :, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h2 id="5-3-Guided-Grad-CAM"><a href="#5-3-Guided-Grad-CAM" class="headerlink" title="5.3 Guided Grad-CAM"></a>5.3 Guided Grad-CAM</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cam_mask = np.zeros(gb.shape)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, gb.shape[<span class="number">0</span>]):</span><br><span class="line">    cam_mask[i, :, :] = mask</span><br><span class="line"></span><br><span class="line">cam_gb = np.multiply(cam_mask, gb)</span><br><span class="line">utils.save_image(torch.from_numpy(cam_gb), <span class="string">'cam_gb.jpg'</span>)</span><br></pre></td></tr></table></figure><h1 id="6-效果显示"><a href="#6-效果显示" class="headerlink" title="6. 效果显示"></a>6. 效果显示</h1><p>原图</p><p><img src="./Grad-CAM/Guided-Grad-CAM3.png" alt="原图"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM3.png" title="原图"></p><p>Grad-CAM</p><p><img src="./Grad-CAM/Guided-Grad-CAM4.jpg" alt="Grad-CAM"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM4.jpg" title="Grad-CAM"></p><p>Guided-Backpropagation</p><p><img src="./Grad-CAM/Guided-Grad-CAM5.jpg" alt="Guided-Backpropagation"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM5.jpg" title="Guided-Backpropagation"></p><p>Guided Grad-CAM</p><p><img src="./Grad-CAM/Guided-Grad-CAM6.jpg" alt="Guided Grad-CAM"><br><img src="/2018/12/14/Grad-CAM/Guided-Grad-CAM6.jpg" title="Guided Grad-CAM"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1610.02391.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="deep learning" scheme="http://yoursite.com/categories/deep-learning/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="CNN" scheme="http://yoursite.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>RotationNet-paper</title>
    <link href="http://yoursite.com/2018/12/11/RotationNet-paper/"/>
    <id>http://yoursite.com/2018/12/11/RotationNet-paper/</id>
    <published>2018-12-11T08:18:45.000Z</published>
    <updated>2018-12-13T06:26:35.932Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-RotationNet-paper"><a href="#1-RotationNet-paper" class="headerlink" title="1. RotationNet-paper"></a>1. RotationNet-paper</h1><a id="more"></a><p>paper: <a href="https://arxiv.org/pdf/1603.06208.pdf" target="_blank" rel="noopener">RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints(CVPR2018)</a></p><p><a href="https://github.com/kanezaki" target="_blank" rel="noopener">Asako Kanezaki</a>, Yasuyuki Matsushita2, and Yoshifumi Nishida.</p><p>Asako Kanezaki 是日本东京研究所专门研究3D的一个老师。</p><p>code-pytorch: <a href="https://github.com/kanezaki/pytorch-rotationnet" target="_blank" rel="noopener">https://github.com/kanezaki/pytorch-rotationnet</a><br>code-caffe: <a href="https://github.com/kanezaki/rotationnet" target="_blank" rel="noopener">https://github.com/kanezaki/rotationnet</a><br>project: <a href="https://kanezaki.github.io/rotationnet/" target="_blank" rel="noopener">https://kanezaki.github.io/rotationnet/</a></p><p>MIMO data: <a href="https://github.com/kanezaki/MIRO" target="_blank" rel="noopener">https://github.com/kanezaki/MIRO</a></p><p>作者是使用caffe版本提交的论文，我也只是看了看代码，作为理解作者论文的辅助，实际没有跑过代码。</p><p>这篇博客以代码和论文混杂，因为是借助代码理解论文的，又因为不主要做这个方向，所以并没有在意精度什么的。</p><h2 id="1-1-出发点"><a href="#1-1-出发点" class="headerlink" title="1.1 出发点"></a>1.1 出发点</h2><p>作者不仅想要预测出图片的类别label，还想预测出图片的view-points.</p><p>我觉得作者的创新点在于对view的状态顺序编码成view-rotation，限定了view的取值空间，使预测的结果变成了哪种view-rotaion的view准确率高。</p><p>因为在一般情况下，想到的是直接预测view，而不是view-rotation.</p><h2 id="1-2-网络架构"><a href="#1-2-网络架构" class="headerlink" title="1.2 网络架构"></a>1.2 网络架构</h2><p><img src="./RotationNet-paper/RotationNet.png" alt="RotationNet的网络架构"><br><img src="/2018/12/11/RotationNet-paper/RotationNet.png" title="RotationNet的网络架构"></p><h3 id="1-2-1-训练过程："><a href="#1-2-1-训练过程：" class="headerlink" title="1.2.1 训练过程："></a>1.2.1 训练过程：</h3><p>以MIRO数据集、case=3为例，nview=160，vcand=(16, 160),view-rotation=16，num-classes=12.</p><p>这里的view-roration，我的理解是view的排列方式，但是还是不太顺。</p><p>输入的图片个数batch-size必须是nview的倍数，以输入一个样本的160个角度的图片为例，即batch-size=160，nsamp=1，不影响后续的分析，因为每个样本没有任何关系。</p><p>输出是output=batch-size x ((num_classes+1) * nview)= 160 x (13 x 160). 可以理解成对每一个图片，输出网络架构的一行，可以理解成160张图片在160个view下属于13个类的概率。</p><p><img src="./RotationNet-paper/RotationNet1.png" alt="模型的输出"><br><img src="/2018/12/11/RotationNet-paper/RotationNet1.png" title="模型的输出"></p><p>预测view rotation: 利用下面的预测view公式，求log并相减得到output- = (160x160) x 12 x 1，可以理解成一个矩阵： (160x160) x 12，每行表示当前图片当前view下的属于各类的概率，第一个160行表示第一张图片在160个view下的概率分布，第二个160行表示第二张图片在160个view下的概率分布。scores = (16 x 12 x 1)。scores可以理解成当前view-rotation下这160张图片一起属于各类的概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">output = model(input_var)</span><br><span class="line">num_classes = int( output.size( <span class="number">1</span> ) / nview ) - <span class="number">1</span></span><br><span class="line">output = output.view( <span class="number">-1</span>, num_classes + <span class="number">1</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute scores and decide target labels</span></span><br><span class="line">output_ = torch.nn.functional.log_softmax( output )</span><br><span class="line">output_ = output_[ :, :<span class="number">-1</span> ] - torch.t( output_[ :, <span class="number">-1</span> ].repeat( <span class="number">1</span>, output_.size(<span class="number">1</span>)<span class="number">-1</span> ).view( output_.size(<span class="number">1</span>)<span class="number">-1</span>, <span class="number">-1</span> ) )</span><br><span class="line">output_ = output_.view( <span class="number">-1</span>, nview * nview, num_classes )</span><br><span class="line">output_ = output_.data.cpu().numpy()</span><br><span class="line">output_ = output_.transpose( <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span> )</span><br><span class="line"></span><br><span class="line">scores = np.zeros( ( vcand.shape[ <span class="number">0</span> ], num_classes, nsamp ) )</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(vcand.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(vcand.shape[<span class="number">1</span>]):</span><br><span class="line">        scores[ j ] = scores[ j ] + output_[ vcand[ j ][ k ] * nview + k ]</span><br></pre></td></tr></table></figure><p>生成动态真值target-：已知这160张图片的真值target[ n * nview ]，假设是第3类，j-max表示第j-max个view-rotation下，预测为第3类的概率最大，继而生成动态真值target-=(target.size(0) x nview)=160 x 160=25600，可以理解成160张图片在160个view下的真值，在j-max个view-rotation对应的view设置为类别3，其余的设置为13.</p><p><img src="./RotationNet-paper/RotationNet2.png" alt="生成动态真值"><br><img src="/2018/12/11/RotationNet-paper/RotationNet2.png" title="生成动态真值"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">target_ = torch.LongTensor( target.size(<span class="number">0</span>) * nview )</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range( nsamp ):</span><br><span class="line">    j_max = np.argmax( scores[ :, target[ n * nview ], n ] )</span><br><span class="line">    <span class="comment"># assign target labels</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(vcand.shape[<span class="number">1</span>]):</span><br><span class="line">        target_[ n * nview * nview + vcand[ j_max ][ k ] * nview + k ] = target[ n * nview ]</span><br></pre></td></tr></table></figure><h3 id="1-2-2-验证过程"><a href="#1-2-2-验证过程" class="headerlink" title="1.2.2 验证过程"></a>1.2.2 验证过程</h3><p>与训练类似，可以得到output- = (160x160) x 12 x 1，可以理解成一个矩阵： (160x160) x 12，每行表示当前图片当前view下的属于各类的概率，第一个160行表示第一张图片在160个view下的概率分布，第二个160行表示第二张图片在160个view下的概率分布。scores = (16 x 12 x 1)。scores可以理解成每个view-rotation下这160张图片一起属于各类的概率。</p><p>j-max表示在第j-max个view-rotation下，scores可以找到最大概率。</p><p>output[n] 表示每连续的160张图片一起属于某类的概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">output = model(input_var)</span><br><span class="line">num_classes = int( output.size( <span class="number">1</span> ) / nview ) - <span class="number">1</span></span><br><span class="line">output = output.view( <span class="number">-1</span>, num_classes + <span class="number">1</span> )</span><br><span class="line">output = torch.nn.functional.log_softmax( output )</span><br><span class="line">output = output[ :, :<span class="number">-1</span> ] - torch.t( output[ :, <span class="number">-1</span> ].repeat( <span class="number">1</span>, output.size(<span class="number">1</span>)<span class="number">-1</span> ).view( output.size(<span class="number">1</span>)<span class="number">-1</span>, <span class="number">-1</span> ) )</span><br><span class="line">output = output.view( <span class="number">-1</span>, nview * nview, num_classes )</span><br><span class="line"></span><br><span class="line"><span class="comment"># measure accuracy and record loss</span></span><br><span class="line">prec1, prec5 = my_accuracy(output.data, target, topk=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># def my_accuracy</span></span><br><span class="line"></span><br><span class="line">target = target[<span class="number">0</span>:<span class="number">-1</span>:nview]</span><br><span class="line">batch_size = target.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">num_classes = output_.size(<span class="number">2</span>)</span><br><span class="line">output_ = output_.cpu().numpy()</span><br><span class="line">output_ = output_.transpose( <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span> )</span><br><span class="line">scores = np.zeros( ( vcand.shape[ <span class="number">0</span> ], num_classes, batch_size ) )</span><br><span class="line">output = torch.zeros( ( batch_size, num_classes ) )</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(vcand.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(vcand.shape[<span class="number">1</span>]):</span><br><span class="line">        scores[ j ] = scores[ j ] + output_[ vcand[ j ][ k ] * nview + k ]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range( batch_size ):</span><br><span class="line">    j_max = int( np.argmax( scores[ :, :, n ] ) / scores.shape[ <span class="number">1</span> ] )</span><br><span class="line">    output[ n ] = torch.FloatTensor( scores[ j_max, :, n ] )</span><br><span class="line">output = output.cuda()</span><br></pre></td></tr></table></figure><h3 id="1-2-3-测试过程"><a href="#1-2-3-测试过程" class="headerlink" title="1.2.3 测试过程"></a>1.2.3 测试过程</h3><p>因为caffe代码没有看懂，所以根据作者的论文和代码猜一下，当输入的图片没有160张，假设只有100张图片，那么又该怎么做？</p><p>模型的输出是output=batch-size x ((num_classes+1) * nview)= 100 x (13 x 160)，那么怎么求scores？</p><p>求scores是需要全部view的信息的。这里不会了，尽管已经给出了公式，但是公式只能算出output-，没有score，不会了。</p><p>坐等作者回复。</p><blockquote><p>Our method is available only when the relative poses of test images are known. For example, if you captured three images where the second image is 22.5 degrees forward from the first image and the third image is 45 degrees forward from the second image, then the images should be indexed as (0, 1, 3). Then you would get 3x160x12 output values. An easy way to proceed is to create a 160x160x12 “output2” which has zero values, and then insert the output values as “output2[0] = output[0]; output2[1] = output[1]; output2[3] = output[2];”. (In our paper, we used LSD-SLAM to calculate relative poses of test images.)</p></blockquote><p>根据作者的回复，不难理解，给定的测试图片是需要预先知道测试图片序列的相对位置的。</p><h2 id="1-3-预测公式"><a href="#1-3-预测公式" class="headerlink" title="1.3 预测公式"></a>1.3 预测公式</h2><script type="math/tex; mode=display">\max_{(v_i)\_{i=1}^M}\prod_{i=1}^M(\log p_{v_{i},y}^{(i)}-\log p_{v_{i},N+1}^{(i)})</script>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-RotationNet-paper&quot;&gt;&lt;a href=&quot;#1-RotationNet-paper&quot; class=&quot;headerlink&quot; title=&quot;1. RotationNet-paper&quot;&gt;&lt;/a&gt;1. RotationNet-paper&lt;/h1&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="RotationNet" scheme="http://yoursite.com/tags/RotationNet/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-2.0</title>
    <link href="http://yoursite.com/2018/12/10/pytorch-2.0/"/>
    <id>http://yoursite.com/2018/12/10/pytorch-2.0/</id>
    <published>2018-12-10T07:18:25.000Z</published>
    <updated>2018-12-23T14:47:18.685Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-eval"><a href="#1-eval" class="headerlink" title="1. eval"></a>1. eval</h1><a id="more"></a><p>pytorch 的eval()只是改变一些模块的状态，并不影响backward过程。</p><p><a href="https://blog.csdn.net/u012436149/article/details/78281553" target="_blank" rel="noopener">https://blog.csdn.net/u012436149/article/details/78281553</a></p><p><a href="https://www.jianshu.com/p/6cb1fd785540" target="_blank" rel="noopener">https://www.jianshu.com/p/6cb1fd785540</a></p><h1 id="2-扩展-torch-autograd"><a href="#2-扩展-torch-autograd" class="headerlink" title="2. 扩展 torch.autograd"></a>2. 扩展 torch.autograd</h1><p>参考链接：</p><p><a href="https://zhuanlan.zhihu.com/p/27783097" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27783097</a></p><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/notes/extending/" target="_blank" rel="noopener">https://pytorch-cn.readthedocs.io/zh/latest/notes/extending/</a></p><p><a href="https://blog.csdn.net/Hungryof/article/details/78346304" target="_blank" rel="noopener">https://blog.csdn.net/Hungryof/article/details/78346304</a></p><h2 id="2-1-Function和Module的差异和应用场景"><a href="#2-1-Function和Module的差异和应用场景" class="headerlink" title="2.1 Function和Module的差异和应用场景"></a>2.1 Function和Module的差异和应用场景</h2><ul><li>Function一般只定义一个操作，因为无法保存参数，例如激活函数、pool操作等；</li><li>Module可以保存参数，适合于定义一个层</li><li>Function需要定义三个方法：__init__, forward, backward;</li><li>Module: 需要定义__init__和forward，而backward由自动求导构成；</li><li>可以简单地认为，Module就是由Function构成的</li><li>Module中不仅包括了Function，还包括了对应的参数、其他变量等等。</li><li>Function: __init__ (optional) - 如果你的operation包含非Variable参数，那么就将其作为__init__的参数传入到operation中。例如：AddConstant Function加一个常数，Transpose Function需要指定哪两个维度需要交换。如果你的operation不需要额外的参数，你可以忽略__init__。</li></ul><h2 id="2-2-定义一个ReLU函数"><a href="#2-2-定义一个ReLU函数" class="headerlink" title="2.2 定义一个ReLU函数"></a>2.2 定义一个ReLU函数</h2><h3 id="2-2-1-定义一个ReLU函数"><a href="#2-2-1-定义一个ReLU函数" class="headerlink" title="2.2.1 定义一个ReLU函数"></a>2.2.1 定义一个ReLU函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">        <span class="comment"># 输入和输出相互对应</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_)</span>:</span></span><br><span class="line">        <span class="comment"># 在forward中，需要定义MyReLU这个运算的forward计算过程</span></span><br><span class="line">        <span class="comment"># 同时可以保存任何在后向传播中需要使用的变量值</span></span><br><span class="line">        self.save_for_backward(input_)         <span class="comment"># 将输入保存起来，在backward时使用</span></span><br><span class="line">        output = input_.clamp(min=<span class="number">0</span>)               <span class="comment"># relu就是截断负数，让所有负数等于0</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        <span class="comment"># 根据BP算法的推导（链式法则），dloss / dx = (dloss / doutput) * (doutput / dx)</span></span><br><span class="line">        <span class="comment"># dloss / doutput就是输入的参数grad_output、</span></span><br><span class="line">        <span class="comment"># 因此只需求relu的导数，在乘以grad_output</span></span><br><span class="line">        input_, = self.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span>                <span class="comment"># 上诉计算的结果就是左式。即ReLU在反向传播中可以看做一个通道选择函数，所有未达到阈值（激活值&lt;0）的单元的梯度都为0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GuidedBackpropReLU</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        positive_mask = (input &gt; <span class="number">0</span>).type_as(input)</span><br><span class="line">        output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)</span><br><span class="line">        self.save_for_backward(input, output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        input, output = self.saved_tensors</span><br><span class="line">        grad_input = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        positive_mask_1 = (input &gt; <span class="number">0</span>).type_as(grad_output)</span><br><span class="line">        positive_mask_2 = (grad_output &gt; <span class="number">0</span>).type_as(grad_output)</span><br><span class="line">        grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input),</span><br><span class="line">                                   torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output,</span><br><span class="line">                                                 positive_mask_1), positive_mask_2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inherit from Function</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># bias is an optional argument</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, weight, bias=None)</span>:</span></span><br><span class="line">        self.save_for_backward(input, weight, bias)</span><br><span class="line">        output = input.mm(weight.t())</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            output += bias.unsqueeze(<span class="number">0</span>).expand_as(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This function has only a single output, so it gets only one gradient</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        <span class="comment"># This is a pattern that is very convenient - at the top of backward</span></span><br><span class="line">        <span class="comment"># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span></span><br><span class="line">        <span class="comment"># None. Thanks to the fact that additional trailing Nones are</span></span><br><span class="line">        <span class="comment"># ignored, the return statement is simple even when the function has</span></span><br><span class="line">        <span class="comment"># optional inputs.</span></span><br><span class="line">        input, weight, bias = self.saved_tensors</span><br><span class="line">        grad_input = grad_weight = grad_bias = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># These needs_input_grad checks are optional and there only to</span></span><br><span class="line">        <span class="comment"># improve efficiency. If you want to make your code simpler, you can</span></span><br><span class="line">        <span class="comment"># skip them. Returning gradients for inputs that don't require it is</span></span><br><span class="line">        <span class="comment"># not an error.</span></span><br><span class="line">        <span class="keyword">if</span> self.needs_input_grad[<span class="number">0</span>]:</span><br><span class="line">            grad_input = grad_output.mm(weight)</span><br><span class="line">        <span class="keyword">if</span> self.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            grad_weight = grad_output.t().mm(input)</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.needs_input_grad[<span class="number">2</span>]:</span><br><span class="line">            grad_bias = grad_output.sum(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grad_input, grad_weight, grad_bias</span><br></pre></td></tr></table></figure><h3 id="2-2-2-Wrap成一个ReLU函数"><a href="#2-2-2-Wrap成一个ReLU函数" class="headerlink" title="2.2.2 Wrap成一个ReLU函数"></a>2.2.2 Wrap成一个ReLU函数</h3><p>可以把刚刚自定义的ReLU类封装成一个函数，方便调用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(input_)</span>:</span></span><br><span class="line">    <span class="comment"># MyReLU()是创建一个MyReLU对象，</span></span><br><span class="line">    <span class="comment"># Function类利用了Python __call__操作，使得可以直接使用对象调用__call__制定的方法</span></span><br><span class="line">    <span class="comment"># __call__指定的方法是forward，因此下面这句MyReLU（）（input_）相当于</span></span><br><span class="line">    <span class="comment"># return MyReLU().forward(input_)</span></span><br><span class="line">    <span class="keyword">return</span> MyReLU()(input_)</span><br><span class="line">input_ = Variable(torch.linspace(<span class="number">-3</span>, <span class="number">3</span>, steps=<span class="number">5</span>))</span><br><span class="line"><span class="keyword">print</span> input_</span><br><span class="line"><span class="keyword">print</span> relu(input_)</span><br></pre></td></tr></table></figure><p>在某些版本中，Function类的使用方法是.apply方法，其和__call__方法类似。</p><p>在某些版本中，forward和backward方法需要是@staticmethod</p><p>在某些版本中，forward中Variable参数会自动转为tensor，backward参数是Variable</p><p>在某些版本中，forward和backward中self会写成ctx</p><p>save_for_backward只能传入Variable或是Tensor的变量，如果是其他类型的，可以用self.xyz = xyz，使其在backward中可以用。</p><h3 id="2-2-3-检查backward是否正确"><a href="#2-2-3-检查backward是否正确" class="headerlink" title="2.2.3 检查backward是否正确"></a>2.2.3 检查backward是否正确</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> gradcheck</span><br><span class="line"></span><br><span class="line"><span class="comment"># gradchek takes a tuple of tensor as input, check if your gradient</span></span><br><span class="line"><span class="comment"># evaluated with these tensors are close enough to numerical</span></span><br><span class="line"><span class="comment"># approximations and returns True if they all verify this condition.</span></span><br><span class="line">input = (Variable(torch.randn(<span class="number">20</span>,<span class="number">20</span>).double(), requires_grad=<span class="keyword">True</span>),)</span><br><span class="line">test = gradcheck.gradcheck(Linear(), input, eps=<span class="number">1e-6</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line">print(test)</span><br></pre></td></tr></table></figure><p>备注：</p><ul><li>自动求导竟然是在backward的操作中创建的图！暂时没懂。</li></ul><h1 id="3-扩展torch-nn"><a href="#3-扩展torch-nn" class="headerlink" title="3. 扩展torch.nn"></a>3. 扩展torch.nn</h1><p>nn包括两种街扩-modules和functional版本。其中，functional版本已经在上面介绍过，不再重复，这一小节主要介绍modules。</p><p>扩展module需要预先实现一个执行计算和梯度的Function。</p><p>扩展module需要实现两个方法：</p><ul><li><p>__init__ (optional) - 输入参数，例如kernel sizes, numbers of features, 等等。同时初始化 parameters和buffers。</p></li><li><p>forward() - 实例化一个执行operation的Function，使用它执行operation。和functional wrapper(上面实现的那个简单的wrapper)十分类似。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_features, output_features, bias=True)</span>:</span></span><br><span class="line">        self.input_features = input_features</span><br><span class="line">        self.output_features = output_features</span><br><span class="line"></span><br><span class="line">        <span class="comment"># nn.Parameter is a special kind of Variable, that will get</span></span><br><span class="line">        <span class="comment"># automatically registered as Module's parameter once it's assigned</span></span><br><span class="line">        <span class="comment"># as an attribute. Parameters and buffers need to be registered, or</span></span><br><span class="line">        <span class="comment"># they won't appear in .parameters() (doesn't apply to buffers), and</span></span><br><span class="line">        <span class="comment"># won't be converted when e.g. .cuda() is called. You can use</span></span><br><span class="line">        <span class="comment"># .register_buffer() to register buffers.</span></span><br><span class="line">        <span class="comment"># nn.Parameters can never be volatile and, different than Variables,</span></span><br><span class="line">        <span class="comment"># they require gradients by default.</span></span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(input_features, output_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(output_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># You should always register all possible parameters, but the</span></span><br><span class="line">            <span class="comment"># optional ones can be None if you want.</span></span><br><span class="line">            self.register_parameter(<span class="string">'bias'</span>, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Not a very smart way to initialize weights</span></span><br><span class="line">        self.weight.data.uniform_(<span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.bias.data.uniform_(<span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="comment"># See the autograd section for explanation of what happens here.</span></span><br><span class="line">        <span class="keyword">return</span> Linear()(input, self.weight, self.bias)</span><br><span class="line">        <span class="comment">#注意这个Linear是之前实现过的Linear</span></span><br></pre></td></tr></table></figure><ul><li>parameter和buffer是一类特殊的Variable。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">self._parameters = OrderedDict()</span><br><span class="line">self._modules = OrderedDict()</span><br><span class="line">self._buffers = OrderedDict()</span><br><span class="line">self._backward_hooks = OrderedDict()</span><br><span class="line">self._forward_hooks = OrderedDict()</span><br><span class="line">self.training = <span class="keyword">True</span></span><br></pre></td></tr></table></figure><ul><li>_parameters: 是一个字典，保存的有：nn.Parameter()和register_parameter()，而nn.Linear(3,4)的parameter不会保存于此。</li><li>_modules:nn.Linear(3,4)会保存于此。</li><li>_buffers:register_buffer()会保存于此.</li><li>named_parameters():会保存_parameters和_modules的parameters。</li><li>__getattr__()方法会从_modules,_parameters和_buffers这三个字典中获取。</li></ul><h1 id="3-retain-graph-and-create-graph"><a href="#3-retain-graph-and-create-graph" class="headerlink" title="3. retain_graph and create_graph"></a>3. retain_graph and create_graph</h1><h2 id="3-1-retain-graph"><a href="#3-1-retain-graph" class="headerlink" title="3.1 retain_graph"></a>3.1 retain_graph</h2><p>retain_graph 是保存前向传播过程中的buffer。是因为如果有两个backward,那么需要retain_graph=true,因为默认一个前向对应一个反向,用于实现多次反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">b = torch.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = torch.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x</span><br><span class="line">z = y + b</span><br><span class="line">z2 = y-b</span><br><span class="line">z.backward()</span><br><span class="line">w.grad</span><br><span class="line"><span class="number">1</span></span><br><span class="line">z2.backward()</span><br><span class="line">RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=<span class="keyword">True</span> when calling backward the first time.</span><br><span class="line">改为</span><br><span class="line">z.backward(retain_graph=<span class="keyword">True</span>) <span class="comment"># 只能保证当前backward后的buffer不被清除.</span></span><br><span class="line">w.grad</span><br><span class="line"><span class="number">1</span></span><br><span class="line">z2.backward()</span><br></pre></td></tr></table></figure><p>发现一个神奇的操作,如果y=w+x,那么不需要retain_graph 也可以多次反向传播,说明问题出现在y上.当y中存储了buffer w或者x后,就需要retain_graph,反之,不需要也能多次反向传播.<br>但是现在找不到这个buffer保存在哪里</p><h2 id="3-2-create-graph"><a href="#3-2-create-graph" class="headerlink" title="3.2 create_graph"></a>3.2 create_graph</h2><p>create_graph用于实现高阶导数,相当于对于叶子节点的导数dx与x是否建立计算图,即是否需要保存f’(x)作为一个计算图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.tensor([<span class="number">5.0</span>], requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">grad_x = torch.autograd.grad(y, x, create_graph=<span class="keyword">True</span>)</span><br><span class="line">grad_x[<span class="number">0</span>].grad_fn</span><br><span class="line">&lt;MulBackward1 at <span class="number">0x7feb27c9e160</span>&gt;</span><br><span class="line">grad_grad_x = torch.autograd.grad(grad_x[<span class="number">0</span>], x)</span><br><span class="line">print(grad_grad_x[<span class="number">0</span>].grad_fn)</span><br><span class="line">grad_grad_grad_x = torch.autograd.grad(grad_grad_x, x)</span><br><span class="line">RuntimeError: element <span class="number">0</span> of tensors does <span class="keyword">not</span> require grad <span class="keyword">and</span> does <span class="keyword">not</span> have a grad_fn</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-eval&quot;&gt;&lt;a href=&quot;#1-eval&quot; class=&quot;headerlink&quot; title=&quot;1. eval&quot;&gt;&lt;/a&gt;1. eval&lt;/h1&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>SPGAN-tensorflow</title>
    <link href="http://yoursite.com/2018/12/10/SPGAN-tensorflow/"/>
    <id>http://yoursite.com/2018/12/10/SPGAN-tensorflow/</id>
    <published>2018-12-10T03:07:15.000Z</published>
    <updated>2018-12-10T07:20:51.859Z</updated>
    
    <content type="html"><![CDATA[<p>在阅读SPGAN代码源码的过程中，学习到的关于tensorflow的一些知识。<br><a id="more"></a></p><h1 id="1-tf-ConfigProto"><a href="#1-tf-ConfigProto" class="headerlink" title="1. tf.ConfigProto"></a>1. tf.ConfigProto</h1><p>参考链接:<a href="https://blog.csdn.net/dcrmg/article/details/79091941" target="_blank" rel="noopener">https://blog.csdn.net/dcrmg/article/details/79091941</a></p><p>tf.ConfigProto用于对sessison会话的参数配置。</p><ul><li>log_device_placement=True: 可以获取到 operations 和 Tensor 被指派到哪个设备(几号CPU或几号GPU)上运行,会在终端打印出各项操作是在哪个设备上运行的</li><li>allow_soft_placement=True: 允许tf自动选择一个存在并且可用的设备来运行操作.在tf中，通过命令 “with tf.device(‘/cpu:0’):”,允许手动设置操作运行的设备</li><li>config.gpu_options.allow_growth = True: 动态申请显存</li><li>config.gpu_options.per_process_gpu_memory_fraction = 0.4: 占用40%显存,限制GPU使用率.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto(allow_soft_placement=<span class="keyword">True</span>)</span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line">sess = tf.Session(config=config)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 限制GPU使用率</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.per_process_gpu_memory_fraction = <span class="number">0.4</span>  <span class="comment">#占用40%显存</span></span><br><span class="line">session = tf.Session(config=config)</span><br><span class="line">等同于</span><br><span class="line">gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=<span class="number">0.4</span>)</span><br><span class="line">config=tf.ConfigProto(gpu_options=gpu_options)</span><br><span class="line">session = tf.Session(config=config)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置使用哪块GPU</span></span><br><span class="line">方法一： 在python中设置</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">'0'</span> <span class="comment">#使用 GPU 0</span></span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">'0,1'</span> <span class="comment"># 使用 GPU 0，1</span></span><br><span class="line">方法二： 在执行时设置</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span> python yourcode.py</span><br></pre></td></tr></table></figure><h1 id="2-tf读取数据集图片的方式"><a href="#2-tf读取数据集图片的方式" class="headerlink" title="2. tf读取数据集图片的方式"></a>2. tf读取数据集图片的方式</h1><p>参考链接:</p><p><a href="https://www.jb51.net/article/134550.htm" target="_blank" rel="noopener">https://www.jb51.net/article/134550.htm</a></p><p><a href="https://www.jb51.net/article/134547.htm" target="_blank" rel="noopener">https://www.jb51.net/article/134547.htm</a></p><p>tf的流程是文件系统—&gt;文件名队列—&gt;内存队列</p><p>推荐使用方法一</p><h2 id="方法一：使用WholeFileReader输入queue，decode输出是Tensor，eval后是ndarray"><a href="#方法一：使用WholeFileReader输入queue，decode输出是Tensor，eval后是ndarray" class="headerlink" title="方法一：使用WholeFileReader输入queue，decode输出是Tensor，eval后是ndarray"></a>方法一：使用WholeFileReader输入queue，decode输出是Tensor，eval后是ndarray</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file_name</span><span class="params">(file_dir)</span>:</span>  <span class="comment">#来自//www.jb51.net/article/134543.htm</span></span><br><span class="line">  <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(file_dir): <span class="comment">#模块os中的walk()函数遍历文件夹下所有的文件</span></span><br><span class="line">    print(root) <span class="comment">#当前目录路径</span></span><br><span class="line">    print(dirs) <span class="comment">#当前路径下所有子目录</span></span><br><span class="line">    print(files) <span class="comment">#当前路径下所有非目录子文件</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file_name2</span><span class="params">(file_dir)</span>:</span>  <span class="comment">#特定类型的文件</span></span><br><span class="line">  L=[]  </span><br><span class="line">  <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(file_dir):</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files: </span><br><span class="line">      <span class="keyword">if</span> os.path.splitext(file)[<span class="number">1</span>] == <span class="string">'.jpg'</span>:  </span><br><span class="line">        L.append(os.path.join(root, file))</span><br><span class="line">  <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line">path = file_name2(<span class="string">'test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#以下参考//www.jb51.net/article/134547.htm (十图详解TensorFlow数据读取机制)</span></span><br><span class="line"><span class="comment">#path2 = tf.train.match_filenames_once(path)</span></span><br><span class="line">file_queue = tf.train.string_input_producer(paths, shuffle=<span class="keyword">True</span>, num_epochs=<span class="number">2</span>) <span class="comment">#创建输入队列</span></span><br><span class="line">image_reader = tf.WholeFileReader()</span><br><span class="line">key, image = image_reader.read(file_queue)</span><br><span class="line">image = tf.image.decode_jpeg(image, channerls=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="comment">#  coord = tf.train.Coordinator() #协同启动的线程</span></span><br><span class="line"><span class="comment">#  threads = tf.train.start_queue_runners(sess=sess, coord=coord) #启动线程运行队列</span></span><br><span class="line"><span class="comment">#  coord.request_stop() #停止所有的线程</span></span><br><span class="line"><span class="comment">#  coord.join(threads)</span></span><br><span class="line"></span><br><span class="line">  tf.local_variables_initializer().run()</span><br><span class="line">  threads = tf.train.start_queue_runners(sess=sess)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#print (type(image))</span></span><br><span class="line">  <span class="comment">#print (type(image.eval()))</span></span><br><span class="line">  <span class="comment">#print(image.eval().shape)</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> path+path:</span><br><span class="line">    plt.figure</span><br><span class="line">    plt.imshow(image.eval())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="方法二：使用gfile读图片，decode输出是Tensor，eval后是ndarray"><a href="#方法二：使用gfile读图片，decode输出是Tensor，eval后是ndarray" class="headerlink" title="方法二：使用gfile读图片，decode输出是Tensor，eval后是ndarray"></a>方法二：使用gfile读图片，decode输出是Tensor，eval后是ndarray</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">print(tf.__version__)</span><br><span class="line"></span><br><span class="line">image_raw = tf.gfile.FastGFile(<span class="string">'test/a.jpg'</span>,<span class="string">'rb'</span>).read()  <span class="comment">#bytes</span></span><br><span class="line">img = tf.image.decode_jpeg(image_raw) <span class="comment">#Tensor</span></span><br><span class="line"><span class="comment">#img2 = tf.image.convert_image_dtype(img, dtype = tf.uint8)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(type(image_raw)) <span class="comment"># bytes</span></span><br><span class="line">  print(type(img)) <span class="comment"># Tensor</span></span><br><span class="line">  <span class="comment">#print(type(img2))</span></span><br><span class="line"></span><br><span class="line">  print(type(img.eval())) <span class="comment"># ndarray !!!</span></span><br><span class="line">  print(img.eval().shape)</span><br><span class="line">  print(img.eval().dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  print(type(img2.eval()))</span></span><br><span class="line"><span class="comment">#  print(img2.eval().shape)</span></span><br><span class="line"><span class="comment">#  print(img2.eval().dtype)</span></span><br><span class="line">  plt.figure(<span class="number">1</span>)</span><br><span class="line">  plt.imshow(img.eval())</span><br><span class="line">  plt.show()</span><br></pre></td></tr></table></figure><h2 id="方法三：使用read-file，decode输出是Tensor，eval后是ndarray"><a href="#方法三：使用read-file，decode输出是Tensor，eval后是ndarray" class="headerlink" title="方法三：使用read_file，decode输出是Tensor，eval后是ndarray"></a>方法三：使用read_file，decode输出是Tensor，eval后是ndarray</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">print(tf.__version__)</span><br><span class="line"></span><br><span class="line">image_raw = tf.gfile.FastGFile(<span class="string">'test/a.jpg'</span>,<span class="string">'rb'</span>).read()  <span class="comment">#bytes</span></span><br><span class="line">img = tf.image.decode_jpeg(image_raw) <span class="comment">#Tensor</span></span><br><span class="line"><span class="comment">#img2 = tf.image.convert_image_dtype(img, dtype = tf.uint8)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(type(image_raw)) <span class="comment"># bytes</span></span><br><span class="line">  print(type(img)) <span class="comment"># Tensor</span></span><br><span class="line">  <span class="comment">#print(type(img2))</span></span><br><span class="line"></span><br><span class="line">  print(type(img.eval())) <span class="comment"># ndarray !!!</span></span><br><span class="line">  print(img.eval().shape)</span><br><span class="line">  print(img.eval().dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  print(type(img2.eval()))</span></span><br><span class="line"><span class="comment">#  print(img2.eval().shape)</span></span><br><span class="line"><span class="comment">#  print(img2.eval().dtype)</span></span><br><span class="line">  plt.figure(<span class="number">1</span>)</span><br><span class="line">  plt.imshow(img.eval())</span><br><span class="line">  plt.show()</span><br></pre></td></tr></table></figure><h1 id="3-tf-train-shuffle-batch"><a href="#3-tf-train-shuffle-batch" class="headerlink" title="3. tf.train.shuffle_batch"></a>3. tf.train.shuffle_batch</h1><p>参考链接:</p><p><a href="https://www.jianshu.com/p/9cfe9cadde06" target="_blank" rel="noopener">https://www.jianshu.com/p/9cfe9cadde06</a></p><p><a href="https://blog.csdn.net/ying86615791/article/details/73864381" target="_blank" rel="noopener">https://blog.csdn.net/ying86615791/article/details/73864381</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">img_batch = tf.train.shuffle_batch([img],</span><br><span class="line">batch_size=batch_size, capacity=capacity,</span><br><span class="line">min_after_dequeue=min_after_dequeue,num_threads=num_threads,</span><br><span class="line">allow_smaller_final_batch=allow_smaller_final_batch)</span><br></pre></td></tr></table></figure><p>tf不是像pytorch一样全局打乱，而是每一次在较短的队列中打乱。其中，队列的最长长度是capacity，最短长度是min_after_dequeue。</p><h1 id="4-tf-summary"><a href="#4-tf-summary" class="headerlink" title="4. tf.summary"></a>4. tf.summary</h1><p>参考链接：<a href="https://blog.csdn.net/hongxue8888/article/details/78610305" target="_blank" rel="noopener">https://blog.csdn.net/hongxue8888/article/details/78610305</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">'./summaries/'</span> + dataset + <span class="string">'_spgan'</span> , sess.graph)</span><br></pre></td></tr></table></figure><h1 id="5-tf-train-Saver"><a href="#5-tf-train-Saver" class="headerlink" title="5. tf.train.Saver"></a>5. tf.train.Saver</h1><p>参考链接：<a href="http://www.cnblogs.com/denny402/p/6940134.html" target="_blank" rel="noopener">http://www.cnblogs.com/denny402/p/6940134.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(max_to_keep= <span class="number">30</span>)</span><br></pre></td></tr></table></figure><h1 id="6-saver-restore"><a href="#6-saver-restore" class="headerlink" title="6. saver.restore"></a>6. saver.restore</h1><p>参考链接：<a href="https://blog.csdn.net/changeforeve/article/details/80268522" target="_blank" rel="noopener">https://blog.csdn.net/changeforeve/article/details/80268522</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_checkpoint</span><span class="params">(checkpoint_dir, sess, saver)</span>:</span></span><br><span class="line">    print(<span class="string">" [*] Loading checkpoint..."</span>)</span><br><span class="line">    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)</span><br><span class="line">    print(ckpt)</span><br><span class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)</span><br><span class="line">        ckpt_path = os.path.join(checkpoint_dir, ckpt_name)</span><br><span class="line">        saver.restore(sess, ckpt_path)</span><br><span class="line">        print(<span class="string">" [*] Loading successful!"</span>)</span><br><span class="line">        <span class="keyword">return</span> ckpt_path</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">" [*] No suitable checkpoint!"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure><h1 id="7-tf-train-Coordinator"><a href="#7-tf-train-Coordinator" class="headerlink" title="7. tf.train.Coordinator"></a>7. tf.train.Coordinator</h1><p>参考链接:</p><p><a href="https://blog.csdn.net/weixin_42052460/article/details/80714539" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42052460/article/details/80714539</a></p><p><a href="https://www.jianshu.com/p/d063804fb272" target="_blank" rel="noopener">https://www.jianshu.com/p/d063804fb272</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">coord = tf.train.Coordinator()</span><br><span class="line">threads = tf.train.start_queue_runners(sess=sess, coord=coord)</span><br></pre></td></tr></table></figure><h1 id="8-tf-identity"><a href="#8-tf-identity" class="headerlink" title="8. tf.identity"></a>8. tf.identity</h1><p>参考链接:</p><p><a href="https://stackoverflow.com/questions/34877523/in-tensorflow-what-is-tf-identity-used-for" target="_blank" rel="noopener">https://stackoverflow.com/questions/34877523/in-tensorflow-what-is-tf-identity-used-for</a></p><p><a href="https://blog.csdn.net/hu_guan_jie/article/details/78495297" target="_blank" rel="noopener">https://blog.csdn.net/hu_guan_jie/article/details/78495297</a></p><p>tf.idenity的逻辑就是等于号，区别是前者在计算图上加了个节点，使得可以多个设备之间可以通信，但是等于号为什么不行呢？</p><h1 id="9-tf-reuse"><a href="#9-tf-reuse" class="headerlink" title="9. tf.reuse"></a>9. tf.reuse</h1><p>参考链接：<a href="https://blog.csdn.net/UESTC_C2_403/article/details/72329786" target="_blank" rel="noopener">https://blog.csdn.net/UESTC_C2_403/article/details/72329786</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在阅读SPGAN代码源码的过程中，学习到的关于tensorflow的一些知识。&lt;br&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/tensorflow/"/>
    
    
      <category term="SPGAN" scheme="http://yoursite.com/tags/SPGAN/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>github-markdown-mathjax</title>
    <link href="http://yoursite.com/2018/12/03/github-markdown-mathjax/"/>
    <id>http://yoursite.com/2018/12/03/github-markdown-mathjax/</id>
    <published>2018-12-03T02:59:51.000Z</published>
    <updated>2018-12-03T03:22:05.493Z</updated>
    
    <content type="html"><![CDATA[<p>前言：在github上传自己的论文记录后发现，github上的显示和在vscode上的显示不同，主要体现在公式在github上不能正确地显示，并且换行也不能正确地显示。<br><a id="more"></a></p><p>参考链接：<br><a href="https://blog.csdn.net/phdsky/article/details/81431279" target="_blank" rel="noopener">https://blog.csdn.net/phdsky/article/details/81431279</a></p><p>搜索之后发现github的markdown不支持mathjax的渲染。<br><a href="https://github.com/github/markup/issues/897" target="_blank" rel="noopener">Github issue - github’s markdown mathjax rending</a><br><a href="https://stackoverflow.com/questions/11256433/how-to-show-math-equations-in-general-githubs-markdownnot-githubs-blog" target="_blank" rel="noopener">Stackoverflow - How to show math equations in general github’s markdown</a></p><p>解决方案或者是公式转图片，或者是使用github内嵌的公式编辑器，或者是适用于chrome的github with MathJax插件。</p><p>我最后采用的是github with MathJax插件。<br><a href="https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima" target="_blank" rel="noopener">GitHub with MathJax 插件</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前言：在github上传自己的论文记录后发现，github上的显示和在vscode上的显示不同，主要体现在公式在github上不能正确地显示，并且换行也不能正确地显示。&lt;br&gt;
    
    </summary>
    
      <category term="github-markdown" scheme="http://yoursite.com/categories/github-markdown/"/>
    
    
      <category term="github" scheme="http://yoursite.com/tags/github/"/>
    
      <category term="markdown" scheme="http://yoursite.com/tags/markdown/"/>
    
      <category term="mathjax" scheme="http://yoursite.com/tags/mathjax/"/>
    
  </entry>
  
  <entry>
    <title>markdown-math</title>
    <link href="http://yoursite.com/2018/12/03/markdown-math/"/>
    <id>http://yoursite.com/2018/12/03/markdown-math/</id>
    <published>2018-12-03T02:17:02.000Z</published>
    <updated>2019-01-18T07:38:35.073Z</updated>
    
    <content type="html"><![CDATA[<p>前言: 因为最近经常用到markdown写数学公式，每次都查感觉有点啰嗦，所以做个简单小结，把平常用的做个记录。这个博客根据平时自己常用到的进行动态增加。</p><a id="more"></a><ul><li>[x] 2019-01-18: hexo更换渲染引擎，详见下文</li><li>[x] 2018-12-27: 使用<a href="https://katex.org/docs/supported.html" target="_blank" rel="noopener">Katex</a></li><li>[x] 2018-12-27：<a href="http://www.mohu.org/info/symbols/symbols.htm" target="_blank" rel="noopener">一份不太简单的LATEX2e介绍</a></li></ul><p>参考链接：</p><p><a href="https://wangcong.info/article/MarkdownWithMath.html" target="_blank" rel="noopener">https://wangcong.info/article/MarkdownWithMath.html</a></p><p><a href="https://blog.csdn.net/deepinC/article/details/81103326" target="_blank" rel="noopener">https://blog.csdn.net/deepinC/article/details/81103326</a></p><p><a href="https://blog.csdn.net/HaleyPKU/article/details/80341932" target="_blank" rel="noopener">https://blog.csdn.net/HaleyPKU/article/details/80341932</a></p><p><a href="https://www.zybuluo.com/fyywy520/note/82980" target="_blank" rel="noopener">https://www.zybuluo.com/fyywy520/note/82980</a></p><h1 id="1-公式使用参考"><a href="#1-公式使用参考" class="headerlink" title="1. 公式使用参考"></a>1. 公式使用参考</h1><h2 id="1-1-插入公式"><a href="#1-1-插入公式" class="headerlink" title="1.1 插入公式"></a>1.1 插入公式</h2><p>插入公式分为行中公式，独立公式和自动编号公式</p><p>1.行中公式 $ a=b $</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> 数学公式 $</span></span><br></pre></td></tr></table></figure><p>2.独立公式 <script type="math/tex">a=b</script></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$ 数学公式 $$</span></span><br></pre></td></tr></table></figure><p>3.编号公式</p><script type="math/tex; mode=display">a=b \tag {1}</script><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$ 数学公式 \tag &#123;1&#125; $$</span></span><br></pre></td></tr></table></figure><p>由公式$(1)$可以得出结论</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由公式$(1)$可以得出结论</span><br></pre></td></tr></table></figure><p>4.自动编号公式<br>自动编号公式在github上显示不出来，原则上是可以的，推荐使用手动编号。</p><script type="math/tex; mode=display">\begin{equation}x^n+y^n=z^n\label{eq:afa}\end{equation}</script><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">数学公式</span><br><span class="line">\label&#123;eq:当前公式名&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure><p>5.自动编号公式的引用方法</p><p>在公式 <script type="math/tex">\eqref{eq:wwqr}</script> 中，我们看到了这个被自动编号的公式。<br>貌似没有成功</p><p>6.单个公式换行</p><p>单个公式很长的时候需要换行，但仅允许生成一个编号时，可以用split标签包围公式代码，在需要转行的地方使用\，每行需要使用1个&amp;来标识对齐的位置，结束后可使用\tag{…}标签编号。</p><script type="math/tex; mode=display">\begin{split}a &= b \\c &= d \\e &= f \end{split}\tag{1.2}</script><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">a &amp;= b \\</span><br><span class="line">c &amp;= d \\</span><br><span class="line">e &amp;= f </span><br><span class="line">\end&#123;split&#125;\tag&#123;1.3&#125;</span><br><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br></pre></td></tr></table></figure><p>7.多行的独立公式</p><p>有时候需要罗列多个公式，可以用eqnarray*标签包围公式代码，在需要转行的地方使用\，每行需要使用2个&amp;来标识对齐位置，两个&amp;…&amp;号之间的是公式间对齐的位置，每行公式后可使用\tag{…}标签编号：</p><p>github貌似对于多行公式显示不出来。</p><script type="math/tex; mode=display">\begin{eqnarray*}x^n+y^n &=& z^n \tag{1.4} \\x+y &=& z \tag{1.5}\end{eqnarray*}</script><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br><span class="line">\begin&#123;eqnarray*&#125;</span><br><span class="line">x^n+y^n &amp;=&amp; z^n \tag&#123;1.4&#125; \\</span><br><span class="line">x+y &amp;=&amp; z \tag&#123;1.5&#125;</span><br><span class="line">\end&#123;eqnarray*&#125;</span><br><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br></pre></td></tr></table></figure><h2 id="1-2-符号"><a href="#1-2-符号" class="headerlink" title="1.2 符号"></a>1.2 符号</h2><div class="table-container"><table><thead><tr><th style="text-align:center">输入</th><th style="text-align:center">显示</th><th style="text-align:center">输入</th><th style="text-align:center">显示</th></tr></thead><tbody><tr><td style="text-align:center">x^y</td><td style="text-align:center">$x^y$</td><td style="text-align:center">x_y</td><td style="text-align:center">$x_y$</td></tr><tr><td style="text-align:center">\sideset{\^1_2}{\^3_4}\bigotimes</td><td style="text-align:center">$\sideset{^1_2}{^3_4}\bigotimes$</td><td style="text-align:center">\langle</td><td style="text-align:center">&lt;</td></tr><tr><td style="text-align:center">\lceil</td><td style="text-align:center">$\lceil$</td><td style="text-align:center">\rceil</td><td style="text-align:center">$\rceil$</td></tr><tr><td style="text-align:center">\lfloor</td><td style="text-align:center">$\lfloor$</td><td style="text-align:center">\frac{a}{b}</td><td style="text-align:center">$\frac{a}{b}$</td></tr><tr><td style="text-align:center">\sqrt[2]{3}</td><td style="text-align:center">$\sqrt[2]{3}$</td><td style="text-align:center">\alpha,\gamma</td><td style="text-align:center">$\alpha$ $\gamma$</td></tr><tr><td style="text-align:center">\frac{a}{b}</td><td style="text-align:center">$\frac{a}{b}$</td><td style="text-align:center">\sum_{n=1}^N{3n}</td><td style="text-align:center">$\sum_{n=1}^N{3n}$</td></tr><tr><td style="text-align:center">\prod_{n=1}^N{3n}</td><td style="text-align:center">$\prod_{n=1}^N{3n}$</td><td style="text-align:center">\sqrt[2]{5}</td><td style="text-align:center">$\sqrt[2]{5}$</td></tr><tr><td style="text-align:center">\int^5_1{f(x)}{\rm d}x</td><td style="text-align:center">$\int^5_1{f(x)}{\rm d}x$</td><td style="text-align:center">\iint^5_1{f(x)}{\rm d}x</td><td style="text-align:center">$\iint^5_1{f(x)}{\rm d}x$</td></tr><tr><td style="text-align:center">+\infty</td><td style="text-align:center">$+\infty$</td><td style="text-align:center">-\infty</td><td style="text-align:center">$-\infty$</td></tr><tr><td style="text-align:center">\lim_{n\rightarrow+\infty} n</td><td style="text-align:center">$\lim_{n\rightarrow+\infty} n$</td><td style="text-align:center">\in</td><td style="text-align:center">$\in$</td></tr><tr><td style="text-align:center">\geq\,\leq</td><td style="text-align:center">$\geq,\leq$</td><td style="text-align:center">\subset,\supset</td><td style="text-align:center">$\subset,\supset$</td></tr><tr><td style="text-align:center">\pm,\cdot</td><td style="text-align:center">$\pm,\cdot$</td><td style="text-align:center">\times,\div</td><td style="text-align:center">$\times,\div$</td></tr><tr><td style="text-align:center">\not=,\not&lt;</td><td style="text-align:center">$\not=,\not&lt;$</td><td style="text-align:center">\not\supset</td><td style="text-align:center">$\not\supset$</td></tr><tr><td style="text-align:center">\log_2{18}</td><td style="text-align:center">$\log_2{18}$</td><td style="text-align:center">\parallel</td><td style="text-align:center">$\parallel$</td></tr><tr><td style="text-align:center">\sum_{n=1}^N{n}</td><td style="text-align:center">$\sum_{n=1}^N{n}$</td><td style="text-align:center">\prod{n}</td><td style="text-align:center">$\prod{n}$</td></tr><tr><td style="text-align:center">\pm,\cdot,\times,\div</td><td style="text-align:center">$\pm,\cdot,\times,\div$</td></tr></tbody></table></div><h2 id="1-3-希腊字母"><a href="#1-3-希腊字母" class="headerlink" title="1.3 希腊字母"></a>1.3 希腊字母</h2><div class="table-container"><table><thead><tr><th style="text-align:center">输入</th><th style="text-align:center">显示</th><th style="text-align:center">输入</th><th style="text-align:center">显示</th></tr></thead><tbody><tr><td style="text-align:center">\alpha,\beta,\gamma</td><td style="text-align:center">$\alpha,\beta,\gamma$</td><td style="text-align:center">\delta,\epsilon, \varepsilon</td><td style="text-align:center">$\delta,\epsilon, \varepsilon$</td></tr><tr><td style="text-align:center">\theta,\lambda,\mu</td><td style="text-align:center">$\theta,\lambda,\mu$</td><td style="text-align:center">\phi,\varphi,\sigma</td><td style="text-align:center">$\phi,\varphi,\sigma$</td></tr><tr><td style="text-align:center">\Sigma</td><td style="text-align:center">$\Sigma$</td></tr></tbody></table></div><h2 id="1-4-空心字母与Fraktur字母"><a href="#1-4-空心字母与Fraktur字母" class="headerlink" title="1.4 空心字母与Fraktur字母"></a>1.4 空心字母与Fraktur字母</h2><div class="table-container"><table><thead><tr><th style="text-align:center">输入</th><th style="text-align:center">显示</th><th style="text-align:center">输入</th><th style="text-align:center">显示</th></tr></thead><tbody><tr><td style="text-align:center">\mathbb{A}</td><td style="text-align:center">$\mathbb{A}$</td><td style="text-align:center">\mathfrak{B}</td><td style="text-align:center">$\mathfrak{B}$</td></tr></tbody></table></div><h2 id="1-5-分段函数"><a href="#1-5-分段函数" class="headerlink" title="1.5 分段函数"></a>1.5 分段函数</h2><script type="math/tex; mode=display">P_{r-j}= \begin{cases}   0 &\mbox{if $r-j$ is odd}\\   r!\,(-1)^{(r-j)/2} &\mbox{if $r-j$ is even}   \end{cases}</script><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br><span class="line">P_&#123;r-j&#125;=</span><br><span class="line"> \begin&#123;cases&#125;</span><br><span class="line">   0 &amp;\mbox&#123;if $r-j$ is odd&#125;\\</span><br><span class="line">   r!\,(-1)^&#123;(r-j)/2&#125; &amp;\mbox&#123;if $r-j$ is even&#125;</span><br><span class="line">   \end&#123;cases&#125;</span><br><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br></pre></td></tr></table></figure><h2 id="1-6-多行对齐公式"><a href="#1-6-多行对齐公式" class="headerlink" title="1.6 多行对齐公式"></a>1.6 多行对齐公式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;align&#125;</span><br><span class="line">h(x) =&amp; \frac&#123;<span class="number">1</span>&#125;&#123;\int_xt(x)\mathrm&#123;d&#125;x&#125; \tag&#123;<span class="number">1</span>&#125;\\</span><br><span class="line">f(x) =&amp; \frac&#123;<span class="number">1</span>&#125;&#123;\int_x\eta(x)\mathrm&#123;d&#125;x&#125;g(x)\tag&#123;<span class="number">2</span>&#125;</span><br><span class="line">\end&#123;align&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><h1 id="2-LaTex公式渲染引擎"><a href="#2-LaTex公式渲染引擎" class="headerlink" title="2. LaTex公式渲染引擎"></a>2. LaTex公式渲染引擎</h1><p>参考链接：<a href="https://www.jianshu.com/p/a9f26f4cd4e6" target="_blank" rel="noopener">https://www.jianshu.com/p/a9f26f4cd4e6</a></p><p>针对Hexo渲染LaTex公式的时候，下划线总是容易被渲染成斜体，所以更换Hexo默认的Markdown渲染引擎。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p>更换渲染引擎只能保证块内公式的下划线的问题，对于行内公式，需要修改\node_modules\kramed\lib\rules\inline.js。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//  em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line">  em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure><p>对于两个连续的花括号，需要中间加个空格。</p><p>对于2019-01-18之前已经发过的博客文章，如公式不进行主动修复。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前言: 因为最近经常用到markdown写数学公式，每次都查感觉有点啰嗦，所以做个简单小结，把平常用的做个记录。这个博客根据平时自己常用到的进行动态增加。&lt;/p&gt;
    
    </summary>
    
      <category term="markdown" scheme="http://yoursite.com/categories/markdown/"/>
    
    
      <category term="markdown" scheme="http://yoursite.com/tags/markdown/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>person-reid-transfer-learning</title>
    <link href="http://yoursite.com/2018/11/29/person-reid-transfer-learning/"/>
    <id>http://yoursite.com/2018/11/29/person-reid-transfer-learning/</id>
    <published>2018-11-29T15:29:52.000Z</published>
    <updated>2018-12-17T07:54:53.982Z</updated>
    
    <content type="html"><![CDATA[<h1 id="transfer-learning"><a href="#transfer-learning" class="headerlink" title="transfer learning"></a>transfer learning</h1><a id="more"></a><p>这个博客主要是因为最近看了几篇关于无监督迁移学习在行人重识别领域的论文，发现隔了几天，自己对论文就忘记得差不多了，所以对论文的关键内容做个简单记录。</p><p>参考链接: <a href="https://github.com/layumi/DukeMTMC-reID_evaluation/blob/master/State-of-the-art/README.md" target="_blank" rel="noopener">Transfer Learning</a></p><p>因为在某些情况下，图片或者公式无法正常显示，所以，我基本会同步到我的博客<br><a href="https://tjjtjjtjj.github.io/2018/11/29/person-reid-transfer-learning/#more" target="_blank" rel="noopener">https://tjjtjjtjj.github.io/2018/11/29/person-reid-transfer-learning/#more</a></p><p>现有方法在transfer learning方向的性能对比</p><p><img src="./pic/transfer/transfer.png" alt="transfer learning"><br><img src="/2018/11/29/person-reid-transfer-learning/transfer.png" title="transfer learning"></p><hr><h2 id="1-ARN"><a href="#1-ARN" class="headerlink" title="1. ARN"></a>1. ARN</h2><p><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w6/Li_Adaptation_and_Re-Identification_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification</a></p><p>Yu-Jhe Li, Fu-En Yang, Yen-Cheng Liu, Yu-Ying Yeh, Xiaofei Du, and Yu-Chiang Frank Wang, CVPR 2018 Workshop</p><p>这篇论文主要分离了数据集的特有特征和行人特征，从而使不同数据集的行人特征投射到统一特征空间中。</p><p>作者是台湾人，没有公布代码。有其他人复现了<a href="https://github.com/huanghoujing/ARN" target="_blank" rel="noopener">代码</a>，但是效果很差。</p><p>我下一步也会尝试复现一下。</p><h3 id="1-1-网络架构"><a href="#1-1-网络架构" class="headerlink" title="1.1 网络架构"></a>1.1 网络架构</h3><p><img src="./pic/ARN/ARN.png" alt="ARN的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/ARN.png" title="ARN的网络架构"></p><p>根据作者的描述，</p><ul><li>$E_I$是resnet50的前四个layer,输入是3X256X256,输出$X^s$是2048X7X7</li><li>$E_T,E_C,E_S$,是相同的网络架构，来自FCN的三层，通过查阅FCN的网络设置，初步猜想是FCN的conv6，conv7，conv8，相应的Decoder暂时按照反卷积来设置。这一部分还需要参考FCN的网络设置。</li><li>$E_T,E_C,E_S$ conv6:7X7X2048,relu6,drop6(0.5),conv7:1X1X2048,relu6,drop6(0.5),conv8:1X1X2048,至于conv6,7的bn和conv8的bn，relu要不要，还需要实验的验证</li><li>在FCN中，逆卷积的使用方式是 deconv(k=64, s=32, p=0)+crop(offset=19)，参考资料:<a href="https://zhuanlan.zhihu.com/p/22976342?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">FCN学习:Semantic Segmentation</a>,<a href="https://blog.csdn.net/zlrai5895/article/details/80473814" target="_blank" rel="noopener">经典网络复现系列（一）：FCN</a></li><li>反卷积的时候一般都是k=2n, s=n,</li><li>参考FCN和pytorch的入门与实践第六章的生成器，我们的Decoder使用deconv(k=1,s=1), deconv(k=1, s=1), deconv(k=7, s=1)</li><li>encoder和decoder都使用bn和relu</li><li>分类层有dropout</li><li>学习率，$E_I=10^{-7}, E_T E_C E_S D_C = 10^{-3}, C_S = 2*10^{-3}  $，并且在前几个epoch只更新$E_I$</li><li>优化器：SGD</li></ul><h3 id="1-2-损失函数"><a href="#1-2-损失函数" class="headerlink" title="1.2 损失函数"></a>1.2 损失函数</h3><p>分类损失</p><script type="math/tex; mode=display">L_{class}=-\sum_{i=1}^{N_s}y_i^s.log\hat{y}_i^s \tag {1}</script><p>对比损失</p><script type="math/tex; mode=display">L_{ctrs}=\sum_{i,j}{\lambda}(e_{c,i}^s-e_{c,j}^s)^2+ ({1-\lambda}) [max(0, m-(e_{c,i}^s-e_{c,j}^s))]^2 \tag {2}</script><p>重构误差</p><script type="math/tex; mode=display">L_{rec} = \sum_{i=1}^{N_s} ||X_i^s-\hat{X_i^s}||^2 + \sum_{i=1}^{N_t} ||X_i^t-\hat{X_i^t}||^2 \tag 3</script><p>差别损失</p><script type="math/tex; mode=display">L_{diff} = || {H_c^s}^T H_p^s ||_F^2 + || {H_c^t}^T H_p^t ||_F^2 \tag 4</script><p>总损失</p><script type="math/tex; mode=display">L_{total} = L_{class} + \alpha L_{ctrs} + \beta L_{rec} + \gamma L_{diff} \tag {5}</script><p>其中</p><script type="math/tex; mode=display">\alpha=0.01, \beta= 2.0, \gamma=1500</script><h3 id="1-3-模块分析"><a href="#1-3-模块分析" class="headerlink" title="1.3 模块分析"></a>1.3 模块分析</h3><p>三个模块:</p><ol><li><strong>$ L_{rec} $</strong></li><li><strong>$ L_{class} $和$ L_{ctrs} $</strong></li><li><strong>$ E_T$和$E_S$</strong></li></ol><h4 id="1-3-1-半监督-L-rec"><a href="#1-3-1-半监督-L-rec" class="headerlink" title="1.3.1 半监督$ L_{rec} $"></a>1.3.1 半监督$ L_{rec} $</h4><p>这里不是很懂这个重构误差损失函数的作用，下面的这个解释也不行。重构损失是半监督损失函数。暂时理解成重构损失保证在获取特征的过程中尽可能减少信息损失。或者说，类似PCA，保留主成分，这个主成分只能保证尽可能地把样本分开。至于这个主成分是否重要，是否有利于分类，不得而知。</p><p>参考链接：<a href="https://blog.csdn.net/hijack00/article/details/52238549" target="_blank" rel="noopener">深度学习中的“重构”</a></p><p>作者在这里提示，当只有重构损失函数的时候，应该保持$E_I$不更新，只更新$E_C$.</p><p>S: Market, T: Duke; S: Duke, T: Market</p><div class="table-container"><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th></tr></thead><tbody><tr><td style="text-align:center">$L_{rec}$</td><td style="text-align:center">44.5</td><td style="text-align:center">20.3</td><td style="text-align:center">31.2</td><td style="text-align:center">18.4</td></tr></tbody></table></div><h4 id="1-3-2-监督-L-rec-L-class-和-L-ctrs"><a href="#1-3-2-监督-L-rec-L-class-和-L-ctrs" class="headerlink" title="1.3.2 监督$ L_{rec} $, $ L_{class} $和$ L_{ctrs} $"></a>1.3.2 监督$ L_{rec} $, $ L_{class} $和$ L_{ctrs} $</h4><p>半监督和监督</p><p>监督损失使得共享空间捕获到行人语义信息。</p><p>S: Market, T: Duke; S: Duke, T: Market</p><div class="table-container"><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th></tr></thead><tbody><tr><td style="text-align:center">w/o $ L_{class} $, $ L_{ctrs} $</td><td style="text-align:center">52.2</td><td style="text-align:center">23.7</td><td style="text-align:center">36.7</td><td style="text-align:center">19.6</td></tr><tr><td style="text-align:center">w $ L_{class} $, $ L_{ctrs} $</td><td style="text-align:center">70.3</td><td style="text-align:center">39.4</td><td style="text-align:center">60.2</td><td style="text-align:center">33.4</td></tr><tr><td style="text-align:center">$L_{rec}$</td><td style="text-align:center">44.5</td><td style="text-align:center">20.3</td><td style="text-align:center">31.2</td><td style="text-align:center">18.4</td></tr><tr><td style="text-align:center">$L_{rec}$, $ L_{class} $和$ L_{ctrs} $</td><td style="text-align:center">60.5</td><td style="text-align:center">28.7</td><td style="text-align:center">48.4</td><td style="text-align:center">26.8</td></tr></tbody></table></div><h4 id="1-3-3-无监督-L-rec-E-T-和-E-S"><a href="#1-3-3-无监督-L-rec-E-T-和-E-S" class="headerlink" title="1.3.3 无监督$ L_{rec} $, $ E_T $和$ E_S $"></a>1.3.3 无监督$ L_{rec} $, $ E_T $和$ E_S $</h4><p>特有特征的提取是为了去除共享空间的噪声。</p><p>假设共享空间存在，且特有特征空间存在，如果没有特有特征的提取，那么得到的行人特征或多或少地都会包含特征空间的基向量。</p><p>当然，这里也隐含了一些假设，共享空间和特有空间一定是线性无关的。空间的基向量是2048维。</p><p>S: Market, T: Duke; S: Duke, T: Market</p><div class="table-container"><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th></tr></thead><tbody><tr><td style="text-align:center">w/o  $ E_T $, $ E_S $</td><td style="text-align:center">60.5</td><td style="text-align:center">28.7</td><td style="text-align:center">48.4</td><td style="text-align:center">26.8</td></tr><tr><td style="text-align:center">w $ L_{class} $, $ L_{ctrs} $</td><td style="text-align:center">70.3</td><td style="text-align:center">39.4</td><td style="text-align:center">60.2</td><td style="text-align:center">33.4</td></tr><tr><td style="text-align:center">$L_{rec}$</td><td style="text-align:center">44.5</td><td style="text-align:center">20.3</td><td style="text-align:center">31.2</td><td style="text-align:center">18.4</td></tr><tr><td style="text-align:center">$ L_{rec} $, $ E_T $和$ E_S $</td><td style="text-align:center">52.2</td><td style="text-align:center">23.7</td><td style="text-align:center">36.7</td><td style="text-align:center">19.6</td></tr></tbody></table></div><hr><h2 id="2-HHL"><a href="#2-HHL" class="headerlink" title="2. HHL"></a>2. HHL</h2><p><a href="https://github.com/zhunzhong07/zhunzhong07.github.io/blob/master/paper/HHL.pdf" target="_blank" rel="noopener">Generalizing A Person Retrieval Model Hetero- and Homogeneously</a></p><p>Zhun Zhong, Liang Zheng, Shaozi Li, Yi Yang, ECCV 2018</p><p>code: <a href="https://github.com/zhunzhong07/HHL" target="_blank" rel="noopener">https://github.com/zhunzhong07/HHL</a></p><p>web: <a href="http://zhunzhong.site/paper/HHL.pdf" target="_blank" rel="noopener">http://zhunzhong.site/paper/HHL.pdf</a></p><p>中文: <a href="http://www.cnblogs.com/Thinker-pcw/p/9787440.html" target="_blank" rel="noopener">http://www.cnblogs.com/Thinker-pcw/p/9787440.html</a></p><p>preson-reid中主要面临的问题：</p><ol><li>数据集之间的差异</li><li>数据集内部摄像头的差异</li></ol><p>解决方法：</p><ol><li>相机差异：利用StarGAN进行风格转化</li><li>数据集差异：将源域/目标域图片视为负匹配</li></ol><p>数据集之间的三元组损失有把不同数据集的行人特征映射到同一特征空间的效果。</p><p>创新点在于使用straGAN和复杂的三元组损失。</p><h3 id="2-1-网络架构"><a href="#2-1-网络架构" class="headerlink" title="2.1 网络架构"></a>2.1 网络架构</h3><p><img src="./pic/HHL/HHL.png" alt="HHL的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL.png" title="HHL的网络架构"></p><p>网络的简要介绍</p><ul><li>CNN是resnet50，网络包括两个分支，一个计算源数据集的分类损失，一个计算相似度学习的triplet损失。</li><li>FC-2014的组成：linear(2048，1024)—&gt;bn(1024)—&gt;relu—&gt;dropout(0.5),相当于一个embedding。</li><li>FC-#ID是linear(1024,751), FC-128是linear(1024, 128), 两个分支的具体情况是：</li><li><ul><li>x1—&gt;linear(2048, 1024)—&gt;x2—&gt;bn(1024)—&gt;x3—&gt;relu—&gt;x4—&gt;dropout(0.5)—&gt;x5—&gt;linear(102, 751)—&gt;x6</li></ul></li><li><ul><li>x1—&gt;linear(2048, 1024)—&gt;x2—&gt;bn(1024)—&gt;x3—&gt;relu—&gt;x4—&gt;linear(1024, 128)</li></ul></li><li>网络的triplet损失是Batch Hard Triplet Loss</li><li>网络的输入设置：在每一个batch中，对于分类损失，source domain随机选取batchsize=128张图片，对于triplet损失，source domain随机选取8个人的共batchsize=64张图片，其中连续的8张图片属于同一个人，target domain随机选取batchsize=16个人的共16X9=144张图片，假设这16个人都是不同的人。实验发现，当source domain的分类损失的图片比较少的时候，无法实现预期效果，其他情况下没有测试。当batchsize是这样的配比时，可以达到作者的效果。理由未知．</li><li>starGAN是离线训练</li><li>学习率设置：base：$10^{-1}$，其他：$10^{-2}$，并且每过40个epoch，学习率阶梯性地乘以0.1.一共训练60个epoch就可以达到预期效果，这部分设置和PCB很类似。不知道是经验还是怎么。</li><li>关于StarGAN待自己复现之后再做进一步解释，现在只复现过StyleGAN。</li><li>triplet损失的margin=0.3</li></ul><h3 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2 损失函数"></a>2.2 损失函数</h3><p>分类损失</p><script type="math/tex; mode=display">L_{cross}=-\sum_{i=1}^{N_s}y_i^s.log\hat{y}_i^s</script><p>triplet损失</p><script type="math/tex; mode=display">L_T=\sum_{x_a, x_p, x_n}[D_{x_a, x_p}+m-D_{x_a, x_n}]_+</script><p>相机不变性的triplet损失</p><p>目标域中一张原始图片作为anchor，StarGAN图片为positive，其他图片为negative</p><script type="math/tex; mode=display">L_C=L_T((x_t^i)^{n_t}\bigcup(x_{t^\*}^i)^{n_t^*})</script><p>域不变性的triplet损失</p><p>源域中一张图片为anchor，同一id的其他图片作为positive，目标域的任一图片为negative</p><script type="math/tex; mode=display">L_D=L_T((x_s^i)^{n_s}\bigcup(x_t^i)^{n_t})</script><p>相机不变性和域不变性的triplet损失</p><p>是将相机不变性和域不变性合为一体，源域的positive不变，negative为源域的其他图片和目标域的图片，目标域的positive不变，negative为源域的图片和目标域的其他行人图片</p><script type="math/tex; mode=display">L_{CD}=L_T((x_s^i)^{n_s}\bigcup(x_t^i)^{n_t}\bigcup(x_{t*}^i)^{n_t^*})</script><p>总损失：</p><script type="math/tex; mode=display">L_{HHL}=L_{cross}+\beta*L_{CD}</script><p>其中：</p><script type="math/tex; mode=display">\beta=0.5</script><h3 id="2-3-模块分析"><a href="#2-3-模块分析" class="headerlink" title="2.3 模块分析"></a>2.3 模块分析</h3><ol><li><strong>starGAN</strong></li><li><strong>sample方法</strong></li></ol><h4 id="2-3-1-starGAN"><a href="#2-3-1-starGAN" class="headerlink" title="2.3.1 starGAN"></a>2.3.1 starGAN</h4><p>在源数据集上训练，在目标数据集上测试不同图像增强方法下的图片距离，通过表格可以得出，预训练的模型对于目标数据集的随机翻转等等有很好的鲁棒性，但是，对于不同摄像头的同一个人，其距离还是很大。因此，利用StarGAN和相机不变性的triplet损失来减少由于摄像头带来的偏差。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Source</th><th style="text-align:center">Target</th><th style="text-align:center">Random Crop</th><th style="text-align:center">Random Flip</th><th style="text-align:center">CamStyle Transfer</th></tr></thead><tbody><tr><td style="text-align:center">Duke</td><td style="text-align:center">Market</td><td style="text-align:center">0.049</td><td style="text-align:center">0.034</td><td style="text-align:center">0.485</td></tr><tr><td style="text-align:center">Market</td><td style="text-align:center">Duke</td><td style="text-align:center">0.059</td><td style="text-align:center">0.044</td><td style="text-align:center">0.614</td></tr></tbody></table></div><h4 id="2-3-2-sample方法"><a href="#2-3-2-sample方法" class="headerlink" title="2.3.2 sample方法"></a>2.3.2 sample方法</h4><p>对于目标域的取样方法，对比了三种方法的性能，分别是随机取样、聚类取样、有监督取样，通过下图可以看出，这三种方法的性能是一样的，最后，作者给的代码是随机取样。</p><p><img src="./pic/HHL/HHL2.png" alt="sample"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL2.png" title="sample"></p><h3 id="2-4-实验设置"><a href="#2-4-实验设置" class="headerlink" title="2.4 实验设置"></a>2.4 实验设置</h3><h4 id="2-4-1-Camera-style-transfer-model：StarGAN"><a href="#2-4-1-Camera-style-transfer-model：StarGAN" class="headerlink" title="2.4.1 Camera style transfer model：StarGAN"></a>2.4.1 Camera style transfer model：StarGAN</h4><p>使用StarGAN进行对于摄像头风格转化。</p><ul><li>2 conv + 6 residual + 2 transposed</li><li>input 128X64</li><li>Adam $\beta_1=0.5, \beta_2=0.999$</li><li>数据初始化:随机翻转和随机裁剪</li><li>学习率：前100个epoch为0.0001，后100个epoch线性衰减到0</li></ul><h4 id="2-4-2-Re-ID-model-training"><a href="#2-4-2-Re-ID-model-training" class="headerlink" title="2.4.2 Re-ID model training"></a>2.4.2 Re-ID model training</h4><ul><li>设置可以参考Zhong, Z., Zheng, L., Zheng, Z., Li, S., Yang, Y.: Camera style adaptation for person re-identification</li><li>input 256*128</li><li>数据初始化：随机裁剪和随机翻转</li><li>dropout=0.5</li><li>学习率：新增的层：0.1，base：0.01，每隔40个epoch乘以0.1</li><li>mini-batch：源域上对于IDE为128，对于tripletloss是64.目标域上对于triplet loss是16.</li><li>epoch=60</li><li>测试：2048-dim计算欧式距离</li></ul><h3 id="2-5-超参数设置"><a href="#2-5-超参数设置" class="headerlink" title="2.5 超参数设置"></a>2.5 超参数设置</h3><ul><li>triplet loss的权重$\beta$</li><li>一个batch中目标域上$n_t$的个数</li></ul><h4 id="2-5-1-参数的设置-beta"><a href="#2-5-1-参数的设置-beta" class="headerlink" title="2.5.1 参数的设置$\beta$"></a>2.5.1 参数的设置$\beta$</h4><p><img src="./pic/HHL/HHL3.png" alt="$\beta$参数的设置"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL3.png" title="$\beta$参数的设置"></p><p>$\beta$应该设置成0.4-0.8</p><h4 id="2-5-2-参数的设置-n-t"><a href="#2-5-2-参数的设置-n-t" class="headerlink" title="2.5.2 参数的设置$n_t$"></a>2.5.2 参数的设置$n_t$</h4><p><img src="./pic/HHL/HHL4.png" alt="$n_t$参数的设置"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL4.png" title="$n_t$参数的设置"></p><p>$n_t$在当前设置(源域上对于IDE为128，对于tripletloss是64)下，应该$n_t&gt;16$</p><p>通过上述参数的设置，结合自己实验时的错误，不妨这么理解，在固定mini-batch=128的情况下</p><ul><li>首先引入源域的triplet_loss，并调整batch和$\beta$，使效果达到最优，,batch的选取2倍数的等间隔，$\beta$可以取等间隔，最后batch=64，即128/2=64，$\beta$则可以先固定成某个值.</li><li>然后引入目标域的triplet_loss，并且要先考虑只有目标域的性能，再考虑结合的性能，每次都需要重新考虑$\beta$和batch的大小</li><li>这么一想，这篇论文做的实验还是很多的。</li></ul><h3 id="2-6-实验结果"><a href="#2-6-实验结果" class="headerlink" title="2.6 实验结果"></a>2.6 实验结果</h3><p><img src="./pic/HHL/HHL5.png" alt="实验结果"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL5.png" title="实验结果"></p><p>通过结果我们看出来，其实提升的效果主要来源于$L_C$，说明预训练的模型对于目标域不同摄像头的图片鲁棒性很差。</p><p>是否说明预训练的模型只学习到了源数据集的跨摄像头的不变行人特征，而对于目标域的摄像头下的不同风格很敏感，而对目标域的同一摄像头下的行人特征很鲁棒。</p><p>$L_T$的提升效果很小是否可以说明目标数据集与源数据集的行人特征空间本身就已经很好地重合了，假如tripl_loss真得具有将不同数据集的行人特征映射到同一特征空间的效果的话。</p><p>通过这篇论文，我们能学到的东西很多，比如对比实验，参数设置实验，想法验证实验等等。</p><h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><h4 id="triplet-loss"><a href="#triplet-loss" class="headerlink" title="triplet_loss"></a>triplet_loss</h4><p>发现triplet_loss很厉害的样子，不妨看看是个什么情况。</p><p>参考链接：<br><a href="https://omoindrot.github.io/triplet-loss" target="_blank" rel="noopener">Triplet Loss and Online Triplet Mining in TensorFlow</a></p><p><a href="http://www.itkeyword.com/doc/2025902251705572502/re-id-with-triplet-loss" target="_blank" rel="noopener">Re-ID with Triplet Loss</a></p><p><a href="https://arxiv.org/pdf/1703.07737.pdf" target="_blank" rel="noopener">In Defense of the Triplet Loss for Person Re-Identification</a></p><p><a href="https://github.com/VisualComputingInstitute/triplet-reid" target="_blank" rel="noopener">code</a></p><p><a href="https://omoindrot.github.io/triplet-loss" target="_blank" rel="noopener">Triplet Loss and Online Triplet Mining in TensorFlow</a>这个博客讲述了triplet_loss的起源、发展和具体使用的几种形式。最后的结论是应该使用在线的batch hard策略。</p><p><a href="http://www.itkeyword.com/doc/2025902251705572502/re-id-with-triplet-loss" target="_blank" rel="noopener">Re-ID with Triplet Loss</a>这篇博客则逻辑性地介绍了各种triplet_loss的变体。最后的结论是batch hard+soft margin效果更好。</p><p>也有提及到，triplet_loss总是不如分类损失强。</p><h4 id="下一步工作"><a href="#下一步工作" class="headerlink" title="下一步工作"></a>下一步工作</h4><p>已经理解源代码</p><hr><h2 id="3-SPGAN"><a href="#3-SPGAN" class="headerlink" title="3. SPGAN"></a>3. SPGAN</h2><p><a href="https://arxiv.org/pdf/1711.07027.pdf" target="_blank" rel="noopener">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</a></p><p>Weijian Deng, Liang Zheng, Guoliang Kang, Yi Yang, Qixiang Ye, Jianbin Jiao, CVPR 2018</p><p>这篇论文主要是构建”Learning via Translation”的框架来进行迁移学习，利用SPGAN(CycleGAN+Simaese net)从源数据集迁移到目标数据集，然后在目标数据集上训练。</p><p>论文的重点是怎么改进CycleGAN。</p><p>web:<a href="http://www.sohu.com/a/208231404_642762" target="_blank" rel="noopener">http://www.sohu.com/a/208231404_642762</a></p><p>code:<a href="https://github.com/Simon4Yan/Learning-via-Translation" target="_blank" rel="noopener">https://github.com/Simon4Yan/Learning-via-Translation</a></p><p>CycleGAN</p><p><a href="https://arxiv.org/pdf/1703.10593.pdf" target="_blank" rel="noopener">Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</a></p><p>code:<a href="https://github.com/zhunzhong07/CamStyle" target="_blank" rel="noopener">https://github.com/zhunzhong07/CamStyle</a></p><p>自己对代码的分析<a href="https://tjjtjjtjj.github.io/2018/11/19/cycleGAN/#more" target="_blank" rel="noopener">https://tjjtjjtjj.github.io/2018/11/19/cycleGAN/#more</a></p><h3 id="3-1-前言"><a href="#3-1-前言" class="headerlink" title="3.1 前言"></a>3.1 前言</h3><p>一般的无监督迁移方法都是假设源域和目标域上有相同ID的图片，不太适用于跨数据集的行人重识别。</p><h3 id="3-2-网络架构"><a href="#3-2-网络架构" class="headerlink" title="3.2 网络架构"></a>3.2 网络架构</h3><p>GAN网络</p><p><img src="./pic/SPGAN/SPGAN1.png" alt="SPGAN"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN1.png" title="SPGAN"></p><p>LMP网络</p><p><img src="./pic/SPGAN/SPGAN3.png" alt="LMP"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN3.png" title="LMP"></p><p>行人重识别整体网络</p><p><img src="./pic/SPGAN/SPGAN2.png" alt="SPGAN"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN2.png" title="SPGAN"></p><p>网络的简要介绍</p><ul><li>整理网络由两部分组成，第一部分是SPGAN，第二部分是常见的行人重识别网络的修改版LMP，重点是第一部分。</li><li>整个网络是用Caffe搭建。</li><li>因为自己没有仔细看caffe的代码，后期有需要的还是要看看超参数设置的。</li><li>SPGAN基本沿用了CycleGAN的设置，epoch=5，更多的epoch没有用。</li><li>SPGAN的正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$,负样本是$G(x_S)$和$x_T$或者$F(x_T)$和$x_S$，作者给的代码中用的正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$，负样本是$x_S$和$x_T$。</li><li>SPGAN的训练分为生成器、判别器、SiaNet。</li><li>$L_{ide}$可以保持转换前后图片颜色保持一致。</li><li>LMP网络直接generated domain上训练。</li><li>在论文的tabel2的注释中，可以看到是分成了7份，与PCB的6份差不多。</li></ul><h3 id="3-3-损失函数"><a href="#3-3-损失函数" class="headerlink" title="3.3 损失函数"></a>3.3 损失函数</h3><h4 id="3-3-1-CycleGAN"><a href="#3-3-1-CycleGAN" class="headerlink" title="3.3.1 CycleGAN"></a>3.3.1 CycleGAN</h4><script type="math/tex; mode=display">L_{T_{adv}}(G,D_T,p_x,p_y)=E_{y\sim p_y}[(D_T(y)-1)^2]+E_{x\sim p_x}[(D_T(G(x))-1)^2]</script><script type="math/tex; mode=display">L_{S_{adv}}(F,D_S,p_x,p_y)=E_{x\sim p_x}[(D_S(x)-1)^2]+E_{y\sim p_y}[(D_S(F(y)))^2]</script><script type="math/tex; mode=display">L_{cyc}(G,F)=E_{x\sim p_x}\parallel F(G(x))-x \parallel_1+E_{y\sim p_y}\parallel G(F(y))-y\parallel_1</script><script type="math/tex; mode=display">L_{ide}(G,F,p_x,p_y)=E_{x\sim p_x}\parallel F(x)-x\parallel_1+E_{y\sim p_y}\parallel G(y)-y\parallel_1</script><h4 id="3-3-2-SPGAN"><a href="#3-3-2-SPGAN" class="headerlink" title="3.3.2 SPGAN"></a>3.3.2 SPGAN</h4><p>Siameses Net:</p><script type="math/tex; mode=display">L_{con}(i,x_1,x_2)=(1-i)(max(0,m-d))^2+id^2</script><p>其中，$m\in [0,2]$，$d=1-cos(\theta)\in [0,2]$表示归一化后的欧式距离.正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$,负样本是$G(x_S)$和$x_T$或者$F(x_T)$和$x_S$。</p><p>Overall objective loss:</p><script type="math/tex; mode=display">L_{sp}=L_{T_{adv}}+L_{S_{adv}}+\lambda_1 L_{cyc}+\lambda_2 L_{ide}+\lambda_3 L_{con}</script><p>其中，$\lambda_1=10，\lambda_2=5，\lambda_3=2, m=2$</p><h4 id="3-3-3-行人重识别网络"><a href="#3-3-3-行人重识别网络" class="headerlink" title="3.3.3 行人重识别网络"></a>3.3.3 行人重识别网络</h4><p>以resnet50为基础网络，和PCB类似，分割成两块。</p><h3 id="3-4-实验设置"><a href="#3-4-实验设置" class="headerlink" title="3.4 实验设置"></a>3.4 实验设置</h3><h4 id="3-4-1-SPGAN"><a href="#3-4-1-SPGAN" class="headerlink" title="3.4.1 SPGAN"></a>3.4.1 SPGAN</h4><p>SPGAN的整体训练过程与CycleGAN基本是一致的，建议先参考CycleGAN，再学习SPGAN。</p><p>$\lambda_1=10，\lambda_2=5，\lambda_3=2, m=2$，学习率为0.0002，batch=1，total_epoch=5</p><p><strong>SiaNet:</strong> </p><p>4个conv+4个max pool+1个FC。</p><p>x(3,256,256)-&gt;conv(3,64,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)</p><p>-&gt;conv(64,128,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)</p><p>-&gt;conv(128,256,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)</p><p>-&gt;conv(256,512,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)(1,1,512)</p><p>-&gt;FC(512, 128)-&gt;leak_relu(0.2)-&gt;dropout(0.5)-&gt;FC(128,64)</p><p>输入预处理：随机左右翻转、resize(286)、crop(256)、img/127.5-1。</p><p>激活函数全部使用leak_relu(0.2)，没有使用bn</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">metric_net</span><span class="params">(img, scope, df_dim=<span class="number">64</span>, reuse=False, train=True)</span>:</span></span><br><span class="line"></span><br><span class="line">    bn = functools.partial(slim.batch_norm, scale=<span class="keyword">True</span>, is_training=train,</span><br><span class="line">                           decay=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, updates_collections=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope + <span class="string">'_discriminator'</span>, reuse=reuse):</span><br><span class="line">        h0 = lrelu(conv(img, df_dim, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h0_conv'</span>))    <span class="comment"># h0 is (128 x 128 x df_dim)</span></span><br><span class="line">        pool1 = Mpool(h0, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        h1 = lrelu(conv(pool1, df_dim * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h1_conv'</span>))  <span class="comment"># h1 is (32 x 32 x df_dim*2)</span></span><br><span class="line">        pool2 = Mpool(h1, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        h2 = lrelu(conv(pool2, df_dim * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h2_conv'</span>))  <span class="comment"># h2 is (8 x 8 x df_dim*4)</span></span><br><span class="line">        pool3 = Mpool(h2, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        h3 = lrelu(conv(pool3, df_dim * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h3_conv'</span>))  <span class="comment"># h3 is (2 x 2 x df_dim*4)</span></span><br><span class="line">        pool4 = Mpool(h3, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        shape = pool4.get_shape()</span><br><span class="line">        flatten_shape = shape[<span class="number">1</span>].value * shape[<span class="number">2</span>].value * shape[<span class="number">3</span>].value</span><br><span class="line">        h3_reshape = tf.reshape(pool4, [<span class="number">-1</span>, flatten_shape], name = <span class="string">'h3_reshape'</span>)</span><br><span class="line"></span><br><span class="line">        fc1 = lrelu(FC(h3_reshape, df_dim*<span class="number">2</span>, scope=<span class="string">'fc1'</span>))</span><br><span class="line">        dropout_fc1 = slim.dropout(fc1, <span class="number">0.5</span>, scope=<span class="string">'dropout_fc1'</span>)  </span><br><span class="line">        net = FC(dropout_fc1, df_dim, scope=<span class="string">'fc2'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#print_activations(net)</span></span><br><span class="line">        <span class="comment">#print_activations(pool4)</span></span><br><span class="line">        <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><h4 id="3-4-2-LMP"><a href="#3-4-2-LMP" class="headerlink" title="3.4.2 LMP"></a>3.4.2 LMP</h4><p>batch_size=16, total_epoch=50, SGD, momentum=0.9, gamma=0.1, lr_ini=0.001, decay to 0.0001 after 40 epochs.</p><p>这部分的设置和IDE基本类似。</p><h3 id="3-5-对比实验"><a href="#3-5-对比实验" class="headerlink" title="3.5 对比实验"></a>3.5 对比实验</h3><h4 id="模块的对比实验"><a href="#模块的对比实验" class="headerlink" title="模块的对比实验"></a>模块的对比实验</h4><p><img src="./pic/SPGAN/SPGAN4.png" alt="对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN4.png" title="对比实验"></p><p><img src="./pic/SPGAN/SPGAN8.png" alt="生成效果"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN8.png" title="生成效果"></p><p>通过对比实验可以看到，以mAP为指标，CycleGAN增加了3个点，SiaNet(m=2)增加了3个点，LMP增加了4个点。说明作者尝试的3个模块都在一定程度上起到了作用。但是个人感觉还是差点什么。比如，为什么会有效？</p><p>假设目标都是为了使源域与目标域的行人特征映射到同一特征空间。这里的CycleGAN做到了这一点。LMP可以认为是加在哪里都有效的一种方式。那SiaNet其实更像是在保证生成的图片不仅要保留源图片的内容，更要保留源图片的行人特征。这种保留是以一种隐空间的形式在保存，而不是明显的分类损失这样子。</p><p>$\lambda_3 $对比实验</p><p><img src="./pic/SPGAN/SPGAN5.png" alt="$\lambda_3 $对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN5.png" title="$\lambda_3 $对比实验"></p><p>pool 和 part的对比实验</p><p><img src="./pic/SPGAN/SPGAN6.png" alt="pool 和 part的对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN6.png" title="pool 和 part的对比实验"></p><p>也就是说，pool的方式和parts的取法是实验得到的，不是凭空想出来的。</p><p>通过上述实验超参数的设置对比实验，与HHL论文比较，都是固定其他，变化一个参数，然后选取最优的参数，是基于局部最优就是全局最优的思想。感觉到作者的实验做得很足。</p><p>不同base model的对比实验</p><p><img src="./pic/SPGAN/SPGAN7.png" alt="不同base model的对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN7.png" title="不同base model的对比实验"></p><h3 id="附录-1"><a href="#附录-1" class="headerlink" title="附录"></a>附录</h3><h4 id="IDE-and-IDE"><a href="#IDE-and-IDE" class="headerlink" title="IDE and $IDE^+$"></a>IDE and $IDE^+$</h4><p><a href="https://arxiv.org/pdf/1604.02531.pdf" target="_blank" rel="noopener">IDE</a></p><p><a href="https://github.com/zhunzhong07/IDE-baseline-Market-1501" target="_blank" rel="noopener">https://github.com/zhunzhong07/IDE-baseline-Market-1501</a></p><blockquote><p>We name the descriptor as ID-discriminative Embedding (IDE).<br>感觉还是没有很好地理解IDE。</p></blockquote><p>对于IDE+没有找到对应的原文，因为不是重点，暂且跳过。</p><p>IDE的pytorch代码</p><p><a href="https://github.com/Simon4Yan/Person_reID_baseline_pytorch" target="_blank" rel="noopener">https://github.com/Simon4Yan/Person_reID_baseline_pytorch</a></p><p>IDE和$IDE^+$的网络模型是一样的：</p><p>resnet50(layer4)+avgpool+Linear(2048,512)+bn1d(512)+LeakReLU(0.1)+Dropout(0.5)+Linear(512, num_class)</p><p>区别在于训练时bn层是否更新：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model.model = resnet50(layer4)+avgpool+Linear(2048,512)+bn1d(512)+LeakReLU(0.1)+Dropout(0.5)</span></span><br><span class="line"><span class="comment"># model.classifier = Linear(512, num_class)</span></span><br><span class="line"><span class="comment"># IDE</span></span><br><span class="line"><span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        model.train(<span class="keyword">True</span>)  <span class="comment"># Set model to training mode</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.train(<span class="keyword">False</span>)  <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"><span class="comment"># IDE+</span></span><br><span class="line"><span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        model.eval()  <span class="comment"># Fix BN of ResNet50</span></span><br><span class="line">        model.model.fc.train(<span class="keyword">True</span>)</span><br><span class="line">        model.classifier.train(<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.train(<span class="keyword">False</span>)  <span class="comment"># Set model to evaluate mode</span></span><br></pre></td></tr></table></figure><h5 id="新增-2018-12-17"><a href="#新增-2018-12-17" class="headerlink" title="新增 2018-12-17"></a>新增 2018-12-17</h5><p>参考论文: <a href="https://arxiv.org/abs/1811.07487" target="_blank" rel="noopener">Re-Identification with Consistent Attentive Siamese Networks</a></p><p>IDE的网络架构<br><img src="./person-reid-transfer-learning/SPGAN9.png" alt="IDE的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN9.png" title="IDE的网络架构"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一般情况下，cut_at_pooling=False，num_features=256, has_embedding为true</span></span><br><span class="line"><span class="comment"># 一般情况下，新增了feat、feat_bn、relu、drop、classifier</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    __factory = &#123;</span><br><span class="line">        <span class="number">18</span>: torchvision.models.resnet18,</span><br><span class="line">        <span class="number">34</span>: torchvision.models.resnet34,</span><br><span class="line">        <span class="number">50</span>: torchvision.models.resnet50,</span><br><span class="line">        <span class="number">101</span>: torchvision.models.resnet101,</span><br><span class="line">        <span class="number">152</span>: torchvision.models.resnet152,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, depth, pretrained=True, cut_at_pooling=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_features=<span class="number">0</span>, norm=False, dropout=<span class="number">0</span>, num_classes=<span class="number">0</span>, triplet_features=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.depth = depth</span><br><span class="line">        self.pretrained = pretrained</span><br><span class="line">        self.cut_at_pooling = cut_at_pooling</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Construct base (pretrained) resnet</span></span><br><span class="line">        <span class="keyword">if</span> depth <span class="keyword">not</span> <span class="keyword">in</span> ResNet.__factory:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">"Unsupported depth:"</span>, depth)</span><br><span class="line">        self.base = ResNet.__factory[depth](pretrained=pretrained)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.cut_at_pooling:</span><br><span class="line">            self.num_features = num_features</span><br><span class="line">            self.norm = norm</span><br><span class="line">            self.dropout = dropout</span><br><span class="line">            self.has_embedding = num_features &gt; <span class="number">0</span></span><br><span class="line">            self.num_classes = num_classes</span><br><span class="line">            self.triplet_features = triplet_features</span><br><span class="line">            out_planes = self.base.fc.in_features</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Append new layers</span></span><br><span class="line">            <span class="keyword">if</span> self.has_embedding:</span><br><span class="line">                self.feat = nn.Linear(out_planes, self.num_features)</span><br><span class="line">                self.feat_bn = nn.BatchNorm1d(self.num_features)</span><br><span class="line">                init.kaiming_normal_(self.feat.weight, mode=<span class="string">'fan_out'</span>)</span><br><span class="line">                init.constant_(self.feat.bias, <span class="number">0</span>)</span><br><span class="line">                init.constant_(self.feat_bn.weight, <span class="number">1</span>)</span><br><span class="line">                init.constant_(self.feat_bn.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Change the num_features to CNN output channels</span></span><br><span class="line">                self.num_features = out_planes</span><br><span class="line">            <span class="keyword">if</span> self.dropout &gt; <span class="number">0</span>:</span><br><span class="line">                self.drop = nn.Dropout(self.dropout)</span><br><span class="line">            <span class="keyword">if</span> self.num_classes &gt; <span class="number">0</span>:</span><br><span class="line">                self.classifier = nn.Linear(self.num_features, self.num_classes)</span><br><span class="line">                init.normal_(self.classifier.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                init.constant_(self.classifier.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> self.triplet_features &gt; <span class="number">0</span>:</span><br><span class="line">                self.triplet = nn.Linear(self.num_features, self.triplet_features)</span><br><span class="line">                init.normal_(self.triplet.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                init.constant_(self.triplet.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.pretrained:</span><br><span class="line">            self.reset_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, output_feature=None)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.base._modules.items():</span><br><span class="line">            <span class="keyword">if</span> name == <span class="string">'avgpool'</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            x = module(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.cut_at_pooling:</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        x = F.avg_pool2d(x, x.size()[<span class="number">2</span>:])</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_feature == <span class="string">'pool5'</span>:</span><br><span class="line">            x = F.normalize(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line">        <span class="keyword">if</span> self.has_embedding:</span><br><span class="line">            x = self.feat(x)</span><br><span class="line">            x = self.feat_bn(x)</span><br><span class="line">        <span class="keyword">if</span> self.norm:</span><br><span class="line">            x = F.normalize(x)</span><br><span class="line">        <span class="keyword">elif</span> self.has_embedding:</span><br><span class="line">            x = F.relu(x)</span><br><span class="line">        <span class="comment"># triplet feature</span></span><br><span class="line">        <span class="keyword">if</span> self.triplet_features &gt; <span class="number">0</span>:</span><br><span class="line">            x_triplet = self.triplet(x)</span><br><span class="line">        <span class="keyword">if</span> self.dropout &gt; <span class="number">0</span>:</span><br><span class="line">            x = self.drop(x)</span><br><span class="line">        <span class="keyword">if</span> self.num_classes &gt; <span class="number">0</span>:</span><br><span class="line">            x_class = self.classifier(x)</span><br><span class="line">        <span class="comment"># two outputs</span></span><br><span class="line">        <span class="keyword">if</span> self.triplet_features &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> x_class, x_triplet</span><br><span class="line">        <span class="keyword">return</span> x_class</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                init.kaiming_normal(m.weight, mode=<span class="string">'fan_out'</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    init.constant(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                init.constant(m.weight, <span class="number">1</span>)</span><br><span class="line">                init.constant(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</span><br><span class="line">                init.normal(m.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    init.constant(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h4 id="Caffe-and-pytorch"><a href="#Caffe-and-pytorch" class="headerlink" title="Caffe and pytorch"></a>Caffe and pytorch</h4><p><a href="https://github.com/Simon4Yan/Learning-via-Translation/issues/1" target="_blank" rel="noopener">Caffe和pytorch中的bn层的计算方式不一样。</a></p><p>在caffe中，bn层在训练时是eval状态，也是只使用Imagenet的mean和variance</p><blockquote><p>The eval mode for BN layer during training, corresponding to Caffe’s batch_norm_param {use_global_stats: true}, means using ImageNet BN mean and variance during training.</p></blockquote><p>在pytorch中，bn层在训练时如果设置成eval装填，才可以达到caffe的精度。</p><h4 id="疑惑"><a href="#疑惑" class="headerlink" title="疑惑"></a>疑惑</h4><p>IDE和IDE+的效果区别为什么会这么大?</p><h3 id="下一步工作-1"><a href="#下一步工作-1" class="headerlink" title="下一步工作"></a>下一步工作</h3><ul><li>[x] 已经理解源代码</li></ul><p>尝试在pytorch上复现结果，现在根据作者提供的代码，感觉并不是很难。主要是SPGAN。</p><hr><h2 id="4-基于GAN的类似论文"><a href="#4-基于GAN的类似论文" class="headerlink" title="4. 基于GAN的类似论文"></a>4. 基于GAN的类似论文</h2><p>类似的采取GAN做person-reid方向的论文还有好多，上面两篇是现在最新的，下面就简单地介绍几篇类似的文章，其中涉及到的原理和前文提到的GAN的方法类似。</p><h3 id="4-1-PTGAN"><a href="#4-1-PTGAN" class="headerlink" title="4.1 PTGAN"></a>4.1 PTGAN</h3><p><a href="https://arxiv.org/pdf/1711.08565.pdf" target="_blank" rel="noopener">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</a></p><p>Longhui Wei1, Shiliang Zhang1, Wen Gao1, Qi Tian</p><p>这篇论文对Cycle-GAN进行了改进，保留ID信息的损失函数如下：</p><script type="math/tex; mode=display">L_{ID}=E_{a \sim p_{data}(a)} [||(G(a)-a) \odot M(a)||_2] + E_{b \sim p_{data}(b)} [||(F(b)-b) \odot M(b)||_2]</script><p>其中，$M(b)$表示使用PSPNet分割后的结果。</p><p>转化效果如下图所示</p><p><img src="./pic/PTGAN/PTGAN.png" alt="PTGAN的转化效果"><br><img src="/2018/11/29/person-reid-transfer-learning/PTGAN.png" title="PTGAN的转化效果"></p><p>这里的Cycle-Gan生成图片的效果和SPGAN生成的效果还是有一些区别的，不是很理解。</p><p>其他的不是本次的重点，不做介绍。</p><h3 id="4-2-DCGAN-CNN"><a href="#4-2-DCGAN-CNN" class="headerlink" title="4.2 DCGAN+CNN"></a>4.2 DCGAN+CNN</h3><p><a href="https://arxiv.org/pdf/1701.07717.pdf" target="_blank" rel="noopener">Unlabeled Samples Generated by GAN Improve the Person Re-Identification Baseline in Vitro</a></p><p>Zhedong Zheng Liang Zheng Yi Yang</p><p>这篇论文主要是利用DCGAN生成新的数据集进行数据集扩充。</p><p>网络架构如图所示：</p><p><img src="./pic/DCGAN+CNN/DCGAN+CNN.png" alt="DCGAN+CNN的网络结构"><br><img src="/2018/11/29/person-reid-transfer-learning/DCGAN+CNN.png" title="DCGAN+CNN的网络结构"></p><p>生成效果图</p><p><img src="./pic/DCGAN+CNN/DCGAN+CNN2.png" alt="生成效果图"><br><img src="/2018/11/29/person-reid-transfer-learning/DCGAN+CNN2.png" title="生成效果图"></p><p>生成图片的标签LSRO</p><script type="math/tex; mode=display">q_{LSR}=\begin{cases} \frac{\epsilon}{K},&k\neq y\\\\                        1-\epsilon+\frac{\epsilon}{K},&k=y \end{cases}</script><script type="math/tex; mode=display">l_{LSR}=-(1-\epsilon)log(p(y))-\frac{\epsilon}{K}\sum_{k=1}^{K}log(p(k))</script><script type="math/tex; mode=display">q_{LSRO}=\frac{1}{K}</script><script type="math/tex; mode=display">l_{LSRO}=-(1-Z)log(p(y))-\frac{Z}{K}\sum_{k=1}^Klog(p(k))</script><p>其中，真实图片的Z=0，生成图片的Z=1.</p><h2 id="5-MMFA"><a href="#5-MMFA" class="headerlink" title="5. MMFA"></a>5. MMFA</h2><p><a href="https://arxiv.org/pdf/1807.01440.pdf" target="_blank" rel="noopener">Multi-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification</a></p><p>Shan Lin, Haoliang Li, Chang-Tsun Li, Alex Chichung Kot, BMVC 2018</p><h3 id="5-1-前言"><a href="#5-1-前言" class="headerlink" title="5.1 前言"></a>5.1 前言</h3><p>其想法也是将源域与目标域映射到同一特征空间。创新点是：</p><ul><li>利用MMD缩小源域与目标域的分布差异</li><li>考虑了属性</li></ul><p><a href="https://blog.csdn.net/a529975125/article/details/81176029" target="_blank" rel="noopener">MMD的参考代码</a></p><h3 id="5-2-网络架构"><a href="#5-2-网络架构" class="headerlink" title="5.2 网络架构"></a>5.2 网络架构</h3><p><img src="./pic/MMFA/MMFA.png" alt="MMFA的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/MMFA.png" title="MMFA的网络架构"></p><p>网络架构的说明:</p><ul><li>每一个batch中包括$n_s$张源域图片，$n_t$张目标域图片。batch=32</li><li>backbone是resnet50，并且修改resnet50的avg_pool为max_pool</li><li>$H_S$是pool层的输出向量，$H_S^{id}$是ID-FC层的输出相邻，$H_S^{attr_m}$Attr-FC-m的输出向量。</li><li>input (256,128,3)</li><li>FC=fc+bn+dropout(0.5)+leaky ReLU+fc</li><li>SGD:momentum=0.9,weight decay=5x10e-4</li><li>lr=0.01,每20个epoch乘以0.1</li><li>测试使用max pool的2048维向量的欧式距离</li><li><a href="http://www.liangzheng.org/Project/project_reid.html" target="_blank" rel="noopener">Market</a>有27个<a href="https://github.com/vana77/Market-1501_Attribute" target="_blank" rel="noopener">属性</a>，<a href="http://vision.cs.duke.edu/DukeMTMC/" target="_blank" rel="noopener">Duke</a>有23个<a href="https://github.com/vana77/DukeMTMC-attribute" target="_blank" rel="noopener">属性</a></li></ul><h3 id="5-3-损失函数"><a href="#5-3-损失函数" class="headerlink" title="5.3 损失函数"></a>5.3 损失函数</h3><p>Identity Loss:</p><script type="math/tex; mode=display">L_{id}=-\frac{1}{n_s}\sum_{i=1}^{n_S}log(p_{id}(h_{S,i}^{id},y_{S,i}))</script><p>Attribute Loss:</p><script type="math/tex; mode=display">L_{attr}=-\frac{1}{M}\frac{1}{n_S}\sum_{m=1}^{M}\sum_{i=1}^{n_S}(a_{S,i}^{m}\cdot log(p_{attr}(h_{S,i}^{attr_m}, m)) -\\\\(1-a_{S,i}^{m})\cdot log(1-p_{attr}(h_{S,i}^{attr_m}, m)))</script><p>Attribute Feature Adaptation</p><script type="math/tex; mode=display">L_{AAL}=\frac{1}{M}\sum_{m=1}^{M}MMD(H_{S}^{attr_m}, H_{T}^{attr_m})^2\\\\         =\frac{1}{M}\sum_{m=1}^{M}\parallel \frac{1}{n_S}\sum_{i=1}^{n_S}\phi(h_{S,i}^{attr_m}) - \frac{1}{n_T}\sum_{i=1}^{n_T}\phi(h_{T,j}^{attr_m}) \parallel \_{H}^2 \\\\         =\frac{1}{M}\sum_{m=1}^{M}[ \frac{1}{(n_S)^2}\sum_{i=1}^{n_S}\sum_{i'=1}^{n_S}k(h_{S,i}^{attr_m}, h_{S,i'}^{attr_m})\\\\         +\frac{1}{(n_T)^2}\sum_{i=1}^{n_T}\sum_{i'=1}^{n_T}k(h_{T,i}^{attr_m}, h_{T,i'}^{attr_m})\\\\         -\frac{2}{n_S\cdot n_T}\sum_{i=1}^{n_S}\sum_{j=1}^{n_T}k(h_{S,i}^{attr_m}, h_{T,j}^{attr_m})  ]</script><script type="math/tex; mode=display">k(h_{S,i}^{attr_m}, h_{T,j}^{attr_m})=exp(-\frac{1}{2\alpha}\parallel  h_{S,i}^{attr_m} - h_{T,j}^{attr_m}\parallel ^2)</script><script type="math/tex; mode=display">\alpha=1,5,10</script><p>Mid-level Deep Feature Adaptation</p><script type="math/tex; mode=display">L_{MDAL}=MMD(H_S,H_T)^2</script><p>Overall loss</p><script type="math/tex; mode=display">L_{all}=L_{id}+\lambda_1 L_{attr}+\lambda_2 L_{AAL}+\lambda_3 L_{MDAL}</script><script type="math/tex; mode=display">\lambda_1=0.1,\lambda_2=1,\lambda_3=1</script><h3 id="5-4-实验分析"><a href="#5-4-实验分析" class="headerlink" title="5.4 实验分析"></a>5.4 实验分析</h3><h4 id="5-4-1-实验结果"><a href="#5-4-1-实验结果" class="headerlink" title="5.4.1 实验结果"></a>5.4.1 实验结果</h4><p><img src="./pic/MMFA/MMFA2.png" alt="实验结果"><br><img src="/2018/11/29/person-reid-transfer-learning/MMFA2.png" title="实验结果"></p><h3 id="5-4-2-实验模块"><a href="#5-4-2-实验模块" class="headerlink" title="5.4.2 实验模块"></a>5.4.2 实验模块</h3><p>实验模块对比实验结果<br><img src="./pic/MMFA/MMFA3.png" alt="实验模块对比实验结果"><br><img src="/2018/11/29/person-reid-transfer-learning/MMFA3.png" title="实验模块对比实验结果"></p><h3 id="5-5-附录"><a href="#5-5-附录" class="headerlink" title="5.5 附录"></a>5.5 附录</h3><p>通过实验结果可以看出，在MMFA模型中，ID+Mid-level Deep Feature Adaptation的贡献最大。</p><p>下一步可以尝试考虑Mid-level Deep Feature Adaptation。</p><p>作者把avg pool 换成max pool。</p><h2 id="6-TJ-AIDL"><a href="#6-TJ-AIDL" class="headerlink" title="6. TJ-AIDL"></a>6. TJ-AIDL</h2><p><a href="http://www.eecs.qmul.ac.uk/~xiatian/papers/WangEtAl_CVPR2018.pdf" target="_blank" rel="noopener">Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification</a></p><p>Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li, ECCV 2018</p><h3 id="6-1-前言"><a href="#6-1-前言" class="headerlink" title="6.1 前言"></a>6.1 前言</h3><p>这篇论文的创新点在于：</p><ul><li>根据属性和id的关系，提出了Identity Inferred Attribute Space。</li></ul><h3 id="6-2-网络架构"><a href="#6-2-网络架构" class="headerlink" title="6.2 网络架构"></a>6.2 网络架构</h3><p>Attribute-Identity Transferable Joint Learning</p><p><img src="./pic/TJ-AIDL/TJ-AIDL.png" alt="TJ-AIDL的网络架构 "><br><img src="/2018/11/29/person-reid-transfer-learning/TJ-AIDL.png" title="TJ-AIDL的网络架构"></p><p>Unsupervised Target Domain Adaptation</p><p><img src="./pic/TJ-AIDL/TJ-AIDL2.png" alt="IJ-AIDL的部分网络架构详解"><br><img src="/2018/11/29/person-reid-transfer-learning/TJ-AIDL2.png" title="IJ-AIDL的部分网络架构详解"></p><p>网络架构的简要说明：</p><ul><li>(a) Identity Branch</li><li>(b) Attribute Branch</li><li>(c) Identity Inferred Attribute (IIA) space</li><li>训练过程分为两步:</li><li><ul><li>(I) 源域训练: Attribute-Identity Transferable Joint Learning</li></ul></li><li><ul><li>(II) 目标域微调: Unsupervised Target Domain Adaptation</li></ul></li><li>一般情况下Identity Branch和Attribute Branch是共享网络，但是本论文中特意分成两个非共享网络</li><li>重点在于对$e_{IIA}$的处理</li><li>IIA-encoder 是3个fc层，512/128/m，decoder是encoder的镜像。</li><li>基准网络是MobileNet</li><li>Adam优化器，lr=0.002，mementum$\beta_1=0.5, \beta_2=0.999$</li><li>batch size=8<blockquote><p>We started with training the identity branch by 100,000 iterations on the source identity labels and then the whole model by 20,000 iterations for both transferable joint learning on the labelled source data and unsupervised domain adaptation on the unlabelled target data</p></blockquote></li></ul><h3 id="6-3-损失函数"><a href="#6-3-损失函数" class="headerlink" title="6.3 损失函数"></a>6.3 损失函数</h3><p>Identity Branch (a) softmax</p><script type="math/tex; mode=display">L_{id}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}log(p_{id}(I_i^S,y_i^S)) \tag{1}</script><p>Attribute Branch(b) sigmoid</p><script type="math/tex; mode=display">L_{att}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}\sum_{j=1}^{m}(a_{i,j}log(p_{att}(I_i,j))+(1-a_{i,j})log(1-p_{att}(I_i,j))) \tag{2}</script><p>Identity Inferred Attribute (IIA) space (c)</p><script type="math/tex; mode=display">L_{rec}=\parallel x_{id}-f_{IIA}(x_{id}) \parallel ^2 \tag{3}</script><script type="math/tex; mode=display">L_{ID-transfer}=\parallel e_{IIA}-\tilde{p}\_{att} \parallel ^2 \tag{4}</script><script type="math/tex; mode=display">L_{att,IIA}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}\sum_{j=1}^{m}(a_{i,j}log(p_{IIA}(I_i,j))+(1-a_{i,j})log(1-p_{IIA}(I_i,j))) \tag{5}</script><script type="math/tex; mode=display">L_{IIA}=L_{att,IIA}+\lambda_1 L_{rec}+\lambda_2 L_{ID-transfer} \tag{6}</script><script type="math/tex; mode=display">\lambda_1=10, \lambda_2=10</script><p>Impact of IIA on Identity and Attribute Branches</p><script type="math/tex; mode=display">L_{att-total}=L_{att}+\lambda_2 L_{ID-transfer} \tag{7}</script><h3 id="6-4-训练与部署流程"><a href="#6-4-训练与部署流程" class="headerlink" title="6.4 训练与部署流程"></a>6.4 训练与部署流程</h3><p><img src="./pic/TJ-AIDL/TJ-AIDL3.png" alt="IJ-AIDL的训练与部署流程"><br></p><h3 id="6-5-模块分析"><a href="#6-5-模块分析" class="headerlink" title="6.5 模块分析"></a>6.5 模块分析</h3><h4 id="6-5-1-ID和Attribute模块分析"><a href="#6-5-1-ID和Attribute模块分析" class="headerlink" title="6.5.1 ID和Attribute模块分析"></a>6.5.1 ID和Attribute模块分析</h4><p><img src="./pic/TJ-AIDL/TJ-AIDL4.png" alt="IJ-AIDL的ID和Attribute模块分析"><br></p><p>通过ID only的mAP和HHL的baseline，可以看出MobileNet和Resnet50对mAP的影响不受很大。</p><p>另外，可以看出，依然是ID占据了很大比重。</p><h4 id="6-5-2-Adapation的作用"><a href="#6-5-2-Adapation的作用" class="headerlink" title="6.5.2 Adapation的作用"></a>6.5.2 Adapation的作用</h4><p><img src="./pic/TJ-AIDL/TJ-AIDL5.png" alt="IJ-AIDL的Adapation的作用"><br><img src="/2018/11/29/person-reid-transfer-learning/TJ-AIDL5.png" title="IJ-AIDL的Adapation的作用"></p><p>从表格中可以看出，Adaptation的作用很小。说明，预训练的模型已经很好地能保持属性的一致性，即不同角度得到的属性是一样的。</p><h3 id="6-6-补充"><a href="#6-6-补充" class="headerlink" title="6.6 补充"></a>6.6 补充</h3><p>还是难以理解作者这么做的出发点，感觉有点凭空就设计出这么多损失函数，可能是哪里还缺点什么东西。</p><p>训练更新的时候方程(7)的出现原因是什么？更新(6)的时候应该就已经对attr进行了影响吧？</p><p>在step(II)中，是怎么更新方程(6)的。</p><p>Identity Inferred Attribute Space的合理性是怎么体现的？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;transfer-learning&quot;&gt;&lt;a href=&quot;#transfer-learning&quot; class=&quot;headerlink&quot; title=&quot;transfer learning&quot;&gt;&lt;/a&gt;transfer learning&lt;/h1&gt;
    
    </summary>
    
      <category term="ind1" scheme="http://yoursite.com/categories/ind1/"/>
    
    
      <category term="person-reid" scheme="http://yoursite.com/tags/person-reid/"/>
    
      <category term="transfer learning" scheme="http://yoursite.com/tags/transfer-learning/"/>
    
  </entry>
  
  <entry>
    <title>cycleGAN</title>
    <link href="http://yoursite.com/2018/11/19/cycleGAN/"/>
    <id>http://yoursite.com/2018/11/19/cycleGAN/</id>
    <published>2018-11-19T01:11:24.000Z</published>
    <updated>2018-12-21T13:25:36.390Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博客主要记录在跟随cycleGAN作者的代码复现学到的东西。<br>title: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(ICCV2017)</p><p>paper: <a href="https://arxiv.org/abs/1703.10593" target="_blank" rel="noopener">https://arxiv.org/abs/1703.10593</a><br>code: <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank" rel="noopener">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a><br>mycode: <a href="https://github.com/TJJTJJTJJ/pytorch_cycleGAN" target="_blank" rel="noopener">https://github.com/TJJTJJTJJ/pytorch_cycleGAN</a><br>cycle_gan的整体框架写得很漂亮，frame可以参考github的frame<br><a id="more"></a></p><h1 id="1-动态导入模块以及文件内的类"><a href="#1-动态导入模块以及文件内的类" class="headerlink" title="1.动态导入模块以及文件内的类"></a>1.动态导入模块以及文件内的类</h1><p>类似这种文件结构<br>.models<br>|— <strong>init</strong>.py<br>|— base_model.py<br>|— cycle_gan_model.py<br>|— networks.py<br>|— pix2pix_model.py<br>`— test_model.py</p><p>在<strong>init</strong>.py这样写两个函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_model_using_name</span><span class="params">(model_name)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    根据model_name导入具体模型'models/model_name_model.py'</span></span><br><span class="line"><span class="string">    :param model_name: eg. cycle_gan</span></span><br><span class="line"><span class="string">    :return: mdoel class eg.cycle_gan_model.CycleGANModle</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># step1 import 'models/model_name_model'</span></span><br><span class="line">    model_filename = <span class="string">'models.'</span>+model_name+<span class="string">'_model'</span></span><br><span class="line">    modellib = importlib.import_module(model_filename)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># step2 get model_name</span></span><br><span class="line">    model = <span class="keyword">None</span></span><br><span class="line">    target_model_name = model_name.replace(<span class="string">'_'</span>,<span class="string">''</span>)+<span class="string">'model'</span></span><br><span class="line">    <span class="keyword">for</span> name, cls <span class="keyword">in</span> modellib.__dict__.items():</span><br><span class="line">        <span class="keyword">if</span> name.lower() == target_model_name.lower() \</span><br><span class="line">                <span class="keyword">and</span> issubclass(cls, BaseModel):</span><br><span class="line">            model = cls</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> model <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        print_str = <span class="string">"In &#123;model_filename&#125;.py, there should be a subclass of BaseModel with class name "</span> \</span><br><span class="line">              <span class="string">"that matches &#123;target_model_name&#125; in lowercase."</span>.format(model_filename=model_filename, \</span><br><span class="line">                                                                      target_model_name=target_model_name)</span><br><span class="line">        print(print_str)</span><br><span class="line">        exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">modellib.__dict__ == vars(modellib)</span><br><span class="line">vars().keys() == dir()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line">modellib = importlib.import_module(model_filename)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> dir(modellib):</span><br><span class="line">    print(k)</span><br><span class="line">CycleGANModel</span><br><span class="line">__builtins__</span><br><span class="line">__cached__</span><br><span class="line">__doc__</span><br><span class="line">__file__</span><br><span class="line">__loader__</span><br><span class="line">__name__</span><br><span class="line">__package__</span><br><span class="line">__spec__</span><br><span class="line"></span><br><span class="line">print(modellib.__dict__)</span><br><span class="line">&#123;<span class="string">'__name__'</span>: <span class="string">'cycle_gan_model'</span>,</span><br><span class="line">...</span><br><span class="line"><span class="string">'CycleGANModel'</span>: cycle_gan_model.CycleGANModel&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">exit(<span class="number">0</span>)无错误退出</span><br><span class="line">exit(<span class="number">1</span>)有错误退出</span><br></pre></td></tr></table></figure><h1 id="2-学习率直线下降"><a href="#2-学习率直线下降" class="headerlink" title="2.学习率直线下降"></a>2.学习率直线下降</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.epoch_count, opt.niter + opt.niter_decay + <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lambda_rule</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    lr_l = <span class="number">1.0</span> - max(<span class="number">0</span>, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> lr_l</span><br><span class="line">scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)</span><br></pre></td></tr></table></figure><h1 id="3-NotImplemented-amp-amp-NotImplementedError"><a href="#3-NotImplemented-amp-amp-NotImplementedError" class="headerlink" title="3.NotImplemented &amp;&amp; NotImplementedError"></a>3.NotImplemented &amp;&amp; NotImplementedError</h1><p>参考:<br><a href="http://www.php.cn/python-tutorials-160083.html" target="_blank" rel="noopener">http://www.php.cn/python-tutorials-160083.html</a><br><a href="https://stackoverflow.com/questions/1062096/python-notimplemented-constant" target="_blank" rel="noopener">https://stackoverflow.com/questions/1062096/python-notimplemented-constant</a></p><p>return NotImplemented<br>raise NotImplementedError(‘initialization method [%s] is not implemented’ % init_type)</p><h1 id="4-parser的修改"><a href="#4-parser的修改" class="headerlink" title="4.parser的修改"></a>4.parser的修改</h1><p>这里既有外界传入的参数,也有自己的参数isTrain,在主函数里调用的时候调用方式是一致的,只是一个可以通过外界传参,一个不能通过外界传参</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainOptions</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(self)</span>:</span></span><br><span class="line">        parser = argparse.ArgumentParser(</span><br><span class="line">                formatter_class=argparse.ArgumentDefaultsHelpFormatter</span><br><span class="line">            )</span><br><span class="line">        parser.add_argument(<span class="string">'--batch_size'</span>, type=int, default=<span class="number">1</span>, help=<span class="string">'input batch size'</span>)</span><br><span class="line">        parser.set_defaults(dataset_mode=<span class="string">'single'</span>)</span><br><span class="line">        opt, _ = parser.parse_known_args()</span><br><span class="line"></span><br><span class="line">        self.isTrain = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">return</span> opt</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self)</span>:</span></span><br><span class="line">        opt = self.initialize()</span><br><span class="line">        opt.isTrain = self.isTrain</span><br><span class="line"></span><br><span class="line">opt = TrainOptions().parse()</span><br></pre></td></tr></table></figure><h1 id="5-eval-和test-函数的结合"><a href="#5-eval-和test-函数的结合" class="headerlink" title="5.eval()和test()函数的结合"></a>5.eval()和test()函数的结合</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    make models eval mode during test time</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> self.model_names:</span><br><span class="line">        <span class="keyword">if</span> isinstance(name, str):</span><br><span class="line">            net = getattr(self, <span class="string">'net'</span>+name)</span><br><span class="line">            net.eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    don't need backprop during test time</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        self.forward()</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">model.test()</span><br></pre></td></tr></table></figure><h1 id="6-多GPU"><a href="#6-多GPU" class="headerlink" title="6.多GPU"></a>6.多GPU</h1><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">modelb = torch.nn.DataParallel(modela, device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="comment"># save</span></span><br><span class="line">torch.save(modelb.module.cpu().state_dict(),path)</span><br><span class="line">modelb.cuda(gpu_ids[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># load</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_networks</span><span class="params">(self, epoch)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> self.model_names:</span><br><span class="line">        <span class="keyword">if</span> isinstance(name, str):</span><br><span class="line">            load_filename = <span class="string">'%s_net_%s.pth'</span> % (epoch, name)</span><br><span class="line">            load_path = os.path.join(self.save_dir, load_filename)</span><br><span class="line">            net = getattr(self, <span class="string">'net'</span> + name)</span><br><span class="line">            <span class="keyword">if</span> isinstance(net, torch.nn.DataParallel):</span><br><span class="line">                net = net.module</span><br><span class="line">            print(<span class="string">'loading the model from %s'</span> % load_path)</span><br><span class="line">            <span class="comment"># if you are using PyTorch newer than 0.4 (e.g., built from</span></span><br><span class="line">            <span class="comment"># GitHub source), you can remove str() on self.device</span></span><br><span class="line">            state_dict = torch.load(load_path, map_location=str(self.device))</span><br><span class="line">            <span class="keyword">if</span> hasattr(state_dict, <span class="string">'_metadata'</span>):</span><br><span class="line">                <span class="keyword">del</span> state_dict._metadata</span><br><span class="line"></span><br><span class="line">            <span class="comment"># patch InstanceNorm checkpoints prior to 0.4</span></span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> list(state_dict.keys()):  <span class="comment"># need to copy keys here because we mutate in loop</span></span><br><span class="line">                self.__patch_instance_norm_state_dict(state_dict, net, key.split(<span class="string">'.'</span>))</span><br><span class="line">            net.load_state_dict(state_dict)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">torch.nn.DataParallel加载预训练模型</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelA</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ModelA, self).__init__()</span><br><span class="line">        self.base1 = torch.nn.Conv2d(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">aa = ModelA()</span><br><span class="line">bb = torch.nn.DataParallel(aa, device_ids=[<span class="number">0</span>])</span><br><span class="line">bb.module.load_state_dict(torch.load(<span class="string">'aa.pth'</span>))</span><br></pre></td></tr></table></figure><p>对于单gpu和Module<br>对于普通的model.cuda,在保存模型会自动变成cpu,需要再次cuda一次<br>对于DataParallel,在保存模型会自动变成cpu,需要再次cuda一次<br>通过源码可以得知,DataParallel的device_ids初始化就已经确定,所以不用担心cuda到第一个GPU上而导致DataParallel忘记自己可以复制到哪些GPU上,会自动复制的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelA</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ModelA, self).__init__()</span><br><span class="line">        self.base = torch.nn.Conv2d(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">aa = ModelA()</span><br><span class="line">print(aa)</span><br><span class="line">ModelA(</span><br><span class="line">  (base): Conv2d(<span class="number">2</span>, <span class="number">2</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">)</span><br><span class="line">print(aa.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]])), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>]))])</span><br><span class="line">aa.cuda()</span><br><span class="line">print(aa.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]], device=<span class="string">'cuda:0'</span>)), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>], device=<span class="string">'cuda:0'</span>))]</span><br><span class="line"></span><br><span class="line">print(aa.cpu().state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.1570</span>, <span class="number">-0.2992</span>],</span><br><span class="line">                        [<span class="number">-0.2927</span>, <span class="number">-0.2748</span>]],</span><br><span class="line">                       [[<span class="number">-0.0097</span>,  <span class="number">0.0346</span>],</span><br><span class="line">                        [<span class="number">-0.3125</span>,  <span class="number">0.2615</span>]]],</span><br><span class="line">                      [[[<span class="number">-0.2506</span>, <span class="number">-0.2632</span>],</span><br><span class="line">                        [ <span class="number">0.1302</span>, <span class="number">-0.2223</span>]],</span><br><span class="line">                       [[ <span class="number">0.1422</span>,  <span class="number">0.0427</span>],</span><br><span class="line">                        [ <span class="number">0.3453</span>,  <span class="number">0.0219</span>]]]])),</span><br><span class="line">             (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1974</span>, <span class="number">-0.1549</span>]))])</span><br><span class="line"></span><br><span class="line">print(aa.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.1570</span>, <span class="number">-0.2992</span>],</span><br><span class="line">          [<span class="number">-0.2927</span>, <span class="number">-0.2748</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">-0.0097</span>,  <span class="number">0.0346</span>],</span><br><span class="line">          [<span class="number">-0.3125</span>,  <span class="number">0.2615</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2506</span>, <span class="number">-0.2632</span>],</span><br><span class="line">          [ <span class="number">0.1302</span>, <span class="number">-0.2223</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.1422</span>,  <span class="number">0.0427</span>],</span><br><span class="line">          [ <span class="number">0.3453</span>,  <span class="number">0.0219</span>]]]])), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1974</span>, <span class="number">-0.1549</span>]))])</span><br><span class="line"></span><br><span class="line">bb = torch.nn.DataParallel(aa, device_ids=[<span class="number">0</span>])</span><br><span class="line">print(bb)</span><br><span class="line">DataParallel(</span><br><span class="line">  (module): ModelA(</span><br><span class="line">    (base): Conv2d(<span class="number">2</span>, <span class="number">2</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">print(bb.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'module.base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]], device=<span class="string">'cuda:0'</span>)), (<span class="string">'module.base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>], device=<span class="string">'cuda:0'</span>))])</span><br><span class="line">print(bb.module.cpu().state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.1570</span>, <span class="number">-0.2992</span>],</span><br><span class="line">                        [<span class="number">-0.2927</span>, <span class="number">-0.2748</span>]],</span><br><span class="line">                       [[<span class="number">-0.0097</span>,  <span class="number">0.0346</span>],</span><br><span class="line">                        [<span class="number">-0.3125</span>,  <span class="number">0.2615</span>]]],</span><br><span class="line"></span><br><span class="line">                      [[[<span class="number">-0.2506</span>, <span class="number">-0.2632</span>],</span><br><span class="line">                        [ <span class="number">0.1302</span>, <span class="number">-0.2223</span>]],</span><br><span class="line">                       [[ <span class="number">0.1422</span>,  <span class="number">0.0427</span>],</span><br><span class="line">                        [ <span class="number">0.3453</span>,  <span class="number">0.0219</span>]]]])),</span><br><span class="line">             (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1974</span>, <span class="number">-0.1549</span>]))])</span><br><span class="line">print(bb)</span><br><span class="line">DataParallel(</span><br><span class="line">  (module): ModelA(</span><br><span class="line">    (base): Conv2d(<span class="number">2</span>, <span class="number">2</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">print(bb.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]])), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>]))])</span><br><span class="line">bb.cuda(gpu_ids[<span class="number">0</span>])</span><br><span class="line">print(bb.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'module.base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]], device=<span class="string">'cuda:0'</span>)), (<span class="string">'module.base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>], device=<span class="string">'cuda:0'</span>))])</span><br></pre></td></tr></table></figure><h1 id="7-Norm"><a href="#7-Norm" class="headerlink" title="7.Norm"></a>7.Norm</h1><p>参考：<br><a href="https://blog.csdn.net/liuxiao214/article/details/81037416" target="_blank" rel="noopener">https://blog.csdn.net/liuxiao214/article/details/81037416</a></p><p>输入图像：[N,C,H,W]<br>BatchNorm: [1,C,1,1]<br>InstanceNorm: [N,C,1,1]</p><p>经过实验,instanceNorm层的weight, bias, running_mean, running_var总是None<br>代码中加载模型的时候对instanceNorm层进行了删除操作,为什么<br>对于pytorch之前的版本instanceNorm层是有running_mean和running_var的,之后的版本修正了之后,就不再需要了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__patch_instance_norm_state_dict</span><span class="params">(self, state_dict, module, keys, i=<span class="number">0</span>)</span>:</span></span><br><span class="line">    key = keys[i]</span><br><span class="line">    <span class="keyword">if</span> i + <span class="number">1</span> == len(keys):  <span class="comment"># at the end, pointing to a parameter/buffer</span></span><br><span class="line">        <span class="keyword">if</span> module.__class__.__name__.startswith(<span class="string">'InstanceNorm'</span>) <span class="keyword">and</span> \</span><br><span class="line">                (key == <span class="string">'running_mean'</span> <span class="keyword">or</span> key == <span class="string">'running_var'</span>):</span><br><span class="line">            <span class="keyword">if</span> getattr(module, key) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                state_dict.pop(<span class="string">'.'</span>.join(keys))</span><br><span class="line">        <span class="keyword">if</span> module.__class__.__name__.startswith(<span class="string">'InstanceNorm'</span>) <span class="keyword">and</span> \</span><br><span class="line">           (key == <span class="string">'num_batches_tracked'</span>):</span><br><span class="line">            state_dict.pop(<span class="string">'.'</span>.join(keys))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="8-functools"><a href="#8-functools" class="headerlink" title="8.functools"></a>8.functools</h1><p>偏函数：适合为多个调用函数提供一致的函数接口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(m,n,p)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> m*n*p</span><br><span class="line">re=partial(f,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">print(re(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 60</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> type(norm_layer) == functools.partial:</span><br><span class="line">    use_bias = norm_layer.func == nn.InstanceNorm2d</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    use_bias = norm_layer == nn.InstanceNorm2d</span><br></pre></td></tr></table></figure><h1 id="9-论文与代码"><a href="#9-论文与代码" class="headerlink" title="9.论文与代码"></a>9.论文与代码</h1><h2 id="ndf"><a href="#ndf" class="headerlink" title="ndf"></a>ndf</h2><p>模型的定义与论文有一个地方不一致,论文写的第一个conv之后通道数是32,但实现是64.<br>与作者沟通得知,第一层不是32,而是64,剩下的也依次递增.</p><p>下采样的时候没有使用reflect进行补充,而是使用了0填充.<br>与作者沟通后，提出的是都可以尝试一下</p><h2 id="unet-model"><a href="#unet-model" class="headerlink" title="unet model"></a>unet model</h2><p>Unet model<br>与网上的不是很一致<br>3-&gt;<em>1-&gt;</em>2-&gt;<em>4-&gt;</em>8-&gt;<em>8-&gt;</em>8<br>3&lt;-<em>2&lt;-</em>4&lt;-<em>8&lt;-</em>16&lt;-<em>16&lt;-</em>16</p><h2 id="参数-no-lsgan"><a href="#参数-no-lsgan" class="headerlink" title="参数 no_lsgan"></a>参数 no_lsgan</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">self.criterionGAN = networks.GANLoss(use_lsgan=<span class="keyword">not</span> opt.no_lsgan).to(self.device)</span><br><span class="line"><span class="comment"># GAN loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GANLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_lsgan=True, target_real_label=<span class="number">1.0</span>, target_fake_label=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(GANLoss, self).__init__()</span><br><span class="line">        self.register_buffer(<span class="string">'real_label'</span>, torch.tensor(target_real_label))</span><br><span class="line">        self.register_buffer(<span class="string">'fake_label'</span>, torch.tensor(target_fake_label))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_lsgan:</span><br><span class="line">            self.loss = nn.MSELoss()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">use_sigmoid = opt.no_lsgan</span><br><span class="line">self.netD_A = define_D(opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm,</span><br><span class="line">                    use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)</span><br></pre></td></tr></table></figure><p>也就是<br>opt.no_lsgan为True时, netD使用sigmoid, GANloss使用BCELoss()<br>opt.no_lsgan为False时, netD不使用sigmoid, GANloss使用MSELoss()</p><p>MSELoss:均方误差 (x-y)<em>*2<br>BCELoss:二分类的交叉熵:使用前需要使用sigmoid函数,input和target的输入维度是一样的.(N,</em>)</p><p>根据作者提供的运行代码,猜测作者使用的是opt.no_lsgan为False,均方误差</p><p>L1loss: |x-y|</p><h2 id="G-and-D-的反向传播过程"><a href="#G-and-D-的反向传播过程" class="headerlink" title="G and D 的反向传播过程"></a>G and D 的反向传播过程</h2><p>回顾一下G和D的反向传播<br><strong>train G</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set_requires_grad(D, <span class="keyword">False</span>)</span><br><span class="line">fake_A = G(real_A)</span><br><span class="line">loss = criterion(D(fake_A), <span class="keyword">True</span>)</span><br><span class="line">optimizers_G.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizers_G.step()</span><br></pre></td></tr></table></figure><p><strong>train D</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set_requires_grad(D, <span class="keyword">False</span>)</span><br><span class="line"><span class="comment">#fake_A = fake_A.detach() # 取消G的grad</span></span><br><span class="line">loss1 = criterion(fake_A.detach(), <span class="keyword">False</span>)</span><br><span class="line">loss2 = criterion(realA, <span class="keyword">True</span>)</span><br><span class="line">loss = loss1 + loss2</span><br><span class="line">optimizers_D.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizers_D.step()</span><br></pre></td></tr></table></figure><h1 id="10-ConTransposed的计算方法"><a href="#10-ConTransposed的计算方法" class="headerlink" title="10.ConTransposed的计算方法"></a>10.ConTransposed的计算方法</h1><p>逆卷积后的图像大小和之前的能对应上，需要output_padding</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nn.ConvTranspose2d(ngf*mult, int(ngf*mult/<span class="number">2</span>), kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>, bias=use_bias)]</span><br><span class="line">-k+<span class="number">2</span>p+s-out_padding是s的整数</span><br><span class="line">k=<span class="number">3</span>,s=<span class="number">2</span>,p=<span class="number">1</span>,则out_padding=<span class="number">1</span></span><br><span class="line">k=<span class="number">3</span>,s=<span class="number">4</span>,p=<span class="number">1</span>,则out_padding=<span class="number">3</span></span><br></pre></td></tr></table></figure><h1 id="11-初始化参数"><a href="#11-初始化参数" class="headerlink" title="11.初始化参数"></a>11.初始化参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(net, init_type=<span class="string">'normal'</span>, gain=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_func</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="comment"># conv, contranspose ,linear, bn</span></span><br><span class="line">        <span class="comment"># type(m) == nn.Conv2d</span></span><br><span class="line">        classname = m.__class__.__name__</span><br><span class="line">        <span class="keyword">if</span> hasattr(m, <span class="string">'weight'</span>) <span class="keyword">and</span> (classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span> <span class="keyword">or</span> classname.find(<span class="string">'Linear'</span>) != <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> init_type == <span class="string">'normal'</span>:</span><br><span class="line">                init.normal_(m.weight.data, <span class="number">0.0</span>, gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'xavier'</span>:</span><br><span class="line">                init.xavier_normal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'kaiming'</span>:</span><br><span class="line">                init.kaiming_normal_(m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_in'</span>)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'orthogonal'</span>:</span><br><span class="line">                init.orthogonal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> NotImplementedError(<span class="string">'initialization method [%s] is not implemented'</span> % init_type)</span><br><span class="line">            <span class="keyword">if</span> hasattr(m, <span class="string">'bias'</span>) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm2d'</span>) != <span class="number">-1</span>:</span><br><span class="line">            init.normal_(m.weight.data, <span class="number">1.0</span>, gain)</span><br><span class="line">            init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'initialize network with %s'</span> % init_type)</span><br><span class="line">    net.apply(init_func)</span><br></pre></td></tr></table></figure><h1 id="12-disriminator-PatchGAN-and-GANLoss"><a href="#12-disriminator-PatchGAN-and-GANLoss" class="headerlink" title="12.disriminator PatchGAN and GANLoss"></a>12.disriminator PatchGAN and GANLoss</h1><p>PatchGAN的kernel是4.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GANLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_lsgan=True, target_real_label=<span class="number">1.0</span>, target_fake_label=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(GANLoss, self).__init__()</span><br><span class="line">        self.register_buffer(<span class="string">'real_label'</span>, torch.tensor(target_real_label))</span><br><span class="line">        self.register_buffer(<span class="string">'fake_label'</span>, torch.tensor(target_fake_label))</span><br><span class="line">        <span class="keyword">if</span> use_lsgan:</span><br><span class="line">            self.loss = nn.MSELoss()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_target_tensor</span><span class="params">(self, input, target_is_real)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> target_is_real:</span><br><span class="line">            target_tensor = self.real_label</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target_tensor = self.fake_label</span><br><span class="line">        <span class="keyword">return</span> target_tensor.expand_as(input)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, input, target_is_real)</span>:</span></span><br><span class="line">        target_tensor = self.get_target_tensor(input, target_is_real)</span><br><span class="line">        <span class="keyword">return</span> self.loss(input, target_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defines the PatchGAN discriminator with the specified arguments.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NLayerDiscriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_nc, ndf=<span class="number">64</span>, n_layers=<span class="number">3</span>, norm_layer=nn.BatchNorm2d, use_sigmoid=False)</span>:</span></span><br><span class="line">        super(NLayerDiscriminator, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> type(norm_layer) == functools.partial:</span><br><span class="line">            use_bias = norm_layer.func == nn.InstanceNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_bias = norm_layer == nn.InstanceNorm2d</span><br><span class="line"></span><br><span class="line">        kw = <span class="number">4</span></span><br><span class="line">        padw = <span class="number">1</span></span><br><span class="line">        sequence = [</span><br><span class="line">            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=<span class="number">2</span>, padding=padw),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        nf_mult = <span class="number">1</span></span><br><span class="line">        nf_mult_prev = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">            nf_mult_prev = nf_mult</span><br><span class="line">            nf_mult = min(<span class="number">2</span>**n, <span class="number">8</span>)</span><br><span class="line">            sequence += [</span><br><span class="line">                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,</span><br><span class="line">                          kernel_size=kw, stride=<span class="number">2</span>, padding=padw, bias=use_bias),</span><br><span class="line">                norm_layer(ndf * nf_mult),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">            ]</span><br><span class="line"></span><br><span class="line">        nf_mult_prev = nf_mult</span><br><span class="line">        nf_mult = min(<span class="number">2</span>**n_layers, <span class="number">8</span>)</span><br><span class="line">        sequence += [</span><br><span class="line">            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,</span><br><span class="line">                      kernel_size=kw, stride=<span class="number">1</span>, padding=padw, bias=use_bias),</span><br><span class="line">            norm_layer(ndf * nf_mult),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        sequence += [nn.Conv2d(ndf * nf_mult, <span class="number">1</span>, kernel_size=kw, stride=<span class="number">1</span>, padding=padw)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_sigmoid:</span><br><span class="line">            sequence += [nn.Sigmoid()]</span><br><span class="line"></span><br><span class="line">        self.model = nn.Sequential(*sequence)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.model(input)</span><br><span class="line"></span><br><span class="line">pred_real = netD(real)</span><br><span class="line">loss_D_real = self.criterionGAN(pred_real, <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="GANLoss的备注"><a href="#GANLoss的备注" class="headerlink" title="GANLoss的备注"></a>GANLoss的备注</h2><p>使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.<strong>call</strong>(input)，在<strong>call</strong>函数中，主要调用的是 layer.forward(x)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。</p><h1 id="13-PatchGAN的感受野"><a href="#13-PatchGAN的感受野" class="headerlink" title="13.PatchGAN的感受野"></a>13.PatchGAN的感受野</h1><p>论文使用的是70X70 PatchGAN<br>PatchGAN:<br>paper:<br>Image-to-Image Translation with Conditional Adversarial Networks<br><a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="noopener">https://arxiv.org/abs/1611.07004</a></p><p>自动计算网址：<a href="https://fomoro.com/tools/receptive-fields/" target="_blank" rel="noopener">https://fomoro.com/tools/receptive-fields/</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">感受野的计算规则</span><br><span class="line">对于第m层,m=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,...,N. hm表示第m层应该有的视野,假设mN=<span class="number">1</span></span><br><span class="line">km, sm, pm,表示第m<span class="number">-1</span>层到第m层的conv的kernel</span><br><span class="line">第一层对于第<span class="number">0</span>层的感受野</span><br><span class="line">h1 = <span class="number">1</span>, </span><br><span class="line">(h0-k1)/s1+<span class="number">1</span>=h1</span><br><span class="line">第二层对于第<span class="number">0</span>层的感受野</span><br><span class="line">h2 = <span class="number">1</span></span><br><span class="line">(h1-k2)/s2+<span class="number">1</span>=h2</span><br><span class="line">(h0-k1)/s1+<span class="number">1</span>=h1</span><br><span class="line">依次类推</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(output_size, ksize, stride)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (output_size - <span class="number">1</span>) * stride + ksize</span><br><span class="line"></span><br><span class="line">last_layer = f(output_size=<span class="number">1</span>, ksize=<span class="number">4</span>, stride=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Receptive field: 4</span></span><br><span class="line">fourth_layer = f(output_size=last_layer, ksize=<span class="number">4</span>, stride=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Receptive field: 7</span></span><br><span class="line">third_layer = f(output_size=fourth_layer, ksize=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Receptive field: 16</span></span><br><span class="line">second_layer = f(output_size=third_layer, ksize=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Receptive field: 34</span></span><br><span class="line">first_layer = f(output_size=second_layer, ksize=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Receptive field: 70</span></span><br><span class="line"></span><br><span class="line">print(first_layer)</span><br></pre></td></tr></table></figure><h1 id="14-torch-tensor-clone"><a href="#14-torch-tensor-clone" class="headerlink" title="14.torch.tensor.clone()"></a>14.torch.tensor.clone()</h1><p>clone()<br>梯度受影响,clone之后的新的tensor的梯度也会影响到原tensor,但是新tensor本身是没有梯度的.<br>clone之后的新tensor的改变不会影响原有的tensor<br>应该这么理解,clone也是计算图中的一个操作,这样的话就可以解释通了.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input = torch.ones(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">input.requires_grad = <span class="keyword">True</span></span><br><span class="line">input2 = input.clone()</span><br><span class="line">print(input2.requires_grad)</span><br><span class="line">y = input.sum()</span><br><span class="line">y.backward()</span><br><span class="line">print(input.grad)</span><br><span class="line"><span class="comment"># 1,1,1...</span></span><br><span class="line">print(input2.grad)</span><br><span class="line"><span class="comment"># None</span></span><br><span class="line">y = input2.sum()</span><br><span class="line">y.backward()</span><br><span class="line">print(input.grad)</span><br><span class="line"><span class="comment"># 2,2,2...</span></span><br><span class="line">print(input2.grad)</span><br><span class="line"><span class="comment"># None</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input = torch.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">input.requires_grad = <span class="keyword">True</span></span><br><span class="line">input2 = input.clone()</span><br><span class="line">input2[<span class="number">1</span>,<span class="number">1</span>] = <span class="number">6</span></span><br><span class="line">print(input2)</span><br><span class="line">print(input)</span><br><span class="line">tensor([[ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">6.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line">tensor([[ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line">input2[<span class="number">1</span>,<span class="number">1</span>] = <span class="number">6</span></span><br><span class="line">y = (input2*input2).sum()</span><br><span class="line">y.backward()</span><br><span class="line">print(input.grad)</span><br><span class="line">print(input2.grad)</span><br><span class="line">tensor([[ <span class="number">8.</span>,  <span class="number">8.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">8.</span>,  <span class="number">8.</span>]])</span><br><span class="line"><span class="keyword">None</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(input2.grad_fn)</span><br><span class="line">&lt;CopySlices object at <span class="number">0x7fe89dc841d0</span>&gt;</span><br></pre></td></tr></table></figure><p>clone的用法<br>tensor保留梯度的交换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tmp = tensor1.clone()</span><br><span class="line">tensor2 = tmp</span><br><span class="line">tensor1 = tensor3</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tensor1, tensor2 = tensor3, tensor1.clone()</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tmp = self.images[random_id].clone()</span><br><span class="line">self.images[random_id] = image</span><br><span class="line">return_images.append(tmp)</span><br></pre></td></tr></table></figure><h1 id="15-from-XX-import"><a href="#15-from-XX-import" class="headerlink" title="15.from XX import"></a>15.from XX import</h1><p>这里还有一些不太对的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .base_model <span class="keyword">import</span> BaseModel <span class="comment"># 同一个文件夹</span></span><br><span class="line"><span class="keyword">from</span> util.image_pool <span class="keyword">import</span> <span class="comment"># 父级文件夹</span></span><br><span class="line"><span class="comment"># 建议</span></span><br><span class="line"><span class="keyword">from</span> .base_model <span class="keyword">import</span> BaseModel <span class="comment"># 同一个文件夹</span></span><br><span class="line"><span class="keyword">from</span> ..util.image_pool <span class="keyword">import</span> <span class="comment"># 父级文件夹</span></span><br></pre></td></tr></table></figure><h1 id="16-register-buffer"><a href="#16-register-buffer" class="headerlink" title="16.register_buffer"></a>16.register_buffer</h1><p>register_buffer<br>self.register_buffer可以将tensor注册成buffer，在forward中使用self.mybuffer, 而不是self.mybuffer_tmp.<br>定义Parameter和buffer都只需要传入Tensor即可。也不需要将其转成gpu。这是因为，当网络进行.cuda()时候，会自动将里面的层的参数,buffer等转换成相应的GPU上。<br>网络存储时也会将buffer存下，当网络load模型时，会将存储的模型的buffer也进行赋值。<br>buffer的更新在forward中，optim.step只能更新nn.Parameter类型的参数。<br>用法<br>self.register_buffer(‘running_mean’, torch.zeros(num_features))</p><h1 id="17-itertools"><a href="#17-itertools" class="headerlink" title="17. itertools"></a>17. itertools</h1><p>无限迭代器<br>itertools，用于创建高效迭代器的函数,<br>itertools.chain 连接多个列表或者迭代器。<br>将多个网络写在一起,使用一个优化器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),</span><br><span class="line">                                    lr=opt.lr, betas=(opt.beta1, <span class="number">0.999</span>))</span><br><span class="line">self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()),</span><br><span class="line">                                    lr=opt.lr, betas=(opt.beta1, <span class="number">0.999</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自然数无限迭代器</span></span><br><span class="line"><span class="comment"># itertools.count</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> itertools</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>natuals = itertools.count(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> natuals:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 序列无限重复</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> itertools</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cs = itertools.cycle(<span class="string">'ABC'</span>) <span class="comment"># 注意字符串也是序列的一种</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> c <span class="keyword">in</span> cs:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> c</span><br><span class="line">...</span><br><span class="line"><span class="string">'A'</span></span><br><span class="line"><span class="string">'B'</span></span><br><span class="line"><span class="string">'C'</span></span><br><span class="line"><span class="string">'A'</span></span><br><span class="line"><span class="string">'B'</span></span><br><span class="line"><span class="string">'C'</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单元素无限重复</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ns = itertools.repeat(<span class="string">'A'</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> ns:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line">打印<span class="number">10</span>次<span class="string">'A'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 无限迭代器中截取有限序列</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>natuals = itertools.count(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ns = itertools.takewhile(<span class="keyword">lambda</span> x: x &lt;= <span class="number">10</span>, natuals)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> ns:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line">打印出<span class="number">1</span>到<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代对象的串联</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> itertools.chain(<span class="string">'ABC'</span>, <span class="string">'XYZ'</span>):</span><br><span class="line">    <span class="keyword">print</span> c</span><br><span class="line"><span class="comment"># 迭代效果：'A' 'B' 'C' 'X' 'Y' 'Z'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代器中相邻的重复元素挑出来放在一起</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> key, group <span class="keyword">in</span> itertools.groupby(<span class="string">'AAABBBCCAAA'</span>):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> key, list(group) <span class="comment"># 为什么这里要用list()函数呢？</span></span><br><span class="line">...</span><br><span class="line">A [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'A'</span>]</span><br><span class="line">B [<span class="string">'B'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">C [<span class="string">'C'</span>, <span class="string">'C'</span>]</span><br><span class="line">A [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'A'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># imap, ifilter</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> x <span class="keyword">in</span> itertools.imap(<span class="keyword">lambda</span> x, y: x * y, [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>], itertools.count(<span class="number">1</span>)):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> x</span><br><span class="line">...</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">40</span></span><br><span class="line"><span class="number">90</span></span><br></pre></td></tr></table></figure><h1 id="18-visdom"><a href="#18-visdom" class="headerlink" title="18.visdom"></a>18.visdom</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.vis = visdom.Visdom(server=opt.display_server, port=opt.display_port, </span><br><span class="line">                         env=opt.display_env, raise_exceptions=<span class="keyword">True</span>, use_incoming_socket=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># env根据_自动分层。e.g. cycle_gan--&gt;cycle,cycle_gan</span></span><br></pre></td></tr></table></figure><h1 id="19-三引号"><a href="#19-三引号" class="headerlink" title="19.三引号"></a>19.三引号</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">三引号的作用</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>str1 = <span class="string">"""List of name:</span></span><br><span class="line"><span class="string"><span class="meta">... </span>Hua Li</span></span><br><span class="line"><span class="string"><span class="meta">... </span>Chao Deng</span></span><br><span class="line"><span class="string"><span class="meta">... </span>&#123;&#125;</span></span><br><span class="line"><span class="string"><span class="meta">... </span>"""</span>.format(<span class="string">'hhh'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(str1)</span><br><span class="line">List of name:</span><br><span class="line">Hua Li</span><br><span class="line">Chao Deng</span><br></pre></td></tr></table></figure><h1 id="20-异常"><a href="#20-异常" class="headerlink" title="20.异常"></a>20.异常</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.触发异常</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mye</span><span class="params">( level )</span>:</span></span><br><span class="line">    <span class="keyword">if</span> level &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"Invalid level!"</span>)</span><br><span class="line">        <span class="comment"># 触发异常后，后面的代码就不会再执行</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    mye(<span class="number">0</span>)            <span class="comment"># 触发异常</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> err:</span><br><span class="line">    print(<span class="number">1</span>,err)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">1</span> Invalid level!</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.自定义异常</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyException</span><span class="params">(Exception)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,message)</span>:</span></span><br><span class="line">        Exception.__init__(self)</span><br><span class="line">        self.message=message </span><br><span class="line">        print(<span class="string">'This is MyException'</span>)</span><br><span class="line">a=<span class="number">7</span></span><br><span class="line"><span class="keyword">if</span> a&lt;<span class="number">10</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">raise</span> MyException(<span class="string">"my excepition is raised "</span>)</span><br><span class="line">    <span class="keyword">except</span> MyException <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">'*****************'</span>)</span><br><span class="line">        print(e.message)    </span><br><span class="line">This <span class="keyword">is</span> MyException</span><br><span class="line">my excepition <span class="keyword">is</span> raised</span><br></pre></td></tr></table></figure><h1 id="21-自定义类的iter"><a href="#21-自定义类的iter" class="headerlink" title="21.自定义类的iter"></a>21.自定义类的iter</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义类的iter</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cl1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.N = <span class="number">10</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            print(i)</span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="number">5</span>:</span><br><span class="line">                <span class="keyword">yield</span> i</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">cc = cl1()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> cc:</span><br><span class="line">    print(<span class="string">'hhh'</span>,i)</span><br><span class="line"><span class="number">0</span></span><br><span class="line">hhh <span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line">hhh <span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">hhh <span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">hhh <span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line">hhh <span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇博客主要记录在跟随cycleGAN作者的代码复现学到的东西。&lt;br&gt;title: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(ICCV2017)&lt;/p&gt;
&lt;p&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1703.10593&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1703.10593&lt;/a&gt;&lt;br&gt;code: &lt;a href=&quot;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&lt;/a&gt;&lt;br&gt;mycode: &lt;a href=&quot;https://github.com/TJJTJJTJJ/pytorch_cycleGAN&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/TJJTJJTJJ/pytorch_cycleGAN&lt;/a&gt;&lt;br&gt;cycle_gan的整体框架写得很漂亮，frame可以参考github的frame&lt;br&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://yoursite.com/categories/DeepLearning/"/>
    
    
      <category term="cycleGAN" scheme="http://yoursite.com/tags/cycleGAN/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow</title>
    <link href="http://yoursite.com/2018/11/05/tensorflow/"/>
    <id>http://yoursite.com/2018/11/05/tensorflow/</id>
    <published>2018-11-05T14:41:03.000Z</published>
    <updated>2018-11-05T15:10:47.797Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇关于tensorflow的博客，这里面很多东西都是很杂碎的，不在此做处理，等积累的多了，理解才能正确。<br><a id="more"></a></p><p>TensorFlow入门教程</p><p>1.TensorFlow深度学习应用实践<br>评价不好</p><ol><li><p>TensorFlow：实战Google深度学习框架（第2版）<br>8.6分，可以用来实践</p></li><li><p>Tensorflow：实战Google深度学习框架<br>8.4分</p></li></ol><p>4.莫烦的tensorlfow教程<br><a href="https://github.com/MorvanZhou" target="_blank" rel="noopener">https://github.com/MorvanZhou</a><br>适合实践</p><p>5.某个网友的自己实现的教程<br><a href="https://www.jianshu.com/p/27a2fb320934" target="_blank" rel="noopener">https://www.jianshu.com/p/27a2fb320934</a><br><a href="https://github.com/zhaozhengcoder/Machine-Learning/tree/master/tensorflow_tutorials" target="_blank" rel="noopener">https://github.com/zhaozhengcoder/Machine-Learning/tree/master/tensorflow_tutorials</a></p><p>6.官网API<br><a href="https://tensorflow.google.cn/api_docs/python/tf" target="_blank" rel="noopener">https://tensorflow.google.cn/api_docs/python/tf</a></p><p>7.深度学习之TensorFlow入门、原理与进阶实战<br>7.6分<br>22章，内容更加详实，偏向理论，可以用来只看不实践</p><p>8.TensorFlow实战<br>7.3分<br>适合看看，内容不深，实践性不强，理论也很浅<br>在github上也没有代码</p><p>不应该总是要求全部，所以应该这样的顺序来学习<br>先学：TensorFlow：实战Google深度学习框架（第2版）<br>再学：莫烦：<a href="https://github.com/MorvanZhou+网页的教程" target="_blank" rel="noopener">https://github.com/MorvanZhou+网页的教程</a><br>基本就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">print(a.graph == tf.get_default_graph())</span><br><span class="line">tf.get_variable() name 必须双引号</span><br><span class="line"></span><br><span class="line">name的作用  https://blog.csdn.net/xiaohuihui1994/article/details/<span class="number">81022043</span></span><br><span class="line"></span><br><span class="line">可以理解成sess需要指定，不能自动加入</span><br><span class="line">.run,.eval能执行的两种方式</span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g1) <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">或者</span><br><span class="line">sess = tf.Session()</span><br><span class="line">tf.global_variables_initializer().run(session=sess)</span><br><span class="line">或者</span><br><span class="line">sess = tf.InteractiveSession() <span class="comment"># 会自动注册为默认会话</span></span><br><span class="line">result.eval()</span><br><span class="line">或者</span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    result.eval()</span><br><span class="line"></span><br><span class="line"><span class="comment">####</span></span><br><span class="line">初始化</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">w1.initializer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">『TensorFlow』使用集合collection控制variables</span><br><span class="line">https://www.cnblogs.com/hellcat/p/<span class="number">9006904.</span>html</span><br><span class="line"></span><br><span class="line">collection</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>], initializer = tf.zeros_initializer()) <span class="comment"># 设置初始值为0</span></span><br><span class="line">    gv= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES <span class="comment"># gv = tf.global_variables()</span></span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> gv:</span><br><span class="line">        print(var)</span><br><span class="line"></span><br><span class="line">现在有了几个概念需要理清楚：</span><br><span class="line">计算图： 不同计算图中的变量是独立的</span><br><span class="line">collection： 不同类型的variable放在不同的collection中，主要是tf.GraphKeys.GLOBAL_VARIABLES和tf.GraphKeys.TRAINABLE_VARIABLES</span><br><span class="line">会话： 会话需要与计算图相连接，完成相应计算图的执行，一个会话对应一个计算图及其执行结果</span><br><span class="line"></span><br><span class="line">tf.add_to_collection</span><br><span class="line">https://www.jianshu.com/p/<span class="number">6612</span>f368e8f4</span><br><span class="line"></span><br><span class="line">这样就不需要传入weighs和biases，这里的reuse实现了定义和使用的一体化，不需要专门对weights定义和调用。</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, reuse=False)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1'</span>, reuse=reuse):</span><br><span class="line">        weights = tf.get_variable(<span class="string">"weights"</span>)</span><br><span class="line">        biases = tf.get_variable(<span class="string">"biases"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer2'</span>, reuse=reuse):</span><br><span class="line">        weights = tf.get_variable(<span class="string">"weights"</span>)</span><br><span class="line">        biases = tf.get_variable(<span class="string">"biases"</span>)</span><br><span class="line"></span><br><span class="line">TFRecord数据格式</span><br><span class="line">https://blog.csdn.net/u012759136/article/details/<span class="number">52232266</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一篇关于tensorflow的博客，这里面很多东西都是很杂碎的，不在此做处理，等积累的多了，理解才能正确。&lt;br&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
</feed>
