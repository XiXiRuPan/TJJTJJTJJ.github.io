<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>田佳杰</title>
  <icon>https://www.gravatar.com/avatar/2d54367ed6dc965439f08c9f1b75cea4</icon>
  <subtitle>代码的搬运工</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-05-24T02:44:44.356Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jiajie Tian</name>
    <email>tianjiajie1881090@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Deep CV-MIML</title>
    <link href="http://yoursite.com/2019/05/22/Deep-CV-MIML/"/>
    <id>http://yoursite.com/2019/05/22/Deep-CV-MIML/</id>
    <published>2019-05-22T02:26:35.000Z</published>
    <updated>2019-05-24T02:44:44.356Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1904.03832" target="_blank" rel="noopener">CVPR2019 Weakly Supervised Person Re-Identification</a></li><li>code:</li></ul><a id="more"></a><p>这篇论文主要的创新点是：不再需要在 video 的每一帧都给出行人的位置和 id，而是直接赋予一段 video 一个 video-level label，即这段 video 中有哪些人。主要目的也是为了降低标注成本。有点重新定义行人重识别问题的意思。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>在传统的行人重识别设定中，每张图片表示用方框圈出来的行人。因此，作者提出了 <strong>weakly supervised setting</strong>，即在训练阶段，只有 video-level label，因此此时每段 video 中有多个行人，有个标签，即 multi-instance multi-label(<strong>MIML</strong>)。</p><p>提出：</p><ul><li>intra-bag alignment</li><li>cross-view bag alignment</li></ul><img src="/2019/05/22/Deep-CV-MIML/weakly_supervised_setting.png" title="weakly supervised setting"><p>基本定义：</p><ul><li>gallery set 中的 every video clip 为一个 bag</li><li>每个 bag 包含多个行人的图片，并标记 video-level label</li><li>probe set 中是经过手工标记的图片，为了统一，也视这些图片为一个 bag</li></ul><p>解决的问题：</p><ul><li>每个 bag 中包含相同的行人: intra-bag alignment</li><li>bag 与 bag （跨摄像头）之间也包含相同的行人: cross-view bag alignment</li></ul><p>现有的 MIML 直接应用到 person re-id 中存在的问题</p><ul><li>现有的 MIML 忽略了 intra-bag variation，即 bag 内会有相同类别的实例</li><li>现有的 MIML 假设 instance-level label 是高度相关的，但是 person re-id 中的行人都相互独立的</li><li>现有的 MIML 没有考虑到 bag 之间的 (cross-view) 相关性</li></ul><h2 id="2-The-Proposed-Approach"><a href="#2-The-Proposed-Approach" class="headerlink" title="2. The Proposed Approach"></a>2. The Proposed Approach</h2><h3 id="2-1-Problem-Statement-and-Notation"><a href="#2-1-Problem-Statement-and-Notation" class="headerlink" title="2.1 Problem Statement and Notation"></a>2.1 Problem Statement and Notation</h3><p>在 weakly supervised person re-id setting 中，不再是以图搜图，而是以图搜视频，或者以视频搜视频，即给定同一个行人组成的视频，在只经过自动画框的视频中找出这个行人。</p><div class="table-container"><table><thead><tr><th style="text-align:left">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">$C$</td><td style="text-align:left">identities</td></tr><tr><td style="text-align:left">$V$</td><td style="text-align:left">camera views</td></tr><tr><td style="text-align:left">$\tilde{C}$</td><td style="text-align:left">$\tilde{C}=C+1$</td></tr><tr><td style="text-align:left">$\cal{X}$</td><td style="text-align:left">the training set</td></tr><tr><td style="text-align:left">$N_{\cal{X}}$</td><td style="text-align:left">$N_{\cal{X}}$ video in the training set</td></tr><tr><td style="text-align:left">$\cal{X}_p, \cal{X}_g$</td><td style="text-align:left">probe set and gallery set,and $\cal{X}_p + \cal{X}_g=\cal{X}$</td></tr><tr><td style="text-align:left">$\cal{X}_p=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_p}$</td><td style="text-align:left">probe set</td></tr><tr><td style="text-align:left">$\cal{X}_g=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_g}$</td><td style="text-align:left">gallery set</td></tr><tr><td style="text-align:left">$v_b \in \lbrace 1,2,…,V \rbrace$</td><td style="text-align:left">摄像头</td></tr><tr><td style="text-align:left">$y_b=[y_b^0, y_b^1,…, y_b^C]\in\lbrace 0,1 \rbrace^{\tilde{C}}$</td><td style="text-align:left">行人标签</td></tr><tr><td style="text-align:left">$X_b=\lbrace x_{b,1}, x_{b,2},…, x_{b,n_b} \rbrace $</td><td style="text-align:left">多实例</td></tr><tr><td style="text-align:left">$x_{b,i}=f_e(I_b;\theta)\in \mathbb{R}^d$</td><td style="text-align:left">图片 $I_b$，特征提取方程 $f_e(\cdot;\theta)$</td></tr></tbody></table></div><p>基本保证每个行人至少在两个摄像头下出现过，并视在 untrimmed videos 中的出现的未知行人为新的一类，即第0类，所以 $\tilde{C}=C+1$</p><p>Question: probe set 是图片，gallery set 是视频，那training set 是什么？也是视频吗？ gallery set 和 training set 是一个集合还是完全不同的集合？</p><p>gallery set 中的视频没有经过人工画框，只有 video-level weak label，其中的行人是自动检测出的，没有人工标定行人标签，所以具体到行人对应哪个标签也是未知的。</p><p>probe set 中，each query 由一系列同一个行人的检测图片组成。为了统一，视 probe set 中的 each query 也为一个 bag，此时 video-level 应该是单实例的。</p><p>所以，可以统一为：$\cal{X}=\lbrace \cal{X}_p, \cal{X}_g \rbrace$，其中 $\cal{X}_p=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_p}$，$\cal{X}_g=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_g}$.其中 probe set 的 bag 是单标签的，gallery set 的 bag label 为0时，表示不确定这个行人是否在这个 video 中出现过。</p><p>重新定义问题需要重新定义好多概念。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1904.03832&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019 Weakly Supervised Person Re-Identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code:&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="person re-id" scheme="http://yoursite.com/categories/person-re-id/"/>
    
    
      <category term="person re-id" scheme="http://yoursite.com/tags/person-re-id/"/>
    
  </entry>
  
  <entry>
    <title>BNNeck</title>
    <link href="http://yoursite.com/2019/05/17/BNNeck/"/>
    <id>http://yoursite.com/2019/05/17/BNNeck/</id>
    <published>2019-05-17T08:20:21.000Z</published>
    <updated>2019-05-18T07:02:18.096Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1903.07071" target="_blank" rel="noopener">Bag of Tricks and A Strong Baseline for Deep Person Re-identification</a></li><li>code:　<a href="https://github.com/michuanhaohao/reid-strong-baseline" target="_blank" rel="noopener">pytorch</a></li></ul><p>这篇论文是旷视科技 Video Team 做的。</p><p>这篇论文主要介绍 re-id 代码中的各种 trick 的作用。</p><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>作者总结了各种 trick 并与 ECCV2018 CVPR2018 的论文做了比较，实验证明，只需要通过各种 trick 的组合，就比提出的算法高很多。</p><img src="/2019/05/17/BNNeck/strong_baseline.png" title="strong baseline"><h2 id="2-Standard-Baseline"><a href="#2-Standard-Baseline" class="headerlink" title="2. Standard Baseline"></a>2. Standard Baseline</h2><ol><li>ResNet50 为基础网络，修改最后一个 fc 层的输出维度为行人类别 $N$.</li><li>随机取 $P$ 个人， 每个人 $K$ 张图片，所以每个 batch $B=P\times K$，作者设置 $P=16, K=4$.</li><li>图片 resize 成 256x128，并添加10个0元素的 pad，然后随机 crop 成 256x128.</li><li>图片随机水平翻转概率设置为0.5.</li><li>图片的像素值转化为 [0,1]，并且 normalize: mean=0.485, 0.456, 0.406, std=0.229, 0.224, 0.225.</li><li>模型提取特征记为 $f$，ID 的预测值为 $p$.</li><li>ReID features $f$ 用于计算 triplet loss，ID prediction logits $p$ 用于计算交叉熵，margin $m=0.3$</li><li>优化器: Adam: lr=0.00035, epoch=120, 第40和第70个 epoch 乘 0.1.</li></ol><h2 id="3-Training-Tricks"><a href="#3-Training-Tricks" class="headerlink" title="3. Training Tricks"></a>3. Training Tricks</h2><img src="/2019/05/17/BNNeck/pipelines_of_baseline.png" title="pipelines of baseline"><h3 id="3-1-Warmup-Learning-Rate"><a href="#3-1-Warmup-Learning-Rate" class="headerlink" title="3.1 Warmup Learning Rate"></a>3.1 Warmup Learning Rate</h3><p>Question: 前10个epoch是不是写错了，应该是-4，不是-5</p><img src="/2019/05/17/BNNeck/warmup.png" title="warmup"><script type="math/tex; mode=display">lr(t)=\begin{cases}3.5\times 10^{-5}\times \frac{t}{10}, &\text{if } 10 \ge t \ge 1 \\3.5\times 10^{-4}, &\text{if } 40 \ge t \ge 10 \\3.5\times 10^{-5}, &\text{if } 70 \ge t \ge 40 \\3.5\times 10^{-6}, &\text{if } 120 \ge t \ge 70 \\\end{cases}</script><h3 id="3-2-Random-Erasing-Augmentation"><a href="#3-2-Random-Erasing-Augmentation" class="headerlink" title="3.2 Random Erasing Augmentation"></a>3.2 Random Erasing Augmentation</h3><img src="/2019/05/17/BNNeck/REA.png" title="REA"><ul><li>probability: $p_e=0.5$</li><li>rectangle region $I_e: S_e=W_e\times H_e$, $0.02&lt;S_e&lt;0.4$</li><li>area ratio: $r_1&lt; r_e=\frac{S_e}{S} &lt; r_2$, $r_1=0.3, r_2=3.33$</li></ul><h3 id="3-3-Label-Smoothing"><a href="#3-3-Label-Smoothing" class="headerlink" title="3.3 Label Smoothing"></a>3.3 Label Smoothing</h3><script type="math/tex; mode=display">q_i=\begin{cases}1-\frac{N-1}{N}\epsilon, &\text{if } i=y \\\frac{\epsilon}{N}, &\text{otherwise}\end{cases}</script><p>$\epsilon=0.1$</p><h3 id="3-4-Last-Stride"><a href="#3-4-Last-Stride" class="headerlink" title="3.4 Last Stride"></a>3.4 Last Stride</h3><ul><li>stride=2: 256x128-&gt;8x4</li><li>stride=1: 256x128-&gt;16x8</li></ul><h3 id="3-5-BNNeck"><a href="#3-5-BNNeck" class="headerlink" title="3.5 BNNeck"></a>3.5 BNNeck</h3><ul><li>ID loss: 更偏向于 cosine distance</li><li>triplet loss: 更偏向于 Euclidean distance</li></ul><img src="/2019/05/17/BNNeck/BNNeck.png" title="BNNeck"><img src="/2019/05/17/BNNeck/loss.png" title="loss"><h3 id="3-6-Center-Loss"><a href="#3-6-Center-Loss" class="headerlink" title="3.6 Center Loss"></a>3.6 Center Loss</h3><p>三元组损失只能使一个 batch 内的正负样本的值相差比较大，却不能考虑全局的正负样本值。</p><p>Triplet loss:</p><script type="math/tex; mode=display">T_{Tri}=[ d_p-d_n+\alpha ]_+</script><p>Center loss:</p><script type="math/tex; mode=display">L_{C}=\frac{1}{2} \sum_{j=1}^B \parallel f_{t_j} -c_{y_j} \parallel_2^2</script><p>其中 $y_j$ 是第 j 张图片的label，$c_{y_j}$ 表示第 $y_j$ 类的特征的中心，$f_{t_j}$ 表示提取的特征 $f_t$.</p><p><strong>Overall</strong>:</p><script type="math/tex; mode=display">L=L_{ID}+L_{Triplet}+\beta L_{C}</script><p>$\beta=0.0005$</p><h2 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4. Experimental Results"></a>4. Experimental Results</h2><p>作者一共做了两组实验，一组是在 source damain 上的，一组是在 cross domain 上的。</p><h3 id="4-1-Influences-of-Each-Trich-Same-domain"><a href="#4-1-Influences-of-Each-Trich-Same-domain" class="headerlink" title="4.1 Influences of Each Trich (Same domain)"></a>4.1 Influences of Each Trich (Same domain)</h3><img src="/2019/05/17/BNNeck/same_domain.png" title="same domain"><p>这些应该是在前面各种 trick 已经有的情况再加一个得到的结果。</p><h3 id="4-2-Analysis-of-BNNeck"><a href="#4-2-Analysis-of-BNNeck" class="headerlink" title="4.2 Analysis of BNNeck"></a>4.2 Analysis of BNNeck</h3><img src="/2019/05/17/BNNeck/ablation_study_of_BNNeck.png" title="ablation study of BNNeck"><p>我觉得这个表格说明了 bn 层是用的，但是在测试的时候取 $f_t$ 还是 $f_i$ ，用 cosine 还是 Euclidean 是无所谓的。</p><h3 id="4-3-Influences-of-Each-Trick-Cross-domain"><a href="#4-3-Influences-of-Each-Trick-Cross-domain" class="headerlink" title="4.3 Influences of Each Trick　(Cross domain)"></a>4.3 Influences of Each Trick　(Cross domain)</h3><img src="/2019/05/17/BNNeck/cross_domain.png" title="cross domain"><p>warmup and label smoothing 更有用一些，stride=1, center loss 没啥用，REA 有负作用。</p><h2 id="5-Supplementary-Experiments"><a href="#5-Supplementary-Experiments" class="headerlink" title="5. Supplementary Experiments"></a>5. Supplementary Experiments</h2><h3 id="5-1-Influences-of-the-Number-of-Batch-Size"><a href="#5-1-Influences-of-the-Number-of-Batch-Size" class="headerlink" title="5.1 Influences of the Number of Batch Size"></a>5.1 Influences of the Number of Batch Size</h3><img src="/2019/05/17/BNNeck/batch_size.png" title="batch size"><p>差别也就在2个点足有，不是特别大，但是更大的 batch 是更有用的。</p><h3 id="5-2-Influences-of-Image-Size"><a href="#5-2-Influences-of-Image-Size" class="headerlink" title="5.2 Influences of Image Size"></a>5.2 Influences of Image Size</h3><img src="/2019/05/17/BNNeck/image_size.png" title="image size"><p>也差不太多</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1903.07071&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bag of Tricks and A Strong Baseline for Deep Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code:　&lt;a href=&quot;https://github.com/michuanhaohao/reid-strong-baseline&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这篇论文是旷视科技 Video Team 做的。&lt;/p&gt;
&lt;p&gt;这篇论文主要介绍 re-id 代码中的各种 trick 的作用。&lt;/p&gt;
    
    </summary>
    
      <category term="re-id" scheme="http://yoursite.com/categories/re-id/"/>
    
    
      <category term="re-id" scheme="http://yoursite.com/tags/re-id/"/>
    
  </entry>
  
  <entry>
    <title>MAR</title>
    <link href="http://yoursite.com/2019/05/13/MAR/"/>
    <id>http://yoursite.com/2019/05/13/MAR/</id>
    <published>2019-05-13T03:29:27.000Z</published>
    <updated>2019-05-15T10:48:41.828Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1903.06325v2" target="_blank" rel="noopener">CVPR2019 Unsupervised Person Re-identification by Soft Multilabel Learning</a></li><li>code: <a href="https://github.com/KovenYu/MAR" target="_blank" rel="noopener">pytorch</a></li><li>参考链接: <a href="https://www.cnblogs.com/Thinker-pcw/p/10807681.html" target="_blank" rel="noopener">https://www.cnblogs.com/Thinker-pcw/p/10807681.html</a></li></ul><p>出发点 Multi-label 很强，效果的确好，就是论文看得有点头晕，有些公式自己之前从来没见过，并且有些公式的出发点没有实验证明。</p><p>这篇论文是腾讯的，今年腾讯优图实验室25篇、腾讯AILab33篇共计55篇论文被 CVPR 2019 录取。</p><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>这是篇跨数据集的行人重识别，source 是 MSMT，target 是 Market 和 Duke。</p><p>作者针对跨数据集行人重识别没有标签问题，提出四个工作：</p><ol><li>soft multilabel learning, 示例如下图</li><li>soft multilabel-guided hard negative mining</li><li>cross-view consistent soft multi-label learning</li><li>reference agent learning</li></ol><img src="/2019/05/13/MAR/soft_multilabel_learning.png" title="soft multilabel learning"><img src="/2019/05/13/MAR/network.png" title="network"><ol><li>用分类结果作为图片的真值，即 soft multilabel</li><li>确定正负样本：特征相似但是 soft multilabel 不相似的作为负样本，特征相似且 soft multilabel 相似的作为正样本</li><li>跨摄像头的一致性，摄像头同一数据集下，应该是不管哪个摄像头下，得到的 soft multilabel 的分布是近似的，</li><li>reference agent 应该满足：有标签数据集中，和 agent 同类别的图片得到的特征应该和 agent 相似，不同类别的图片得到的特征应该不相似，在无标签数据集中，图片得到的特征和 agent 应该不相似。</li></ol><p>这几个点感觉完全没有联系啊，作者是咋想到的并放在一起的呢？</p><p>剩下的三个创新点后面依次阐述，其理解还是有点费劲的。</p><p>注:</p><ul><li>本论文中的 auxiliary dataset 等价于 source dataset</li><li>agent 是一个单独的 classx2048 维的数据，代码中是直接调用的 fc.weight。</li></ul><h2 id="2-Deep-Soft-Multilabel-Reference-Learning"><a href="#2-Deep-Soft-Multilabel-Reference-Learning" class="headerlink" title="2. Deep Soft Multilabel Reference Learning"></a>2. Deep Soft Multilabel Reference Learning</h2><h3 id="2-1-Problem-formulation-and-Overview"><a href="#2-1-Problem-formulation-and-Overview" class="headerlink" title="2.1 Problem formulation and Overview"></a>2.1 Problem formulation and Overview</h3><div class="table-container"><table><thead><tr><th style="text-align:left">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">$X=\lbrace x_i \rbrace_{i=1}^{N_u}$</td><td style="text-align:left">没有标签的数据集，$N_u$张图片</td></tr><tr><td style="text-align:left">$Z=\lbrace z_i, w_i \rbrace_{i=1}^{N_a}, \text{where }w_i=1,2,…, N_p$</td><td style="text-align:left">有标签的数据集 auxiliary，$z_i$表示图片，$w_i$表示 label，$N_a$ 张图片，$N_p$个人，有标签数据集和无标签数据集人物没有重叠</td></tr><tr><td style="text-align:left">$f(\cdot)$</td><td style="text-align:left">discriminative deep feature embedding，应该是特征提取模型，即 $f(x)$，满足 $\parallel f(\cdot) \parallel_2=1$</td></tr><tr><td style="text-align:left">$\lbrace a_i \rbrace_{i=1}^{N_p}$</td><td style="text-align:left">reference person feature, $\parallel a_i \parallel_2=1$</td></tr><tr><td style="text-align:left">$y=l(f(x),\lbrace a_i \rbrace_{i=1}^{N_p})\in R^{N_p}$</td><td style="text-align:left">soft multilabel function $l(\cdot)$, where $y=(y^{(1)}, y^{(2)}, …, y^{(N_p)})$, $\sum_i^{N_p} y^{(i)}=1, y^{(i)}\in [0, 1]$</td></tr></tbody></table></div><p>一共有两个内容需要学习: $f(\cdot)$, $\lbrace a_i \rbrace_{i=1}^{N_p}$</p><h3 id="2-2-Soft-multilabel-guided-hard-negative-mining"><a href="#2-2-Soft-multilabel-guided-hard-negative-mining" class="headerlink" title="2.2 Soft multilabel-guided hard negative mining"></a>2.2 Soft multilabel-guided hard negative mining</h3><p>大哥，你的上下标能不能提前说清楚啊[捂脸]，算了算了，腾讯的，惹不起惹不起。</p><img src="/2019/05/13/MAR/soft_multilabel_guided_hard_negative_mining.png" title="soft multilabel guided hard negative mining"><p><strong>定义</strong> soft multilabel function:</p><script type="math/tex; mode=display">y^{(k)}=l(f(x),\lbrace a_i \rbrace_{i=1}^{N_p})^{(k)}=\frac{\exp(a_k^Tf(x))}{\sum_i \exp(a_i^Tf(x))}</script><p>也就是说，用 $f(x)$ 去依次点乘 $a_k$， 然后用一个 softmax。</p><p>按照这个公式来说的话，是在特征空间上做的操作，有点类似 ECN 中的预测概率，越相似，值越大，而不是通过分类器预测概率。</p><p>代码中有温度T。</p><p><strong>假设</strong>：如果一对样本 $x_i, x_j$ 有很高的特征相似性, 即 $f(x_i)^Tf(x_j)$，称之为相似样本。如果这对相似样本的其他特性也相似，则大概率为一对正样本，如果其他特性不相似，则大概率为一个难负样本 hard negative pair。</p><p>注：这里并没有实验证明一对 hard negative pair 的其他特性（主要指下文提到的 soft multilabel agreement）大概率不相似。所以表示存疑其假设的正确性。又想了想，在难采样三元组损失中，hard negative pair 就是指特征相似但是 label 不同的样本，positive pair 指 label 相同的样本的，easy positive pair 指 label 相同特征相似的样本，hard positive pair 指 label 相同特征不相似的样本，这样的话可以把 soft multilabel 看成样本的 label 的话，也是可以说得通的。</p><p><strong>引理1：其他特性的相似性</strong>：作者选用 soft multilabel 作为其他特性，soft multilabel agreement $A(\cdot, \cdot)$ 表示作为其他特性的相似性。定义为</p><script type="math/tex; mode=display">A(y_i,y_j)=y_i \land y_j=\sum_k \min(y_i^{(k)},y_j^{(k)})=1-\frac{\parallel y_i-y_j \parallel_1}{2} \in [0,1]</script><p>越相似，值越大。最后一个等号通过画图很容易求得，就不解释了。Question: 这里的相似性定义成了向量之间的一范，没有定义成熟悉的点积，暂时不知道原因。</p><p><strong>引理2： hard negative pair</strong>：对于无标签数据集 $X$ 的所有样本对 $M=N_u\times (N_u-1)/2$，设置比例 $p$，取 $pM$ 个特征最相似的样本对，即 $\hat{M}=\lbrace (i,j)|f(x_i)^T f(x_j)\ge S\rbrace, \parallel \hat{M} \parallel=pM$, 其中 $S$ 表示 $pM$ 个特征最相似样本对的阈值，动态变化，不是很重要的，重要的是取 $pM$个样本对。然后根据 label 的相似性将这些样本对划分为 positive set $P$ and hard negative set $N$，即</p><script type="math/tex; mode=display">P=\lbrace (i,j)|f(x_i)^T f(x_j)\ge S, A(y_i, y_j)\ge T \rbrace</script><script type="math/tex; mode=display">N=\lbrace (k,l)|f(x_k)^T f(x_l)\ge S, A(y_k, y_l)< T \rbrace</script><p>其中 $T$ 表示 soft multilabel agreement 的阈值。会更新。</p><p><strong>loss</strong>：soft Multilabel-guided Discriminative embedding Learning:</p><script type="math/tex; mode=display">L_{MDL}=-\log \frac{\bar{P}}{\bar{P}+\bar{N}}</script><p>where,</p><script type="math/tex; mode=display">\bar{P}=\frac{1}{|P|}\sum_{(i,j)\in P}\exp(-\parallel f(x_i)-f(x_j) \parallel_2^2)</script><script type="math/tex; mode=display">\bar{N}=\frac{1}{|N|}\sum_{(k,l)\in N}\exp(-\parallel f(x_k)-f(x_l) \parallel_2^2)</script><p>so,</p><script type="math/tex; mode=display">\begin{aligned}L_{MDL}&=-\log \frac{\bar{P}}{\bar{P}+\bar{N}} \\       &=-\log \frac{\frac{1}{|P|}\sum_{(i,j)\in P}\exp(-\parallel f(x_i)-f(x_j) \parallel_2^2)}{\frac{1}{|P|}\sum_{(i,j)\in P}\exp(-\parallel f(x_i)-f(x_j) \parallel_2^2)+\frac{1}{|N|}\sum_{(k,l)\in N}\exp(-\parallel f(x_k)-f(x_l) \parallel_2^2)}\end{aligned}</script><p>Question: 这是个啥公式啊，都没有见过类似的公式，作者也没有给出解释。</p><p>此时固定 agent $\lbrace a_i \rbrace_{i=1}^{N_p}$ ，学习 $f(\cdot)$.</p><p>实际训练时，$M=M_{batch}=N_{batch}\times (N_{batch}-1)/2$</p><h3 id="2-3-Cross-view-consistent-soft-multilabel-learning"><a href="#2-3-Cross-view-consistent-soft-multilabel-learning" class="headerlink" title="2.3 Cross-view consistent soft multilabel learning"></a>2.3 Cross-view consistent soft multilabel learning</h3><p>因为行人重识别要求跨摄像头识别，所以考虑到行人的分布应该与摄像头无关。</p><p><strong>Loss</strong>:</p><script type="math/tex; mode=display">L_{CML}=\sum_v d(P_v(y), P(y))^2</script><p>其中，$P(y)$ 表示数据集 $X$ 的 soft multilabel 分布，$P_v(y)$ 表示数据集 $X$ 在摄像头 $v$ 的 soft multilabel 分布，$d(\cdot, \cdot)$ 表示分布的距离，可以是 KL divergence 或者 <a href="https://blog.csdn.net/yzxnuaa/article/details/79725014" target="_blank" rel="noopener">Wasserstein distance</a>.因为实际观察到服从 log-normal 分布，所以采取 simplified 2-Wasserstein distance。</p><script type="math/tex; mode=display">L_{CML}=\sum_v \parallel \mu_v-\mu \parallel_2^2 + \parallel \sigma_v-\sigma\parallel_2^2</script><p>其中，$\mu/\sigma$表示总体数据集的 log-soft multilabel 的均值和方差，$\mu_v/\sigma_v$表示总体数据集在摄像头$v$的 log-soft multilabel 的均值和方差.<br>Question: 这个公式又是咋推出来的，这是妥妥地写出来也看不懂系列。</p><p>此时固定 agent $\lbrace a_i \rbrace_{i=1}^{N_p}$ ，学习 $f(\cdot)$.</p><h3 id="2-4-Reference-agent-Learning"><a href="#2-4-Reference-agent-Learning" class="headerlink" title="2.4 Reference agent Learning"></a>2.4 Reference agent Learning</h3><p>考虑到 referentce agent 需要与 soft multilabel function $l(\cdot, \cdot)$ 有关，因此得到损失函数</p><script type="math/tex; mode=display">L_{AL}=\sum_k -\log l(f(z_k), {a_i})^{(w_k)}=\sum_k -\log \frac{\exp(a_{w_k}^T f(z_k))}{\sum_j \exp(a_j^T f(z_k))}</script><p>其中，$z_k$ 表示有标签数据集 $Z$ 中标签为 $w_k$ 的第 $k$ 张图片。</p><p>这里可以理解成 $z_k$ 的预测概率和真实概率的交叉熵损失。这个损失函数不仅训练 $a_i$ 更接近第i个人的所有图片的特征，也训练 feature embedding $f$，使 $l(\cdot, \cdot)$ 得到的标签更具有表示同一个人的能力，符合 soft multilabel-guided hard negative mining 的假设：特征相似，但是 soft multilabel 不相似的为 hard negative mining。</p><p>这个公式更新的是 $f$ 和 agent $a_i$.</p><p>注：该论文中的公式其实按照从广义的定义到实际的应用的具体化过程，所以刚开始才会感觉有点乱，公式里面的字符也会一变再变，其实是从理论的公式到具体化实际代码的过程。</p><p><strong>Joint embedding learning for reference comparability</strong>: 为了更好地提高 soft multilabel function 表示无标签数据集图片的正确性，提出 Joint embedding learning for reference comparability，为了修正 domain shift，利用无标签数据集 $f(x)$ 和 $a_i$ 肯定不是一对，提出 loss:</p><script type="math/tex; mode=display">L_{RJ}=\sum_i \sum_{j\in M_i} \sum_{k:w_k=i}[m-\parallel a_i-f(x_j) \parallel_2^2]_+ + \parallel a_i-f(z_k) \parallel_2^2</script><p>其中，其目的是为了保证$a_i$所表示的有标签数据集中的同一id的图片和$a_i$特征相似，$a_i$和所有无标签数据集中的图片特征都不相似。 $M_i=\lbrace j| \parallel a_i-f(x_j) \parallel_2^2 &lt; m \rbrace$，表示对第 $i$ 个 agent $a_i$ 而言，特征最为相似 ($\parallel a_i-f(x_j) \parallel_2^2=2(1-a_i^Tf(x_j))$, 越相似，值越小) 的无标签数据集中的图片，按照作者推荐的论文 <a href="https://arxiv.org/abs/1704.06369" target="_blank" rel="noopener">44:Normface: l2 hypersphere embedding for face verification</a>，建议 $m=1$。</p><p>此时固定 agent $\lbrace a_i \rbrace_{i=1}^{N_p}$ ，学习 $f(\cdot)$.</p><p>对 $L_{RJ}$ 根据代码再次做出解释，$L_{RJ}$ 的目的是学习更好的特征提取器 $f$，使有标签数据集提取出的特征与同类别的 agent 的特征相似，与不同类别的agent不相似，无标签数据集提取出的特征与 agent 都不相似，有点三元组损失的意思。此时的 $a_i$ 是常量，不进行反向求导的。其二，对于任意一个 agent $a_i$，有标签数据集中，label 等于 $i$ 的图片视为正样本，其他图片视为负样本，对于无标签数据集，则直接视为 $a_i$ 的负样本。具体来说，就是对每一张有标签数据集的图片，$a_{label}$ 为正， 其余 $a_i$ 为负，对于每一张无标签数据集的图片，$a_i$ 都为负。</p><p>所以总的 reference agent learning loss为:</p><script type="math/tex; mode=display">L_{RAL}=L_{AL}+\beta L_{RJ}</script><h3 id="2-5-1-Model-training-and-testing"><a href="#2-5-1-Model-training-and-testing" class="headerlink" title="2.5.1 Model training and testing"></a>2.5.1 Model training and testing</h3><script type="math/tex; mode=display">L_{MAR}=L_{MDL}+\lambda_1 L_{CML}+\lambda_2 L_{RAL}</script><h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h2><p>MSMT17 为辅助数据集， Market-1501、Duke 为无标签数据集。</p><img src="/2019/05/13/MAR/comparison.png" title="comparison"><img src="/2019/05/13/MAR/Ablation.png" title="Ablation study"><p>备注: 论文中的 agent 其实并不是之前以为通过图片输入模型得到的特征求出来的，而是 ResNet-50 的 fc.weight(classx2048) ，也就是分类器的分类向量。和 ECN 论文中的使用方法有很大的不同吧。在 ECN 中，使用的就是图片输入模型得到的特征，可能是因为 ECN 中一张图片对应一个特征，而本论文中是多个图片对应一个特征。</p><h2 id="4-code"><a href="#4-code" class="headerlink" title="4. code"></a>4. code</h2><h3 id="4-1-Logger"><a href="#4-1-Logger" class="headerlink" title="4.1 Logger"></a>4.1 Logger</h3><p>两种logger</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种：定义简单，使用繁琐</span></span><br><span class="line"><span class="comment"># 定义</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_string</span><span class="params">()</span>:</span></span><br><span class="line">    ISOTIMEFORMAT = <span class="string">'%Y-%m-%d %X'</span></span><br><span class="line">    string = <span class="string">'[&#123;&#125;]'</span>.format(time.strftime(ISOTIMEFORMAT, time.localtime(time.time())))</span><br><span class="line">    <span class="keyword">return</span> string</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Logger</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, save_path)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(save_path):</span><br><span class="line">            os.makedirs(save_path)</span><br><span class="line">        self.file = open(os.path.join(save_path, <span class="string">'log_&#123;&#125;.txt'</span>.format(time_string())), <span class="string">'w'</span>)</span><br><span class="line">        self.print_log(<span class="string">"python version : &#123;&#125;"</span>.format(sys.version.replace(<span class="string">'\n'</span>, <span class="string">' '</span>)))</span><br><span class="line">        self.print_log(<span class="string">"torch  version : &#123;&#125;"</span>.format(torch.__version__))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_log</span><span class="params">(self, string)</span>:</span></span><br><span class="line">        self.file.write(<span class="string">"&#123;&#125;\n"</span>.format(string))</span><br><span class="line">        self.file.flush()</span><br><span class="line">        print(string)</span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line">logger = Logger(args.save_path)</span><br><span class="line">logger.print_log(<span class="string">"=&gt; loading checkpoint '&#123;&#125;'"</span>.format(load_path))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二种，定义繁琐，使用简单，重定向</span></span><br><span class="line"><span class="comment"># 推荐用这种</span></span><br><span class="line"><span class="comment"># 定义</span></span><br><span class="line"><span class="comment"># .\logging.py</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> errno</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkdir_if_missing</span><span class="params">(dir_path)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        os.makedirs(dir_path)</span><br><span class="line">    <span class="keyword">except</span> OSError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">if</span> e.errno != errno.EEXIST:</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Logger</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, fpath=None)</span>:</span></span><br><span class="line">        self.console = sys.stdout</span><br><span class="line">        self.file = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> fpath <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            mkdir_if_missing(os.path.dirname(fpath))</span><br><span class="line">            self.file = open(fpath, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(self, msg)</span>:</span></span><br><span class="line">        self.console.write(msg)</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.write(msg)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">flush</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.console.flush()</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.flush()</span><br><span class="line">            os.fsync(self.file.fileno())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.console.close()</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.close()</span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line">sys.stdout = Logger(osp.join(args.logs_dir, <span class="string">'log.txt'</span>))</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><h3 id="4-2-model"><a href="#4-2-model" class="headerlink" title="4.2 model"></a>4.2 model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 3x384x128</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        feature_maps = self.layer4(x)</span><br><span class="line">        <span class="comment"># 2048x12x4</span></span><br><span class="line">        x = self.avgpool(feature_maps)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># bx2048</span></span><br><span class="line">        <span class="comment"># renorm 使每一行的向量的2范进行了截断处理</span></span><br><span class="line">        <span class="comment"># 使之变成[0,1e-5]，再线性变成[0,1]</span></span><br><span class="line">        <span class="comment"># 这里的renorm可以暂时理解成进行了二范处理，</span></span><br><span class="line">        feature = x.renorm(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1e-5</span>).mul(<span class="number">1e5</span>)</span><br><span class="line">        <span class="comment"># bx2048</span></span><br><span class="line">        w = self.fc.weight</span><br><span class="line">        <span class="comment"># 注: 这个self.fc.weight(classx2048)就是论文中的agent</span></span><br><span class="line">        ww = w.renorm(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1e-5</span>).mul(<span class="number">1e5</span>)</span><br><span class="line">        sim = feature.mm(ww.t())</span><br><span class="line">        <span class="comment"># sim: bxclass</span></span><br><span class="line">        <span class="comment"># feature(f): bx2048, sim(y): bxclass, feature_maps: 2048x12x4</span></span><br><span class="line">        <span class="keyword">return</span> feature, sim, feature_maps</span><br></pre></td></tr></table></figure><h3 id="4-3-optim"><a href="#4-3-optim" class="headerlink" title="4.3 optim"></a>4.3 optim</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bn_params, other_params = partition_params(self.net, <span class="string">'bn'</span>)</span><br><span class="line">self.optimizer = torch.optim.SGD([&#123;<span class="string">'params'</span>: bn_params, <span class="string">'weight_decay'</span>: <span class="number">0</span>&#125;,</span><br><span class="line">                                  &#123;<span class="string">'params'</span>: other_params&#125;], lr=args.lr, momentum=<span class="number">0.9</span>, weight_decay=args.wd)</span><br></pre></td></tr></table></figure><h3 id="4-4-trainer-init-losses"><a href="#4-4-trainer-init-losses" class="headerlink" title="4.4 trainer/init_losses"></a>4.4 trainer/init_losses</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReidTrainer</span><span class="params">(Trainer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args, logger)</span>:</span></span><br><span class="line">        self.al_loss = nn.CrossEntropyLoss().cuda()</span><br><span class="line">        self.rj_loss = JointLoss(args.margin).cuda()</span><br><span class="line">        self.cml_loss = MultilabelLoss(args.batch_size).cuda()  <span class="comment"># L_CML</span></span><br><span class="line">        self.mdl_loss = DiscriminativeLoss(args.mining_ratio).cuda() <span class="comment"># L_MDL</span></span><br><span class="line">        self.net = resnet50(pretrained=<span class="keyword">False</span>, num_classes=self.args.num_classes)</span><br><span class="line">        self.multilabel_memory = torch.zeros(N_target_samples, <span class="number">4101</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_losses</span><span class="params">(self, target_loader)</span>:</span></span><br><span class="line">        self.logger.print_log(<span class="string">'initializing centers/threshold ...'</span>)</span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(self.args.ml_path):</span><br><span class="line">            (multilabels, views, pairwise_agreements) = torch.load(self.args.ml_path)</span><br><span class="line">            self.logger.print_log(<span class="string">'loaded ml from &#123;&#125;'</span>.format(self.args.ml_path))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.logger.print_log(<span class="string">'not found &#123;&#125;. computing ml...'</span>.format(self.args.ml_path))</span><br><span class="line">            sim, _, views = extract_features(target_loader, self.net, index_feature=<span class="number">1</span>, return_numpy=<span class="keyword">False</span>)</span><br><span class="line">            <span class="comment"># sim: bxclass, views: bx1</span></span><br><span class="line">            multilabels = F.softmax(sim * self.args.scala_ce, dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># multilabels: bxclass 这里应该对于soft multilabel funtion得到的结果y^&#123;(k)&#125;</span></span><br><span class="line">            <span class="comment"># Question: sim*self.args.scala_ce 是什么意思</span></span><br><span class="line">            ml_np = multilabels.cpu().numpy()</span><br><span class="line">            pairwise_agreements = <span class="number">1</span> - pdist(ml_np, <span class="string">'minkowski'</span>, p=<span class="number">1</span>)/<span class="number">2</span></span><br><span class="line">            <span class="comment"># pairwise_agreements: soft multilabel agreement A(.,.) 公式2</span></span><br><span class="line">        log_multilabels = torch.log(multilabels)</span><br><span class="line">        self.cml_loss.init_centers(log_multilabels, views)</span><br><span class="line">        self.logger.print_log(<span class="string">'initializing centers done.'</span>)</span><br><span class="line">        self.mdl_loss.init_threshold(pairwise_agreements)</span><br><span class="line">        self.logger.print_log(<span class="string">'initializing threshold done.'</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_epoch</span><span class="params">(self, source_loader, target_loader, epoch)</span>:</span></span><br><span class="line">        self.lr_scheduler.step()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.cml_loss.initialized <span class="keyword">or</span> <span class="keyword">not</span> self.mdl_loss.initialized:</span><br><span class="line">            self.init_losses(target_loader)</span><br><span class="line">        batch_time_meter = AverageMeter()</span><br><span class="line">        stats = (<span class="string">'loss_source'</span>, <span class="string">'loss_st'</span>, <span class="string">'loss_ml'</span>, <span class="string">'loss_target'</span>, <span class="string">'loss_total'</span>)</span><br><span class="line">        meters_trn = &#123;stat: AverageMeter() <span class="keyword">for</span> stat <span class="keyword">in</span> stats&#125;</span><br><span class="line">        self.train()</span><br><span class="line"></span><br><span class="line">        end = time.time()</span><br><span class="line">        target_iter = iter(target_loader)</span><br><span class="line">        <span class="keyword">for</span> i, source_tuple <span class="keyword">in</span> enumerate(source_loader):</span><br><span class="line">            imgs = source_tuple[<span class="number">0</span>].cuda()</span><br><span class="line">            labels = source_tuple[<span class="number">1</span>].cuda()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                target_tuple = next(target_iter)</span><br><span class="line">            <span class="keyword">except</span> StopIteration:</span><br><span class="line">                target_iter = iter(target_loader)</span><br><span class="line">                target_tuple = next(target_iter)</span><br><span class="line">            imgs_target = target_tuple[<span class="number">0</span>].cuda()</span><br><span class="line">            labels_target = target_tuple[<span class="number">1</span>].cuda()</span><br><span class="line">            views_target = target_tuple[<span class="number">2</span>].cuda()</span><br><span class="line">            idx_target = target_tuple[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">            features, similarity, _ = self.net(imgs)</span><br><span class="line">            features_target, similarity_target, _ = self.net(imgs_target)</span><br><span class="line">            <span class="comment"># features: bx2048, similarity: bxclass</span></span><br><span class="line">            scores = similarity * self.args.scala_ce</span><br><span class="line">            loss_source = self.al_loss(scores, labels) <span class="comment"># 公式7，同时训练 agent 和 f()</span></span><br><span class="line">            agents = self.net.module.fc.weight.renorm(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1e-5</span>).mul(<span class="number">1e5</span>)</span><br><span class="line">            <span class="comment"># features: bx2048, agents: classx2048, labels: bx1, similarity: bxclass</span></span><br><span class="line">            loss_st = self.rj_loss(features, agents.detach(), labels, similarity.detach(), features_target, similarity_target.detach())</span><br><span class="line">            multilabels = F.softmax(features_target.mm(agents.detach().t_()*self.args.scala_ce), dim=<span class="number">1</span>)</span><br><span class="line">            loss_ml = self.cml_loss(torch.log(multilabels), views_target)</span><br><span class="line">            <span class="keyword">if</span> epoch &lt; <span class="number">1</span>:</span><br><span class="line">                loss_target = torch.Tensor([<span class="number">0</span>]).cuda()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                multilabels_cpu = multilabels.detach().cpu()</span><br><span class="line">                is_init_batch = self.initialized[idx_target]</span><br><span class="line">                initialized_idx = idx_target[is_init_batch]</span><br><span class="line">                uninitialized_idx = idx_target[~is_init_batch]</span><br><span class="line">                self.multilabel_memory[uninitialized_idx] = multilabels_cpu[~is_init_batch]</span><br><span class="line">                self.initialized[uninitialized_idx] = <span class="number">1</span></span><br><span class="line">                self.multilabel_memory[initialized_idx] = <span class="number">0.9</span> * self.multilabel_memory[initialized_idx] \</span><br><span class="line">                                                          + <span class="number">0.1</span> * multilabels_cpu[is_init_batch]</span><br><span class="line">                loss_target = self.mdl_loss(features_target, self.multilabel_memory[idx_target], labels_target)</span><br><span class="line"></span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            loss_total = loss_target + self.args.lamb_1 * loss_ml + self.args.lamb_2 * \</span><br><span class="line">                         (loss_source + self.args.beta * loss_st)</span><br><span class="line">            loss_total.backward()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> stats:</span><br><span class="line">                v = locals()[k]</span><br><span class="line">                meters_trn[k].update(v.item(), self.args.batch_size)</span><br><span class="line"></span><br><span class="line">            batch_time_meter.update(time.time() - end)</span><br><span class="line">            freq = self.args.batch_size / batch_time_meter.avg</span><br><span class="line">            end = time.time()</span><br><span class="line">            <span class="keyword">if</span> i % self.args.print_freq == <span class="number">0</span>:</span><br><span class="line">                self.logger.print_log(<span class="string">'  Iter: [&#123;:03d&#125;/&#123;:03d&#125;]   Freq &#123;:.1f&#125;   '</span>.format(</span><br><span class="line">                    i, len(source_loader), freq) + create_stat_string(meters_trn) + time_string())</span><br><span class="line"></span><br><span class="line">        save_checkpoint(self, epoch, os.path.join(self.args.save_path, <span class="string">"checkpoints.pth"</span>))</span><br><span class="line">        <span class="keyword">return</span> meters_trn</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L_CML 公式6</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultilabelLoss</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, batch_size, use_std=True)</span>:</span></span><br><span class="line">        super(MultilabelLoss, self).__init__()</span><br><span class="line">        self.use_std = use_std</span><br><span class="line">        self.moment = batch_size / <span class="number">10000</span></span><br><span class="line">        self.initialized = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_centers</span><span class="params">(self, log_multilabels, views)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param log_multilabels: shape=(N, n_class)</span></span><br><span class="line"><span class="string">        :param views: (N,)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        # 用于初始化全局的均值和方差</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        univiews = torch.unique(views)</span><br><span class="line">        mean_ml = []</span><br><span class="line">        std_ml = []</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> univiews:</span><br><span class="line">            ml_in_v = log_multilabels[views == v]</span><br><span class="line">            mean = ml_in_v.mean(dim=<span class="number">0</span>)</span><br><span class="line">            std = ml_in_v.std(dim=<span class="number">0</span>)</span><br><span class="line">            mean_ml.append(mean)</span><br><span class="line">            std_ml.append(std)</span><br><span class="line">        center_mean = torch.mean(torch.stack(mean_ml), dim=<span class="number">0</span>)</span><br><span class="line">        center_std = torch.mean(torch.stack(std_ml), dim=<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'center_mean'</span>, center_mean)</span><br><span class="line">        self.register_buffer(<span class="string">'center_std'</span>, center_std)</span><br><span class="line">        self.initialized = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_centers</span><span class="params">(self, log_multilabels, views)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param log_multilabels: shape=(BS, n_class)</span></span><br><span class="line"><span class="string">        :param views: shape=(BS,)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        univiews = torch.unique(views)</span><br><span class="line">        means = []</span><br><span class="line">        stds = []</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> univiews:</span><br><span class="line">            ml_in_v = log_multilabels[views == v]</span><br><span class="line">            <span class="keyword">if</span> len(ml_in_v) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            mean = ml_in_v.mean(dim=<span class="number">0</span>)</span><br><span class="line">            means.append(mean)</span><br><span class="line">            <span class="keyword">if</span> self.use_std:</span><br><span class="line">                std = ml_in_v.std(dim=<span class="number">0</span>)</span><br><span class="line">                stds.append(std)</span><br><span class="line">        new_mean = torch.mean(torch.stack(means), dim=<span class="number">0</span>)</span><br><span class="line">        self.center_mean = self.center_mean * (<span class="number">1</span> - self.moment) + new_mean * self.moment</span><br><span class="line">        <span class="keyword">if</span> self.use_std:</span><br><span class="line">            new_std = torch.mean(torch.stack(stds), dim=<span class="number">0</span>)</span><br><span class="line">            self.center_std = self.center_std * (<span class="number">1</span> - self.moment) + new_std * self.moment</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, log_multilabels, views)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param log_multilabels: shape=(BS, n_class)</span></span><br><span class="line"><span class="string">        :param views: shape=(BS,)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._update_centers(log_multilabels.detach(), views)</span><br><span class="line"></span><br><span class="line">        univiews = torch.unique(views)</span><br><span class="line">        loss_terms = []</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> univiews:</span><br><span class="line">            ml_in_v = log_multilabels[views == v]</span><br><span class="line">            <span class="keyword">if</span> len(ml_in_v) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            mean = ml_in_v.mean(dim=<span class="number">0</span>)</span><br><span class="line">            loss_mean = (mean - self.center_mean).pow(<span class="number">2</span>).sum()</span><br><span class="line">            loss_terms.append(loss_mean)</span><br><span class="line">            <span class="keyword">if</span> self.use_std:</span><br><span class="line">                std = ml_in_v.std(dim=<span class="number">0</span>)</span><br><span class="line">                loss_std = (std - self.center_std).pow(<span class="number">2</span>).sum()</span><br><span class="line">                loss_terms.append(loss_std)</span><br><span class="line">        loss_total = torch.mean(torch.stack(loss_terms))</span><br><span class="line">        <span class="keyword">return</span> loss_total</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L_MDL 公式4</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiscriminativeLoss</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mining_ratio=<span class="number">0.001</span>)</span>:</span></span><br><span class="line">        super(DiscriminativeLoss, self).__init__()</span><br><span class="line">        self.mining_ratio = mining_ratio</span><br><span class="line">        self.register_buffer(<span class="string">'n_pos_pairs'</span>, torch.Tensor([<span class="number">0</span>]))</span><br><span class="line">        self.register_buffer(<span class="string">'rate_TP'</span>, torch.Tensor([<span class="number">0</span>]))</span><br><span class="line">        self.moment = <span class="number">0.1</span></span><br><span class="line">        self.initialized = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_threshold</span><span class="params">(self, pairwise_agreements)</span>:</span></span><br><span class="line">        <span class="comment"># Question：论文中还有一个限制条件，f(x_i)f(x_j)&gt;S，代码只考虑了A(y_i, y_j)</span></span><br><span class="line">        pos = int(len(pairwise_agreements) * self.mining_ratio)</span><br><span class="line">        sorted_agreements = np.sort(pairwise_agreements)</span><br><span class="line">        t = sorted_agreements[-pos]</span><br><span class="line">        self.register_buffer(<span class="string">'threshold'</span>, torch.Tensor([t]).cuda())</span><br><span class="line">        self.initialized = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, multilabels, labels)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param features: shape=(BS, dim)</span></span><br><span class="line"><span class="string">        :param multilabels: (BS, n_class)</span></span><br><span class="line"><span class="string">        :param labels: (BS,)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        P, N = self._partition_sets(features.detach(), multilabels, labels)</span><br><span class="line">        <span class="keyword">if</span> P <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            pos_exponant = torch.Tensor([<span class="number">1</span>]).cuda()</span><br><span class="line">            num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sdist_pos_pairs = []</span><br><span class="line">            <span class="keyword">for</span> (i, j) <span class="keyword">in</span> zip(P[<span class="number">0</span>], P[<span class="number">1</span>]):</span><br><span class="line">                sdist_pos_pair = (features[i] - features[j]).pow(<span class="number">2</span>).sum()</span><br><span class="line">                sdist_pos_pairs.append(sdist_pos_pair)</span><br><span class="line">            pos_exponant = torch.exp(- torch.stack(sdist_pos_pairs)).mean()</span><br><span class="line">            num = -torch.log(pos_exponant)</span><br><span class="line">        <span class="keyword">if</span> N <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            neg_exponant = torch.Tensor([<span class="number">0.5</span>]).cuda()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sdist_neg_pairs = []</span><br><span class="line">            <span class="keyword">for</span> (i, j) <span class="keyword">in</span> zip(N[<span class="number">0</span>], N[<span class="number">1</span>]):</span><br><span class="line">                sdist_neg_pair = (features[i] - features[j]).pow(<span class="number">2</span>).sum()</span><br><span class="line">                sdist_neg_pairs.append(sdist_neg_pair)</span><br><span class="line">            neg_exponant = torch.exp(- torch.stack(sdist_neg_pairs)).mean()</span><br><span class="line">        den = torch.log(pos_exponant + neg_exponant)</span><br><span class="line">        loss = num + den</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_partition_sets</span><span class="params">(self, features, multilabels, labels)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        partition the batch into confident positive, hard negative and others</span></span><br><span class="line"><span class="string">        :param features: shape=(BS, dim)</span></span><br><span class="line"><span class="string">        :param multilabels: shape=(BS, n_class)</span></span><br><span class="line"><span class="string">        :param labels: shape=(BS,)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        P: positive pair set. tuple of 2 np.array i and j.</span></span><br><span class="line"><span class="string">            i contains smaller indices and j larger indices in the batch.</span></span><br><span class="line"><span class="string">            if P is None, no positive pair found in this batch.</span></span><br><span class="line"><span class="string">        N: negative pair set. similar to P, but will never be None.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        f_np = features.cpu().numpy()</span><br><span class="line">        ml_np = multilabels.cpu().numpy()</span><br><span class="line">        p_dist = pdist(f_np)</span><br><span class="line">        p_agree = <span class="number">1</span> - pdist(ml_np, <span class="string">'minkowski'</span>, p=<span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">        sorting_idx = np.argsort(p_dist)</span><br><span class="line">        n_similar = int(len(p_dist) * self.mining_ratio)</span><br><span class="line">        similar_idx = sorting_idx[:n_similar]</span><br><span class="line">        is_positive = p_agree[similar_idx] &gt; self.threshold.item()</span><br><span class="line">        pos_idx = similar_idx[is_positive]</span><br><span class="line">        neg_idx = similar_idx[~is_positive]</span><br><span class="line">        P = dist_idx_to_pair_idx(len(f_np), pos_idx)</span><br><span class="line">        N = dist_idx_to_pair_idx(len(f_np), neg_idx)</span><br><span class="line">        self._update_threshold(p_agree)</span><br><span class="line">        self._update_buffers(P, labels)</span><br><span class="line">        <span class="keyword">return</span> P, N</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_threshold</span><span class="params">(self, pairwise_agreements)</span>:</span></span><br><span class="line">        pos = int(len(pairwise_agreements) * self.mining_ratio)</span><br><span class="line">        sorted_agreements = np.sort(pairwise_agreements)</span><br><span class="line">        t = torch.Tensor([sorted_agreements[-pos]]).cuda()</span><br><span class="line">        self.threshold = self.threshold * (<span class="number">1</span> - self.moment) + t * self.moment</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_buffers</span><span class="params">(self, P, labels)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> P <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            self.n_pos_pairs = <span class="number">0.9</span> * self.n_pos_pairs</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        n_pos_pairs = len(P[<span class="number">0</span>])</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> (i, j) <span class="keyword">in</span> zip(P[<span class="number">0</span>], P[<span class="number">1</span>]):</span><br><span class="line">            count += labels[i] == labels[j]</span><br><span class="line">        rate_TP = float(count) / n_pos_pairs</span><br><span class="line">        self.n_pos_pairs = <span class="number">0.9</span> * self.n_pos_pairs + <span class="number">0.1</span> * n_pos_pairs</span><br><span class="line">        self.rate_TP = <span class="number">0.9</span> * self.rate_TP + <span class="number">0.1</span> * rate_TP</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L_RJ 公式 8</span></span><br><span class="line"><span class="comment"># 与公式8略有不同</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JointLoss</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, margin=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(JointLoss, self).__init__()</span><br><span class="line">        self.margin = margin</span><br><span class="line">        self.sim_margin = <span class="number">1</span> - margin / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, agents, labels, similarity, features_target, similarity_target)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param features: shape=(BS/2, dim)</span></span><br><span class="line"><span class="string">        :param agents: shape=(n_class, dim)</span></span><br><span class="line"><span class="string">        :param labels: shape=(BS/2,)</span></span><br><span class="line"><span class="string">        :param features_target: shape=(BS/2, n_class)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss_terms = []</span><br><span class="line">        arange = torch.arange(len(agents)).cuda()</span><br><span class="line">        zero = torch.Tensor([<span class="number">0</span>]).cuda()</span><br><span class="line">        <span class="keyword">for</span> (f, l, s) <span class="keyword">in</span> zip(features, labels, similarity):</span><br><span class="line">            loss_pos = (f - agents[l]).pow(<span class="number">2</span>).sum() <span class="comment"># 公式8的最后一项，a_i-f(z_k)</span></span><br><span class="line">            loss_terms.append(loss_pos)</span><br><span class="line">            neg_idx = arange != l</span><br><span class="line">            <span class="comment"># 从agent中选出与当前图片特征相似度高于阈值，但不是同一类的的agent</span></span><br><span class="line">            hard_agent_idx = neg_idx &amp; (s &gt; self.sim_margin) <span class="comment"># 越相似，值越大</span></span><br><span class="line">            <span class="keyword">if</span> torch.any(hard_agent_idx):</span><br><span class="line">                hard_neg_sdist = (f - agents[hard_agent_idx]).pow(<span class="number">2</span>).sum(dim=<span class="number">1</span>)</span><br><span class="line">                loss_neg = torch.max(zero, self.margin - hard_neg_sdist).mean()</span><br><span class="line">                loss_terms.append(loss_neg)</span><br><span class="line">        <span class="keyword">for</span> (f, s) <span class="keyword">in</span> zip(features_target, similarity_target):</span><br><span class="line">            hard_agent_idx = s &gt; self.sim_margin</span><br><span class="line">            <span class="keyword">if</span> torch.any(hard_agent_idx):</span><br><span class="line">                hard_neg_sdist = (f - agents[hard_agent_idx]).pow(<span class="number">2</span>).sum(dim=<span class="number">1</span>)</span><br><span class="line">                loss_neg = torch.max(zero, self.margin - hard_neg_sdist).mean()</span><br><span class="line">                loss_terms.append(loss_neg)</span><br><span class="line">        loss_total = torch.mean(torch.stack(loss_terms))</span><br><span class="line">        <span class="keyword">return</span> loss_total</span><br></pre></td></tr></table></figure><p>根据代码，重新明确两个定义:</p><ul><li>similarity 指的是图片的 feature1 和 agent 的 feature2 的特征相似性: feature1*feature2</li><li>multilabels 指的是 similarity.mul(self.args.scala_ce) 再softmax得到的</li></ul><p>算了，有些代码还是跑的时候看吧，因为有些更新方式有些看不懂。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1903.06325v2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019 Unsupervised Person Re-identification by Soft Multilabel Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/KovenYu/MAR&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;参考链接: &lt;a href=&quot;https://www.cnblogs.com/Thinker-pcw/p/10807681.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/Thinker-pcw/p/10807681.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;出发点 Multi-label 很强，效果的确好，就是论文看得有点头晕，有些公式自己之前从来没见过，并且有些公式的出发点没有实验证明。&lt;/p&gt;
&lt;p&gt;这篇论文是腾讯的，今年腾讯优图实验室25篇、腾讯AILab33篇共计55篇论文被 CVPR 2019 录取。&lt;/p&gt;
    
    </summary>
    
      <category term="deep learning" scheme="http://yoursite.com/categories/deep-learning/"/>
    
    
      <category term="person re-id" scheme="http://yoursite.com/tags/person-re-id/"/>
    
  </entry>
  
  <entry>
    <title>residual_attention</title>
    <link href="http://yoursite.com/2019/05/10/residual-attention_and_CBAM_GCNet/"/>
    <id>http://yoursite.com/2019/05/10/residual-attention_and_CBAM_GCNet/</id>
    <published>2019-05-10T02:12:48.000Z</published>
    <updated>2019-05-10T08:14:31.646Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><ul><li>paper: <a href="https://arxiv.org/abs/1704.06904" target="_blank" rel="noopener">商汤 CVPR2017 Residual Attention_Network for Image Classification</a></li><li>code: <a href="https://github.com/fwang91/residual-attention-network" target="_blank" rel="noopener">caffe</a>, <a href="http://ethereon.github.io/netscope/#/editor" target="_blank" rel="noopener">caffe网络可视化工具 Netscope</a>, <a href="https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch" target="_blank" rel="noopener">pytorch</a></li><li>paper: <a href="https://arxiv.org/abs/1807.06521" target="_blank" rel="noopener">ECCV2018_CBAM: Convolutional Block Attention Module</a></li><li>code: <a href="https://github.com/luuuyi/CBAM.PyTorch" target="_blank" rel="noopener">pytorch</a></li><li>paper: <a href="https://arxiv.org/abs/1904.11492" target="_blank" rel="noopener">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li>code: <a href="https://github.com/xvjiarui/GCNet" target="_blank" rel="noopener">pytorch</a><a id="more"></a>这篇论文也是讲 attention map 的，主要用于分类，说实话，没有太理解其中的创新点，可能是因为不懂整个 attention map 的进程，前人做到了什么地步，从效果图上看，感觉比 Dual Attention 那篇论文还是差一些。不过能上 CVPR 的肯定有牛的地方，只是自己水平不够。</li></ul><h2 id="1-Residual-Attention-Network"><a href="#1-Residual-Attention-Network" class="headerlink" title="1. Residual Attention Network"></a>1. Residual Attention Network</h2><p>每个 Attention Module 都分为两个分支：mask branch and trunk branch。</p><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/mask_trunk_branch.png" title="mask branch and trunk branch"><script type="math/tex; mode=display">H_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)</script><p>其中，c表示通道，i表示所有的位置，$M_{i,c}(x)\in[0,1]$</p><h3 id="1-1-Spatial-Attention-and-Channel-Attention"><a href="#1-1-Spatial-Attention-and-Channel-Attention" class="headerlink" title="1.1 Spatial Attention and Channel Attention"></a>1.1 Spatial Attention and Channel Attention</h3><p>Mixed attention:</p><script type="math/tex; mode=display">f_1(x_{i,c})=\frac{1}{1+\exp (-x_{i,c})}</script><p>Channel Attention:</p><script type="math/tex; mode=display">f_2(x_{i,c})=\frac{x_{i,c}}{\parallel x_i \parallel}</script><p>Spatial Attention:</p><script type="math/tex; mode=display">f_3(x_{i,c})=\frac{1}{1+\exp (-(x_{i,c}-mean_c)/std_c)}</script><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/activation_function.png" title="different activation functions"><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/network.png" title="Residual Attention Network"><h2 id="2-code"><a href="#2-code" class="headerlink" title="2. code"></a>2. code</h2><p>代码有点绕，暂时没有看懂。</p><h2 id="3-CBAM"><a href="#3-CBAM" class="headerlink" title="3. CBAM"></a>3. CBAM</h2><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/CBAM.png" title="CBAM"><h2 id="4-code"><a href="#4-code" class="headerlink" title="4. code"></a>4. code</h2><p>简单明了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChannelAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_planes, ratio=<span class="number">16</span>)</span>:</span></span><br><span class="line">        super(ChannelAttention, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1   = nn.Conv2d(in_planes, in_planes // <span class="number">16</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2   = nn.Conv2d(in_planes // <span class="number">16</span>, in_planes, <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))</span><br><span class="line">        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))</span><br><span class="line">        out = avg_out + max_out</span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(out)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, planes, stride=<span class="number">1</span>, downsample=None)</span>:</span></span><br><span class="line">        super(Bottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                               padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv3 = nn.Conv2d(planes, planes * <span class="number">4</span>, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(planes * <span class="number">4</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        self.ca = ChannelAttention(planes * <span class="number">4</span>)</span><br><span class="line">        self.sa = SpatialAttention()</span><br><span class="line"></span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        out = self.ca(out) * out</span><br><span class="line">        out = self.sa(out) * out</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="5-GCNet"><a href="#5-GCNet" class="headerlink" title="5. GCNet"></a>5. GCNet</h2><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/GCNet.png" title="GCNet"><p>综合考虑了 SE-block 和 NL-block。</p><p>代码有点绕，暂时不看。</p><h2 id="5-优化器-optimizer"><a href="#5-优化器-optimizer" class="headerlink" title="5. 优化器 optimizer"></a>5. 优化器 optimizer</h2><p><a href="https://zhuanlan.zhihu.com/p/64882877?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=589386839161311232" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/64882877?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=589386839161311232</a></p><p><a href="https://gitlab.com/snowhitiger/learn_deep_learning/blob/master/sgd_visualization.ipynb" target="_blank" rel="noopener">code</a></p><p>不同的优化器，对结果也有很大的影响，先只记录一下 SGD, Adam 的常见参数配置。</p><p><strong>原始 SGD</strong>：</p><ul><li>学习率太小 0.20 ，有限步内无法达到最优，全程都没有震荡</li><li>学习率太大 0.61 ，会造成剧烈震荡，随着学习率的增加，后期震荡逐渐变大，0.52时后期震荡就已经无法收敛了</li><li>学习率适中 0.42 ，会成功达到最优点，前期会有震荡，后期没有震荡</li><li>对学习率很敏感</li></ul><img src="/2019/05/10/residual-attention_and_CBAM_GCNet/optimizer_ori_SGD.jpg" title="optimizer_ori_SGD"><p><strong>带动量的 SGD</strong>：</p><ul><li>适中的动量 beta=0.42 可以减少大学习率 lr=0.61 的震荡，达到最优点</li><li>较大的动量 beta=0.81 会对大学习率 lr=0.61 引入更大的震荡</li><li>一般的做法是大的动量 beta=0.9 和小的学习率 lr=0.02 or 0.03，会以比较平缓的方式加速达到最优</li><li>当 lr=0.01 时，beta不管怎么取都达不到最优，或者没到，或者超过</li><li>当 beta=0.9 时，对学习率也比较敏感，0.01会到不了最优点、0.02会到最优点前面、0.03会到最优点后面，区别比较大</li><li>常用设置：beta=0.9，weight-decay=5e-4，nesterov=True，之后再调节 lr 吧。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.optim.SGD(params, lr, momentum=<span class="number">0.9</span>,  weight_decay=<span class="number">5e-4</span>, nesterov=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><strong>Adam</strong>：</p><ul><li>对学习率不敏感</li><li>训练稳定</li><li>在最优点附件</li><li>参数设置：beta1=0.9, beta2=0.999, eta=1e-8</li></ul><p>虽然 Adam 对初始学习率不敏感，训练也比较稳定，但最终能达到的精度没有手动调好的SGD来得高。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.optim.Adam(params, lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>, weight_decay=<span class="number">1e-4</span>)</span><br><span class="line"><span class="comment"># 更推荐 0.5, 0.999</span></span><br><span class="line">torch.optim.Adam(params, lr=<span class="number">0.001</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1704.06904&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;商汤 CVPR2017 Residual Attention_Network for Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/fwang91/residual-attention-network&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;caffe&lt;/a&gt;, &lt;a href=&quot;http://ethereon.github.io/netscope/#/editor&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;caffe网络可视化工具 Netscope&lt;/a&gt;, &lt;a href=&quot;https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1807.06521&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ECCV2018_CBAM: Convolutional Block Attention Module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/luuuyi/CBAM.PyTorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1904.11492&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/xvjiarui/GCNet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;
    
    </summary>
    
      <category term="attention" scheme="http://yoursite.com/categories/attention/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>ECN</title>
    <link href="http://yoursite.com/2019/05/06/ECN/"/>
    <id>http://yoursite.com/2019/05/06/ECN/</id>
    <published>2019-05-06T07:32:26.000Z</published>
    <updated>2019-05-09T08:31:18.129Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>钟准团队的CCPR2019文章，牛逼。思想有一丢丢类似聚类，但是比聚类强的是，赋予了id。与HHL文章有很大的不同。什也不说了，虚心学习。</p><ul><li>paper: <a href="https://arxiv.org/abs/1904.01990" target="_blank" rel="noopener">CVPR2019_Invariance Matters Exemplar Memory for Domain Adaptive Person Re-identification</a></li><li>code: <a href="https://github.com/zhunzhong07/ECN" target="_blank" rel="noopener">pytorch</a></li></ul><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>作者的注意力还是集中在如何区分 target domain 中的图片。考虑了三个因素，exemplar-invariance, camera-invariance and neighborhood-invariance. 可以简单地理解成每个人是一类，StarGAN生成的图片是一类，特征相似的是一类，并且为了实现上述目标，需要存储 target domain 中 train 数据集的所有特征，这一点有点疑问，需要的存储空间和计算时间消耗大吗？作者在第一节的最后有解答，消耗的空间和时间都很小。等自己实验的时候就知道了。它是用了两个分类器，一个用于训练 source 数据集， label 是 source 的label，一个用于训练 target 数据集， label 是 target 的label，两个label不一样。网络模型与HHL略有不同。</p><p>第一，通过分类损失使 target domain 的图片全部可分。分类模型学到的更可能是图片视觉上的相似性而不是语义相似性，并且数据集中同一id的图片在视觉上变化很大，所以，索性将target数据集上的每张图片作为一类，迫使不同id的相似图片变得不相似。行吧，这个理由，(⊙o⊙)…，怎么有点牵强吧，因为和后面的特征相似的作为一类有一丢丢矛盾。不对，有点类似既然不知道你们的id，那就把让你们之间先全部可分，这个思路在哪篇论文见过，想不起来了。</p><p>第二，同一 content 不同 camera style 的图片视为一类。这个和HHL的数据集处理方法一样，用 StarGAN 生成同一图片在不同 camera 下的图片，并视为一类，因为刚刚对图片使用了分类损失，所以这时候可以直接用到分类损失上，而不用再考虑三元组损失，(我觉得可能是基于分类损失的作用一直大于二元组损失，三元组损失)，这时可以保证同一 content 不同 camera style 的图片视为一类。这时 StarGAN 生成图片中的人的形态等基本保留，感觉更多是亮度上的变化(肉眼可见)，偶尔会有衣服颜色的变换，但是不能变换这个人的姿势，比如正面走变成侧面走这种，HHL 已经证明了 camera style 的影响，但是我觉得姿势的变换也是一个重要的因素才对，因为分类模型的起家就是识别同一类物体不同形态的图片。好像 CVPR 2019 这个团队有做这方面的内容，抽空膜拜一波。</p><p>第三，k近邻的图片视为一类。这应该像是之间看到的一些关于聚类的或者k近邻的论文，总体思想是因为不知道图片的真id，那就可能赋予一个比较接近真实的id，又因为同一id的图片的特征应该是临近的，所以把临近的图片作为同一id也不过分，其实有的论文用的是聚类，但本质是一样的。</p><p>我觉得钟准厉害的地方不仅仅是能想到这三个创新点，更重要的是能把这三个创新点融合到一起，因为这三个创新点在别的论文里或多或少都可以看到影子，但是因为实现的的方法、思路、具体过程都不一样，很难看见一个创新点就把融到一起，看到一个想法就放到自己的模型里，或者看到一个idea就觉得有用并且一试还真有用，并能给出属于自己的解释，而不是明显的生搬硬造，比如这里的 memory module 很有想法。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>Unsupervised domain adaptation. 一种方法是对齐两个域的特征，其基本假设是源域和目标域的类别一样。另一种方法是丢弃目标域的未知类别样本，学习源域到目标域的映射。</p><p>Unsupervised person re-identification. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.pdf" target="_blank" rel="noopener">ECCV2018_HHL</a> 可能忽略了 target domain 中其他因素对 id 的影响，比如姿势等。与 <a href="https://arxiv.org/pdf/1808.07301.pdf" target="_blank" rel="noopener">BMVC2018_DAL</a> 不同的是，作者设计了软分类损失。</p><h2 id="3-The-Proposed-Method"><a href="#3-The-Proposed-Method" class="headerlink" title="3. The Proposed Method"></a>3. The Proposed Method</h2><p>简单定义下数学符号，直接用英文吧，能知道就行。</p><div class="table-container"><table><thead><tr><th style="text-align:center">数学符号</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">$\lbrace X_s, Y_s \rbrace, N_s$</td><td style="text-align:center">source domain, $N_s$ person images, $(x_{si}, y_{s,i})$, $M$ person identities</td></tr><tr><td style="text-align:center">$X_t, N_t$</td><td style="text-align:center">target domain, $N_t$ person images</td></tr></tbody></table></div><h3 id="3-1-Overview-of-Framework"><a href="#3-1-Overview-of-Framework" class="headerlink" title="3.1 Overview of Framework"></a>3.1 Overview of Framework</h3><img src="/2019/05/06/ECN/framework.png" title="Framework of the proposed approach"><p>其实模型没有画的这么复杂，基本还是保留ResNet-50的基本网络，后面加个FC-4049，FC-#id/ exemplar memory，其中FC-#id用于 source domain 的数据， exemplar memory 用于计算 target domain 的数据，等下直接看代码一目了然。</p><h3 id="3-2-Supervised-Learning-for-Source-Domain"><a href="#3-2-Supervised-Learning-for-Source-Domain" class="headerlink" title="3.2 Supervised Learning for Source Domain"></a>3.2 Supervised Learning for Source Domain</h3><p>这个容易理解， source domain 直接用个分类损失。</p><script type="math/tex; mode=display">L_{src}=-\frac{1}{n_s}\sum_{i=1}^{n_s}\log p(y_{s,i}|x_{s,i})</script><h3 id="3-3-Exemplar-Memory"><a href="#3-3-Exemplar-Memory" class="headerlink" title="3.3 Exemplar Memory"></a>3.3 Exemplar Memory</h3><p>Exemplar Memory 借鉴的是 <a href="https://arxiv.org/abs/1604.01850v3" target="_blank" rel="noopener">Joint Detection and Identification Feature Learning for Person Search</a>，（已经有两篇论文没有看过了，自己菜不是没有原因的），采用的是 key-value 架构 $K,V$，其中在 key-memory 中存储每张图片的 FC-4096 特征， value-memory 中存储图片的lable，其中视每张图片为一类，index 即为 label。key-memory 中的特征初始化成0，value-memory 初始化成 $V[i]=i$，并且在整个训练过程中保持不变。key-memory 的更新策略是:</p><script type="math/tex; mode=display">K[i]\gets \alpha K[i]+(1-\alpha)f(x_{t,i})</script><p>其中$\alpha\in [0,1]$，$K[i]\gets \parallel K[i] \parallel_2$，也就是每次都会进行一次归一化。这里的 $\alpha$ 是线性变化的，初始化为 0.01, 后续变化为 $\alpha = 0.01*epoch $。其中每次更新都是发生在反向传播时，顺便进行更新的。写的很巧妙。</p><h3 id="3-4-Invariance-Learning-for-Target-Domain"><a href="#3-4-Invariance-Learning-for-Target-Domain" class="headerlink" title="3.4 Invariance Learning for Target Domain"></a>3.4 Invariance Learning for Target Domain</h3><p>开始祭出三板斧了。</p><p><strong>Exemplar-invariance</strong>: 因为同一id的图片在外观上也会有很大的变化，即每一张图片都应该只与自己相似，与其他图片差别很大，因此，可以视每一张图片为一类。具体过程是，对于给定图片$x_{t,i}$，先计算 $x_{t,i}$ 的特征与其他图片在 key-memory 存储的特征的 cosine 距离，距离越大越相似。然后用这个距离去预测 $x_{t,i}$ 属于第 i 类的概率。这种预测与我见过的分类不太一样，因为分类模型一般都是直接用特征去预测类别，但这里是用距离去预测类别，或者说用距离当预测概率，还是第一次见。</p><script type="math/tex; mode=display">p(i|x_{t,i})=\frac{exp(K[i]^T f(x_{t,i})/\beta)}{\sum_{j=1}^{N_t}exp(K[j]^T f(x_{t,i})/\beta)}</script><p>其中 $\beta\in (0,1]$ ，用于平衡分布的趋势，记为 temperature，$\beta=0.05$，根据代码来看，$K[i]$ 表示当前 model 运行之前存储的， $f(x_{t,i})$表示当前 model 的结果，也就是说两者不一样。下面求概率的过程同理。</p><p>这个公式和 distillation 很像。</p><script type="math/tex; mode=display">q_i=\frac{exp(z_i/T)}{\sum_j exp(z_j/T)}</script><p>其中，$z_i$是类别预测概率。</p><p>所以分类损失可以写成：</p><script type="math/tex; mode=display">L_{ei}=-\log p(i|x_{t,i})</script><p><strong>Camera-invariance</strong>：视经过 StarGAN 转换的图片$\hat{x}_{t,i}$是同一类，与HHL一样，但是与HHL不同的是，HHL用于三元组损失，ECN用于分类损失，类似 Exemplar-invariance 的概率求解和loss损失，得：</p><script type="math/tex; mode=display">p(i|\hat{x} _ {t,i})=\frac{exp(K[i]^T f(\hat{x} _ {t,i})/\beta)}{\sum_{j=1}^{N_t}exp(K[j]^T f(\hat{x}_{t,i})/\beta)}</script><script type="math/tex; mode=display">L_{ci}=-\log p(i|\hat{x}_{t,i})</script><p>其中，$\hat{x}_ {t,i}$是在生成的C张图片中$\lbrace \hat{x}_ {t,i,1},…, \hat{x}_{t,i,C} \rbrace$随机选了一张。</p><p>这里有个问题，Exemplar Memory 中是否包含 CamStyle 生成的图片。等看代码就知道了。</p><p><strong>Neighborhood-invariance</strong>: 对每一张图片，在数据集中一定至少存在一张与之相同id的图片，如果能得到这些图片，那就更好了，这样就可以获得同一id不同姿态的图片。具体方法是在 exemplar memory 中计算 cos 距离，用k近邻方法得到最近的k张图片的index，记为 $M(x_{t,i},k)$，显然最近的是i. $k=6$</p><p>基于假设k近邻得到的图片$M(x_{t,i},k)$属于同一类，因此可以得到 $x_{t,i}$ 属于第 j 类的概率的权重是:</p><script type="math/tex; mode=display">w_{i,j}=\begin{cases}\frac{1}{k}, j\not = i \\1, j=i\end{cases}, \forall j \in M(x_{t,i},k)</script><p>这种赋予权重的方法是把 $x_{t,i}$ 属于另一张图片类别的概率，而不是我之前以为的 $M(x_{t,i},k)$ 中的图片属于 $x_{t,i}$ 类别的概率。</p><p>因此损失可以写成：</p><script type="math/tex; mode=display">L_{n,i}=-\sum_{j\not = i} w_{i,j} \log p(j|x_{t,i}), \forall j \in M(x_{t,i},k)</script><p>其中，为了区分 Exemplar-invariance 和 Neighborhood-invariance ，这里没有再次计算 $x_{t,i}$ 属于第 i 类的损失。</p><p><strong>Overall loss of invariance learning</strong>: invariance learning 的总损失可以表示成：</p><script type="math/tex; mode=display">L_{tgt}=-\frac{1}{n_t} \sum_{i=1}^{n_t} \sum_{j \in M(x_{t,i},k)} w_{i,j} \log p(j|x^*_{t,i})</script><p>其中，$x^*_ {t,i}$ 表示随机从$\lbrace x_{t,i}, \hat{x}_ {t,i,1},…, \hat{x}_{t,i,C}, \rbrace$ 选一张。</p><p>看看代码再说这个损失的具体实现吧。</p><h3 id="3-5-Final-Loss-for-Network"><a href="#3-5-Final-Loss-for-Network" class="headerlink" title="3.5 Final Loss for Network"></a>3.5 Final Loss for Network</h3><script type="math/tex; mode=display">L=(1-\lambda)L_{src}+\lambda L_{tgt}</script><p>其中，$\lambda \in (0, 1]$， $\lambda=0.3$</p><p>这里的损失没有用常见的加法，而是一个线性组合？</p><h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4. Experiment"></a>4. Experiment</h2><h3 id="4-1-Experiment-Setting"><a href="#4-1-Experiment-Setting" class="headerlink" title="4.1 Experiment Setting"></a>4.1 Experiment Setting</h3><p>前5个 epoch 只训练 exemplar-invariance and camera-invariance，也就是通用的交叉熵损失，5个epoch之后再加入 neighborhood-invariance。</p><h3 id="4-2-Parameter-Analysis"><a href="#4-2-Parameter-Analysis" class="headerlink" title="4.2 Parameter Analysis"></a>4.2 Parameter Analysis</h3><p>temperature fact $\beta$, weight of loss $\lambda$, number of candidate positive samples $k$.</p><p><strong>Temperature fact $\beta$</strong></p><img src="/2019/05/06/ECN/temperature.png" title="different values of $\beta$"><p>当 $\beta$ 比较小的时候，分布越陡，值越大，损失越小，结果也越好。最好的结果是 $\beta$ 在0.05 左右。同时通过表格可以看出，当 $\beta$ 变化从0.05到0.5时，rank-1下降了17。但同时，在最优解的附近也接近最优解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在蒸馏网络中，T越大，分布越平缓</span></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">fun1 = F.softmax</span><br><span class="line">aa = torch.tensor([<span class="number">0.2</span>, <span class="number">0.8</span>, <span class="number">0.2</span>])</span><br><span class="line">fun1(aa, dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">0.2616</span>, <span class="number">0.4767</span>, <span class="number">0.2616</span>])</span><br><span class="line">bb = torch.tensor([<span class="number">0.2</span>, <span class="number">0.8</span>, <span class="number">0.2</span>])/<span class="number">0.1</span></span><br><span class="line">fun1(bb, dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">0.0025</span>, <span class="number">0.9951</span>, <span class="number">0.0025</span>])</span><br></pre></td></tr></table></figure><p><strong>The weight of source and target losses $\lambda$</strong>:</p><img src="/2019/05/06/ECN/weight.png" title="The weight of source and target losses $\lambda$"><p>可以看出即便只有 target loss， 性能也超过 baseline， 当取其他值的时候，性能几乎不变。</p><p><strong>Number of positive samples $k$</strong>:</p><img src="/2019/05/06/ECN/number.png" title="Number of positive samples $k$"><p>可以看出，性能对k挺敏感的。</p><h3 id="4-4-Evaluation"><a href="#4-4-Evaluation" class="headerlink" title="4.4 Evaluation"></a>4.4 Evaluation</h3><p>通过表格中的数据集可以计算出，C+N&gt;C,N，C和N共同使用互相还有促进作用。</p><p>作者也在 MSMT 数据集上做了训练测试。</p><img src="/2019/05/06/ECN/MSMT.png" title="performance evaluation when tested on MSMT17"><p>感觉大家都开始在 MSMT17 上做实验了，不怎么管 CUHK 了。</p><h2 id="5-code"><a href="#5-code" class="headerlink" title="5. code"></a>5. code</h2><p>代码和HHL的差不多</p><h3 id="5-1-model"><a href="#5-1-model" class="headerlink" title="5.1 model"></a>5.1 model</h3><h4 id="5-1-1-baseline"><a href="#5-1-1-baseline" class="headerlink" title="5.1.1 baseline"></a>5.1.1 baseline</h4><p>有以下几个改变：</p><ul><li>conv1 ~ layer2 的参数不再更新</li><li>source: layer4-avgpool-Linear(4096)-bn-relu-drop(0.5)-Linear(num_class)</li><li>target: layer4-avgpool-Linear(4096)-bn-F.normalize-drop(0.5)</li></ul><p>Question：先normalize后drop的话就不满足归一化的定义了</p><p>Question: conv1~layer2 固定参数是有什么含义吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    __factory = &#123;</span><br><span class="line">        <span class="number">18</span>: torchvision.models.resnet18,</span><br><span class="line">        <span class="number">34</span>: torchvision.models.resnet34,</span><br><span class="line">        <span class="number">50</span>: torchvision.models.resnet50,</span><br><span class="line">        <span class="number">101</span>: torchvision.models.resnet101,</span><br><span class="line">        <span class="number">152</span>: torchvision.models.resnet152,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, depth, pretrained=True, cut_at_pooling=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_features=<span class="number">0</span>, norm=False, dropout=<span class="number">0</span>, num_classes=<span class="number">0</span>, num_triplet_features=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.depth = depth</span><br><span class="line">        self.pretrained = pretrained</span><br><span class="line">        self.cut_at_pooling = cut_at_pooling</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Construct base (pretrained) resnet</span></span><br><span class="line">        <span class="keyword">if</span> depth <span class="keyword">not</span> <span class="keyword">in</span> ResNet.__factory:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">"Unsupported depth:"</span>, depth)</span><br><span class="line">        self.base = ResNet.__factory[depth](pretrained=pretrained)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Fix layers [conv1 ~ layer2]</span></span><br><span class="line">        fixed_names = []</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.base._modules.items():</span><br><span class="line">            <span class="keyword">if</span> name == <span class="string">"layer3"</span>:</span><br><span class="line">                <span class="comment"># assert fixed_names == ["conv1", "bn1", "relu", "maxpool", "layer1", "layer2"]</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            fixed_names.append(name)</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> module.parameters():</span><br><span class="line">                param.requires_grad = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.cut_at_pooling:</span><br><span class="line">            self.num_features = num_features</span><br><span class="line">            self.norm = norm</span><br><span class="line">            self.dropout = dropout</span><br><span class="line">            self.has_embedding = num_features &gt; <span class="number">0</span></span><br><span class="line">            self.num_classes = num_classes</span><br><span class="line">            self.num_triplet_features = num_triplet_features</span><br><span class="line"></span><br><span class="line">            self.l2norm = Normalize(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            out_planes = self.base.fc.in_features</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Append new layers</span></span><br><span class="line">            <span class="keyword">if</span> self.has_embedding:</span><br><span class="line">                self.feat = nn.Linear(out_planes, self.num_features)</span><br><span class="line">                self.feat_bn = nn.BatchNorm1d(self.num_features)</span><br><span class="line">                init.kaiming_normal_(self.feat.weight, mode=<span class="string">'fan_out'</span>)</span><br><span class="line">                init.constant_(self.feat.bias, <span class="number">0</span>)</span><br><span class="line">                init.constant_(self.feat_bn.weight, <span class="number">1</span>)</span><br><span class="line">                init.constant_(self.feat_bn.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Change the num_features to CNN output channels</span></span><br><span class="line">                self.num_features = out_planes</span><br><span class="line">            <span class="keyword">if</span> self.dropout &gt;= <span class="number">0</span>:</span><br><span class="line">                self.drop = nn.Dropout(self.dropout)</span><br><span class="line">            <span class="keyword">if</span> self.num_classes &gt; <span class="number">0</span>:</span><br><span class="line">                self.classifier = nn.Linear(self.num_features, self.num_classes)</span><br><span class="line">                init.normal_(self.classifier.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                init.constant_(self.classifier.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.pretrained:</span><br><span class="line">            self.reset_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, output_feature=None)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.base._modules.items():</span><br><span class="line">            <span class="keyword">if</span> name == <span class="string">'avgpool'</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = module(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.cut_at_pooling:</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        x = F.avg_pool2d(x, x.size()[<span class="number">2</span>:])</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_feature == <span class="string">'pool5'</span>:</span><br><span class="line">            x = F.normalize(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.has_embedding:</span><br><span class="line">            x = self.feat(x)</span><br><span class="line">            x = self.feat_bn(x)</span><br><span class="line">            tgt_feat = F.normalize(x)</span><br><span class="line">            tgt_feat = self.drop(tgt_feat)</span><br><span class="line">            <span class="keyword">if</span> output_feature == <span class="string">'tgt_feat'</span>:</span><br><span class="line">                <span class="keyword">return</span> tgt_feat</span><br><span class="line">        <span class="keyword">if</span> self.norm:</span><br><span class="line">            x = F.normalize(x)</span><br><span class="line">        <span class="keyword">elif</span> self.has_embedding:</span><br><span class="line">            x = F.relu(x)</span><br><span class="line">        <span class="keyword">if</span> self.dropout &gt; <span class="number">0</span>:</span><br><span class="line">            x = self.drop(x)</span><br><span class="line">        <span class="keyword">if</span> self.num_classes &gt; <span class="number">0</span>:</span><br><span class="line">            x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                init.kaiming_normal(m.weight, mode=<span class="string">'fan_out'</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    init.constant(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                init.constant(m.weight, <span class="number">1</span>)</span><br><span class="line">                init.constant(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</span><br><span class="line">                init.normal(m.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    init.constant(m.bias, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>L2 正则化定义了专门的层，但没有使用，可能是嫌慢吧。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Normalize</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, power=<span class="number">2</span>)</span>:</span></span><br><span class="line">        super(Normalize, self).__init__()</span><br><span class="line">        self.power = power</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        norm = x.pow(self.power).sum(<span class="number">1</span>, keepdim=<span class="keyword">True</span>).pow(<span class="number">1.</span>/self.power)</span><br><span class="line">        out = x.div(norm)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h4 id="5-1-2-InvNet"><a href="#5-1-2-InvNet" class="headerlink" title="5.1.2 InvNet"></a>5.1.2 InvNet</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pytorch&gt;=1.0.0</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, autograd</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable, Function</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment"># 这应该是最新版的自定义 Function 的实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExemplarMemory</span><span class="params">(Function)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, em, alpha=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        super(ExemplarMemory, self).__init__()</span><br><span class="line">        self.em = em</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: b*2048, targets: b*1 (index)</span></span><br><span class="line">        self.save_for_backward(inputs, targets)</span><br><span class="line">        <span class="comment"># outputs: b*12936</span></span><br><span class="line">        outputs = inputs.mm(self.em.t())</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_outputs)</span>:</span></span><br><span class="line">        <span class="comment"># 这个backward 主要用于更新 em</span></span><br><span class="line">        inputs, targets = self.saved_tensors</span><br><span class="line">        grad_inputs = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> self.needs_input_grad[<span class="number">0</span>]:</span><br><span class="line">            grad_inputs = grad_outputs.mm(self.em)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(inputs, targets):</span><br><span class="line">            self.em[y] = self.alpha * self.em[y] + (<span class="number">1.</span> - self.alpha) * x</span><br><span class="line">            self.em[y] /= self.em[y].norm()</span><br><span class="line">        <span class="keyword">return</span> grad_inputs, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Invariance learning loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InvNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, num_classes, beta=<span class="number">0.05</span>, knn=<span class="number">6</span>, alpha=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        super(InvNet, self).__init__()</span><br><span class="line">        self.device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">        self.num_features = num_features</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.alpha = alpha  <span class="comment"># Memory update rate</span></span><br><span class="line">        self.beta = beta  <span class="comment"># Temperature fact</span></span><br><span class="line">        self.knn = knn  <span class="comment"># Knn for neighborhood invariance</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Exemplar memory, 12936x2048</span></span><br><span class="line">        self.em = nn.Parameter(torch.zeros(num_classes, num_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, targets, epoch=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs: b*2048, targets: b*1 (index)</span></span><br><span class="line">        alpha = self.alpha * epoch</span><br><span class="line">        <span class="comment"># 每次都是重建一个 ExemplarMemory，我觉得可能是因为alpha每次要改变。</span></span><br><span class="line">        inputs = ExemplarMemory(self.em, alpha=alpha)(inputs, targets)</span><br><span class="line">        <span class="comment"># inputs: b*12936</span></span><br><span class="line"></span><br><span class="line">        inputs /= self.beta</span><br><span class="line">        <span class="keyword">if</span> self.knn &gt; <span class="number">0</span> <span class="keyword">and</span> epoch &gt; <span class="number">4</span>:</span><br><span class="line">            <span class="comment"># With neighborhood invariance</span></span><br><span class="line">            loss = self.smooth_loss(inputs, targets)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Without neighborhood invariance</span></span><br><span class="line">            loss = F.cross_entropy(inputs, targets)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">smooth_loss</span><span class="params">(self, inputs, targets)</span>:</span></span><br><span class="line">        <span class="comment"># overall loss of invariance loss</span></span><br><span class="line">        <span class="comment"># inputs: b*12936, targets: b*1 (index)</span></span><br><span class="line">        targets = self.smooth_hot(inputs.detach().clone(), targets.detach().clone(), self.knn)</span><br><span class="line">        <span class="comment"># targets: b*12936, weights</span></span><br><span class="line">        outputs = F.log_softmax(inputs, dim=<span class="number">1</span>)</span><br><span class="line">        loss = - (targets * outputs)</span><br><span class="line">        loss = loss.sum(dim=<span class="number">1</span>)</span><br><span class="line">        loss = loss.mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">smooth_hot</span><span class="params">(self, inputs, targets, k=<span class="number">6</span>)</span>:</span></span><br><span class="line">        <span class="comment"># Sort</span></span><br><span class="line">        <span class="comment"># inputs: b*12936, targets: b*1 (index)</span></span><br><span class="line">        <span class="comment"># targets_onehot: b*12936</span></span><br><span class="line">        <span class="comment"># targets_onehot: 记录的是当前样本属于其他类的概率的权重 1/k or 1</span></span><br><span class="line">        _, index_sorted = torch.sort(inputs, dim=<span class="number">1</span>, descending=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        ones_mat = torch.ones(targets.size(<span class="number">0</span>), k).to(self.device)</span><br><span class="line">        targets = torch.unsqueeze(targets, <span class="number">1</span>)</span><br><span class="line">        targets_onehot = torch.zeros(inputs.size()).to(self.device)</span><br><span class="line">        <span class="comment"># 这里的 weights 应该是 1/k， 因为每个值都一样，所以 softmax 之后就是 1/k</span></span><br><span class="line">        <span class="comment"># 猜测作者这里刚开始不是直接想要 1/k 的权重，而是根据距离远近赋予权重，比如选</span></span><br><span class="line">        <span class="comment"># 择6个最近的，然后根据相似性赋予权重（等同于概率的求解）</span></span><br><span class="line">        <span class="comment"># Question:weights = F.softmax(ones_mat, dim=1)</span></span><br><span class="line">        <span class="comment"># targets_onehot.scatter_(1, index_sorted[:, 0:k], ones_mat * weights)</span></span><br><span class="line">        weights = F.softmax(ones_mat, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 根据位置填充权重</span></span><br><span class="line">        targets_onehot.scatter_(<span class="number">1</span>, index_sorted[:, <span class="number">0</span>:k], ones_mat * weights)</span><br><span class="line">        <span class="comment"># Question: 怎么保证前k个就一定没有index？</span></span><br><span class="line">        targets_onehot.scatter_(<span class="number">1</span>, targets, float(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> targets_onehot</span><br></pre></td></tr></table></figure><p>补充 <strong>gather</strong> 和 <strong>scatter</strong> 的用法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gather</span></span><br><span class="line"><span class="comment"># torch.gather(input, dim, index, out=None)</span></span><br><span class="line"><span class="comment"># output[i][j][k]=input[index[[i][j][k]]][j][k] # if dim=0</span></span><br><span class="line"><span class="comment"># output[i][j][k]=input[i][index[[i][j][k]]][k] # if dim=1</span></span><br><span class="line"><span class="comment"># output[i][j][k]=input[i][j][index[[i][j][k]]] # if dim=2</span></span><br><span class="line"><span class="comment"># 光看公式很绕，</span></span><br><span class="line"><span class="comment"># 其中 out 的size和 index 的size相同</span></span><br><span class="line"><span class="comment"># input: a0*a1*a2*...ai-1*ai*ai+1,...,an-1</span></span><br><span class="line"><span class="comment"># index: a0*a1*a2*...ai-1*y*ai+1,...,an-1</span></span><br><span class="line"><span class="comment"># output:a0*a1*a2*...ai-1*y*ai+1,...,an-1</span></span><br><span class="line"><span class="comment"># 除了第dim维，其他维度上 index 的size和input的size相同</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者我们换个角度，不要管公式，把index分成y份，index_0，index_1, ..., index_y-1</span></span><br><span class="line"><span class="comment"># index_0: a0*a1*a2*...ai-1*1*ai+1,...,an-1</span></span><br><span class="line"><span class="comment"># index_0 就是在第i个维度上选取对应位置第y[0]个通道的数字，</span></span><br><span class="line"><span class="comment"># 以此类推，下面用二维矩阵做个示范</span></span><br><span class="line"><span class="comment"># 可以理解成降维</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]])</span><br><span class="line"><span class="comment"># [0,0] 表示选取第0维第0个通道的数字，第0维第0个通道的数字</span></span><br><span class="line">torch.gather(input, <span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">0</span>]]) )</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>]])</span><br><span class="line"><span class="comment"># [0,0] 表示选取第0维第0个通道的数字，第0维第1个通道的数字</span></span><br><span class="line">torch.gather(input, <span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>]]) )</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">4.</span>]])</span><br><span class="line">torch.gather(input, <span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>]]) )</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">4.</span>]])</span><br><span class="line"><span class="comment"># 三维矩阵</span></span><br><span class="line"><span class="comment"># 三维矩阵更好理解，可以通过投射的方式理解</span></span><br><span class="line">a = torch.arange(<span class="number">0</span>,<span class="number">27</span>).view(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 第0维第0个通道</span></span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>]],</span><br><span class="line"><span class="comment"># 第0维第1个通道</span></span><br><span class="line">        [[ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">         [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>],</span><br><span class="line">         [<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>]],</span><br><span class="line"><span class="comment"># 第0维第2个通道</span></span><br><span class="line">        [[<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>],</span><br><span class="line">         [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>]]])</span><br><span class="line"><span class="comment"># 在对应位置，分别选取第0维的第0个，第1个，第2个通道的数字</span></span><br><span class="line">c = [[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]]]</span><br><span class="line">index = torch.LongTensor(c)</span><br><span class="line">a.gather(<span class="number">0</span>, index)</span><br><span class="line">tensor([[[ <span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">         [ <span class="number">3</span>, <span class="number">13</span>, <span class="number">23</span>],</span><br><span class="line">         [ <span class="number">6</span>, <span class="number">16</span>, <span class="number">26</span>]]])</span><br><span class="line"><span class="comment"># output的第0维第0个通道：分别选取0,1,2</span></span><br><span class="line"><span class="comment"># output的第0维第1个通道：分别选取0,1,0</span></span><br><span class="line">c = [[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]], [[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]]</span><br><span class="line">index = torch.LongTensor(c)</span><br><span class="line">a.gather(<span class="number">0</span>, index)</span><br><span class="line">tensor([[[ <span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">         [ <span class="number">3</span>, <span class="number">13</span>, <span class="number">23</span>],</span><br><span class="line">         [ <span class="number">6</span>, <span class="number">16</span>, <span class="number">26</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0</span>, <span class="number">10</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>, <span class="number">13</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>, <span class="number">16</span>,  <span class="number">8</span>]]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scatter</span></span><br><span class="line"><span class="comment"># torch.scatter_(dim, index, src)</span></span><br><span class="line"><span class="comment"># 可以理解成gather的相反操作</span></span><br><span class="line"><span class="comment"># src 和 index 的size相同</span></span><br><span class="line"><span class="comment"># out[index[i][j][k]][j][k]=src[i][j][k] if dim=0</span></span><br><span class="line"><span class="comment"># out[i][index[i][j][k]][k]=src[i][j][k] if dim=1</span></span><br><span class="line"><span class="comment"># out[i][j][index[i][j][k]]=src[i][j][k] if dim=2</span></span><br><span class="line">x = torch.rand(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"> <span class="number">0.4319</span>  <span class="number">0.6500</span>  <span class="number">0.4080</span>  <span class="number">0.8760</span>  <span class="number">0.2355</span></span><br><span class="line"> <span class="number">0.2609</span>  <span class="number">0.4711</span>  <span class="number">0.8486</span>  <span class="number">0.8573</span>  <span class="number">0.1029</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">2</span>x5]</span><br><span class="line">torch.zeros(<span class="number">3</span>, <span class="number">5</span>).scatter_(<span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]), x)</span><br><span class="line"> <span class="number">0.4319</span>  <span class="number">0.4711</span>  <span class="number">0.8486</span>  <span class="number">0.8760</span>  <span class="number">0.2355</span></span><br><span class="line"> <span class="number">0.0000</span>  <span class="number">0.6500</span>  <span class="number">0.0000</span>  <span class="number">0.8573</span>  <span class="number">0.0000</span></span><br><span class="line"> <span class="number">0.2609</span>  <span class="number">0.0000</span>  <span class="number">0.4080</span>  <span class="number">0.0000</span>  <span class="number">0.1029</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">3</span>x5]</span><br></pre></td></tr></table></figure><h3 id="5-2-data"><a href="#5-2-data" class="headerlink" title="5.2 data"></a>5.2 data</h3><p>对于 target domain 中的图片，每次都是从原始图片和生成图片组成的集合中随机取一张图片，当选取的 camid 是原始图片的 camid 时，取原始图片，当取到的 camid 不是原始图片的 camid 时，取生成图片，实际只使用了 C-1 张生成图片和 1 张原始图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># reid/utils/data/preprocessor.py</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_single_item</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        fname, pid, camid = self.dataset[index]</span><br><span class="line">        sel_cam = torch.randperm(self.num_cam)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> sel_cam == camid:</span><br><span class="line">            fpath = osp.join(self.root, fname)</span><br><span class="line">            img = Image.open(fpath).convert(<span class="string">'RGB'</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'msmt'</span> <span class="keyword">in</span> self.root:</span><br><span class="line">                fname = fname[:<span class="number">-4</span>] + <span class="string">'_fake_'</span> + str(sel_cam.numpy() + <span class="number">1</span>) + <span class="string">'.jpg'</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                fname = fname[:<span class="number">-4</span>] + <span class="string">'_fake_'</span> + str(camid + <span class="number">1</span>) + <span class="string">'to'</span> + str(sel_cam.numpy() + <span class="number">1</span>) + <span class="string">'.jpg'</span></span><br><span class="line">            fpath = osp.join(self.camstyle_root, fname)</span><br><span class="line">            img = Image.open(fpath).convert(<span class="string">'RGB'</span>)</span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        <span class="keyword">return</span> img, fname, pid, index</span><br></pre></td></tr></table></figure><h3 id="5-3-optimizer"><a href="#5-3-optimizer" class="headerlink" title="5.3 optimizer"></a>5.3 optimizer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个写的可以，清晰明了，只是在HHL中，只有classifier层设置为1.0，其他新层则是0.0，这里则把新层全部设置成了1.0，应该也是实验所得吧。</span></span><br><span class="line">base_param_ids = set(map(id, model.module.base.parameters()))</span><br><span class="line"></span><br><span class="line">base_params_need_for_grad = filter(<span class="keyword">lambda</span> p: p.requires_grad, model.module.base.parameters())</span><br><span class="line"></span><br><span class="line">new_params = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span></span><br><span class="line">                id(p) <span class="keyword">not</span> <span class="keyword">in</span> base_param_ids]</span><br><span class="line">param_groups = [</span><br><span class="line">    &#123;<span class="string">'params'</span>: base_params_need_for_grad, <span class="string">'lr_mult'</span>: <span class="number">0.1</span>&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: new_params, <span class="string">'lr_mult'</span>: <span class="number">1.0</span>&#125;]</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(param_groups, lr=args.lr,</span><br><span class="line">                            momentum=args.momentum,</span><br><span class="line">                            weight_decay=args.weight_decay,</span><br><span class="line">                            nesterov=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h3 id="5-4-train"><a href="#5-4-train" class="headerlink" title="5.4 train"></a>5.4 train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># main.py and reid/trainers.py</span></span><br><span class="line"><span class="comment"># self.model: ResNet-50</span></span><br><span class="line"><span class="comment"># self.model_inv : InvNet</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># source domain 的交叉熵损失</span></span><br><span class="line">inputs, pids = self._parse_data(inputs)</span><br><span class="line">outputs = self.model(inputs)</span><br><span class="line">source_pid_loss = self.pid_criterion(outputs, pids)</span><br><span class="line"><span class="comment"># target domain 的损失</span></span><br><span class="line">outputs = self.model(inputs_target, <span class="string">'tgt_feat'</span>)</span><br><span class="line">loss_un = self.model_inv(outputs, index_target, epoch=epoch)</span><br><span class="line"><span class="comment"># overall loss</span></span><br><span class="line">loss = (<span class="number">1</span> - self.lmd) * source_pid_loss + self.lmd * loss_un</span><br></pre></td></tr></table></figure><p>知识点补充：因为 ResNet-50 的 conv1~layer2 的参数已经设置不参与更新，自然这些层的 bn 层也应该是 eval 状态。</p><p>Question: 上面的知识点补充是否正确？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_model_train</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.model.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fix first BN</span></span><br><span class="line">    fixed_bns = []</span><br><span class="line">    <span class="keyword">for</span> idx, (name, module) <span class="keyword">in</span> enumerate(self.model.module.named_modules()):</span><br><span class="line">        <span class="keyword">if</span> name.find(<span class="string">"layer3"</span>) != <span class="number">-1</span>:</span><br><span class="line">            <span class="comment"># assert len(fixed_bns) == 22</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> name.find(<span class="string">"bn"</span>) != <span class="number">-1</span>:</span><br><span class="line">            fixed_bns.append(name)</span><br><span class="line">            module.eval()</span><br><span class="line">len(fixed_bns)</span><br><span class="line"><span class="number">22</span></span><br><span class="line">[<span class="string">'bn1'</span>,</span><br><span class="line"> <span class="string">'layer1.0.bn1'</span>,</span><br><span class="line"> <span class="string">'layer1.0.bn2'</span>,</span><br><span class="line"> <span class="string">'layer1.0.bn3'</span>,</span><br><span class="line"> <span class="string">'layer1.1.bn1'</span>,</span><br><span class="line"> <span class="string">'layer1.1.bn2'</span>,</span><br><span class="line"> <span class="string">'layer1.1.bn3'</span>,</span><br><span class="line"> <span class="string">'layer1.2.bn1'</span>,</span><br><span class="line"> <span class="string">'layer1.2.bn2'</span>,</span><br><span class="line"> <span class="string">'layer1.2.bn3'</span>,</span><br><span class="line"> <span class="string">'layer2.0.bn1'</span>,</span><br><span class="line"> <span class="string">'layer2.0.bn2'</span>,</span><br><span class="line"> <span class="string">'layer2.0.bn3'</span>,</span><br><span class="line"> <span class="string">'layer2.1.bn1'</span>,</span><br><span class="line"> <span class="string">'layer2.1.bn2'</span>,</span><br><span class="line"> <span class="string">'layer2.1.bn3'</span>,</span><br><span class="line"> <span class="string">'layer2.2.bn1'</span>,</span><br><span class="line"> <span class="string">'layer2.2.bn2'</span>,</span><br><span class="line"> <span class="string">'layer2.2.bn3'</span>,</span><br><span class="line"> <span class="string">'layer2.3.bn1'</span>,</span><br><span class="line"> <span class="string">'layer2.3.bn2'</span>,</span><br><span class="line"> <span class="string">'layer2.3.bn3'</span>]</span><br></pre></td></tr></table></figure><p>到此应该代码全部看完了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;钟准团队的CCPR2019文章，牛逼。思想有一丢丢类似聚类，但是比聚类强的是，赋予了id。与HHL文章有很大的不同。什也不说了，虚心学习。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1904.01990&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2019_Invariance Matters Exemplar Memory for Domain Adaptive Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/zhunzhong07/ECN&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="preson re-id" scheme="http://yoursite.com/categories/preson-re-id/"/>
    
    
      <category term="person re-id" scheme="http://yoursite.com/tags/person-re-id/"/>
    
      <category term="cross domain" scheme="http://yoursite.com/tags/cross-domain/"/>
    
  </entry>
  
  <entry>
    <title>Dual Attention Network for Scene Segmentation</title>
    <link href="http://yoursite.com/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/"/>
    <id>http://yoursite.com/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/</id>
    <published>2019-05-05T02:21:36.000Z</published>
    <updated>2019-05-20T07:45:23.148Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>这篇文章的重点在于 dual attention 的作用，并且attention的使用和之前看到的 SE block 还不太一样。dual attention 主要解决了全局依赖性，即其他位置的物体对当前位置的的物体的特征的影响。重点不是场景分割，自己也不是很懂分割的代码和实现，暂时对分割不做过多研究。</p><ul><li>paper: <a href="https://arxiv.org/pdf/1809.02983.pdf" target="_blank" rel="noopener">CPVR2019: Dual Attention Network for Scene Segmentation</a></li><li>code: <a href="https://github.com/junfu1115/DANet/" target="_blank" rel="noopener">pytorch</a></li><li>team: 中科院自动化所图像与视频分析团队（IVA），隶属于模式识别国家重点实验室，在 ICCV 2017 COCO-Places 场景解析竞赛、京东 AI 时尚挑战赛和阿里巴巴大规模图像搜索大赛踢馆赛等多次拔得头筹。嗯，一句话，很牛逼。</li><li><a href="https://blog.csdn.net/P_LarT/article/details/89043620" target="_blank" rel="noopener">解读</a></li></ul><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>作者提出了 <strong>Dual Attention Network(DANet)</strong> 来融合局部特征。具体的过程是在 dilated FCN 上添加了两种 attention modules: <strong>the position attention module</strong> and <strong>the channel attention module</strong>，这两个attention主要解决的是全局依赖性。</p><p>场景分割需要解决的两个问题：区分相似的东西(田地和草)，识别不同大小外观的同一个东西(车)。因此，场景分割模型需要提高像素级别识别的特征表示。</p><p>一种方法是<strong>多尺度</strong>融合来识别不同大小的物体，但是不能以全局地角度来很好地处理物体与物体之间的关系。应该是指最后的特征的感受野有大有小，可以理解成不同大小的物体都能识别到。</p><p>还有一种方法是利用了LSTM来实现 long-range dependencies，可以理解成物体的识别不仅依靠自己的特征，还依赖于其他物体的的特征，即<strong>全局依赖</strong>或者<strong>空间依赖性</strong>。</p><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/Dual_Attention_Network.png" title="Dual Attention Network"><p>其中 the position attention module 主要用于解决全局的空间位置依赖问题，the channel attention module 解决的是全局的通道依赖性。</p><p>所以作者主要解决的是全局依赖性，并没有考虑不同大小的物体的分割问题。</p><p>按照作者的说法，DANet 有两种作用：第一，可以避免显眼的大的物体的特征影响不起眼的小的物体的标签；第二，可以在一定程度上融合不同尺寸的物体的相似特征；第三，利用空间和通道的依懒性解决全局依赖问题。</p><h2 id="2-Dual-Attention-Network"><a href="#2-Dual-Attention-Network" class="headerlink" title="2. Dual Attention Network"></a>2. Dual Attention Network</h2><h3 id="3-1-Overview"><a href="#3-1-Overview" class="headerlink" title="3.1 Overview"></a>3.1 Overview</h3><p>基准网络是 dilated FCN.</p><h3 id="3-2-Position-Attention-Module"><a href="#3-2-Position-Attention-Module" class="headerlink" title="3.2 Position Attention Module"></a>3.2 Position Attention Module</h3><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/Position_and_Channel_Attention_Module.png" title="Position and Channel Attention Module"><p>通过特征提取网络得到特征图 $A\in R^{C\times H\times W}$，分别通过一个卷积层得两个特征图 $\lbrace B,C \rbrace \in R^{C\times H\times W}$，并且 reshape 成 $R^{C\times N}$，其中$N=H\times W$，然后得到$S\in R^{N\times N}$，此时把$R^{C}$看成这个位置的特征。下面阐述下具体的过程：</p><script type="math/tex; mode=display">\begin{aligned}\hat{S}&=C^T\times B, (where, \hat{S}\in R^{N\times N}) \\       &=[C_1, C_2,..., C_N]^T\times [B_1, B_2,..., B_N] \\       &=\begin{bmatrix}        C_1^T\times B_1 & C_1^T\times B_2 & ... & C_1^T\times B_N \\        C_2^T\times B_1 & C_2^T\times B_2 & ... & C_2^T\times B_N \\        ... & ... & ... & ... \\        C_N^T\times B_1 & C_N^T\times B_2 & ... & C_N^T\times B_N       \end{bmatrix}(where,C_j,B_i\in R^C)       \\\hat{s}_{ji}&=B_i^T \times C_j\end{aligned}</script><p>从而得到$S$</p><script type="math/tex; mode=display">s_{ji}=\frac{\exp(B_i \cdot C_j)}{\sum_{i=1}^N \exp(B_i \cdot C_j)}</script><p>可以理解成对$\hat{S}$的每一行都做一次softmax，即 S 的每一行和为1，可以解释成C中的点与B中所有点的相似性，越相似值越大。其中B和C是对称的。</p><p>同时，将A送进第三个滤波器得到 $D\in R^{C\times H\times W}$ 并且 reshape 成 $R^{C\times N}$ ，从而得到最后的输出$E$，下面阐述具体计算过程:</p><script type="math/tex; mode=display">\begin{aligned}\hat{E}&=D\times S^T (where,\hat{E}\in R^{C\times N}, D\in R^{C\times N}, C\in R^{N\times N})\\       &=[D_1, D_2,..., D_N] \times \begin{bmatrix}        s_{11} & s_{12} & ... & s_{1N} \\        s_{21} & s_{22} & ... & s_{2N} \\        ... & ... & ... & ... \\        s_{N1} & s_{N2} & ... & s_{NN}       \end{bmatrix}^T(where,D_i\in R^C) \\       &=[D_1\cdot s_{11}+D_2\cdot s_{12}+...+D_N\cdot s_{1N}, ..., D_1\cdot s_{N1}+D_2\cdot s_{N2}+...+D_N\cdot s_{NN}] \\       &=[\sum_i s_{1i}D_i, \sum_i s_{2i}D_i,... ,\sum_i s_{Ni}D_i] \\\hat{e}_{j}&=\sum_i^N(s_{ji}D_i)\end{aligned}</script><p>从而得到$E\in R^{C\times N}$,</p><script type="math/tex; mode=display">E_j=\alpha \sum_i^N(s_{ji}D_i)+A_j</script><p>相当于 $\hat{E}$ 与 $A$ 进行了线性组合，并对其reshape变成$E\in R^{C\times H\times W}$，其中$\alpha$是一个可学习参数，网络自动学习，初始化为0。</p><p>如果不考虑其中的 softmax, 可以写成:</p><script type="math/tex; mode=display">E=\alpha D\times (C^T \times B)^T+A</script><h3 id="3-3-Channel-Attention-Module"><a href="#3-3-Channel-Attention-Module" class="headerlink" title="3.3 Channel Attention Module"></a>3.3 Channel Attention Module</h3><p>Position Attention Module 是把每个位置的通道作为其特征 $R^C$ ，Channel Attention Module 是把每个通道的特征图作为其特征 $R^N$。</p><p>与 Position Attention Module 不同的地方还有没有经过三个滤波器得到 $B,C,D$ ，而是直接使用A。</p><p>仍然是先把 A reshape 成 $A\in R^{C\times N}$，然后进行和上述类似的操作，可以令$B,C,D=A^T$下面阐述具体过程:</p><script type="math/tex; mode=display">\begin{aligned}\hat{X}&=A\cdot A^T (where, A\in R^{C\times N}, X\in R^{C\times C}) \\\hat{x}_{ji}&=A_i^T \cdot A_j (where, A_i \in R^N)\end{aligned}</script><p>结合后面的代码分析，从而得到$X\in R^{C\times C}$:</p><script type="math/tex; mode=display">x_{ji}=\frac{\exp(-A_i \cdot A_j)}{\sum_{i=1}^N \exp(-A_i \cdot A_j)}</script><p>同样可以理解成对 $\hat{X}$ 的每一行做一次softmax，可以理解成A的自相关性。结合后面的 channel attention 的可视化，不同通道代表的类别不同，所以这里应该是越不相似值越大。</p><p>然后类似地我们得到$E\in R^{C\times H \times W}$:</p><script type="math/tex; mode=display">E_j=\beta \sum_i^N(x_{ji}A_i)+A_j</script><p>如果不考虑其中的 softmax, 可以写成:</p><script type="math/tex; mode=display">E=\beta (A \times A^T)\times A+A</script><p>这里给我的感觉更多地是在加法，而不是 SE block 用的乘法。</p><p>其实看到这里我是表示很怀疑的，这种 attention 能有效果吗？后面的可视化证明了作者的思路是正确的。</p><p>其中$\beta$也是一个可学习参数，网络自动学习，初始化为0。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Implementation-Details"><a href="#4-1-Implementation-Details" class="headerlink" title="4.1 Implementation Details"></a>4.1 Implementation Details</h3><p>学习率: 多项式衰减</p><script type="math/tex; mode=display">(1-\frac{iter}{total iter})^{0.9}</script><p>下面专门做一个学习率衰减的情况。</p><h3 id="4-2-Results-on-Datasets"><a href="#4-2-Results-on-Datasets" class="headerlink" title="4.2 Results on Datasets"></a>4.2 Results on Datasets</h3><h4 id="4-2-1-Ablation-Study-for-Attention-Modules"><a href="#4-2-1-Ablation-Study-for-Attention-Modules" class="headerlink" title="4.2.1 Ablation Study for Attention Modules"></a>4.2.1 Ablation Study for Attention Modules</h4><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/ablation_on_Cityscapes.png" title="ablation study on Cityscapes"><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/visualization_on_Cityscapes.png" title="visualization results on Cityscapes"><p>从实验结果可以看出， the position module 和 the channel module 互为补充，两个合起来后的提升效果没有单个的提升效果明显。</p><h3 id="4-2-3-Visualization-of-Attention-Module"><a href="#4-2-3-Visualization-of-Attention-Module" class="headerlink" title="4.2.3 Visualization of Attention Module"></a>4.2.3 Visualization of Attention Module</h3><p>这一小节很有意思的。</p><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/visualization_of_attention_module.png" title="visualization of attention module"><p>对于 position attention，得到的 $E\in R^{(H\times W)\times (H\times W)}$ ，可以理解点与点之间的相似性，对每个图片，选两个点，记为 ( #1 and #2 )，并且展示这两个点的 position attention map. 第一张图 #1 标记的是建筑物， #2 标记的是车，第二张图分别标记的是交通标记和行人，第三行标记的是植物和行人。可以看出来，同一类事物哪怕离得远也可以标记出来，不同事物哪怕离得近也标记不出来。或者说， position attention 具有在全局的角度来标记同一类事物，哪怕离得远，哪怕事物很小，同时区分近距离的不同事物。</p><p>对于 channel attention, 从图片中可以看出来，主要是同一通道得到的是同一类别。</p><p>现在还不知道是怎么可视化的。</p><h3 id="4-2-4-Comparing-with-State-of-the-art"><a href="#4-2-4-Comparing-with-State-of-the-art" class="headerlink" title="4.2.4 Comparing with State-of-the-art"></a>4.2.4 Comparing with State-of-the-art</h3><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/results.png" title="results"><p>嗯，比其他方法都强。</p><p>作者一共在四个数据集上做了实验，说明是真的强。</p><h2 id="5-Learning-rate"><a href="#5-Learning-rate" class="headerlink" title="5. Learning rate"></a>5. Learning rate</h2><p>以前虽然一直在用一些学习率衰减方式，但是都不系统。</p><h3 id="5-1-fixed"><a href="#5-1-fixed" class="headerlink" title="5.1 fixed"></a>5.1 fixed</h3><script type="math/tex; mode=display">lr=base\_lr</script><h3 id="5-2-step"><a href="#5-2-step" class="headerlink" title="5.2 step"></a>5.2 step</h3><p>离散的学习率变化策略</p><script type="math/tex; mode=display">lr=base\_lr\cdot \gamma^{epoch//step\_size}</script><p>其中，向下取整，并且 $\gamma$ 和 step_size 都需要设置<br><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/step.png" title="step"><br>gamma一般取0.1， step_wise一般取40</p><h3 id="5-3-exp"><a href="#5-3-exp" class="headerlink" title="5.3 exp"></a>5.3 exp</h3><script type="math/tex; mode=display">lr=base\_lr\cdot \gamma^{epoch}</script><p>其中 $\gamma$ 需要设置<br><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/exp.jpg" title="exp"><br>gamma一般取0.99</p><h3 id="5-4-inv"><a href="#5-4-inv" class="headerlink" title="5.4 inv"></a>5.4 inv</h3><script type="math/tex; mode=display">lr=base\_lr\cdot (1+y\cdot epoch)^{-power}</script><p>其中$\gamma$和power都需要设置<br><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/inv.jpg" title="inv"><br>gamma控制下降速率，power控制曲线在饱和状态下学习率达到的最低值。可以理解成当epoch达到最大值的时候，学习率在不同的power下最低值不一样。</p><h3 id="5-5-multistep"><a href="#5-5-multistep" class="headerlink" title="5.5 multistep"></a>5.5 multistep</h3><p>多次step，只是学习率改变的迭代次数不均匀</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lr_policy: <span class="string">"multistep"</span></span><br><span class="line">gamma: <span class="number">0.5</span></span><br><span class="line">stepvalue: <span class="number">10000</span></span><br><span class="line">stepvalue: <span class="number">30000</span></span><br><span class="line">stepvalue: <span class="number">60000</span></span><br></pre></td></tr></table></figure><h3 id="5-6-poly"><a href="#5-6-poly" class="headerlink" title="5.6 poly"></a>5.6 poly</h3><script type="math/tex; mode=display">lr=base\_lr\cdot (1-epoch/max\_epoch)^{power}</script><p>其中，power需要设置，并且epoch为0时，lr是base_lr，当达到最大次数时，学习率变成0.</p><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/poly.jpg" title="poly"><h3 id="5-7-sigmoid"><a href="#5-7-sigmoid" class="headerlink" title="5.7 sigmoid"></a>5.7 sigmoid</h3><script type="math/tex; mode=display">lr=base\_lr\cdot \frac{1}{1+exp^{-\gamma \cdot (epoch-step\_size)}}</script><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/sigmoid.jpg" title="sigmoid"><p>其中step_size控制sigmoid为0.5的位置，gamma学习率的变化速率。</p><h3 id="5-8-warm-up"><a href="#5-8-warm-up" class="headerlink" title="5.8 warm up"></a>5.8 warm up</h3><p>在前10个epoch使用较小的lr，之后正常使用</p><h3 id="5-9-all"><a href="#5-9-all" class="headerlink" title="5.9 all"></a>5.9 all</h3><img src="/2019/05/05/Dual-Attention-Network-for-Scene-Segmentation/all.jpg" title="all"><p>其中，step和multi_step最好，其次是exp,ploy，最差的是 inv,sigmoid.</p><h3 id="5-10-code"><a href="#5-10-code" class="headerlink" title="5.10 code"></a>5.10 code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = list(range(<span class="number">1000</span>))</span><br><span class="line">base_lr=<span class="number">0.01</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_lr</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    step_wise=<span class="number">50</span></span><br><span class="line">    gamma=<span class="number">0.1</span></span><br><span class="line">    <span class="keyword">return</span> base_lr*gamma**(epoch//step_wise)</span><br><span class="line">y = [step_lr(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x,y)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exp_lr</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    gamma=<span class="number">0.999</span></span><br><span class="line">    <span class="keyword">return</span> base_lr*gamma**epoch</span><br><span class="line">y = [exp_lr(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x,y)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inv_lr</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    gamma=<span class="number">0.1</span></span><br><span class="line">    power=<span class="number">0.75</span></span><br><span class="line">    <span class="keyword">return</span> base_lr*(<span class="number">1</span>+gamma*epoch)**(-power)</span><br><span class="line">y = [inv_lr(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x,y)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_lr</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    step_wise1=<span class="number">200</span></span><br><span class="line">    step_wise2=<span class="number">300</span></span><br><span class="line">    step_wise3=<span class="number">400</span></span><br><span class="line">    gamma=<span class="number">0.5</span></span><br><span class="line">    power = [<span class="number">0</span>]*step_wise1 + [<span class="number">1</span>]*step_wise2+[<span class="number">2</span>]*step_wise3</span><br><span class="line">    <span class="keyword">if</span> epoch&lt;len(power):</span><br><span class="line">        <span class="keyword">return</span> base_lr*gamma**power[epoch]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> base_lr*gamma**<span class="number">3</span></span><br><span class="line">y = [multi_lr(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x,y)</span><br></pre></td></tr></table></figure><h2 id="6-code"><a href="#6-code" class="headerlink" title="6. code"></a>6. code</h2><p>这次的代码很有含金量，用到了多GPU。</p><p>一是涉及到的代码有点多，二是自己没有跑过分割的代码，不清楚具体的代码组织形式。所以下面从小到大一个个讲关键的地方。有些代码和作者的论文描述不是非常一致，但不影响总体。</p><h3 id="6-1-PAM-and-CAM"><a href="#6-1-PAM-and-CAM" class="headerlink" title="6.1 PAM and CAM"></a>6.1 PAM and CAM</h3><p>position attention and channel attention</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PAM_Module</span><span class="params">(Module)</span>:</span></span><br><span class="line">    <span class="string">""" Position attention module"""</span></span><br><span class="line">    <span class="comment">#Ref from SAGAN</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim)</span>:</span></span><br><span class="line">        super(PAM_Module, self).__init__()</span><br><span class="line">        self.chanel_in = in_dim</span><br><span class="line"></span><br><span class="line">        self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//<span class="number">8</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//<span class="number">8</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.gamma = Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.softmax = Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            inputs :</span></span><br><span class="line"><span class="string">                x : input feature maps( B X C X H X W)</span></span><br><span class="line"><span class="string">            returns :</span></span><br><span class="line"><span class="string">                out : attention value + input feature</span></span><br><span class="line"><span class="string">                attention: B X (HxW) X (HxW)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># C: 512, C//8: 64</span></span><br><span class="line">        m_batchsize, C, height, width = x.size()</span><br><span class="line">        <span class="comment"># x: B,C,H,W</span></span><br><span class="line">        proj_query = self.query_conv(x).view(m_batchsize, <span class="number">-1</span>, width*height).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># C': B,HxW,C//8</span></span><br><span class="line">        proj_key = self.key_conv(x).view(m_batchsize, <span class="number">-1</span>, width*height)</span><br><span class="line">        <span class="comment"># B: B,C//8,HxW</span></span><br><span class="line">        energy = torch.bmm(proj_query, proj_key)</span><br><span class="line">        <span class="comment"># \hat&#123;S&#125; = C'xB : B,HxW,HxW</span></span><br><span class="line">        attention = self.softmax(energy)</span><br><span class="line">        <span class="comment"># S: B,HxW,HxW</span></span><br><span class="line">        proj_value = self.value_conv(x).view(m_batchsize, <span class="number">-1</span>, width*height)</span><br><span class="line">        <span class="comment"># D: B,C,HxW</span></span><br><span class="line">        out = torch.bmm(proj_value, attention.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># \hat&#123;E&#125; = DxS': B,C,HxW</span></span><br><span class="line">        out = out.view(m_batchsize, C, height, width)</span><br><span class="line">        <span class="comment"># \hat&#123;E&#125; : B,C,H,W</span></span><br><span class="line">        out = self.gamma*out + x</span><br><span class="line">        <span class="comment"># E: B,C,H,W</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CAM_Module</span><span class="params">(Module)</span>:</span></span><br><span class="line">    <span class="string">""" Channel attention module"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim)</span>:</span></span><br><span class="line">        super(CAM_Module, self).__init__()</span><br><span class="line">        self.chanel_in = in_dim</span><br><span class="line">        self.gamma = Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line">        self.softmax  = Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            inputs :</span></span><br><span class="line"><span class="string">                x : input feature maps( B X C X H X W)</span></span><br><span class="line"><span class="string">            returns :</span></span><br><span class="line"><span class="string">                out : attention value + input feature</span></span><br><span class="line"><span class="string">                attention: B X C X C</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        m_batchsize, C, height, width = x.size()</span><br><span class="line">        <span class="comment"># x: B,C,H,W</span></span><br><span class="line">        proj_query = x.view(m_batchsize, C, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># A: B,C,HxW</span></span><br><span class="line">        proj_key = x.view(m_batchsize, C, <span class="number">-1</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># A': B,HxW,C</span></span><br><span class="line">        energy = torch.bmm(proj_query, proj_key)</span><br><span class="line">        <span class="comment"># \hat&#123;X&#125; = AxA': B,C,C</span></span><br><span class="line">        energy_new = torch.max(energy, <span class="number">-1</span>, keepdim=<span class="keyword">True</span>)[<span class="number">0</span>].expand_as(energy)-energy</span><br><span class="line">        <span class="comment"># note that, 作者在这里用了一次 max-v_i，而不是常见的v_i-max，按照github上的解释，</span></span><br><span class="line">        <span class="comment"># 作者选用前者而不是后者的原因是后者的效果不好，不知道该怎么反驳，</span></span><br><span class="line">        <span class="comment"># channel attention 主要衡量的是通道与通道之间的相似性，</span></span><br><span class="line">        <span class="comment"># 按照这个公式，结合channel的可视化，只能强行解释成，希望通道之间不相似，越不相似给的值越高，</span></span><br><span class="line">        attention = self.softmax(energy_new)</span><br><span class="line">        <span class="comment"># X: B,C,C</span></span><br><span class="line">        proj_value = x.view(m_batchsize, C, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># D: B,C,HxW</span></span><br><span class="line">        out = torch.bmm(attention, proj_value)</span><br><span class="line">        <span class="comment"># \hat&#123;E&#125; = XxD : B,C,HxW，这里也满足\hat&#123;E&#125;中的每个元素的系数之后为1</span></span><br><span class="line">        out = out.view(m_batchsize, C, height, width)</span><br><span class="line">        <span class="comment"># \hat&#123;E&#125;: B,C,H,W</span></span><br><span class="line">        out = self.gamma*out + x</span><br><span class="line">        <span class="comment"># E: B,C,H,W</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="6-2-DANetHead"><a href="#6-2-DANetHead" class="headerlink" title="6.2 DANetHead"></a>6.2 DANetHead</h3><p>从代码上看，过程大概是：</p><ol><li>是在进入 attention module 会进行一次通道缩小，2048-&gt;512，</li><li>position attention module: sa_conv, channel attention module: sc_conv</li><li>得到三种预测的类别：sa_conv+sc_conv-&gt;sasc_output, sa_conv-&gt;sa_output, sc_conv-&gt;sc_output</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DANetHead</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, norm_layer)</span>:</span></span><br><span class="line">        <span class="comment"># in_channels: 2048</span></span><br><span class="line">        <span class="comment"># out_channels: dataset.num_classes</span></span><br><span class="line">        super(DANetHead, self).__init__()</span><br><span class="line">        inter_channels = in_channels // <span class="number">4</span></span><br><span class="line">        self.conv5a = nn.Sequential(nn.Conv2d(in_channels, inter_channels, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                                   norm_layer(inter_channels),</span><br><span class="line">                                   nn.ReLU())</span><br><span class="line">        self.sa = PAM_Module(inter_channels)</span><br><span class="line">        self.conv51 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                                   norm_layer(inter_channels),</span><br><span class="line">                                   nn.ReLU())</span><br><span class="line">        self.conv6 = nn.Sequential(nn.Dropout2d(<span class="number">0.1</span>, <span class="keyword">False</span>), nn.Conv2d(<span class="number">512</span>, out_channels, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.conv5c = nn.Sequential(nn.Conv2d(in_channels, inter_channels, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                                   norm_layer(inter_channels),</span><br><span class="line">                                   nn.ReLU())</span><br><span class="line">        self.sc = CAM_Module(inter_channels)</span><br><span class="line">        self.conv52 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                                   norm_layer(inter_channels),</span><br><span class="line">                                   nn.ReLU())</span><br><span class="line">        self.conv7 = nn.Sequential(nn.Dropout2d(<span class="number">0.1</span>, <span class="keyword">False</span>), nn.Conv2d(<span class="number">512</span>, out_channels, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.conv8 = nn.Sequential(nn.Dropout2d(<span class="number">0.1</span>, <span class="keyword">False</span>), nn.Conv2d(<span class="number">512</span>, out_channels, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x: B,C,H,W: C 2048</span></span><br><span class="line">        feat1 = self.conv5a(x)</span><br><span class="line">        <span class="comment"># feat1: B,C//4,H,W</span></span><br><span class="line">        sa_feat = self.sa(feat1)</span><br><span class="line">        <span class="comment"># sa_feat: B,C//4,H,W</span></span><br><span class="line">        sa_conv = self.conv51(sa_feat)</span><br><span class="line">        <span class="comment"># sa_conv: B,C//4,H,W</span></span><br><span class="line">        sa_output = self.conv6(sa_conv)</span><br><span class="line">        <span class="comment"># sa_output: B,C_out,H,W</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># x: B,C,H,W: C 2048</span></span><br><span class="line">        feat2 = self.conv5c(x)</span><br><span class="line">        <span class="comment"># feat2: B,C//4,H,W</span></span><br><span class="line">        sc_feat = self.sc(feat2)</span><br><span class="line">        <span class="comment"># sc_feat: B,C//4,H,W</span></span><br><span class="line">        sc_conv = self.conv52(sc_feat)</span><br><span class="line">        <span class="comment"># sc_conv: B,C//4,H,W</span></span><br><span class="line">        sc_output = self.conv7(sc_conv)</span><br><span class="line">        <span class="comment"># sc_output: B,C_out,H,W</span></span><br><span class="line"></span><br><span class="line">        feat_sum = sa_conv+sc_conv</span><br><span class="line">        <span class="comment"># feat_sum: B,C//4,H,W</span></span><br><span class="line">        sasc_output = self.conv8(feat_sum)</span><br><span class="line">        <span class="comment"># sasc_output: B,C_out,H,W</span></span><br><span class="line"></span><br><span class="line">        output = [sasc_output]</span><br><span class="line">        output.append(sa_output)</span><br><span class="line">        output.append(sc_output)</span><br><span class="line">        <span class="comment"># output:[sasc_output, sa_output, sc_output]: 3,B,C_out,H,W</span></span><br><span class="line">        <span class="keyword">return</span> tuple(output)</span><br></pre></td></tr></table></figure><h3 id="6-3-BaseNet"><a href="#6-3-BaseNet" class="headerlink" title="6.3 BaseNet"></a>6.3 BaseNet</h3><p>以ResNet-50为例，相当于求得每一个layer的输出 [c1, c2, c3, c4]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nclass, backbone, aux, se_loss, dilated=True, norm_layer=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 base_size=<span class="number">576</span>, crop_size=<span class="number">608</span>, mean=[<span class="number">.485</span>, <span class="number">.456</span>, <span class="number">.406</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                 std=[<span class="number">.229</span>, <span class="number">.224</span>, <span class="number">.225</span>], root=<span class="string">'./pretrain_models'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 multi_grid=False, multi_dilation=None)</span>:</span></span><br><span class="line">        super(BaseNet, self).__init__()</span><br><span class="line">        self.nclass = nclass</span><br><span class="line">        self.aux = aux</span><br><span class="line">        self.se_loss = se_loss</span><br><span class="line">        self.mean = mean</span><br><span class="line">        self.std = std</span><br><span class="line">        self.base_size = base_size</span><br><span class="line">        self.crop_size = crop_size</span><br><span class="line">        <span class="comment"># copying modules from pretrained models</span></span><br><span class="line">        <span class="keyword">if</span> backbone == <span class="string">'resnet50'</span>:</span><br><span class="line">            self.pretrained = resnet.resnet50(pretrained=<span class="keyword">True</span>, dilated=dilated,</span><br><span class="line">                                              norm_layer=norm_layer, root=root,</span><br><span class="line">                                              multi_grid=multi_grid, multi_dilation=multi_dilation)</span><br><span class="line">        <span class="keyword">elif</span> backbone == <span class="string">'resnet101'</span>:</span><br><span class="line">            self.pretrained = resnet.resnet101(pretrained=<span class="keyword">True</span>, dilated=dilated,</span><br><span class="line">                                               norm_layer=norm_layer, root=root,</span><br><span class="line">                                               multi_grid=multi_grid,multi_dilation=multi_dilation)</span><br><span class="line">        <span class="keyword">elif</span> backbone == <span class="string">'resnet152'</span>:</span><br><span class="line">            self.pretrained = resnet.resnet152(pretrained=<span class="keyword">True</span>, dilated=dilated,</span><br><span class="line">                                               norm_layer=norm_layer, root=root,</span><br><span class="line">                                               multi_grid=multi_grid, multi_dilation=multi_dilation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'unknown backbone: &#123;&#125;'</span>.format(backbone))</span><br><span class="line">        <span class="comment"># bilinear upsample options</span></span><br><span class="line">        self._up_kwargs = up_kwargs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">base_forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pretrained.conv1(x)</span><br><span class="line">        x = self.pretrained.bn1(x)</span><br><span class="line">        x = self.pretrained.relu(x)</span><br><span class="line">        x = self.pretrained.maxpool(x)</span><br><span class="line">        c1 = self.pretrained.layer1(x)</span><br><span class="line">        c2 = self.pretrained.layer2(c1)</span><br><span class="line">        c3 = self.pretrained.layer3(c2)</span><br><span class="line">        c4 = self.pretrained.layer4(c3)</span><br><span class="line">        <span class="keyword">return</span> c1, c2, c3, c4</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, x, target=None)</span>:</span></span><br><span class="line">        pred = self.forward(x)</span><br><span class="line">        <span class="keyword">if</span> isinstance(pred, (tuple, list)):</span><br><span class="line">            pred = pred[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> target <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> pred</span><br><span class="line">        correct, labeled = batch_pix_accuracy(pred.data, target.data)</span><br><span class="line">        inter, union = batch_intersection_union(pred.data, target.data, self.nclass)</span><br><span class="line">        <span class="keyword">return</span> correct, labeled, inter, union</span><br></pre></td></tr></table></figure><h3 id="6-5-DANet"><a href="#6-5-DANet" class="headerlink" title="6.5 DANet"></a>6.5 DANet</h3><p>相当于求这三种的预测：sa_conv+sc_conv-&gt;sasc_output, sa_conv-&gt;sa_output, sc_conv-&gt;sc_output</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DANet</span><span class="params">(BaseNet)</span>:</span></span><br><span class="line">    <span class="string">r"""Fully Convolutional Networks for Semantic Segmentation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    nclass : int</span></span><br><span class="line"><span class="string">        Number of categories for the training dataset.</span></span><br><span class="line"><span class="string">    backbone : string</span></span><br><span class="line"><span class="string">        Pre-trained dilated backbone network type (default:'resnet50'; 'resnet50',</span></span><br><span class="line"><span class="string">        'resnet101' or 'resnet152').</span></span><br><span class="line"><span class="string">    norm_layer : object</span></span><br><span class="line"><span class="string">        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Reference:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks</span></span><br><span class="line"><span class="string">        for semantic segmentation." *CVPR*, 2015</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nclass, backbone, aux=False, se_loss=False, norm_layer=nn.BatchNorm2d, **kwargs)</span>:</span></span><br><span class="line">        super(DANet, self).__init__(nclass, backbone, aux, se_loss, norm_layer=norm_layer, **kwargs)</span><br><span class="line">        self.head = DANetHead(<span class="number">2048</span>, nclass, norm_layer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 具体的图片大小还是需要看图像分割的输入，这里以标准的224为例</span></span><br><span class="line">        <span class="comment"># x: 3,H,W &amp;&amp; 224, 224</span></span><br><span class="line">        imsize = x.size()[<span class="number">2</span>:]</span><br><span class="line">        _, _, c3, c4 = self.base_forward(x)</span><br><span class="line">        <span class="comment"># c3, c4: ResNet-50 的 layer3 和 layer 4 的输出</span></span><br><span class="line">        <span class="comment"># c3: 1024, H//16, W//16 &amp;&amp; 1024, 14, 14=224//16</span></span><br><span class="line">        <span class="comment"># c4: 2018, H//32, W//32 &amp;&amp; 7, 7=224//32</span></span><br><span class="line">        x = self.head(c4)</span><br><span class="line">        <span class="comment"># x: [sasc_output, sa_output, sc_output]: 3,B,dataset.num_classes,H//32, W//32 &amp;&amp; 7, 7=224//32</span></span><br><span class="line">        x = list(x)</span><br><span class="line">        x[<span class="number">0</span>] = upsample(x[<span class="number">0</span>], imsize, **self._up_kwargs)</span><br><span class="line">        x[<span class="number">1</span>] = upsample(x[<span class="number">1</span>], imsize, **self._up_kwargs)</span><br><span class="line">        x[<span class="number">2</span>] = upsample(x[<span class="number">2</span>], imsize, **self._up_kwargs)</span><br><span class="line">        <span class="comment"># 上采样</span></span><br><span class="line">        <span class="comment"># x: [sasc_output, sa_output, sc_output]: 3,B,dataset.num_classes,H,W &amp;&amp; 224, 224</span></span><br><span class="line">        outputs = [x[<span class="number">0</span>]]</span><br><span class="line">        outputs.append(x[<span class="number">1</span>])</span><br><span class="line">        outputs.append(x[<span class="number">2</span>])</span><br><span class="line">        <span class="comment"># x: [sasc_output, sa_output, sc_output]: 3,B,dataset.num_classes,H,W &amp;&amp; 224, 224</span></span><br><span class="line">        <span class="keyword">return</span> tuple(outputs)</span><br></pre></td></tr></table></figure><h3 id="6-6-SegmentationMultiLosses"><a href="#6-6-SegmentationMultiLosses" class="headerlink" title="6.6 SegmentationMultiLosses"></a>6.6 SegmentationMultiLosses</h3><p>希望 position+channel attetion, position attention, channel attention 三种预测都准确</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SegmentationMultiLosses</span><span class="params">(CrossEntropyLoss)</span>:</span></span><br><span class="line">    <span class="string">"""2D Cross Entropy Loss with Multi-L1oss"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nclass=<span class="number">-1</span>, weight=None,size_average=True, ignore_index=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        super(SegmentationMultiLosses, self).__init__(weight, size_average, ignore_index)</span><br><span class="line">        self.nclass = nclass</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, *inputs)</span>:</span></span><br><span class="line"></span><br><span class="line">        *preds, target = tuple(inputs)</span><br><span class="line">        pred1, pred2 ,pred3= tuple(preds)</span><br><span class="line">        <span class="comment"># sa_conv+sc_conv-&gt;sasc_output, sa_conv-&gt;sa_output, sc_conv-&gt;sc_output</span></span><br><span class="line">        loss1 = super(SegmentationMultiLosses, self).forward(pred1, target)</span><br><span class="line">        loss2 = super(SegmentationMultiLosses, self).forward(pred2, target)</span><br><span class="line">        loss3 = super(SegmentationMultiLosses, self).forward(pred3, target)</span><br><span class="line">        loss = loss1 + loss2 + loss3</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="6-7-其他"><a href="#6-7-其他" class="headerlink" title="6.7 其他"></a>6.7 其他</h3><p>其他的代码暂时就不看了，只是记录一个自己没有看到过的函数</p><p>Synchronized Cross-GPU Batch Normalization functions</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;这篇文章的重点在于 dual attention 的作用，并且attention的使用和之前看到的 SE block 还不太一样。dual attention 主要解决了全局依赖性，即其他位置的物体对当前位置的的物体的特征的影响。重点不是场景分割，自己也不是很懂分割的代码和实现，暂时对分割不做过多研究。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/1809.02983.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CPVR2019: Dual Attention Network for Scene Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/junfu1115/DANet/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;team: 中科院自动化所图像与视频分析团队（IVA），隶属于模式识别国家重点实验室，在 ICCV 2017 COCO-Places 场景解析竞赛、京东 AI 时尚挑战赛和阿里巴巴大规模图像搜索大赛踢馆赛等多次拔得头筹。嗯，一句话，很牛逼。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/P_LarT/article/details/89043620&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;解读&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="segmentation" scheme="http://yoursite.com/categories/segmentation/"/>
    
    
      <category term="segmentation" scheme="http://yoursite.com/tags/segmentation/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>SE block</title>
    <link href="http://yoursite.com/2019/05/04/SE-block/"/>
    <id>http://yoursite.com/2019/05/04/SE-block/</id>
    <published>2019-05-04T08:15:35.000Z</published>
    <updated>2019-05-04T12:42:48.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>这篇文章主要是通过在神经网络中加入 SE-block 来加强通道之间的关系，提高性能，理论上讲可以加入任意网络任意任务，并且这篇文章获得了 ImageNet2017 的冠军。很牛逼。</p><ul><li>paper: <a href="https://arxiv.org/pdf/1709.01507.pdf" target="_blank" rel="noopener">ECCV2018_Squeeze-and-Excitation Networks</a></li><li>code: <a href="https://github.com/hujie-frank/SENet" target="_blank" rel="noopener">Caffe, TensorFlow, MatConvNet, MXNet, Pytorch, Chainer</a>, <a href="https://github.com/miraclewkf/SENet-PyTorch" target="_blank" rel="noopener">pytorch</a></li></ul><p>这篇文章清晰易懂，讲得很细(没用的话也比较多)，很work，一起拜读一下。<br><a id="more"></a></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul><li>SE block: Squeeze and Excitation block</li></ul><img src="/2019/05/04/SE-block/squeeze_and_excitation_block.png" title="Squeeze and Excitation block"><p>符号表达：$F_{tr}:X\to U, X\in R^{H’×W’×C’}, U\in R^{H×W×C}$</p><p>SE block 在底层时更偏向于提取任务之间的共享特征，在高层时更偏向于提取任务相关的特征。</p><h2 id="3-Squeeze-and-Excitation-Blocks"><a href="#3-Squeeze-and-Excitation-Blocks" class="headerlink" title="3. Squeeze and Excitation Blocks"></a>3. Squeeze and Excitation Blocks</h2><h3 id="3-1-Squeeze-Global-Information-Embedding"><a href="#3-1-Squeeze-Global-Information-Embedding" class="headerlink" title="3.1 Squeeze: Global Information Embedding"></a>3.1 Squeeze: Global Information Embedding</h3><script type="math/tex; mode=display">z_c=F_{sq}(u_c)=\frac{1}{H×W}\sum_{i=1}^H \sum_{j=1}^W u_c(i,j)</script><p>其中，$z\in R^C$</p><h3 id="3-2-Excitation-Adaptive-Recalibration"><a href="#3-2-Excitation-Adaptive-Recalibration" class="headerlink" title="3.2 Excitation: Adaptive Recalibration"></a>3.2 Excitation: Adaptive Recalibration</h3><script type="math/tex; mode=display">s=F_{ex}(z,W)=\sigma(g(z,W))=\sigma(W_2\delta (W_1z))</script><p>其中，$\delta$表示ReLU函数，$W_1\in R^{\frac{C}{r}×C}$ 并且 $W_2\in R^{C×\frac{C}{r}}$，也就是两个FC层。</p><script type="math/tex; mode=display">\tilde{x}_c=F_{scale}(u_c, s_c)=s_c\cdot u_c</script><h3 id="3-3-Instantiations"><a href="#3-3-Instantiations" class="headerlink" title="3.3 Instantiations"></a>3.3 Instantiations</h3><img src="/2019/05/04/SE-block/Instantiations.png" title="instantiation"><img src="/2019/05/04/SE-block/concrete_examples.png" title="concrete_examples of ResNet"><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><p>主要是从分类等场景出发，说明了 SE block 在ResNet,Inception等各种网络和ImageNet, cifar-100等数据集上表现都好。</p><h2 id="5-Ablation-Study"><a href="#5-Ablation-Study" class="headerlink" title="5. Ablation Study"></a>5. Ablation Study</h2><h3 id="5-1-Reduction-ratio"><a href="#5-1-Reduction-ratio" class="headerlink" title="5.1 Reduction ratio"></a>5.1 Reduction ratio</h3><p>Reduction ratio r in Excitation</p><img src="/2019/05/04/SE-block/different_reduction_ratio.png" title="different reduction ratio"><p>作者设置为r=16.</p><h3 id="5-2-Squeeze-Operator"><a href="#5-2-Squeeze-Operator" class="headerlink" title="5.2 Squeeze Operator"></a>5.2 Squeeze Operator</h3><p>作者只比较了max pooling 和avg pooling.这两种方法差不多。</p><h3 id="5-3-Excitation-Operator"><a href="#5-3-Excitation-Operator" class="headerlink" title="5.3 Excitation Operator"></a>5.3 Excitation Operator</h3><img src="/2019/05/04/SE-block/different_non-linearities.png" title="different non-linearities"><p>作者比较了 ReLU, Tanh, Sigmoid三种函数，实验证明 Sigmoid 函数更好一些，这里指的是第二个激活函数。</p><h3 id="5-4-Different-stages"><a href="#5-4-Different-stages" class="headerlink" title="5.4 Different stages"></a>5.4 Different stages</h3><img src="/2019/05/04/SE-block/different_stages.png" title="different stage"><h3 id="5-5-Integration-strategy"><a href="#5-5-Integration-strategy" class="headerlink" title="5.5 Integration strategy"></a>5.5 Integration strategy</h3><img src="/2019/05/04/SE-block/different_integration_designs.png" title="different integration designs"><img src="/2019/05/04/SE-block/effect_integration_designs.png" title="effect integration designs"><h2 id="6-code"><a href="#6-code" class="headerlink" title="6. code"></a>6. code</h2><p>代码还是很简单的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SE ResNet50</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SELayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channel, reduction=<span class="number">16</span>)</span>:</span></span><br><span class="line">        super(SELayer, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(channel, channel // reduction, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Linear(channel // reduction, channel, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y = self.avg_pool(x).view(b, c)</span><br><span class="line">        y = self.fc(y).view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x * y.expand_as(x)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SEBasicBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, planes, stride=<span class="number">1</span>, downsample=None, reduction=<span class="number">16</span>)</span>:</span></span><br><span class="line">        super(SEBasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes, <span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.se = SELayer(planes, reduction)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        residual = x</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.se(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;这篇文章主要是通过在神经网络中加入 SE-block 来加强通道之间的关系，提高性能，理论上讲可以加入任意网络任意任务，并且这篇文章获得了 ImageNet2017 的冠军。很牛逼。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/1709.01507.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ECCV2018_Squeeze-and-Excitation Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/hujie-frank/SENet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Caffe, TensorFlow, MatConvNet, MXNet, Pytorch, Chainer&lt;/a&gt;, &lt;a href=&quot;https://github.com/miraclewkf/SENet-PyTorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这篇文章清晰易懂，讲得很细(没用的话也比较多)，很work，一起拜读一下。&lt;br&gt;
    
    </summary>
    
      <category term="attention" scheme="http://yoursite.com/categories/attention/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="SE" scheme="http://yoursite.com/tags/SE/"/>
    
  </entry>
  
  <entry>
    <title>Reality Oriented Adaptation</title>
    <link href="http://yoursite.com/2019/05/03/Reality-Oriented-Adaptation/"/>
    <id>http://yoursite.com/2019/05/03/Reality-Oriented-Adaptation/</id>
    <published>2019-05-03T07:52:58.000Z</published>
    <updated>2019-05-04T08:18:41.523Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><h3 id="0-0-前言"><a href="#0-0-前言" class="headerlink" title="0.0 前言"></a>0.0 前言</h3><p>这篇文章主要解决的问题是：当语义分割网络在合成数据集(有标签)上训练好，在真实数据集(没有标签)上性能下降比较多。作者认为有两个原因：对合成数据过拟合，合成数据与真实数据存在分布差异。(好吧，我认为这两是一个原因)。作者提出target guided distillation 和 spatial-aware adaptation 来改进性能，效果还挺好的。我主要看target guided distillaiton。</p><ul><li>paper: <a href="https://arxiv.org/pdf/1711.11556.pdf" target="_blank" rel="noopener">CVPR2018_ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes</a></li><li>code: 无</li></ul><h3 id="0-1-HydraPlus-Net"><a href="#0-1-HydraPlus-Net" class="headerlink" title="0.1 HydraPlus-Net"></a>0.1 HydraPlus-Net</h3><p>顺便记录下刚看的论文 HydraPlus-Net，因为这篇论文是caffe代码且对我的帮助不大，所以只是简单地记录下其中的创新点。</p><a id="more"></a><ul><li>paper: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_HydraPlus-Net_Attentive_Deep_ICCV_2017_paper.pdf" target="_blank" rel="noopener">ICCV2017: HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</a></li><li>code: <a href="https://github.com/xh-liu/HydraPlus-Net" target="_blank" rel="noopener">caffe</a></li></ul><img src="/2019/05/03/Reality-Oriented-Adaptation/HydraPlus.png" title="HydraPlus network"><p>其主要创新点在于：</p><ul><li>attention不仅可以用于本block，也可以用于其他block</li><li>一个block可以生成多个attention map</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>针对合成数据集的模型在真实数据集上性能很差，作者提出原因可能是：过拟合和分布不一致，因此提出模型：ROAD-Net。下面对作者提出的名词做出解释</p><ul><li>Reality Oriented Adaptation Networks(ROAD-Net)</li><li>real style orientation &amp; target guided distillation: 为了避免过拟合合成数据集，使用 distillation 使 model 的输出和预训练的模型的输出一致，注意，这里 distillation 针对的是真实数据集，而不是合成数据集，这种方法称为 target guided distillation.</li><li>real distribution orientation &amp; spatial-aware adaptation &amp; domain classifier: 因为合成数据集和真实数据集的分布不一致，提出 <a href="https://arxiv.org/pdf/1409.7495v2.pdf" target="_blank" rel="noopener">DANN</a> 也就是 domain classifier，其实类似GAN的D，来使得合成数据集和真实数据集的特征分布一致。主要过程就是将合成数据集和真实数据集的特征图分割成几个区域，然后判断这几个区域是不是同一个domain。</li></ul><img src="/2019/05/03/Reality-Oriented-Adaptation/ROAD_Net.png" title="ROAD_Net"><h2 id="2-Reality-Oriented-Adaptation-Networks"><a href="#2-Reality-Oriented-Adaptation-Networks" class="headerlink" title="2. Reality Oriented Adaptation Networks"></a>2. Reality Oriented Adaptation Networks</h2><h3 id="2-1-Target-Guided-Distillation"><a href="#2-1-Target-Guided-Distillation" class="headerlink" title="2.1 Target Guided Distillation"></a>2.1 Target Guided Distillation</h3><img src="/2019/05/03/Reality-Oriented-Adaptation/target_guided_distillation.png" title="illustration of target guided distillaion"><p>其中，pretrained model 在训练的时候不更新.</p><p><strong>distillation loss</strong>：</p><script type="math/tex; mode=display">L_{dist}=\frac{1}{N}\sum_{i,j}\parallel x_{i,j}-z_{i,j} \parallel _2</script><p>其中，$x_{i,j}, z_{i,j}$分别表示 segmentation model 和 pretrained model 得到的 feature map 在位置 $(i,j)$ 的值，二范是简单的欧式距离。这个损失简单粗暴，后续的实验也证明了这种方法的确要更好一些。</p><p>我觉得这个是一个思路，这种 distillation 可以在一定程度上使得 segmentation model 学习到真实数据集的特征分布。</p><p>当然，除了 distillation loss 用来防止过拟合，也有其他方法用来防止过拟合，比如冻结一些层然后循环，或者用 source (合成数据集) 用来进行 distillation，是 <a href="https://arxiv.org/pdf/1606.09282.pdf" target="_blank" rel="noopener">learning without forgetting</a>，这篇文章也很有用，等下简单地讲下这篇文章。</p><p>其实这里大有文章可做。</p><h3 id="2-2-Spatial-Aware-Adaptation"><a href="#2-2-Spatial-Aware-Adaptation" class="headerlink" title="2.2 Spatial-Aware Adaptation"></a>2.2 Spatial-Aware Adaptation</h3><img src="/2019/05/03/Reality-Oriented-Adaptation/spatial_aware_adaptation.png" title="spatial-aware adaptation"><p>假设把特征图分割成了$m=1,…,M$块，每一块的区域坐标集合表示为$(u,v)\in R_m$，记点(u,v)对应的特征图为$x_{u,v}$，记区域对应的特征图为$X_m^s={x_{u,v}^s | (u,v)\in R_m}$和$X_m^t={x_{u,v}^t | (u,v)\in R_m}$，定义其loss为:</p><script type="math/tex; mode=display">L_{spt}=\sum_{m=1}^M L_{da}(X_m^s, X_m^t)</script><p>其中，$L_{da}$表示 domain adaptation loss，其实就是domain classifier loss,具体表示如下。</p><script type="math/tex; mode=display">L_H(X^s, X^t)=\frac{1}{|X|}\sum_{x\in X} l(h(x),d)</script><p>其中$h:x\to {0,1}$，采用的<a href="https://arxiv.org/pdf/1409.7495v2.pdf" target="_blank" rel="noopener">DANN</a>模型，应该是类似GAN中的D，$d\in {0,1}$.</p><p>这个domain classifer的目的是为了使生成的source和target生成的特征尽量相似。</p><h3 id="2-3-Network-Overview"><a href="#2-3-Network-Overview" class="headerlink" title="2.3 Network Overview"></a>2.3 Network Overview</h3><script type="math/tex; mode=display">L_{ROAD}=L_{seg}+\lambda_1 L_{dist}+\lambda_2 L_{spt}</script><h2 id="3-Experimental-Results"><a href="#3-Experimental-Results" class="headerlink" title="3. Experimental Results"></a>3. Experimental Results</h2><h3 id="3-1-Experimental-Results"><a href="#3-1-Experimental-Results" class="headerlink" title="3.1 Experimental Results"></a>3.1 Experimental Results</h3><ul><li>dst: target guided distillation</li><li>spt: spatial-aware adaptation</li></ul><img src="/2019/05/03/Reality-Oriented-Adaptation/results.png" title="results"><p>通过结果可以看出，这两个创新点是有用的。</p><h3 id="3-2-Analysis-on-Real-Style-Orientation"><a href="#3-2-Analysis-on-Real-Style-Orientation" class="headerlink" title="3.2 Analysis on Real Style Orientation"></a>3.2 Analysis on Real Style Orientation</h3><img src="/2019/05/03/Reality-Oriented-Adaptation/td.png" title="target domain distillation"><h3 id="3-3-Analysis-on-Real-Distribution-Orientation"><a href="#3-3-Analysis-on-Real-Distribution-Orientation" class="headerlink" title="3.3 Analysis on Real Distribution Orientation"></a>3.3 Analysis on Real Distribution Orientation</h3><img src="/2019/05/03/Reality-Oriented-Adaptation/spt.png" title="spatial-aware adaptation"><h2 id="4-Others"><a href="#4-Others" class="headerlink" title="4. Others"></a>4. Others</h2><p>这里主要简单介绍下论文中提到的几篇参考文献的主要内容，并没有细读这几篇参考文献。</p><h3 id="4-1-Domain-Adaptation"><a href="#4-1-Domain-Adaptation" class="headerlink" title="4.1 Domain Adaptation"></a>4.1 Domain Adaptation</h3><p><a href="https://arxiv.org/pdf/1409.7495v2.pdf" target="_blank" rel="noopener">Unsupervised Domain Adaptation by Backpropagation</a></p><img src="/2019/05/03/Reality-Oriented-Adaptation/domain_classifier.png" title="domain classifier"><p><a href="https://blog.csdn.net/MataFela/article/details/77827217" target="_blank" rel="noopener">Domain Adaptation</a></p><h3 id="4-2-Distillation"><a href="#4-2-Distillation" class="headerlink" title="4.2 Distillation"></a>4.2 Distillation</h3><script type="math/tex; mode=display">q_i=\frac{exp(z_i/T)}{\sum_j exp(z_j/T)}</script><script type="math/tex; mode=display">L(p,q)=\sum_i p_i * (-\log q_i)</script><p><a href="https://github.com/dkozlov/awesome-knowledge-distillation" target="_blank" rel="noopener">Awesome Knowledge Distillation</a>中有相关的论文和一部分实现。</p><p><a href="https://arxiv.org/pdf/1606.09282.pdf" target="_blank" rel="noopener">Learning without Forgetting</a></p><p><a href="https://www.jianshu.com/p/e73851f32c9f" target="_blank" rel="noopener">模型压缩总览</a></p><p><a href="https://blog.csdn.net/nature553863/article/details/80568658" target="_blank" rel="noopener">知识蒸馏</a></p><p><a href="https://blog.csdn.net/qzrdypbuqk/article/details/81482598" target="_blank" rel="noopener">Knowledge Distillation</a></p><p>终于找到一个 KD (knowledge-distillation) loss 的<a href="https://github.com/peterliht/knowledge-distillation-pytorch" target="_blank" rel="noopener">代码</a>. <strong>这个我看懂了</strong>。</p><script type="math/tex; mode=display">D_{KL}(P||Q)=\sum P(i)\frac{P(i)}{Q(i)}</script><script type="math/tex; mode=display">H(p,q)=E_p[-\log q]=H(p)+D_{KL}(P||Q)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准的KD，利用的是交叉熵求KD</span></span><br><span class="line"><span class="comment"># 但速度很慢</span></span><br><span class="line">x, target = x.to(device), target.to(device)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    out = teacher(x)</span><br><span class="line">    soft_target = F.softmax(out/T, dim=<span class="number">1</span>)</span><br><span class="line">hard_target = target</span><br><span class="line">out = student(x)  <span class="comment">## this is the input to softmax</span></span><br><span class="line">logp = F.log_softmax(out/T, dim=<span class="number">1</span>)</span><br><span class="line">loss_soft_target = -torch.mean(torch.sum(soft_target * logp, dim=<span class="number">1</span>))</span><br><span class="line">loss_hard_target = nn.CrossEntropyLoss()(out, hard_target)</span><br><span class="line">loss = loss_soft_target * T * T + alpha * loss_hard_target</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">L_{KD}=\alpha T^2 KLdiv(Q_S^T,Q_T^T)+(1-\alpha)CrossEntrop(Q_s,y_{true})</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用KL散度求KD</span></span><br><span class="line"><span class="comment"># 理论上和实际上KL散度等同于交叉熵，速度很快，结果相同</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn_kd</span><span class="params">(outputs, labels, teacher_outputs, params)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the knowledge-distillation (KD) loss given outputs, labels.</span></span><br><span class="line"><span class="string">    "Hyperparameters": temperature and alpha</span></span><br><span class="line"><span class="string">    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher</span></span><br><span class="line"><span class="string">    and student expects the input tensor to be log probabilities! See Issue #2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    alpha = params.alpha</span><br><span class="line">    T = params.temperature</span><br><span class="line">    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=<span class="number">1</span>),</span><br><span class="line">                             F.softmax(teacher_outputs/T, dim=<span class="number">1</span>)) * (alpha * T * T) + \</span><br><span class="line">              F.cross_entropy(outputs, labels) * (<span class="number">1.</span> - alpha)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> KD_loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这段代码证明了DL散度和交叉熵的反向传播是一样的</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># sample number</span></span><br><span class="line">N = <span class="number">10</span></span><br><span class="line"><span class="comment"># category number</span></span><br><span class="line">C = <span class="number">5</span></span><br><span class="line"><span class="comment"># softmax output of teacher</span></span><br><span class="line">p = torch.softmax(torch.rand(N, C), dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># logit output of student</span></span><br><span class="line">s = torch.rand(N, C, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># softmax output of student, T = 1</span></span><br><span class="line">q = torch.softmax(s, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># KL Diverse</span></span><br><span class="line"><span class="comment"># this is the implementation of the author's</span></span><br><span class="line"><span class="comment"># torch will do element mean because it is the default option</span></span><br><span class="line"><span class="comment"># kl_loss = nn.KLDivLoss()(torch.log(q), p)</span></span><br><span class="line"><span class="comment"># I think this should be the right solution</span></span><br><span class="line">kl_loss = (nn.KLDivLoss(reduction=<span class="string">'none'</span>)(torch.log(q), p)).sum(dim=<span class="number">1</span>).mean()</span><br><span class="line">kl_loss.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'grad using KL DivLoss'</span></span><br><span class="line"><span class="keyword">print</span> s.grad</span><br><span class="line"><span class="comment"># clear the grad</span></span><br><span class="line">s.grad.zero_()</span><br><span class="line"><span class="comment"># bug2: should not do element wise mean operation</span></span><br><span class="line"><span class="comment"># ce_loss = torch.mean(-torch.log(q) * p)</span></span><br><span class="line">ce_loss = torch.mean(torch.sum(-torch.log(q) * p, dim=<span class="number">1</span>))</span><br><span class="line">ce_loss.backward()</span><br><span class="line"><span class="keyword">print</span> <span class="string">'grad using ce loss'</span></span><br><span class="line"><span class="keyword">print</span> s.grad</span><br><span class="line"><span class="comment"># the real gradient of s should be `(q - p) / batch_size`</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'real grad, should be (q-p) / batch_size'</span></span><br><span class="line"><span class="keyword">print</span> (q - p) / N</span><br></pre></td></tr></table></figure><p>或者也可以参考: <a href="https://github.com/PolarisShi/distillation/blob/master/student.py" target="_blank" rel="noopener">code</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;h3 id=&quot;0-0-前言&quot;&gt;&lt;a href=&quot;#0-0-前言&quot; class=&quot;headerlink&quot; title=&quot;0.0 前言&quot;&gt;&lt;/a&gt;0.0 前言&lt;/h3&gt;&lt;p&gt;这篇文章主要解决的问题是：当语义分割网络在合成数据集(有标签)上训练好，在真实数据集(没有标签)上性能下降比较多。作者认为有两个原因：对合成数据过拟合，合成数据与真实数据存在分布差异。(好吧，我认为这两是一个原因)。作者提出target guided distillation 和 spatial-aware adaptation 来改进性能，效果还挺好的。我主要看target guided distillaiton。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/1711.11556.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2018_ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: 无&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;0-1-HydraPlus-Net&quot;&gt;&lt;a href=&quot;#0-1-HydraPlus-Net&quot; class=&quot;headerlink&quot; title=&quot;0.1 HydraPlus-Net&quot;&gt;&lt;/a&gt;0.1 HydraPlus-Net&lt;/h3&gt;&lt;p&gt;顺便记录下刚看的论文 HydraPlus-Net，因为这篇论文是caffe代码且对我的帮助不大，所以只是简单地记录下其中的创新点。&lt;/p&gt;
    
    </summary>
    
      <category term="semantice segmentation" scheme="http://yoursite.com/categories/semantice-segmentation/"/>
    
    
      <category term="semantice segmentation" scheme="http://yoursite.com/tags/semantice-segmentation/"/>
    
      <category term="domain guided distillation" scheme="http://yoursite.com/tags/domain-guided-distillation/"/>
    
  </entry>
  
  <entry>
    <title>Domain Guided Dropout</title>
    <link href="http://yoursite.com/2019/04/30/Domain-Guided-Dropout/"/>
    <id>http://yoursite.com/2019/04/30/Domain-Guided-Dropout/</id>
    <published>2019-04-30T07:38:36.000Z</published>
    <updated>2019-05-02T12:05:58.980Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0.前言"></a>0.前言</h2><p>这篇文章的主要任务是，给定多个有标签行人重识别数据集，如何训练一个模型，在各个数据集上表现都好，并且这个模型能“隐式识别出”（Domain Guided Dropout）是哪个数据集。</p><ul><li>paper: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Xiao_Learning_Deep_Feature_CVPR_2016_paper.pdf" target="_blank" rel="noopener">CVPR2016 Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification</a></li><li>code: <a href="https://github.com/Cysu/dgd_person_reid" target="_blank" rel="noopener">caffe</a></li></ul><a id="more"></a><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>对于单个数据集，模型已经能达到很好的效果，对于多个数据集，训练一个模型，怎么才能达到也是很好的效果。</p><p>思路分为两步：</p><ul><li>第一步，将多个数据集的训练集融合到一起，用单分类器训练一个模型baseline</li><li>第二步，用Domain Guided Dropout(DGD)替换标准的dropout，再训练几个epoch，相当于用DGD对模型有针对性的微调。</li></ul><p>结论：fine-tune+DGD的组合作用更大一些。</p><p>DGD有两种方案：deterministic DGD 和 stochastic DGD.</p><p>等会儿看看能不能从caffe代码中找到一些代码，毕竟看不懂caffe代码很伤啊。</p><p>实验结果证明了：1. 多个数据集混合训练得到的baseline比单数据集的模型效果要好；2. deterministic DGD 对baseline有提升，而且stochastic DGD 提升效果更好。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>现在对于多个数据集训练一个模型的方法集中在如何学习一个共同的域不变的特征表示空间，但是作者这篇论文提出的模型允许模型学习到与域相关的components和域无关的特征表示空间，我觉得这个思路要更好一些，因为测试是在各个数据集上分别进行测试的，那么如果能学到一些与域相关的components，将特征共享空间与component组合之后得到域相关空间。</p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><img src="/2019/04/30/Domain-Guided-Dropout/pipeline.png" title="overview of pipeline"><p>针对这张图，我是这么理解的，假设模型在训练时特征提取了2048维特征，那么这2048维特征可能前1500维特征是和 domain A 有关，而后1500维特征和 domain B 有关，这里的有关可以暂时理解成对后续的分类结果有比较大的影响，举个极端的例子，在 domain A 中，主要根据第100维特征分类，那么第100维特征的存在对分类结果有很大的影响，而其他维的特征对 domain A 中的数据集分类无关，那么模型只需要每次都把其他维的特征直接变成0即可，只是因为其他域的存在，使得模型不能把其他维的特征每次都变成0，从而影响了 domain A 的特征提取。</p><p>后面作者也专门做了研究特征的影响。</p><h3 id="3-1-公式定义"><a href="#3-1-公式定义" class="headerlink" title="3.1 公式定义"></a>3.1 公式定义</h3><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:center">D</td><td style="text-align:left">D个数据集</td></tr><tr><td style="text-align:center">$\lbrace (x_i^j, y_i^j)_{j=1}^{N_i}\rbrace_{i=1}^D$</td><td style="text-align:left">数据集的表示，第i个数据集第j个图片</td></tr><tr><td style="text-align:center">$g(\cdot)$</td><td style="text-align:left">generic feature extractor</td></tr></tbody></table></div><h3 id="3-2-baseline"><a href="#3-2-baseline" class="headerlink" title="3.2 baseline"></a>3.2 baseline</h3><p>baseline 值得注意的是作者没有采用多个分类器的方法，而是用一个分类器，把所有数据集混在一起，重新排id，来训练。</p><p>多个分类器$f_1, f_2, …, f_D$</p><script type="math/tex; mode=display">\sum_{i=1}^D \sum_{j=1}^{N_i} L(f_i(g(x_i^j)), y_i^j)</script><p>单分类器$f$</p><script type="math/tex; mode=display">\sum_{i=1}^D \sum_{j=1}^{N_i} L(f(g(x_i^j)), y_i^j)</script><p>按照作者的说法，这种单分类器能同时学习到domain biases and person id，后续有实验分析证明这个理论。</p><p><strong>网络架构</strong>：</p><img src="/2019/04/30/Domain-Guided-Dropout/structure.png" title="structure of CNN"><p>其中M表示行人类别。其中 fc7 包括：Linear, ReLU, Dropout, 再结合后续的DGD，不难猜测测试特征就是fc7的输出，也就是g(x)，这里的dropout灵活处理，因为DGD就是取代的dropout.</p><h3 id="3-3-Domain-Guided-Dropout"><a href="#3-3-Domain-Guided-Dropout" class="headerlink" title="3.3 Domain Guided Dropout"></a>3.3 Domain Guided Dropout</h3><p>定义：$g(x)\in R^d$，第i个元素的影响因子定义为:</p><script type="math/tex; mode=display">s_i=L(g(x)_{\i})-L(g(x))</script><p>其中$g(x)_{\i}$表示将i个元素变成0.</p><p>对于每个域，遍历所有图片得到第i个元素的影响因子:</p><script type="math/tex; mode=display">\bar{s}_i=E_{x\in D}[s_i]</script><img src="/2019/04/30/Domain-Guided-Dropout/impact_scores.png" title="impact scores"><p>这张图片可以清晰地表明对于不同的 domain，第i个特征的影响程度不同，换句话说，有些特征对 domain A 影响大，对 domain B 影响小，如果能自动学习到哪些特征影响作用大，那么就可以有针对性地训练这些特征并减少其他特征对测试的影响。作者的思路很清晰。</p><p>作者实际是用的二阶泰勒展开逼近的$s_i$:</p><script type="math/tex; mode=display">s_i \approx -\frac{\partial L}{\partial g(x)_i}g(x)_i+\frac{1}{2}\frac{\partial^2 L}{\partial g(x)_i^2}g(x)_i^2</script><p>实验证明，这种逼近的精度是可以满足需求的：</p><img src="/2019/04/30/Domain-Guided-Dropout/approximated.png" title="true and approximated"><p>定义：mask m，根据上一步得到的s得到mask m，有两种方案：</p><p>方案一：deterministic</p><script type="math/tex; mode=display">m_i = \begin{cases}1 &\text{if} s_i>0 \\0 &\text{if} s_i\eqslantless 0\end{cases}</script><p>方法二：stochastic</p><script type="math/tex; mode=display">p(m_i=1)=\frac{1}{1+e^{-s_i/T}}</script><p>然后用mask m代替fc7的dropout，再训练10个epoch.在测试的时候，特征也是需要经过mask m的。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><ul><li>JSTL: jointly with a single-task learning objective</li><li>DGD: Domain Guided Dropout</li><li>JSTL+DGD: improve JSTL with deterministic Domain Guided Dropout: work on all the domains simultaneously.</li><li>FT-JSTL+DGD: fine-tune the CNN separately on each domain with the stochastic Domain Guided Dropout</li><li>FT-JSTL: fine-tune the CNN separately on each domain with standard dropout</li></ul><img src="/2019/04/30/Domain-Guided-Dropout/different_methods.png" title="different methods"><p>从上图中可以看出，联合训练对大数据集的作用不明显，对小数据集的作用比较明显，从随机初始化开始训练的话，deterministic DGD的作用几乎没有，fine-tune的作用是有的，fine-tune和stochastic DGD 共同起的作用更大一些，也就是说fine-tune本身的作用最大，如果在fine-tune的过程中加入DGD，发挥的作用会更好一些，</p><img src="/2019/04/30/Domain-Guided-Dropout/different_Dropout.png" title="different Dropout"><p>其中a图表示对所有的域同时训练，b图表示分别对域微调得到不同的模型。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0.前言&quot;&gt;&lt;/a&gt;0.前言&lt;/h2&gt;&lt;p&gt;这篇文章的主要任务是，给定多个有标签行人重识别数据集，如何训练一个模型，在各个数据集上表现都好，并且这个模型能“隐式识别出”（Domain Guided Dropout）是哪个数据集。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Xiao_Learning_Deep_Feature_CVPR_2016_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR2016 Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/Cysu/dgd_person_reid&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;caffe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="person re-id" scheme="http://yoursite.com/categories/person-re-id/"/>
    
    
      <category term="person re-id" scheme="http://yoursite.com/tags/person-re-id/"/>
    
      <category term="multi-domain" scheme="http://yoursite.com/tags/multi-domain/"/>
    
  </entry>
  
  <entry>
    <title>One_Example_reID</title>
    <link href="http://yoursite.com/2019/03/27/One-Example-reID/"/>
    <id>http://yoursite.com/2019/03/27/One-Example-reID/</id>
    <published>2019-03-27T07:25:37.000Z</published>
    <updated>2019-03-29T10:33:27.247Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这篇文章主要的任务也是为了解决re-ID中需要全部标签的问题，核心思想只对一个摄像头下的全部行人各取一张图片（对于没有出现的行人，从其他摄像头下取一张），然后通过训练模型，聚类给假标签，逐步识别出所有图片。</p><ul><li>paper: <a href="https://yu-wu.net/pdf/TIP2019_One-Example-reID.pdf" target="_blank" rel="noopener">TIP 2019 Progressive Learning for Person Re-Identification with One Example</a></li><li>author: Yu Wu, Yutian Lin, Yi Yang (University of Technology Sydney (UTS))</li><li>code: <a href="https://github.com/Yu-Wu/One-Example-Person-ReID" target="_blank" rel="noopener">https://github.com/Yu-Wu/One-Example-Person-ReID</a></li><li>project: <a href="https://zhuanlan.zhihu.com/p/54576174" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/54576174</a></li></ul><a id="more"></a><p>关于这篇文章的作者也多提几句。</p><p>关于行人重识别(person re-ID)，我看到的多是<strong>悉尼科技大学的郑良团队</strong>，所以简单介绍一下郑良团队的成员。</p><ul><li>郑良：<a href="http://www.liangzheng.com.cn/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/liangzheng06" target="_blank" rel="noopener">github</a>，2015年清华博士毕业，有学生郑哲东，钟准，孙奕帆。</li><li>郑哲东：<a href="http://www.zdzheng.xyz/" target="_blank" rel="noopener">主页</a>, <a href="https://github.com/layumi?tab=repositories" target="_blank" rel="noopener">github</a>，悉尼科技大学博三。</li><li>钟准：<a href="http://zhunzhong.site/" target="_blank" rel="noopener">主页</a>, <a href="https://github.com/zhunzhong07" target="_blank" rel="noopener">github</a>（鸣人），厦门大学交换生博四。</li><li>孙奕帆：<a href="https://syfafterzy.github.io/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/syfafterzy" target="_blank" rel="noopener">github</a>，清华博二。</li><li>林雨恬：<a href="https://vana77.github.io/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/vana77" target="_blank" rel="noopener">github</a>，很漂亮也很厉害的一个小姐姐.</li><li>Wu Yu：<a href="https://yu-wu.net/" target="_blank" rel="noopener">主页</a>，<a href="https://github.com/Yu-Wu" target="_blank" rel="noopener">github</a>，<a href="https://www.zhihu.com/people/wu2008yu/activities" target="_blank" rel="noopener">知乎</a>，悉尼科技大学博一。</li></ul><p>人家牛逼，虚心学习。</p><p>[-] 2019-03-29 </p><p>看到一个大神廖星宇的github,中国科学技术大学,应用数学研究生,旷世科技工程师,他对re-id的研究的浓缩,我觉得就已经可以秒杀90%的论文了.</p><ul><li>廖星宇:<a href="https://l1aoxingyu.github.io/" target="_blank" rel="noopener">主页</a>, <a href="https://github.com/L1aoXingyu/reid_baseline" target="_blank" rel="noopener">github</a></li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>person re-ID 有监督模型，半监督模型。作者采用的是 one-shot image-based setting，也就是每个行人只有一个样例。</p><p>具体来说，作者将数据集分为三部分：labeled data, selected pseudo-labeled data and index-labeled data，其中 labeled data 和 selected pseudo-labeled data 用分类损失，index-labeled data 用 exclusive loss。exclusive loss 的目标是尽可能地使图片之间都离得比较远。</p><img src="/2019/03/27/One-Example-reID/join_training_procedure.png" title="join training procedure"><p>作者主要的<strong>改进点</strong>在于提出了 <strong>exclusive loss</strong> 和 label estimation</p><p>作者的另一篇相关文章是在视频上做的</p><ul><li>paper: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf" target="_blank" rel="noopener">CVPR 2018 Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning</a></li><li>知乎: <a href="https://www.leiphone.com/news/201806/o8a3H5um1H2zXrof.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201806/o8a3H5um1H2zXrof.html</a></li><li>code: <a href="https://github.com/Yu-Wu/Exploit-Unknown-Gradually" target="_blank" rel="noopener">https://github.com/Yu-Wu/Exploit-Unknown-Gradually</a></li></ul><h1 id="2-The-progressive-model"><a href="#2-The-progressive-model" class="headerlink" title="2. The progressive model"></a>2. The progressive model</h1><h2 id="2-1-Framework-overview"><a href="#2-1-Framework-overview" class="headerlink" title="2.1 Framework overview"></a>2.1 Framework overview</h2><p>训练模型分为两步：在三个数据集上训练CNN模型—&gt;在 unlabeled data 上选择一些数据放到 pseudo-labeled data 上。</p><h2 id="2-2-Preliminaries"><a href="#2-2-Preliminaries" class="headerlink" title="2.2 Preliminaries"></a>2.2 Preliminaries</h2><ul><li>labeled data set $\mathcal{L}$</li><li>unlabeled data set $\mathcal{U}$</li><li><ul><li>pseudo-labeled data set $\mathcal{S}^t$</li></ul></li><li><ul><li>index-labeled data set $\mathcal{M}^t$</li></ul></li></ul><h2 id="2-3-The-joint-learning-method"><a href="#2-3-The-joint-learning-method" class="headerlink" title="2.3 The joint learning method"></a>2.3 The joint learning method</h2><p><strong>The Exclusive Loss</strong>: index-labeled data set $\mathcal{M}^t$,</p><script type="math/tex; mode=display">\max \limits_{\theta}\parallel \phi(\theta;x_i) - \phi(\theta;x_j)\parallel</script><p>令$||v_i||=\tilde{\phi}(\theta;x_i)$是$x_i$的归一化后的特征，因此可以将最大化欧氏距离近似为最小化cos距离。</p><ul><li><strong>这个公式没有推导出来，还在询问作者的答案</strong>.</li><li>在<a href="https://arxiv.org/pdf/1604.01850.pdf" target="_blank" rel="noopener">Joint Detection and Identification Feature Learning for Person Search</a>看到了类似的公式，并且有了新理解，在下面补充。</li></ul><script type="math/tex; mode=display">l(V)=-log \frac{exp(v_i^T\tilde{\phi}(x_i)/\tau)}{\sum_{j=1}^{|M^t|}exp(v_j^T\tilde{\phi}(x_i)/\tau)}</script><p>其中，V是所有数据的归一化特征，更高的$\tau$导致softer probability distribution.在更新时，先计算当前数据与所有数据的cos距离，在反向传播时，$v_i=1\2*(v_i+\tilde{\phi}(x_i)$并且归一化。</p><p><strong>The cross-entropy loss</strong>:labeled data set $\mathcal{L}$ and pseudo-labeled data set $\mathcal{S}^t$</p><p>不再陈述</p><p><strong>The total loss</strong>:</p><script type="math/tex; mode=display">\min \lambda l(\mathcal{L}) + \lambda l(\mathcal{S}^t) + (1-\lambda) l(V)(\mathcal{M}^t)</script><h2 id="2-4-The-effective-sampling-criterion"><a href="#2-4-The-effective-sampling-criterion" class="headerlink" title="2.4 The effective sampling criterion"></a>2.4 The effective sampling criterion</h2><p>没有采用分类损失，而是利用最近邻赋予假标签并且假标签的真实性为与最近邻真值的距离，每次取$m_t = m_{t-1}+p\cdot n_u$</p><p>验证集是另外一个re-ID的训练集。</p><p>整体算法如下：</p><img src="/2019/03/27/One-Example-reID/Algorithm.png" title="algorithm"><h2 id="2-5-补充公式"><a href="#2-5-补充公式" class="headerlink" title="2.5 补充公式"></a>2.5 补充公式</h2><p><a href="https://arxiv.org/pdf/1604.01850.pdf" target="_blank" rel="noopener">Joint Detection and Identification Feature Learning for Person Search</a></p><p>定义：n个数据，每个数据表示成向量，即L-2归一化向量$V={v_i}_{i=1}^{n}$，则新向量$x$属于$v_i$类的概率定义为:</p><script type="math/tex; mode=display">p_i=\frac{exp(v_i^Tx/\tau)}{\sum_{j=1}^{n}exp(v_i^Tx/\tau)}</script><p>其中，$\tau$更高，概率分布更平缓，设为0.1.</p><p>这里有两种含义。</p><p>第一种，$v_i$有标签，但$V$的类别多，属于同一类的数据较少，$x$不属于$V$，$x$有标签t，那么n个数据的概率最大值为x的标签，目标应该是最大化x属于t类的概率，其目的是为了分类更加准确。</p><p>第二种，$v_i$没有标签，$x=v_k$属于$V$，目标是最大化x属于第k个元素的概率，最大也就是1，此时$x=v_k$与其他向量$v_j$都正交，其目的是为了令各个向量都离得比较远，也就是作者的目的。作者这里没有用正交来做损失函数，而是用了softmax，很厉害。</p><h1 id="3-code"><a href="#3-code" class="headerlink" title="3. code"></a>3. code</h1><h2 id="3-1-数据集加载"><a href="#3-1-数据集加载" class="headerlink" title="3.1 数据集加载"></a>3.1 数据集加载</h2><p>两个文件 ./reid/datasets/duke.py ./reid/utils/data/dataset.py</p><p>./reid/datasets/duke.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Duke</span><span class="params">(Dataset)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 预处理数据集,使其变成统一形式,从而使用load进行加载</span></span><br></pre></td></tr></table></figure><p>./reid/utils/data/dataset.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self, verbose=True)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_check_integrity</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 检查文件的完整性 images  meta.json splits.json</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这篇文章主要的任务也是为了解决re-ID中需要全部标签的问题，核心思想只对一个摄像头下的全部行人各取一张图片（对于没有出现的行人，从其他摄像头下取一张），然后通过训练模型，聚类给假标签，逐步识别出所有图片。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://yu-wu.net/pdf/TIP2019_One-Example-reID.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TIP 2019 Progressive Learning for Person Re-Identification with One Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;author: Yu Wu, Yutian Lin, Yi Yang (University of Technology Sydney (UTS))&lt;/li&gt;
&lt;li&gt;code: &lt;a href=&quot;https://github.com/Yu-Wu/One-Example-Person-ReID&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Yu-Wu/One-Example-Person-ReID&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;project: &lt;a href=&quot;https://zhuanlan.zhihu.com/p/54576174&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/54576174&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="re-ID" scheme="http://yoursite.com/categories/re-ID/"/>
    
    
      <category term="one_example" scheme="http://yoursite.com/tags/one-example/"/>
    
      <category term="re-ID" scheme="http://yoursite.com/tags/re-ID/"/>
    
  </entry>
  
  <entry>
    <title>GANimation</title>
    <link href="http://yoursite.com/2019/01/24/GANimation/"/>
    <id>http://yoursite.com/2019/01/24/GANimation/</id>
    <published>2019-01-24T07:53:06.000Z</published>
    <updated>2019-03-28T08:15:53.709Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这篇文章是根据GANnotation的一个公式查过来的，感觉还挺厉害。</p><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><ul><li>paper: <a href="https://www.albertpumarola.com/publications/files/pumarola2018ganimation.pdf" target="_blank" rel="noopener">GANimation Anatomically-aware Facial Animation from a Single Image</a></li><li>github: <a href="https://github.com/albertpumarola/GANimation" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation</a></li><li>project: <a href="https://www.albertpumarola.com/research/GANimation/index.html" target="_blank" rel="noopener">https://www.albertpumarola.com/research/GANimation/index.html</a></li></ul><p>关键词：starGAN的改进、连续的表情变换、贴回去能够一致</p><a id="more"></a><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>在人脸转换中，StarGAN是最成功的GAN，但是只能生成离散的人脸。作者要做的就是生成连续的表情变化。</p><h1 id="2-Problem-Formulation"><a href="#2-Problem-Formulation" class="headerlink" title="2. Problem Formulation"></a>2. Problem Formulation</h1><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">$\mathrm{I}_{y_r}\in \mathbb{R}^{H×W×3}$</td><td style="text-align:center">输入图片</td></tr><tr><td style="text-align:center">$\mathrm{y}_r=(y_1,…,y_N)^T$</td><td style="text-align:center">其中，每一个$y_i$表示第i个action unit的程度，在0~1之间</td></tr><tr><td style="text-align:center">$\mathrm{I}_{y_g}$</td><td style="text-align:center">输出图片</td></tr><tr><td style="text-align:center">$\mathcal{M}$</td><td style="text-align:center">映射函数M: $(\mathrm{I}_{y_r},\mathrm{y}_g)$—&gt;$\mathrm{I}_{y_g}$</td></tr></tbody></table></div><p>非成对图片</p><h1 id="3-Our-Approach"><a href="#3-Our-Approach" class="headerlink" title="3. Our Approach"></a>3. Our Approach</h1><p><img src="./GANimation/GANimation1.png" alt="网络结构"><br><img src="/2019/01/24/GANimation/GANimation1.png" title="网络结构"></p><h2 id="3-1-Network-Architechture"><a href="#3-1-Network-Architechture" class="headerlink" title="3.1 Network Architechture"></a>3.1 Network Architechture</h2><h3 id="3-1-1-Generator"><a href="#3-1-1-Generator" class="headerlink" title="3.1.1 Generator"></a>3.1.1 Generator</h3><p>对于G的改进，为了能够使G只聚焦于对于新表情的生成，而保留其他元素，引入attention机制，也就是G生成的不是一整张图片，而是两个mask，color mask C 和 attention mask A.即：</p><script type="math/tex; mode=display">\mathrm{I}_{\mathrm{y}_f}=(1-A)\cdot C + A\cdot \mathrm{I}_{\mathrm{y}_o}</script><p>其中，$\mathrm{A}=G_A(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f)\in {0,…,1}^{H×W}$，$\mathrm{C}=G_C(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f)\in {0,…,1}^{H×W×3}$</p><p><img src="./GANimation/GANimation2.png" alt="生成器结构"><br><img src="/2019/01/24/GANimation/GANimation2.png" title="生成器结构"></p><h3 id="3-1-2-Conditional-Critic"><a href="#3-1-2-Conditional-Critic" class="headerlink" title="3.1.2 Conditional Critic"></a>3.1.2 Conditional Critic</h3><p>PatchGAN: 输入图像 $\mathrm{I}\dashrightarrow \mathrm{Y}_{\mathrm{I}}\in \mathbb{R}^{H/2^6×W/2^6}$</p><p>并且对判别器进行改进，加入额外的回归判别类别。</p><h2 id="3-2-Learning-the-model"><a href="#3-2-Learning-the-model" class="headerlink" title="3.2 Learning the model"></a>3.2 Learning the model</h2><p>损失函数</p><h3 id="3-2-1-Image-Adversarial-Loss"><a href="#3-2-1-Image-Adversarial-Loss" class="headerlink" title="3.2.1 Image Adversarial Loss"></a>3.2.1 Image Adversarial Loss</h3><p>判断图片是生成的还是真实的。</p><p>和StarGAN的损失一样。</p><script type="math/tex; mode=display">L_I(G,D_I,I_{y_o},y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[D_\mathrm{I}(G(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f))]-\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[D_\mathrm{I}(\mathrm{I}_{\mathrm{y}_o})]+\lambda_{gp} \mathbb{E}_{\tilde{I}\thicksim \mathbb{P}_{\tilde{I}}}[(\parallel \nabla_{\tilde{I}}D_I(\tilde{I}) \parallel _2-1)^2]</script><h3 id="3-2-2-Attention-Loss"><a href="#3-2-2-Attention-Loss" class="headerlink" title="3.2.2 Attention Loss"></a>3.2.2 Attention Loss</h3><p>这个损失是针对attention mask A 和 color mask C.</p><p>Total Variation Regularization</p><script type="math/tex; mode=display">L_A(G,I_{y_o},y_f)=\lambda_{TV}\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\sum_{i,j}^{H,W}[(A_{i+1,j}-A_{i,j})^2+(A_{i,j+1}-A_{i,j})^2]]+\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel A \parallel_2]</script><p>这个公式的初步感觉是A要尽可能平缓，并且A中的元素尽可能小。</p><p>根据作者的说法，是为了保证A不变成全是1的矩阵，并且为了保证更加平滑的空间结合。以代码为准。</p><h3 id="3-2-3-Conditional-Expression-Loss"><a href="#3-2-3-Conditional-Expression-Loss" class="headerlink" title="3.2.3 Conditional Expression Loss"></a>3.2.3 Conditional Expression Loss</h3><p>这个应该和starGAN的判断图片属性分类正确损失是一样的。</p><script type="math/tex; mode=display">L_y(G,D_y,I_{y_o},y_o,y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel D_y(G(I_{y_o}|y_f))-y_f \parallel]+\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel D_y(I_{y_o})-y_o \parallel]</script><h3 id="3-2-4-Identity-Loss"><a href="#3-2-4-Identity-Loss" class="headerlink" title="3.2.4 Identity Loss"></a>3.2.4 Identity Loss</h3><p>这个应该就是starGAN的重构损失</p><script type="math/tex; mode=display">L_{idg}(G,I_{y_o},y_o,y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel G(G(I_{y_o}|y_f)|y_o)-I_{y_o} \parallel _1]</script><p>这个损失是为了保证生成前后图片的id是一样的。</p><h3 id="3-2-5-Full-Loss"><a href="#3-2-5-Full-Loss" class="headerlink" title="3.2.5 Full Loss"></a>3.2.5 Full Loss</h3><script type="math/tex; mode=display">L=L_I+\lambda_y L_y+\lambda_A (L_A(G,I_{y_g},y_r)+L_A(G,I_{y_r},y_g))+\lambda_{idt}L_{idt}</script><script type="math/tex; mode=display">\lambda_{gp}=10, \lambda_A=0.1, \lambda_{TV}=0.0001, \lambda_y=4000, \lambda_{idt}=10</script><h1 id="4-Implementation-Details"><a href="#4-Implementation-Details" class="headerlink" title="4. Implementation Details"></a>4. Implementation Details</h1><p><strong>The attention mechanism guaranties a smooth transition between the morphed cropped face and the original image.</strong></p><p>也就是说 attention mechanism 能够保证生成的图片很好地再贴回去。</p><h2 id="4-1-Single-Action-Units-Edition"><a href="#4-1-Single-Action-Units-Edition" class="headerlink" title="4.1 Single Action Units Edition"></a>4.1 Single Action Units Edition</h2><p><img src="./GANimation/GANimation3.png" alt="Single Action Units Edition"><br><img src="/2019/01/24/GANimation/GANimation3.png" title="Single Action Units Edition"></p><ul><li>[x] 这里的AU是什么？ intensity怎么理解？</li></ul><p>AU:<a href="https://www.cs.cmu.edu/~face/facs.htm" target="_blank" rel="noopener">https://www.cs.cmu.edu/~face/facs.htm</a></p><p>intensity: <a href="https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units" target="_blank" rel="noopener">https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units</a></p><p><img src="./GANimation/GANimation4.png" alt="Attention Model"><br><img src="/2019/01/24/GANimation/GANimation4.png" title="Attention Model"></p><h2 id="4-2-Simultaneous-Edition-of-Multiple-AUs"><a href="#4-2-Simultaneous-Edition-of-Multiple-AUs" class="headerlink" title="4.2 Simultaneous Edition of Multiple AUs"></a>4.2 Simultaneous Edition of Multiple AUs</h2><p><img src="./GANimation/GANimation5.png" alt="Facial animation from a single image"><br><img src="/2019/01/24/GANimation/GANimation5.png" title="Facial animation from a single image"></p><script type="math/tex; mode=display">\alpha y_g+(1-\alpha)y_r</script><h2 id="4-3-Discrete-Emotions-Editing"><a href="#4-3-Discrete-Emotions-Editing" class="headerlink" title="4.3 Discrete Emotions Editing"></a>4.3 Discrete Emotions Editing</h2><p><img src="./GANimation/GANimation6.png" alt="Qualitative comparison"><br><img src="/2019/01/24/GANimation/GANimation6.png" title="Qualitative comparison"></p><p>作者生成的图片比StarGAN更清晰。</p><h2 id="4-4-High-Expressions-Variability"><a href="#4-4-High-Expressions-Variability" class="headerlink" title="4.4 High Expressions Variability"></a>4.4 High Expressions Variability</h2><h2 id="4-5-Images-in-the-Wild"><a href="#4-5-Images-in-the-Wild" class="headerlink" title="4.5 Images in the Wild"></a>4.5 Images in the Wild</h2><p><img src="./GANimation/GANimation7.png" alt="Qualitative evaluation on images in the wild"><br><img src="/2019/01/24/GANimation/GANimation7.png" title="Qualitative evaluation on images in the wild"></p><p>作者先检测到人脸，然后扣下来，做训练测试，然后再贴回去，与原图保持了一样的清晰度，个人猜测是因为表情的变化只在人脸的中央就可以完成，不涉及到背景的变换，如果涉及到背景的变换，那么是否还能保证贴回去与原图保持一致性。</p><h2 id="4-6-Pushing-the-Limits-of-the-Model"><a href="#4-6-Pushing-the-Limits-of-the-Model" class="headerlink" title="4.6 Pushing the Limits of the Model"></a>4.6 Pushing the Limits of the Model</h2><p><img src="./GANimation/GANimation8.png" alt="Success and Failure Cases"><br><img src="/2019/01/24/GANimation/GANimation8.png" title="Success and Failure Cases."></p><h1 id="5-code"><a href="#5-code" class="headerlink" title="5. code"></a>5. code</h1><h2 id="5-1-生成器Generator"><a href="#5-1-生成器Generator" class="headerlink" title="5.1 生成器Generator"></a>5.1 生成器Generator</h2><p>GANimation的Generator的主体网络和starGAN的Generator的主体网络一致，只是多加了一个conv</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(NetworkBase)</span>:</span></span><br><span class="line">    <span class="string">"""Generator. Encoder-Decoder Architecture."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conv_dim=<span class="number">64</span>, c_dim=<span class="number">5</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self._name = <span class="string">'generator_wgan'</span></span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">3</span>+c_dim, conv_dim, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.InstanceNorm2d(conv_dim, affine=<span class="keyword">True</span>))</span><br><span class="line">        layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Down-Sampling</span></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            layers.append(nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">            layers.append(nn.InstanceNorm2d(curr_dim*<span class="number">2</span>, affine=<span class="keyword">True</span>))</span><br><span class="line">            layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Bottleneck</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(repeat_num):</span><br><span class="line">            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Up-Sampling</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">            layers.append(nn.InstanceNorm2d(curr_dim//<span class="number">2</span>, affine=<span class="keyword">True</span>))</span><br><span class="line">            layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.main = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(curr_dim, <span class="number">3</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.Tanh())</span><br><span class="line">        self.img_reg = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.Sigmoid())</span><br><span class="line">        self.attetion_reg = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, c)</span>:</span></span><br><span class="line">        <span class="comment"># replicate spatially and concatenate domain information</span></span><br><span class="line">        c = c.unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">        c = c.expand(c.size(<span class="number">0</span>), c.size(<span class="number">1</span>), x.size(<span class="number">2</span>), x.size(<span class="number">3</span>))</span><br><span class="line">        x = torch.cat([x, c], dim=<span class="number">1</span>)</span><br><span class="line">        features = self.main(x)</span><br><span class="line">        <span class="keyword">return</span> self.img_reg(features), self.attetion_reg(features)</span><br></pre></td></tr></table></figure><h2 id="5-2-Discriminator"><a href="#5-2-Discriminator" class="headerlink" title="5.2 Discriminator"></a>5.2 Discriminator</h2><p>Discriminator和StarGAN 的Discriminator完全一样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(NetworkBase)</span>:</span></span><br><span class="line">    <span class="string">"""Discriminator. PatchGAN."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_size=<span class="number">128</span>, conv_dim=<span class="number">64</span>, c_dim=<span class="number">5</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self._name = <span class="string">'discriminator_wgan'</span></span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">3</span>, conv_dim, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.LeakyReLU(<span class="number">0.01</span>, inplace=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, repeat_num):</span><br><span class="line">            layers.append(nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">            layers.append(nn.LeakyReLU(<span class="number">0.01</span>, inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        k_size = int(image_size / np.power(<span class="number">2</span>, repeat_num))</span><br><span class="line">        self.main = nn.Sequential(*layers)</span><br><span class="line">        self.conv1 = nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=k_size, bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        h = self.main(x)</span><br><span class="line">        out_real = self.conv1(h)</span><br><span class="line">        out_aux = self.conv2(h)</span><br><span class="line">        <span class="keyword">return</span> out_real.squeeze(), out_aux.squeeze()</span><br></pre></td></tr></table></figure><h2 id="5-3-train-D"><a href="#5-3-train-D" class="headerlink" title="5.3 train D"></a>5.3 train D</h2><p>这里训练D的过程和starGAN有所不同，并且超参数也有所不同。</p><script type="math/tex; mode=display">\lambda_{D-cond}=4000, \lambda_{gp}=10</script><p>starGAN:</p><script type="math/tex; mode=display">\lambda_{cls}=1, \lambda_{gp}=10</script><ul><li>[ ] 为什么和怎么使用的MSELoss</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># starGAN是把Image Adversarial Loss的三项一起反向传播，但GANimation是分开反向传播的，不确定这么做是否有影响。</span></span><br><span class="line">loss_D, fake_imgs_masked = self._forward_D()</span><br><span class="line">self._optimizer_D.zero_grad()</span><br><span class="line">loss_D.backward()</span><br><span class="line">self._optimizer_D.step()</span><br><span class="line"></span><br><span class="line">loss_D_gp= self._gradinet_penalty_D(fake_imgs_masked)</span><br><span class="line">self._optimizer_D.zero_grad()</span><br><span class="line">loss_D_gp.backward()</span><br><span class="line">self._optimizer_D.step()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_forward_D</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># generate fake images</span></span><br><span class="line">    fake_imgs, fake_img_mask = self._G.forward(self._real_img, self._desired_cond)</span><br><span class="line">    fake_img_mask = self._do_if_necessary_saturate_mask(fake_img_mask, saturate=self._opt.do_saturate_mask)</span><br><span class="line">    fake_imgs_masked = fake_img_mask * self._real_img + (<span class="number">1</span> - fake_img_mask) * fake_imgs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># D(real_I)</span></span><br><span class="line">    <span class="comment"># 识别真图片为真，(Image Adversarial Loss)</span></span><br><span class="line">    <span class="comment"># 图片类别分类准确，这里的分类用的不是交叉熵，而是MSELoss，(Conditional Expression Loss)</span></span><br><span class="line">    <span class="comment"># 刚刚发现一个问题，如果是分类损失，MSELoss的输入必须是同样大小的，按照starGAN，D的输出是类别大小(batch*classification)，G的输入是(batch*1)，那这个样子肯定是没法进行MSELoss的，所以还需要看了数据的处理之后才能明白怎么回事。</span></span><br><span class="line">    <span class="comment"># self._criterion_D_cond = torch.nn.MSELoss().cuda()</span></span><br><span class="line">    d_real_img_prob, d_real_img_cond = self._D.forward(self._real_img)</span><br><span class="line">    self._loss_d_real = self._compute_loss_D(d_real_img_prob, <span class="keyword">True</span>) * self._opt.lambda_D_prob</span><br><span class="line">    self._loss_d_cond = self._criterion_D_cond(d_real_img_cond, self._real_cond) / self._B * self._opt.lambda_D_cond</span><br><span class="line"></span><br><span class="line">    <span class="comment"># D(fake_I)</span></span><br><span class="line">    <span class="comment"># 识别假图片为假，(Image Adversarial Loss)</span></span><br><span class="line">    d_fake_desired_img_prob, _ = self._D.forward(fake_imgs_masked.detach())</span><br><span class="line">    self._loss_d_fake = self._compute_loss_D(d_fake_desired_img_prob, <span class="keyword">False</span>) * self._opt.lambda_D_prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># combine losses</span></span><br><span class="line">    <span class="keyword">return</span> self._loss_d_real + self._loss_d_cond + self._loss_d_fake, fake_imgs_masked</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_loss_D</span><span class="params">(self, estim, is_real)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -torch.mean(estim) <span class="keyword">if</span> is_real <span class="keyword">else</span> torch.mean(estim)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gradinet_penalty_D</span><span class="params">(self, fake_imgs_masked)</span>:</span></span><br><span class="line">    <span class="comment"># (Image Adversarial Loss)的第三项</span></span><br><span class="line">    <span class="comment"># interpolate sample</span></span><br><span class="line">    alpha = torch.rand(self._B, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).cuda().expand_as(self._real_img)</span><br><span class="line">    interpolated = Variable(alpha * self._real_img.data + (<span class="number">1</span> - alpha) * fake_imgs_masked.data, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">    interpolated_prob, _ = self._D(interpolated)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute gradients</span></span><br><span class="line">    grad = torch.autograd.grad(outputs=interpolated_prob,</span><br><span class="line">                                inputs=interpolated,</span><br><span class="line">                                grad_outputs=torch.ones(interpolated_prob.size()).cuda(),</span><br><span class="line">                                retain_graph=<span class="keyword">True</span>,</span><br><span class="line">                                create_graph=<span class="keyword">True</span>,</span><br><span class="line">                                only_inputs=<span class="keyword">True</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># penalize gradients</span></span><br><span class="line">    grad = grad.view(grad.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    grad_l2norm = torch.sqrt(torch.sum(grad ** <span class="number">2</span>, dim=<span class="number">1</span>))</span><br><span class="line">    self._loss_d_gp = torch.mean((grad_l2norm - <span class="number">1</span>) ** <span class="number">2</span>) * self._opt.lambda_D_gp</span><br></pre></td></tr></table></figure><h2 id="5-4-train-G"><a href="#5-4-train-G" class="headerlink" title="5.4 train G"></a>5.4 train G</h2><p>这一部分和starGAN的训练类似，比starGAN多一个mask的平滑loss。</p><script type="math/tex; mode=display">\lambda_{D-cond}=4000, \lambda_{cyc}=10, \lambda_{mask}=0.1, \lambda_{mask-smooth}=1*e^{-5}</script><p>starGAN:</p><script type="math/tex; mode=display">\lambda_{cls}=1, \lambda_{cyc}=10</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_forward_G</span><span class="params">(self, keep_data_for_visuals)</span>:</span></span><br><span class="line">    <span class="comment"># generate fake images</span></span><br><span class="line">    fake_imgs, fake_img_mask = self._G.forward(self._real_img, self._desired_cond)</span><br><span class="line">    fake_img_mask = self._do_if_necessary_saturate_mask(fake_img_mask, saturate=self._opt.do_saturate_mask)</span><br><span class="line">    fake_imgs_masked = fake_img_mask * self._real_img + (<span class="number">1</span> - fake_img_mask) * fake_imgs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># D(G(Ic1, c2)*M) masked</span></span><br><span class="line">    <span class="comment"># 生成图片为真 (Image Adversarial Loss)</span></span><br><span class="line">    <span class="comment"># 生成图片的属性为真 (Conditional Expression Loss)</span></span><br><span class="line">    d_fake_desired_img_masked_prob, d_fake_desired_img_masked_cond = self._D.forward(fake_imgs_masked)</span><br><span class="line">    self._loss_g_masked_fake = self._compute_loss_D(d_fake_desired_img_masked_prob, <span class="keyword">True</span>) * self._opt.lambda_D_prob</span><br><span class="line">    self._loss_g_masked_cond = self._criterion_D_cond(d_fake_desired_img_masked_cond, self._desired_cond) / self._B * self._opt.lambda_D_cond</span><br><span class="line"></span><br><span class="line">    <span class="comment"># G(G(Ic1,c2), c1)</span></span><br><span class="line">    <span class="comment"># 重构损失 (Identity Loss)</span></span><br><span class="line">    rec_real_img_rgb, rec_real_img_mask = self._G.forward(fake_imgs_masked, self._real_cond)</span><br><span class="line">    rec_real_img_mask = self._do_if_necessary_saturate_mask(rec_real_img_mask, saturate=self._opt.do_saturate_mask)</span><br><span class="line">    rec_real_imgs = rec_real_img_mask * fake_imgs_masked + (<span class="number">1</span> - rec_real_img_mask) * rec_real_img_rgb</span><br><span class="line"></span><br><span class="line">    <span class="comment"># l_cyc(G(G(Ic1,c2), c1)*M)</span></span><br><span class="line">    self._loss_g_cyc = self._criterion_cycle(rec_real_imgs, self._real_img) * self._opt.lambda_cyc</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss mask</span></span><br><span class="line">    <span class="comment"># (Attention Loss) 不仅对生成的mask进行了平滑，也对重构生成的mask进行了平滑损失</span></span><br><span class="line">    self._loss_g_mask_1 = torch.mean(fake_img_mask) * self._opt.lambda_mask</span><br><span class="line">    self._loss_g_mask_2 = torch.mean(rec_real_img_mask) * self._opt.lambda_mask</span><br><span class="line">    self._loss_g_mask_1_smooth = self._compute_loss_smooth(fake_img_mask) * self._opt.lambda_mask_smooth</span><br><span class="line">    self._loss_g_mask_2_smooth = self._compute_loss_smooth(rec_real_img_mask) * self._opt.lambda_mask_smooth</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_loss_smooth</span><span class="params">(self, mat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.sum(torch.abs(mat[:, :, :, :<span class="number">-1</span>] - mat[:, :, :, <span class="number">1</span>:])) + \</span><br><span class="line">            torch.sum(torch.abs(mat[:, :, :<span class="number">-1</span>, :] - mat[:, :, <span class="number">1</span>:, :]))</span><br></pre></td></tr></table></figure><h2 id="5-5-保存图片"><a href="#5-5-保存图片" class="headerlink" title="5.5 保存图片"></a>5.5 保存图片</h2><p>这个保存图片在starGAN就没有太理解，在这里又看到了类似的，才理解这是对输入图片归一化的反向操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># starGAN</span></span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">denorm</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="string">"""Convert the range from [-1, 1] to [0, 1]."""</span></span><br><span class="line">    out = (x + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> out.clamp_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GANimation</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">mean = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line">std = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line"><span class="keyword">for</span> i, m, s <span class="keyword">in</span> zip(img, mean, std):</span><br><span class="line">  i.mul_(s).add_(m)</span><br><span class="line">image_numpy = img.numpy()</span><br><span class="line">image_numpy_t = np.transpose(image_numpy, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">image_numpy_t = image_numpy_t*<span class="number">254.0</span></span><br><span class="line">image_numpy_t.astype(np.uint8)</span><br></pre></td></tr></table></figure><h2 id="5-6-其他"><a href="#5-6-其他" class="headerlink" title="5.6 其他"></a>5.6 其他</h2><p>没有实际跑这个代码，所以对于一些细节不是很清晰，尤其是数据处理那里，暂时根据查到的AU资料理解成17个AU(但1, 2, 4, 5, 6, 7, 9, 10, 12, 14, 15, 17, 20, 23, 25, 26, 28, and 45是18个AU)，每个AU是一个0~5的数字。</p><p>但是对于作者所说的能够生成连续的表情变换，这一点只能在测试代码中看出，但是在训练的时候并没有特意去表示连续的变化，暂时对于连续的变化存疑。</p><p>主要是openface这个库有点晕，等数据集下载之后试试。</p><p><a href="https://github.com/albertpumarola/GANimation/issues/45" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/45</a><br><a href="https://github.com/albertpumarola/GANimation/issues/62" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/62</a><br><a href="https://github.com/albertpumarola/GANimation/issues/43" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/43</a><br><a href="https://github.com/albertpumarola/GANimation/issues/32" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/32</a><br><a href="https://github.com/albertpumarola/GANimation/issues/25" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimation/issues/25</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这篇文章是根据GANnotation的一个公式查过来的，感觉还挺厉害。&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{I}=(1-M) \circ C + M \circ I&lt;/script&gt;&lt;ul&gt;
&lt;li&gt;paper: &lt;a href=&quot;https://www.albertpumarola.com/publications/files/pumarola2018ganimation.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GANimation Anatomically-aware Facial Animation from a Single Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;github: &lt;a href=&quot;https://github.com/albertpumarola/GANimation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/albertpumarola/GANimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;project: &lt;a href=&quot;https://www.albertpumarola.com/research/GANimation/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.albertpumarola.com/research/GANimation/index.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关键词：starGAN的改进、连续的表情变换、贴回去能够一致&lt;/p&gt;
    
    </summary>
    
      <category term="GAN" scheme="http://yoursite.com/categories/GAN/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="mask and colour" scheme="http://yoursite.com/tags/mask-and-colour/"/>
    
  </entry>
  
  <entry>
    <title>GANnotation</title>
    <link href="http://yoursite.com/2019/01/17/GANnotation/"/>
    <id>http://yoursite.com/2019/01/17/GANnotation/</id>
    <published>2019-01-17T09:00:01.000Z</published>
    <updated>2019-01-24T07:25:49.642Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这是对GAN应用到人脸合成的改进，只是在arxiv上，先看看再说。</p><a id="more"></a><ul><li>paper: <a href="https://arxiv.org/abs/1811.03492" target="_blank" rel="noopener">https://arxiv.org/abs/1811.03492</a></li><li>github: <a href="https://github.com/ESanchezLozano/GANnotation" target="_blank" rel="noopener">https://github.com/ESanchezLozano/GANnotation</a></li><li>youtube: <a href="https://youtu.be/-8r7zexg4yg" target="_blank" rel="noopener">https://youtu.be/-8r7zexg4yg</a></li></ul><p>key word: self-consistency loss, triple consistency loss, progressive image translation</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>作者对 <strong>self-consistency loss</strong> 的有效性进行了论证， self-consistency loss 可以有效地使图片转换前后 <strong>preserve identity</strong>，并且提出了新的loss <strong>triple consistency loss</strong>.</p><p><img src="./GANnotation/GANnotation2.png" alt="triple consistency loss"><br><img src="/2019/01/17/GANnotation/GANnotation2.png" title="triple consistency loss"></p><p>作者观察到当生成的图片再次经过生成另一种属性的图片时，生成的效果很差。记生成的图片再次经过网络生成新的图片的过程为 <strong>“progressive image translation”</strong>。比如输入一张图片a，先通过网络生成图片b，再将图片b送入网络，得到生成图片c。</p><p><img src="./GANnotation/GANnotation1.png" alt="progressive image translation"><br><img src="/2019/01/17/GANnotation/GANnotation1.png" title="progressive image translation"></p><p>并且作者提出了<strong>GAN-notation</strong>，即 <strong>unconstrained landmark guided face-to-face synthesis</strong>, 同时改变人脸的姿势和表情(simultaneous change in pose and expression)， 论证了 triple consistency loss 的有效性。</p><h1 id="2-Proposed-approach"><a href="#2-Proposed-approach" class="headerlink" title="2. Proposed approach"></a>2. Proposed approach</h1><p><img src="./GANnotation/GANnotation3.png" alt="Proposed approach"><br><img src="/2019/01/17/GANnotation/GANnotation3.png" title="Proposed approach"></p><h2 id="2-1-Notation"><a href="#2-1-Notation" class="headerlink" title="2.1 Notation"></a>2.1 Notation</h2><p>符号说明：</p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">$I\in \mathcal{I}, w × h$</td><td style="text-align:center">人脸图片$I$</td></tr><tr><td style="text-align:center">$s_i \in \mathbb{R}^{2n}$</td><td style="text-align:center">$n$个有序点的集合</td></tr><tr><td style="text-align:center">$H(s_t) \in \mathcal{H}, H(s_t) \in \mathbb{R}^{n×w×h}$</td><td style="text-align:center">$s_i$编码成 heatmap，以均值为点的高斯分布的形式呈现</td></tr><tr><td style="text-align:center">$G: \mathcal{I}×\mathcal{H} \to \hat{\mathcal{I}}$</td><td style="text-align:center">$\hat{\mathcal{I}}$是生成图片的集合</td></tr><tr><td style="text-align:center">$\hat{I} = G(I;H(s_t))$</td><td style="text-align:center">其中$I$和$H(s_t)$是在通道维拼接而成的</td></tr><tr><td style="text-align:center">$\mathbb{P}_{\mathcal{I}}$</td><td style="text-align:center">图片$I$的分布</td></tr><tr><td style="text-align:center">$\mathbb{P}_{\hat{\mathcal{I}}}$</td><td style="text-align:center">图片$\hat{I}$的分布</td></tr></tbody></table></div><h2 id="2-2-Architecture"><a href="#2-2-Architecture" class="headerlink" title="2.2 Architecture"></a>2.2 Architecture</h2><p><strong>G</strong>:</p><ul><li>two spatial downsampling convolutions</li><li>followed by a set of residual blocks</li><li>two spatial upsampling blocks with 1=2 strided convolutions</li><li>output: colour image $C$ and a mask $M$.</li></ul><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><p><strong>D</strong>: PatchGAN, 128×128—&gt;4×4×512</p><h2 id="2-3-Training"><a href="#2-3-Training" class="headerlink" title="2.3 Training"></a>2.3 Training</h2><h3 id="2-3-1-Adversarial-loss"><a href="#2-3-1-Adversarial-loss" class="headerlink" title="2.3.1 Adversarial loss"></a>2.3.1 Adversarial loss</h3><p>作者使用的是 hinge adversarial loss。</p><p>我在参考<a href="https://arxiv.org/pdf/1705.02894.pdf" target="_blank" rel="noopener">文献19</a>中没有找到类似的loss，自己理解的是，对于判别器D而言，希望真图片的真值是$y \leqslant -1$，生成图片的真值是$y \geqslant 1$， 对于生成器G而言，希望生成的图片的真值是$y \geqslant 0$</p><script type="math/tex; mode=display">\begin{align}L_{adv}(D) = &-\mathbb{E}_{\hat{I}\backsim \mathbb{P}_{\hat{\mathcal{I}}}}[min(0, -1+D(\hat{I}))]+ \\&-\mathbb{E}_{I\backsim \mathbb{P}_{\mathcal{I}}}[min(0, -1+D(\hat{I}))]\end{align}</script><script type="math/tex; mode=display">L_{adv}(G)=-\mathbb{E}_{I \backsim \mathbb{P}_I } [D(\hat{I})]</script><h3 id="2-3-2-Pixel-loss"><a href="#2-3-2-Pixel-loss" class="headerlink" title="2.3.2 Pixel loss"></a>2.3.2 Pixel loss</h3><p>这个公式应该需要成对图片</p><script type="math/tex; mode=display">L_{pix} = \parallel G(I;H(s_t)) - I_t \parallel</script><h3 id="2-3-3-Consistency-loss"><a href="#2-3-3-Consistency-loss" class="headerlink" title="2.3.3 Consistency loss"></a>2.3.3 Consistency loss</h3><script type="math/tex; mode=display">L_{self}=\parallel G(G(I;H(s_t)); H(s_i)) - I \parallel</script><h3 id="2-3-4-Triple-Consistency-loss"><a href="#2-3-4-Triple-Consistency-loss" class="headerlink" title="2.3.4 Triple Consistency loss"></a>2.3.4 Triple Consistency loss</h3><script type="math/tex; mode=display">L_{triple}=\parallel G(\hat{I};H(s_n)) - G(I;H(s_n)) \parallel ^2</script><p>其中，$ \hat{I} = G(I;H(s_t)) $.</p><h3 id="2-3-5-Identity-preserving-loss"><a href="#2-3-5-Identity-preserving-loss" class="headerlink" title="2.3.5 Identity preserving loss"></a>2.3.5 Identity preserving loss</h3><script type="math/tex; mode=display">L_{id}=\sum_{l=fc,p} \parallel \Phi_{CNN}^l(I) - \Phi_{CNN}^l(\hat{I})  \parallel</script><p>其中，这个公式的目的是为了preserve the identity(???), 使用 Light CNN 的 fully connected layer 和 last pooling layer 提取出的特征。</p><p>不懂，为啥子呢？之前看StarGAN已经不使用这个公式了。</p><h3 id="2-3-6-Perceptual-loss"><a href="#2-3-6-Perceptual-loss" class="headerlink" title="2.3.6 Perceptual loss"></a>2.3.6 Perceptual loss</h3><p>这个应该就是之前见过的Vgg提取特征做损失。</p><script type="math/tex; mode=display">\begin{align}L_{pp} =& \sum_{l} \parallel \Phi_{VGG}^l(I) \Phi_{VGG}^l(\hat{I}) \parallel \\&+\parallel  \Gamma(\Phi_{VGG}^{relu3_3}(I)) - \Phi_{VGG}^{relu3_3}(\hat{I}) \parallel_F\end{align}</script><p>其中，$l=\lbrace relu1_2, relu2_2, relu3_3, relu4_3\rbrace$</p><p>感觉用到的损失太多了。</p><h3 id="2-3-7-Full-loss"><a href="#2-3-7-Full-loss" class="headerlink" title="2.3.7 Full loss"></a>2.3.7 Full loss</h3><script type="math/tex; mode=display">\begin{align}L(G) =& \lambda_{adv}L_{adv} + \lambda_{pix}L_{pix}+\lambda_{self}L_{self}+\\&\lambda_{triple}L_{triple}+\lambda_{id}L_{id}+\lambda_{pp}L_{pp}+\lambda_{tv}L_{tv}\end{align}</script><p>其中，$\lambda_{adv}=1, \lambda_{pix}=10, \lambda_{self}=100, \lambda_{triple}=100, \lambda_{id}=1, \lambda_{pp}=10, \lambda_{tv}=10^{-4} $</p><p>在作者给的<a href="https://arxiv.org/pdf/1603.08155.pdf" target="_blank" rel="noopener">参考文献14</a>中，也没有明确找到$L_{tv}$的表达式。之后看代码再确定一下吧。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h1><ul><li>Adam, $\beta_1=0.5, \beta_2=0.999$</li></ul><h2 id="3-1-On-the-use-of-a-triple-consistency-loss"><a href="#3-1-On-the-use-of-a-triple-consistency-loss" class="headerlink" title="3.1 On the use of a triple consistency loss"></a>3.1 On the use of a triple consistency loss</h2><p><img src="./GANnotation/GANnotation4.png" alt="StarGAN with/without triple consistency loss"><br><img src="/2019/01/17/GANnotation/GANnotation4.png" title="StarGAN with/without triple consistency loss"></p><p>可以看到即使是 one-to-one ， with triple consistency loss 的效果也要更好一些。</p><h2 id="3-2-GANnotation"><a href="#3-2-GANnotation" class="headerlink" title="3.2 GANnotation"></a>3.2 GANnotation</h2><p>StarGAN只能对很粗的概念进行转换，但是GANnotation可以直接对比较细的属性pose进行转换，而且还可以连续变换，更厉害一些。</p><p><img src="./GANnotation/GANnotation5.png" alt="GANnotation"><br><img src="/2019/01/17/GANnotation/GANnotation5.png" title="GANnotation"></p><p><img src="./GANnotation/GANnotation6.png" alt="GANnotation.vs.CR-GAN"><br><img src="/2019/01/17/GANnotation/GANnotation6.png" title="GANnotation.vs.CR-GAN"></p><h1 id="4-code"><a href="#4-code" class="headerlink" title="4. code"></a>4. code</h1><h2 id="4-1-生成器G"><a href="#4-1-生成器G" class="headerlink" title="4.1 生成器G"></a>4.1 生成器G</h2><p>生成器G输入是一张3通道的人脸图片和66通道的目标人脸关键点热力图，输出是3通道的colour image 和1通道的mask图。</p><p>生成器G共分为5部分：down_conv、bottleneck、feature_layer、colour_layer、mask_layer。</p><ul><li>[x] colour_image 和 mask 怎么理解？</li></ul><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conv_dim=<span class="number">64</span>, c_dim=<span class="number">66</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        initial_layer = [nn.Conv2d(<span class="number">3</span>+c_dim, conv_dim, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>,padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">        initial_layer += [nn.InstanceNorm2d(conv_dim, affine=<span class="keyword">True</span>)]</span><br><span class="line">        initial_layer += [nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>)]</span><br><span class="line"></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            initial_layer += [nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">            initial_layer += [nn.InstanceNorm2d(curr_dim*<span class="number">2</span>, affine=<span class="keyword">True</span>)]</span><br><span class="line">            initial_layer += [nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>)]</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.down_conv = nn.Sequential(*initial_layer)</span><br><span class="line"></span><br><span class="line">        bottleneck = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(repeat_num):</span><br><span class="line">            bottleneck += [ResidualBlock(curr_dim)]</span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(*bottleneck)</span><br><span class="line"></span><br><span class="line">        features = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            features += [nn.ConvTranspose2d(curr_dim, curr_dim//<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">            features += [nn.InstanceNorm2d(curr_dim//<span class="number">2</span>, affine=<span class="keyword">True</span>)]</span><br><span class="line">            features += [nn.LeakyReLU(<span class="number">0.2</span>,inplace=<span class="keyword">True</span>)]</span><br><span class="line">            curr_dim = curr_dim // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.feature_layer = nn.Sequential(*features)</span><br><span class="line"></span><br><span class="line">        colour = [nn.Conv2d(curr_dim, <span class="number">3</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">        colour += [nn.Tanh()]</span><br><span class="line">        self.colour_layer = nn.Sequential(*colour)</span><br><span class="line"></span><br><span class="line">        mask = [nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)]</span><br><span class="line">        mask += [nn.Sigmoid()]</span><br><span class="line">        self.mask_layer = nn.Sequential(*mask)</span><br><span class="line">        init_weights(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">( self, x )</span>:</span></span><br><span class="line">        down = self.down_conv(x)</span><br><span class="line">        bottle = self.bottleneck(down)</span><br><span class="line">        features = self.feature_layer(bottle)</span><br><span class="line">        col = self.colour_layer(features)</span><br><span class="line">        mask = self.mask_layer(features)</span><br><span class="line">        output = mask * ( x[:,<span class="number">0</span>:<span class="number">3</span>,:,:] - col ) + col</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(net, init_type=<span class="string">'normal'</span>, gain=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_func</span><span class="params">(m)</span>:</span></span><br><span class="line">        classname = m.__class__.__name__</span><br><span class="line">        <span class="keyword">if</span> hasattr(m, <span class="string">'weight'</span>) <span class="keyword">and</span> (classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span> <span class="keyword">or</span> classname.find(<span class="string">'Linear'</span>) != <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> init_type == <span class="string">'normal'</span>:</span><br><span class="line">                weight_init.normal_(m.weight.data, <span class="number">0.0</span>, gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'xavier'</span>:</span><br><span class="line">                weight_init.xavier_normal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'kaiming'</span>:</span><br><span class="line">                weight_init.kaiming_normal_(m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_in'</span>)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'orthogonal'</span>:</span><br><span class="line">                weight_init.orthogonal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> NotImplementedError(<span class="string">'initialization method [%s] is not implemented'</span> % init_type)</span><br><span class="line">            <span class="keyword">if</span> hasattr(m, <span class="string">'bias'</span>) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                weight_init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm2d'</span>) != <span class="number">-1</span>:</span><br><span class="line">            weight_init.normal_(m.weight.data, <span class="number">1.0</span>, gain)</span><br><span class="line">            weight_init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'initialize network with %s'</span> % init_type)</span><br><span class="line">    net.apply(init_func)</span><br></pre></td></tr></table></figure><p>np.loadtxt</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">points = np.loadtxt(<span class="string">'test_images/test_1.txt'</span>).transpose().reshape(<span class="number">66</span>,<span class="number">2</span>,<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="4-2-其他"><a href="#4-2-其他" class="headerlink" title="4.2 其他"></a>4.2 其他</h2><p>作者现在只公布了demo代码。但不久就会公布训练代码，搓搓小手手期待中。</p><h2 id="4-3-StarGAN-with-Triple-Consistency-Loss"><a href="#4-3-StarGAN-with-Triple-Consistency-Loss" class="headerlink" title="4.3 StarGAN-with-Triple-Consistency-Loss"></a>4.3 StarGAN-with-Triple-Consistency-Loss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate target domain labels randomly.</span></span><br><span class="line">rand_idx = torch.randperm(label_org.size(<span class="number">0</span>))</span><br><span class="line">label_trg = label_org[rand_idx]</span><br><span class="line"></span><br><span class="line">rand_idx = torch.randperm(label_org.size(<span class="number">0</span>))</span><br><span class="line">label_third = label_org[rand_idx]</span><br><span class="line"><span class="comment"># ================================================================================#</span></span><br><span class="line"><span class="comment">#                               Triple consistency loss.                          #</span></span><br><span class="line"><span class="comment"># ================================================================================#</span></span><br><span class="line">x_third = self.G(x_fake, c_third)</span><br><span class="line">x_third_real = self.G(x_real, c_third)</span><br><span class="line">g_loss_triple = torch.mean(torch.abs(x_third - x_third_real))</span><br></pre></td></tr></table></figure><h1 id="5-答疑解惑"><a href="#5-答疑解惑" class="headerlink" title="5. 答疑解惑"></a>5. 答疑解惑</h1><h2 id="5-1-colour-image-和-mask-怎么理解？"><a href="#5-1-colour-image-和-mask-怎么理解？" class="headerlink" title="5.1 colour_image 和 mask 怎么理解？"></a>5.1 colour_image 和 mask 怎么理解？</h2><p>这个公式来源于</p><p>paper: <a href="http://arxiv.org/abs/1807.09251" target="_blank" rel="noopener">Ganimation: Anatomically-aware facial animation from a single image</a></p><p>github: <a href="https://github.com/albertpumarola/GANimatio" target="_blank" rel="noopener">https://github.com/albertpumarola/GANimatio</a></p><p>project: <a href="http://www.albertpumarola.com/#projects" target="_blank" rel="noopener">http://www.albertpumarola.com/#projects</a></p><script type="math/tex; mode=display">\hat{I}=(1-M) \circ C + M \circ I</script><p>网络生成器没有直接回归整个图像，而是输出两个掩码，一个着色掩码C和一个注意力掩码A，其中，掩码A表示C的每个像素在多大程度上对输出图像有贡献，这样生成器就无需渲染与表情无关的元素，仅聚焦于定义了人脸表情的像素上。</p><p>这个公式貌似比StarGAN还牛逼。能不能放在StarGAN上呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这是对GAN应用到人脸合成的改进，只是在arxiv上，先看看再说。&lt;/p&gt;
    
    </summary>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="face synthesis" scheme="http://yoursite.com/tags/face-synthesis/"/>
    
  </entry>
  
  <entry>
    <title>tensorboard</title>
    <link href="http://yoursite.com/2019/01/15/tensorboard/"/>
    <id>http://yoursite.com/2019/01/15/tensorboard/</id>
    <published>2019-01-15T09:15:48.000Z</published>
    <updated>2019-01-17T08:20:10.969Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>新装pytorch之后，可视化工具visdom就不能用了，所以改用tensorboard，以下是tensorboard及其变体的使用命令。</p><a id="more"></a><h1 id="1-原理"><a href="#1-原理" class="headerlink" title="1. 原理"></a>1. 原理</h1><p>tensorboard与visdom不同，前者是直接读取文件进行展示，需要程序先把要展示的内容保存成文件，然后tensorboard再读取文件进行展示，后者是代码直接展示，在程序中就直接传递给visdom了。</p><p>相同点是两者都需要在程序外面启动。</p><h1 id="2-API以及使用流程"><a href="#2-API以及使用流程" class="headerlink" title="2. API以及使用流程"></a>2. API以及使用流程</h1><h2 id="2-1-API"><a href="#2-1-API" class="headerlink" title="2.1 API"></a>2.1 API</h2><ul><li>tf.summary.FileWriter——用于将汇总数据写入磁盘 </li><li>tf.summary.scalar——对标量数据汇总和记录 </li><li>tf.summary.histogram——记录数据的直方图 </li><li>tf.summary.image——将图像写入summary </li><li>tf.summary.merge——对各类的汇总进行一次合并 </li><li>tf.summary.merge_all——合并默认图像中的所有汇总</li></ul><h2 id="2-2-使用流程"><a href="#2-2-使用流程" class="headerlink" title="2.2 使用流程"></a>2.2 使用流程</h2><ol><li>添加记录节点：tf.summary.scalar/image/histogram()等</li><li>汇总记录节点：merged = tf.summary.merge_all()</li><li>运行汇总节点：summary = sess.run(merged)，得到汇总结果</li><li>日志书写器实例化：summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph)，实例化的同时传入 graph 将当前计算图写入日志</li><li>调用日志书写器实例对象summary_writer的add_summary(summary, global_step=i)方法将所有汇总日志写入文件</li><li>调用日志书写器实例对象summary_writer的close()方法写入内存，否则它每隔120s写入一次， close() 之后就无法再次写入了，需要重新打开reopen()，这里可以替代为summary_writer.flush()。</li></ol><p><img src="./tensorboard/tensorboard.jpg" alt="tensorboard使用流程图"><br><img src="/2019/01/15/tensorboard/tensorboard.jpg" title="tensorboard使用流程图"></p><h1 id="3-启动"><a href="#3-启动" class="headerlink" title="3. 启动"></a>3. 启动</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir /path/to/log</span><br></pre></td></tr></table></figure><h1 id="4-初始化"><a href="#4-初始化" class="headerlink" title="4. 初始化"></a>4. 初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.summary.FileWriter(logdir, graph=None, flush_secs=120, max_queue=10)</span></span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>, sess.graph(), flush_secs = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">writer.add_graph(sess.graph())</span><br></pre></td></tr></table></figure><p>其他常用API</p><ul><li>add_event(event)：Adds an event to the event file</li><li>add_graph(graph, global_step=None)：Adds a Graph to the event file，Most users pass a graph in the constructor instead</li><li>add_summary(summary, global_step=None)：Adds a Summary protocol buffer to the event file，一定注意要传入 global_step</li><li>close()：Flushes the event file to disk and close the file</li><li>flush()：Flushes the event file to disk</li><li>add_meta_graph(meta_graph_def,global_step=None)</li><li>add_run_metadata(run_metadata, tag, global_step=None)</li></ul><p>备注：这个event还是没有看懂怎么使用。</p><h1 id="5-scalar"><a href="#5-scalar" class="headerlink" title="5. scalar"></a>5. scalar</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 三个工具：writer用于指定路径，sess用于运行，summary_op用于执行。</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy) <span class="comment"># tag, value</span></span><br><span class="line"></span><br><span class="line">summary_op = tf.summary.merge_all()</span><br><span class="line">summary_str = sess.run(summary_op)</span><br><span class="line">summary_writer.add_summary(summary_str, global_step) <span class="comment"># y, x</span></span><br><span class="line"><span class="comment"># 备注：global_step总是int型，即使输入0.4，在图像显示也是0.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or family</span></span><br><span class="line">tf.summary.scalar(<span class="string">'loss1'</span>, <span class="number">1</span>, family=<span class="string">'loss'</span>)</span><br><span class="line">tf.summary.scalar(<span class="string">'loss2'</span>, <span class="number">2</span>, family=<span class="string">'loss'</span>)</span><br><span class="line">tf.summary.scalar(<span class="string">'loss3'</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 示意如下图</span></span><br></pre></td></tr></table></figure><p><img src="./tensorboard/tensorboard3.png" alt="family"><br><img src="/2019/01/15/tensorboard/tensorboard3.png" title="family"></p><p>或者自定义数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 两个工具 summary_writer: 用于连接路径， summary: 用于指定y值</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">summary = tf.Summary(value=[</span><br><span class="line">  tf.Summary.Value(tag=<span class="string">'test2'</span>, simple_value=<span class="number">0</span>),</span><br><span class="line">  tf.Summary.Value(tag=<span class="string">'test3'</span>, simple_value=<span class="number">1</span>),</span><br><span class="line">])</span><br><span class="line">summary_writer.add_summary(summary, global_step) <span class="comment"># y, x</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">"/path/to/log"</span>)</span><br><span class="line">summary = tf.Summary()</span><br><span class="line">summary.value.add(tag=<span class="string">'test4'</span>, simple_value=<span class="number">0</span>)</span><br><span class="line">summary.value.add(tag=<span class="string">'test5'</span>, simple_value=<span class="number">1</span>)</span><br><span class="line">sumamry_writer.add_summary(summary, global_step)</span><br></pre></td></tr></table></figure><h1 id="6-histogram"><a href="#6-histogram" class="headerlink" title="6. histogram"></a>6. histogram</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.histogram(<span class="string">'layer'</span>+str(i+<span class="number">1</span>)+<span class="string">'weights'</span>,weights)</span><br></pre></td></tr></table></figure><p>这里的weights可以是list型，也可以是pytorch的tensor型，其他类型没试，但可以推断，一般情况下的都可以，猜测会统一转换为numpy型。</p><h1 id="7-image"><a href="#7-image" class="headerlink" title="7. image"></a>7. image</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.image(<span class="string">'input'</span>, x_image, max_outputs=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tf.summary.image(<span class="string">'test'</span>, tf.reshape(images, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]), <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tf.summary.image(<span class="string">'test1'</span>, torch.rand(<span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>备注：x_image必须是uint8或者float32型的4-D Tensor[batch_size, height, width, channels]，其中channels为1， 3 或者4</p><p>对于numpy</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line">image_ = Image.open(<span class="string">'path'</span>)</span><br><span class="line">image_numpy = numpy.array(image_) <span class="comment"># 271, 108, 3</span></span><br><span class="line">tf.summary.image(<span class="string">'test1'</span>, image_numpy.reshape(<span class="number">1</span>, <span class="number">271</span>, <span class="number">108</span>,<span class="number">3</span>))</span><br><span class="line">summary_op = tf.summary.merge_all()</span><br><span class="line">summary_str = sess.run(summary_op)</span><br><span class="line">summary_writer.add_summary(summary_str, global_step)</span><br></pre></td></tr></table></figure><p>对于pytorch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image_ = Image.open(<span class="string">'path'</span>) <span class="comment"># 3, 271, 108</span></span><br><span class="line">image_tensor = T.ToTensor()(image_)</span><br><span class="line">tf.summary.image(<span class="string">'test9'</span>, image_tensor.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">271</span>, <span class="number">108</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 备注：将permute改为reshape之后不知道为什么会展示9空格的相同的黑白图片</span></span><br><span class="line">summary_op = tf.summary.merge_all()</span><br><span class="line">summary_str = sess.run(summary_op)</span><br><span class="line">summary_writer.add_summary(summary_str, global_step)</span><br></pre></td></tr></table></figure><p>uint8或者float32型的4-D Tensor[batch_size, height, width, channels]，其中channels为1， 3 或者4</p><h1 id="8-多个event的可视化"><a href="#8-多个event的可视化" class="headerlink" title="8. 多个event的可视化"></a>8. 多个event的可视化</h1><p>如果 logdir 目录的子目录中包含另一次运行时的数据(多个 event)，那么 TensorBoard 会展示所有运行的数据(主要是scalar)，这样可以用于比较不同参数下模型的效果，调节模型的参数，让其达到最好的效果！</p><p>这个还有待测试</p><h1 id="9-tensorboard-logger"><a href="#9-tensorboard-logger" class="headerlink" title="9. tensorboard_logger"></a>9. tensorboard_logger</h1><p>tensorboard_logger可以暂时理解成简化版的tensorboard，或者是tensorboard的高级API。</p><p>tensorboard_logger依赖于tensorboard。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">pip install tensorboard_logger</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">tensorboard --logdir /path/to/log --port /port <span class="comment"># 实际还是通过tensorboard来使用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">from</span> tensorboard_logger <span class="keyword">import</span> Logger</span><br><span class="line">logger = Logger(logdir=<span class="string">'./logs'</span>, flush_secs=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># logger.logdir = './logs2'  # 可以修改，尝试着修改了一下，发现没用，暂时用不着。</span></span><br><span class="line"><span class="comment"># logger.flush_secs = 3   # 可以修改</span></span><br><span class="line"><span class="comment"># API</span></span><br><span class="line">logger.log_value(<span class="string">'loss'</span>, <span class="number">10</span>, step=<span class="number">3</span>) <span class="comment"># step 可以不写，推荐写</span></span><br><span class="line"><span class="comment"># image_path = 'test.jpg'</span></span><br><span class="line"><span class="comment"># image_ = Image.open(image_path)</span></span><br><span class="line"><span class="comment"># image_numpy = np.array(image_)</span></span><br><span class="line">logger.log_images(<span class="string">'images'</span>, image_numpy.reshape(<span class="number">1</span>,<span class="number">271</span>,<span class="number">168</span>,<span class="number">3</span>), step=<span class="number">2</span>) <span class="comment"># 规则同上</span></span><br><span class="line">logger.log_histogram(<span class="string">'weights'</span>, torch.rand(<span class="number">2</span>,<span class="number">3</span>)+<span class="number">10</span>, step=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数tensorboard_logger.configure, tensorboard_logger.log_value与tensorboard_logger.Logger可以达到一样的效果</span></span><br><span class="line"><span class="keyword">from</span> tensorboard_logger <span class="keyword">import</span> configure, log_value</span><br><span class="line">configure(<span class="string">"runs/run-1234"</span>, flush_secs=<span class="number">5</span>)</span><br><span class="line">log_value(<span class="string">'v1'</span>, v1, step)</span><br></pre></td></tr></table></figure><h1 id="10-tensorboardX"><a href="#10-tensorboardX" class="headerlink" title="10. tensorboardX"></a>10. tensorboardX</h1><p><a href="https://github.com/lanpa/tensorboardX" target="_blank" rel="noopener">https://github.com/lanpa/tensorboardX</a></p><p>tensorboardX也依赖于tensorflow和tensorboard。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">pip install tensorboardX</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">tensorboard --logdir /path/to/log --port /port</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(logdir = <span class="string">'tensorboard7'</span>, comment = <span class="string">'test1'</span>, flush_secs = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 当有logdir时，comment被忽略，</span></span><br><span class="line"><span class="comment"># 当没有logdir时，只有comment(writer = SummaryWriter(comment = 'tensorboard7_test1', flush_secs = 1))会作为补充自动生成这样的文件目录：</span></span><br><span class="line"><span class="comment"># ./runs/Jan17_11-24-12_zbp-PowerEdge-T630tensorboard7_test1/events....</span></span><br><span class="line"><span class="comment"># 此外，任意tf.summary.FileWriter的参数都可以加上去。</span></span><br><span class="line">writer.close()</span><br><span class="line"><span class="comment"># 没有writer.flush()</span></span><br></pre></td></tr></table></figure><p>另一种写法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> SummaryWriter(logdir = <span class="string">'tensorboard7'</span>) <span class="keyword">as</span> w:</span><br><span class="line">    w.add_something()</span><br></pre></td></tr></table></figure><p>不推荐with的写法，会自动重新创建一个文件，乱。</p><h2 id="10-1-add-scalar"><a href="#10-1-add-scalar" class="headerlink" title="10.1 add_scalar"></a>10.1 add_scalar</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># API</span></span><br><span class="line">writer.add_scalar(<span class="string">'loss1'</span>, <span class="number">2</span>, global_step=<span class="number">2</span>)</span><br><span class="line">writer.add_scalars(<span class="string">'loss'</span>, &#123;<span class="string">'loss1'</span>:<span class="number">2</span>, <span class="string">'loss2'</span>:<span class="number">3</span>&#125;, global_step=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 示例如下图，add_scalars可以将多个loss显示同一个图上，实现类似visdom的功能。</span></span><br><span class="line"><span class="comment"># add_scalars(main_tag,tag_scalar_dict,global_step=None)</span></span><br></pre></td></tr></table></figure><p><img src="./tensorboard/tensorboard1.png" alt="add_scalar&amp;add_scalars"><br><img src="/2019/01/15/tensorboard/tensorboard1.png" title="add_scalar&add_scalars"></p><p><strong>备注：</strong>:</p><ol><li><strong>SummaryWriter以及之前的类似Writer都可以自动创建文件夹</strong>。</li><li><strong>tensorboard —logdir /path/to/log会自动迭代该文件下的所有文件夹和文件，将event展示出来，并且将同名字的scalar放在一起，可以用于对比修改前后的结果</strong>。</li></ol><h2 id="10-2-add-image"><a href="#10-2-add-image" class="headerlink" title="10.2 add_image"></a>10.2 add_image</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">writer.add_image(tag, img_tensor, global_step=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># img_tensor: 图像数据，shape（3，H，W) 配合torchvision.utils.make_grid使用</span></span><br><span class="line"><span class="comment"># writer.add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW'),</span></span><br><span class="line"><span class="comment">#  dataformats: 'CHW', 'HWC', 'HW'</span></span><br><span class="line"></span><br><span class="line">image_path = <span class="string">'test.jpg'</span></span><br><span class="line">image_ = Image.open(image_path)</span><br><span class="line">image_tensor = T.ToTensor()(image_)</span><br><span class="line">writer.add_image(<span class="string">'test1'</span>, image_tensor, global_step=<span class="number">2</span>)</span><br><span class="line">writer.add_image(<span class="string">'test2'</span>, torchvision.utils.make_grid(torch.rand(<span class="number">16</span>, <span class="number">3</span>, <span class="number">256</span>,<span class="number">256</span>), nrow=<span class="number">8</span>, padding=<span class="number">20</span>), global_step=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="10-3-add-histogram"><a href="#10-3-add-histogram" class="headerlink" title="10.3 add_histogram"></a>10.3 add_histogram</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">writer.add_histogram(tag,values,global_step=<span class="keyword">None</span>,bins=<span class="string">'tensorflow'</span>)</span><br><span class="line"><span class="comment"># bins:  (string): one of &#123;'tensorflow','auto', 'fd', ...&#125;, this determines</span></span><br><span class="line"><span class="comment"># how the bins are made. You can find other options in:</span></span><br><span class="line"><span class="comment"># https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html</span></span><br><span class="line">writer.add_histogram(<span class="string">'test1'</span>, torch.rand(<span class="number">16</span>, <span class="number">3</span>, <span class="number">256</span>,<span class="number">256</span>), global_step=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 感觉 bins 这个参数也差不多么</span></span><br></pre></td></tr></table></figure><h2 id="10-4-add-graph"><a href="#10-4-add-graph" class="headerlink" title="10.4 add_graph"></a>10.4 add_graph</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net1</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net1, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x )</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">10</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = torch.nn.functional.softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">input = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">model = Net1()</span><br><span class="line">writer.add_graph(model, (input,))</span><br><span class="line"></span><br><span class="line">add_graph(model, input_to_model, verbose=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># verbose 用于是否print</span></span><br></pre></td></tr></table></figure><p><img src="./tensorboard/tensorboard4.png" alt="add_graph"><br><img src="/2019/01/15/tensorboard/tensorboard4.png" title="add_graph"></p><p>终于pytorch也能显示graph了，不容易，应该提前熟悉这些API的。</p><h1 id="11-结论"><a href="#11-结论" class="headerlink" title="11. 结论"></a>11. 结论</h1><p>综上所述，我觉得：</p><ul><li>tensorboardX最适合非tensorflow的深度学习框架</li><li>tensorboard适合tensoflow</li><li>tensorboard_logger也适合非tensorflow的深度学习框架，可以看成简化版的tensorboardX</li><li>tensorboardX和tensorboard_logger可以互相替换</li><li>开始使用tensorboardX作为我的新的可视化工具。</li></ul><p>不太懂tensorboardX和tensorboard_logger之间的区别，难道是开发的人不一样？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;新装pytorch之后，可视化工具visdom就不能用了，所以改用tensorboard，以下是tensorboard及其变体的使用命令。&lt;/p&gt;
    
    </summary>
    
      <category term="tensorboard" scheme="http://yoursite.com/categories/tensorboard/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
      <category term="tensorboard" scheme="http://yoursite.com/tags/tensorboard/"/>
    
  </entry>
  
  <entry>
    <title>SyRI</title>
    <link href="http://yoursite.com/2019/01/14/SyRI/"/>
    <id>http://yoursite.com/2019/01/14/SyRI/</id>
    <published>2019-01-14T03:32:20.000Z</published>
    <updated>2019-01-14T07:07:45.946Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这也是一篇合成数据集的论文。</p><a id="more"></a><p>paper: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Slawomir_Bak_Domain_Adaptation_through_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Domain Adaptation through Synthesis for Unsupervised Person Re-identification</a></p><p>github: 无</p><p>项目地址: 无</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>针对的问题是行人重识别数据缺乏在不同光照条件下的多样性，所以作者进行了数据合成。</p><p><strong>HDR maps</strong>: high dynamic range environment maps.</p><p><strong>three-step domain adaption technique</strong>:</p><p><img src="./SyRI/SyRI1.png" alt="three-step domain adaption technique"><br><img src="/2019/01/14/SyRI/SyRI1.png" title="three-step domain adaption technique"></p><ol><li>Illumination inference</li><li>Domain translation</li><li>Fine-tuning</li></ol><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>这篇文章看得我有点迷迷糊糊的，感觉其中用到了cycleGAN，优化了cycleGAN的一个损失函数，但是怎么进行的illuminaion inference、fine-tuning，没有充分理解。</p><h1 id="罗浩对这篇文章的解读"><a href="#罗浩对这篇文章的解读" class="headerlink" title="罗浩对这篇文章的解读"></a>罗浩对这篇文章的解读</h1><p><a href="https://zhuanlan.zhihu.com/p/44212707" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44212707</a></p><blockquote><p>由于训练数据的缺乏，以及3D建模的技术增强，利用游戏等合成的逼真数据来进行视觉研究已经逐渐打入各个领域。这一篇就是利用电脑合成数据在进行ReID的论文。论文提出了新的合成数据集SyRI dataset，该数据集通过改变HDR参数等，一个行人ID可以拥有一百多个环境下图像数据。此外，为了在未见过的真实场景上实现更好的域适应效果，论文基于这个合成数据集提出了一种新的方法。</p><p>整个Pipeline包括三个环节。第一步是拿到了target domain $R_{M+1}$ 的一些未标注图片之后，要对光照进行一个推理，在合成数据集 $S_{k^*}$ 里面找到最接近样本 。然后利用CycleGAN将合成数据生成target domain style的数据。最后利用生成的数据对ReID网络进行fune-tuning。整体来说pipeline比较简单.</p><p>例如，在CycleGAN生成图环节，为了得到更高质量的图像，论文使用了和PTGAN一样的前景mask的思想。不同点在于PTGAN是借助于语义分割网络得到一个行人前景的mask，本文是直接使用了一个2D高斯核作为mask</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;这也是一篇合成数据集的论文。&lt;/p&gt;
    
    </summary>
    
      <category term="person re-id" scheme="http://yoursite.com/categories/person-re-id/"/>
    
    
      <category term="person re-identification" scheme="http://yoursite.com/tags/person-re-identification/"/>
    
      <category term="domain adaption" scheme="http://yoursite.com/tags/domain-adaption/"/>
    
  </entry>
  
  <entry>
    <title>InstaGAN</title>
    <link href="http://yoursite.com/2019/01/04/InstaGAN/"/>
    <id>http://yoursite.com/2019/01/04/InstaGAN/</id>
    <published>2019-01-04T03:25:47.000Z</published>
    <updated>2019-01-10T14:57:05.795Z</updated>
    
    <content type="html"><![CDATA[<p>实例转换</p><a id="more"></a><h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p><a href="https://arxiv.org/pdf/1812.10889.pdf" target="_blank" rel="noopener">InstaGAN Instance-aware image-to-image translation</a></p><p>Sangwoo Mo, Minsu Cho, Jinwoo Shin</p><p>github: <a href="https://github.com/sangwoomo/instagan" target="_blank" rel="noopener">https://github.com/sangwoomo/instagan</a></p><p>project: <a href="https://openreview.net/forum?id=ryxwJhC9YX" target="_blank" rel="noopener">https://openreview.net/forum?id=ryxwJhC9YX</a></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p><img src="./InstaGAN/InstaGAN1.png" alt="Tranlation results"><br><img src="/2019/01/04/InstaGAN/InstaGAN1.png" title="Tranlation results"></p><p>整体分为三部分:</p><ol><li>an instance-augmented neural architecture</li><li>a context preserving loss</li><li>a sequential mini-batch inference/training technique</li></ol><ul><li>an instance-augmented neural architecture: an image and the corresponding set of instance attributes.</li><li>a context preserving loss: target instances and an identity function</li><li>a sequential mini-batch inference/training technique: translating the mini-batches of instance attributes sequentially</li></ul><h1 id="2-InstaGAN"><a href="#2-InstaGAN" class="headerlink" title="2. InstaGAN"></a>2. InstaGAN</h1><p>符号说明：</p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">$\mathcal{X}$, $\mathcal{Y}$</td><td style="text-align:center">image domain</td></tr><tr><td style="text-align:center">$\mathcal{A}, \mathcal{B}$</td><td style="text-align:center">a space of set of instance attributes</td></tr><tr><td style="text-align:center">$\boldsymbol{a} = \lbrace a_i \rbrace _{i=1}^N $</td><td style="text-align:center">set of instance attributes</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">instance segmentation mask</td></tr><tr><td style="text-align:center">$G_{XY}:\mathcal{X}-&gt;\mathcal{Y}, G_{YX}:\mathcal{Y}-&gt;\mathcal{X}$</td><td style="text-align:center">tranlation function</td></tr></tbody></table></div><h2 id="2-1-InstaGAN-architecture"><a href="#2-1-InstaGAN-architecture" class="headerlink" title="2.1 InstaGAN architecture"></a>2.1 InstaGAN architecture</h2><p><img src="./InstaGAN/InstaGAN2.png" alt="InstaGAN architecture"><br><img src="/2019/01/04/InstaGAN/InstaGAN2.png" title="InstaGAN architecture"></p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">$f_{GX}$</td><td style="text-align:center">image feature extractor</td></tr><tr><td style="text-align:center">$f_{GA}$</td><td style="text-align:center">attribute feature extractor</td></tr><tr><td style="text-align:center">$H_{GX}(x,a)=[f_{GX}(x);\sum_{i=1}^Nf_{GA}(a_i)]$</td><td style="text-align:center">image representation</td></tr><tr><td style="text-align:center">$H_{GA}^n(x,a)=[f_{GX}(x);\sum_{i=1}^Nf_{GA}(a_i);f_{GA}(a_n)]$</td><td style="text-align:center">image representation</td></tr><tr><td style="text-align:center">$h_{DX}(x,a)=[f_{DX}(x);\sum_{i=1}^Nf_{DA}(a_i)]$</td><td style="text-align:center">image representation for discriminator</td></tr><tr><td style="text-align:center">$f_{GX},f_{GA},f_{DX},f_{DA},g_{GX},G_{GA},G_{DX}$</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">$(x,a)-&gt;(y’,b’)$</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">$(y,b)-&gt;(x’,a’)$</td></tr></tbody></table></div><p><strong>作者为了能实现mask顺序不变性，采用相加的方式</strong>。</p><h2 id="2-2-Training-loss"><a href="#2-2-Training-loss" class="headerlink" title="2.2 Training loss"></a>2.2 Training loss</h2><ol><li><strong>domain loss</strong>: GAN loss</li><li><strong>content loss</strong>: cycle-consistency loss and identity mapping loss and context preserving loss</li></ol><p><strong>LSGAN</strong>: 判断图片是原始的还是生成的</p><script type="math/tex; mode=display">L_{LSGAN}=(D_X(x,a)-1)^2+(D_X(G_{YX}(y,b)))^2+(D_Y(y,b)-1)^2+(D_Y(G_{XY}(y,b)))^2</script><p><strong>cycle-consistency loss</strong>: 循环一致性</p><script type="math/tex; mode=display">L_{cyc}=\parallel G_{YX}(G_{XY}(x,a))-(x,a) \parallel \_1+\parallel G_{XY}(G_{YX}(y,b))-(y,b) \parallel _1</script><p><strong>identity mapping loss</strong>: 恒等映射</p><script type="math/tex; mode=display">L_{idt}=\parallel G_{XY}(y,b)-(y,b) \parallel \_1 + \parallel G_{YX}(x,a)-(x,a) \parallel _1</script><p><strong>context preserving loss</strong>: 保留背景</p><script type="math/tex; mode=display">L_{ctx}=\parallel w(a,b')\odot(x-y') \parallel _1 + \parallel w(b,a')\odot(y-x') \parallel _1</script><p>其中，$w(a,b’), w(b,a’)$表示在原图片和生成图片都是背景的位置的权重是1.</p><p><strong>Total loss</strong>:</p><script type="math/tex; mode=display">\begin{align}L_{InstaGAN}&=L_{LSGAN} + \lambda_{cyc} L_{cyc}+ \lambda_{idt} L_{idt}+ \lambda_{ctx} L_{ctx} \\&=L_{LSGAN}+L_{content}\end{align}</script><h2 id="2-3-sequential-mini-batch-translation"><a href="#2-3-sequential-mini-batch-translation" class="headerlink" title="2.3 sequential mini-batch translation"></a>2.3 sequential mini-batch translation</h2><p>考虑到在图片上的实例可能很多，而GPU的所需空间随之线性增长，可能不符合现实情况，所以需要考虑在图片上可以转化一小部分实例。</p><p><img src="./InstaGAN/InstaGAN3.png" alt="sequential mini-batch translation"><br><img src="/2019/01/04/InstaGAN/InstaGAN3.png" title="sequential mini-batch translation"></p><p>符号说明：</p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">$a=\cup_{i=1}^Ma_i$</td><td style="text-align:center">divide the set of instance masks a into mini-batch $a_1,a_2,…,a_M$</td></tr><tr><td style="text-align:center">$(x_m, a_m)-&gt;(y’_m, b’_m) or (x_{m+1}, a_{m+1})$</td><td style="text-align:center">mini-batch translation</td></tr><tr><td style="text-align:center">$(y’_m, b’_{1:m})=(y’_m, \cup_{i=1}^m b’_i)$</td><td style="text-align:center">用于判断真假</td></tr></tbody></table></div><p>在这种情况下，不同的损失函数作用的范围发生改变，第m次时，content loss作用在$(x_m, a_m), (y’_m, b’_m)$，domain loss 作用在$(x,a), (y’_m, b’_{1:m})$，即</p><script type="math/tex; mode=display">L_{InstaGAN-SM}=\sum_{m=1}^M \lbrace L_{LSGAN}((x,a),(y'_m, b'_{1:m}))+L_{content}((x_m, a_m),(y'_m, b'_m)) \rbrace</script><script type="math/tex; mode=display">L_{content}=\lambda_{cyc} L_{cyc}+\lambda_{idt} L_{idt}+\lambda_{ctx} L_{ctx}</script><ul><li>每m个迭代detach一次，来使用固定大小的GPU。</li><li>划分mini-batch的原则：size of instances, 由大到小</li></ul><h1 id="3-experimental-results"><a href="#3-experimental-results" class="headerlink" title="3. experimental results"></a>3. experimental results</h1><h2 id="3-1-image-to-image-translation-results"><a href="#3-1-image-to-image-translation-results" class="headerlink" title="3.1 image-to-image translation results"></a>3.1 image-to-image translation results</h2><p><img src="./InstaGAN/InstaGAN4.png" alt="translation results"><br><img src="/2019/01/04/InstaGAN/InstaGAN4.png" title="translation results"></p><p>通过上述结果的展示，我可以认为在这方面InstaGAN要比CycleGAN的效果更好，更能得到想要的指定的结果。</p><p><img src="./InstaGAN/InstaGAN5.png" alt="results of translation "><br><img src="/2019/01/04/InstaGAN/InstaGAN5.png" title="results of translation"></p><p>第一个结果表明可以通过控制掩码来控制生成的图片。</p><p>第二个结果表明可以使用预测的掩码进行转换图片，从而减少获取掩码的成本。</p><h2 id="3-2-ablation-study"><a href="#3-2-ablation-study" class="headerlink" title="3.2 ablation study"></a>3.2 ablation study</h2><p><img src="./InstaGAN/InstaGAN6.png" alt="ablation study"><br><img src="/2019/01/04/InstaGAN/InstaGAN6.png" title="ablation study"></p><p>Fig.9 主要使研究作者提出的三部分功能的作用，instance mask，损失函数，mini-batch的影响，从效果上看，还是最后一张图片效果更好一些。</p><p><img src="./InstaGAN/InstaGAN7.png" alt="ablation  study on the effects of the sequential mini-batch inference/training technique "><br><img src="/2019/01/04/InstaGAN/InstaGAN7.png" title="ablation study on the effects of the sequential mini-batch inference/training technique"></p><p>Fig.10分别表示在training和inference中使用one-step还是sequential方法，我觉得都差不多，但是对于有限的GPU是个很好的方法。</p><h1 id="4-Appendix"><a href="#4-Appendix" class="headerlink" title="4. Appendix"></a>4. Appendix</h1><h2 id="4-1-architecture-details"><a href="#4-1-architecture-details" class="headerlink" title="4.1 architecture details"></a>4.1 architecture details</h2><blockquote><p>PatchGAN discriminator is composed of 5 convolutional layers, including normalization and non-linearity layers. We used the first 3 convolution layers for feature extractors, and the last 2 convolution layers for classifier.</p></blockquote><h2 id="4-2-traning-details"><a href="#4-2-traning-details" class="headerlink" title="4.2 traning details"></a>4.2 traning details</h2><ul><li>$\lambda_{cyc}=10, \lambda_{idt}=10, \lambda_{ctx}=10$</li><li>Adam: $\beta_1=0.5, \beta_2=0.999$</li><li>batch_size=4</li><li>GPU = 4</li><li>learning rate: 0.0002 for G, 0.0001 for D, 前m个epoch保持不变，后n个epoch线性衰减为0.不同的数据集的m和n不同</li><li>size对于不同的数据集也不同。</li></ul><h2 id="4-3-trend-of-translation-results"><a href="#4-3-trend-of-translation-results" class="headerlink" title="4.3 trend of translation results"></a>4.3 trend of translation results</h2><p><img src="./InstaGAN/InstaGAN8.png" alt="trend of translation results"><br><img src="/2019/01/04/InstaGAN/InstaGAN8.png" title="trend of translation results"></p><h2 id="4-4-其他"><a href="#4-4-其他" class="headerlink" title="4.4 其他"></a>4.4 其他</h2><p>我觉得这是相当于对于CycleGAN，加上了指向性生成，不再是单独地生成目标域风格的图片，而是对指定区域生成目标域风格的图片。</p><p>刚刚想到一个问题，InstaGAN可以生成指定形状的图片，但是对于同一形状的不同物体，比如生成红色的裙子和黑色的裙子这样子的任务，可能不行。</p><h2 id="4-5-video-translation-results"><a href="#4-5-video-translation-results" class="headerlink" title="4.5 video translation results"></a>4.5 video translation results</h2><p><img src="./InstaGAN/InstaGAN9.png" alt="video translation results"><br><img src="/2019/01/04/InstaGAN/InstaGAN9.png" title="video translation results"></p><p>作者使用pix2pix作为分割。</p><p>感觉在视频上，裤子换成裙子后，能保持所有帧的裙子都是一样的，说明转换的稳定性很好。</p><h1 id="5-code"><a href="#5-code" class="headerlink" title="5. code"></a>5. code</h1><p>看细节还是需要看代码的实现过程。</p><h2 id="5-1-文件目录"><a href="#5-1-文件目录" class="headerlink" title="5.1 文件目录"></a>5.1 文件目录</h2><p><img src="./InstaGAN/InstaGAN10.png" alt="文件目录"><br><img src="/2019/01/04/InstaGAN/InstaGAN10.png" title="文件目录"></p><p>通过文件组织，可以发现cycleGAN尽可能地考虑了可扩展性。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">|-- LICENSE</span><br><span class="line">|-- README.md</span><br><span class="line">|-- data</span><br><span class="line">|   |-- __init__.py</span><br><span class="line">|   |-- aligned_dataset.py</span><br><span class="line">|   |-- base_data_loader.py</span><br><span class="line">|   |-- base_dataset.py</span><br><span class="line">|   |-- image_folder.py</span><br><span class="line">|   |-- single_dataset.py</span><br><span class="line">|   |-- unaligned_dataset.py</span><br><span class="line">|   `-- unaligned_seg_dataset.py</span><br><span class="line">|-- datasets</span><br><span class="line">|   |-- combine_A_and_B.py</span><br><span class="line">|   |-- download_coco.sh</span><br><span class="line">|   |-- download_cyclegan_dataset.sh</span><br><span class="line">|   |-- download_pix2pix_dataset.sh</span><br><span class="line">|   |-- generate_ccp_dataset.py</span><br><span class="line">|   |-- generate_coco_dataset.py</span><br><span class="line">|   |-- generate_mhp_dataset.py</span><br><span class="line">|   |-- make_dataset_aligned.py</span><br><span class="line">|   |-- pants2skirt_mhp</span><br><span class="line">|-- docs</span><br><span class="line">|   `-- more_results.md</span><br><span class="line">|-- environment.yml</span><br><span class="line">|-- models</span><br><span class="line">|   |-- __init__.py</span><br><span class="line">|   |-- base_model.py</span><br><span class="line">|   |-- cycle_gan_model.py</span><br><span class="line">|   |-- insta_gan_model.py</span><br><span class="line">|   |-- networks.py</span><br><span class="line">|   |-- pix2pix_model.py</span><br><span class="line">|   `-- test_model.py</span><br><span class="line">|-- options</span><br><span class="line">|   |-- __init__.py</span><br><span class="line">|   |-- base_options.py</span><br><span class="line">|   |-- test_options.py</span><br><span class="line">|   `-- train_options.py</span><br><span class="line">|-- requirements.txt</span><br><span class="line">|-- scripts</span><br><span class="line">|   |-- conda_deps.sh</span><br><span class="line">|   |-- download_cyclegan_model.sh</span><br><span class="line">|   |-- download_pix2pix_model.sh</span><br><span class="line">|   |-- install_deps.sh</span><br><span class="line">|   |-- test_before_push.py</span><br><span class="line">|   |-- test_cyclegan.sh</span><br><span class="line">|   |-- test_pix2pix.sh</span><br><span class="line">|   |-- test_single.sh</span><br><span class="line">|   |-- train_cyclegan.sh</span><br><span class="line">|   `-- train_pix2pix.sh</span><br><span class="line">|-- test.py</span><br><span class="line">|-- train.py</span><br><span class="line">`-- util</span><br><span class="line">    |-- __init__.py</span><br><span class="line">    |-- get_data.py</span><br><span class="line">    |-- html.py</span><br><span class="line">    |-- image_pool.py</span><br><span class="line">    |-- util.py</span><br><span class="line">    `-- visualizer.py</span><br></pre></td></tr></table></figure><h2 id="5-2-seg"><a href="#5-2-seg" class="headerlink" title="5.2 seg"></a>5.2 seg</h2><p>从下面的代码可以看出，需要读取固定数量的instance的segmentation。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># self.max_instances = 20</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_segs</span><span class="params">(self, seg_path, seed)</span>:</span></span><br><span class="line">  segs = list()</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(self.max_instances):</span><br><span class="line">    path = seg_path.replace(<span class="string">'.png'</span>, <span class="string">'_&#123;&#125;.png'</span>.format(i))</span><br><span class="line">    <span class="keyword">if</span> os.path.isfile(path):</span><br><span class="line">      seg = Image.open(path).convert(<span class="string">'L'</span>)</span><br><span class="line">      seg = self.fixed_transform(seg, seed)</span><br><span class="line">      segs.append(seg)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      segs.append(-torch.ones(segs[<span class="number">0</span>].size()))</span><br><span class="line">  <span class="keyword">return</span> torch.cat(segs)</span><br></pre></td></tr></table></figure><p><strong>备注</strong>: 原始图片transforms之后,0~1变成了-1~1; 分割图片transforms之后-1表示背景,取值-1~1,这也是为什么补充的时候用-1补充的原因.</p><h2 id="5-3-generator"><a href="#5-3-generator" class="headerlink" title="5.3 generator"></a>5.3 generator</h2><blockquote><p>ResNet generator is composed of downsampling blocks, residual blocks, and upsampling blocks. We used downsampling blocks and residual blocks for encoders, and used upsampling blocks for generators.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResnetSetGenerator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_nc=<span class="number">3</span>, output_nc=<span class="number">3</span>, ngf=<span class="number">64</span>, norm_layer=nn.InstanceNorm2d, use_dropout=False, n_blocks=<span class="number">9</span>, padding_type=<span class="string">'reflect'</span>)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> (n_blocks &gt;= <span class="number">0</span>)</span><br><span class="line">        super(ResnetSetGenerator, self).__init__()</span><br><span class="line">        self.input_nc = input_nc</span><br><span class="line">        self.output_nc = output_nc</span><br><span class="line">        self.ngf = ngf</span><br><span class="line">        <span class="keyword">if</span> type(norm_layer) == functools.partial:</span><br><span class="line">            use_bias = norm_layer.func == nn.InstanceNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_bias = norm_layer == nn.InstanceNorm2d</span><br><span class="line"></span><br><span class="line">        n_downsampling = <span class="number">2</span></span><br><span class="line">        self.encoder_img = self.get_encoder(input_nc, n_downsampling, ngf, norm_layer, use_dropout, n_blocks, padding_type, use_bias)</span><br><span class="line">        self.encoder_seg = self.get_encoder(<span class="number">1</span>, n_downsampling, ngf, norm_layer, use_dropout, n_blocks, padding_type, use_bias)</span><br><span class="line">        self.decoder_img = self.get_decoder(output_nc, n_downsampling, <span class="number">2</span> * ngf, norm_layer, use_bias)  <span class="comment"># 2*ngf</span></span><br><span class="line">        self.decoder_seg = self.get_decoder(<span class="number">1</span>, n_downsampling, <span class="number">3</span> * ngf, norm_layer, use_bias)  <span class="comment"># 3*ngf</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_encoder</span><span class="params">(self, input_nc, n_downsampling, ngf, norm_layer, use_dropout, n_blocks, padding_type, use_bias)</span>:</span></span><br><span class="line">        model = [nn.ReflectionPad2d(<span class="number">3</span>),</span><br><span class="line">                 nn.Conv2d(input_nc, ngf, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>, bias=use_bias),</span><br><span class="line">                 norm_layer(ngf),</span><br><span class="line">                 nn.ReLU(<span class="keyword">True</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_downsampling):</span><br><span class="line">            mult = <span class="number">2</span> ** i</span><br><span class="line">            model += [nn.Conv2d(ngf * mult, ngf * mult * <span class="number">2</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=use_bias),</span><br><span class="line">                      norm_layer(ngf * mult * <span class="number">2</span>),</span><br><span class="line">                      nn.ReLU(<span class="keyword">True</span>)]</span><br><span class="line"></span><br><span class="line">        mult = <span class="number">2</span> ** n_downsampling</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_blocks):</span><br><span class="line">            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_decoder</span><span class="params">(self, output_nc, n_downsampling, ngf, norm_layer, use_bias)</span>:</span></span><br><span class="line">        model = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_downsampling):</span><br><span class="line">            mult = <span class="number">2</span>**(n_downsampling - i)</span><br><span class="line">            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / <span class="number">2</span>), kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>, bias=use_bias),</span><br><span class="line">                      norm_layer(int(ngf * mult / <span class="number">2</span>)),</span><br><span class="line">                      nn.ReLU(<span class="keyword">True</span>)]</span><br><span class="line">        model += [nn.ReflectionPad2d(<span class="number">3</span>)]</span><br><span class="line">        model += [nn.Conv2d(ngf, output_nc, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>)]</span><br><span class="line">        model += [nn.Tanh()]</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inp)</span>:</span></span><br><span class="line">        <span class="comment"># split data</span></span><br><span class="line">        img = inp[:, :self.input_nc, :, :]  <span class="comment"># (B, CX, W, H)</span></span><br><span class="line">        segs = inp[:, self.input_nc:, :, :]  <span class="comment"># (B, CA, W, H)</span></span><br><span class="line">        mean = (segs + <span class="number">1</span>).mean(<span class="number">0</span>).mean(<span class="number">-1</span>).mean(<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">if</span> mean.sum() == <span class="number">0</span>:</span><br><span class="line">            mean[<span class="number">0</span>] = <span class="number">1</span>  <span class="comment"># forward at least one segmentation</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># run encoder</span></span><br><span class="line">        enc_img = self.encoder_img(img)</span><br><span class="line">        enc_segs = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(segs.size(<span class="number">1</span>)): <span class="comment"># 第i个instance</span></span><br><span class="line">            <span class="keyword">if</span> mean[i] &gt; <span class="number">0</span>:  <span class="comment"># skip empty segmentation</span></span><br><span class="line">                seg = segs[:, i, :, :].unsqueeze(<span class="number">1</span>)</span><br><span class="line">                enc_segs.append(self.encoder_seg(seg))</span><br><span class="line">        enc_segs = torch.cat(enc_segs)</span><br><span class="line">        enc_segs_sum = torch.sum(enc_segs, dim=<span class="number">0</span>, keepdim=<span class="keyword">True</span>)  <span class="comment"># aggregated set feature</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># run decoder</span></span><br><span class="line">        feat = torch.cat([enc_img, enc_segs_sum], dim=<span class="number">1</span>)</span><br><span class="line">        out = [self.decoder_img(feat)]</span><br><span class="line">        idx = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(segs.size(<span class="number">1</span>)):</span><br><span class="line">            <span class="keyword">if</span> mean[i] &gt; <span class="number">0</span>:</span><br><span class="line">                enc_seg = enc_segs[idx].unsqueeze(<span class="number">0</span>)  <span class="comment"># (1, ngf, w, h)</span></span><br><span class="line">                idx += <span class="number">1</span>  <span class="comment"># move to next index</span></span><br><span class="line">                feat = torch.cat([enc_seg, enc_img, enc_segs_sum], dim=<span class="number">1</span>)</span><br><span class="line">                out += [self.decoder_seg(feat)]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out += [segs[:, i, :, :].unsqueeze(<span class="number">1</span>)]  <span class="comment"># skip empty segmentation</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(out, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="5-4-Discriminator"><a href="#5-4-Discriminator" class="headerlink" title="5.4 Discriminator"></a>5.4 Discriminator</h2><blockquote><p>On the other hand, PatchGAN discriminator is composed of 5 convolutional layers, including normalization and non-linearity layers. We used the first 3 convolution layers for feature extractors, and the last 2 convolution layers for classifier.</p><p>In addition, we observed that applying Spectral Normalization (SN) (Miyato et al., 2018) for discriminators significantly improve the performance, although we used LSGAN (Mao et al., 2017), while the original motivation of SN was to enforce Lipschitz condition to match with the theory of WGAN (Arjovsky et al., 2017; Gulrajani et al., 2017).</p></blockquote><ul><li>[x] SpectralNorm 这个是怎么运行的？ <a href="http://www.twistedwg.com/2018/10/13/SNGAN.html" target="_blank" rel="noopener">http://www.twistedwg.com/2018/10/13/SNGAN.html</a></li></ul><p>虽然还是没有太搞懂其原理，但大致清楚了，是求矩阵的谱范数，因为难以求解，便用迭代的方式计算u、v。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define spectral normalization layer</span></span><br><span class="line"><span class="comment"># Code from Christian Cosgrove's repository</span></span><br><span class="line"><span class="comment"># https://github.com/christiancosgrove/pytorch-spectral-normalization-gan/blob/master/spectral_normalization.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2normalize</span><span class="params">(v, eps=<span class="number">1e-12</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> v / (v.norm() + eps)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpectralNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, module, name=<span class="string">'weight'</span>, power_iterations=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(SpectralNorm, self).__init__()</span><br><span class="line">        self.module = module</span><br><span class="line">        self.name = name</span><br><span class="line">        self.power_iterations = power_iterations</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._made_params():</span><br><span class="line">            self._make_params()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_u_v</span><span class="params">(self)</span>:</span></span><br><span class="line">        u = getattr(self.module, self.name + <span class="string">"_u"</span>)</span><br><span class="line">        v = getattr(self.module, self.name + <span class="string">"_v"</span>)</span><br><span class="line">        w = getattr(self.module, self.name + <span class="string">"_bar"</span>)</span><br><span class="line"></span><br><span class="line">        height = w.data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.power_iterations):</span><br><span class="line">            v.data = l2normalize(torch.mv(torch.t(w.view(height, <span class="number">-1</span>).data), u.data))</span><br><span class="line">            u.data = l2normalize(torch.mv(w.view(height, <span class="number">-1</span>).data, v.data))</span><br><span class="line"></span><br><span class="line">        sigma = u.dot(w.view(height, <span class="number">-1</span>).mv(v))</span><br><span class="line">        setattr(self.module, self.name, w / sigma.expand_as(w))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_made_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            u = getattr(self.module, self.name + <span class="string">"_u"</span>)</span><br><span class="line">            v = getattr(self.module, self.name + <span class="string">"_v"</span>)</span><br><span class="line">            w = getattr(self.module, self.name + <span class="string">"_bar"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">except</span> AttributeError:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_params</span><span class="params">(self)</span>:</span></span><br><span class="line">        w = getattr(self.module, self.name) <span class="comment"># shape: (64,3,4,4)</span></span><br><span class="line"></span><br><span class="line">        height = w.data.shape[<span class="number">0</span>]  <span class="comment"># int 64</span></span><br><span class="line">        width = w.view(height, <span class="number">-1</span>).data.shape[<span class="number">1</span>] <span class="comment"># int 48</span></span><br><span class="line"></span><br><span class="line">        u = nn.Parameter(w.data.new(height).normal_(<span class="number">0</span>, <span class="number">1</span>), requires_grad=<span class="keyword">False</span>) <span class="comment"># shape (64)</span></span><br><span class="line">        v = nn.Parameter(w.data.new(width).normal_(<span class="number">0</span>, <span class="number">1</span>), requires_grad=<span class="keyword">False</span>) <span class="comment"># shape (48)</span></span><br><span class="line">        u.data = l2normalize(u.data) </span><br><span class="line">        v.data = l2normalize(v.data)</span><br><span class="line">        w_bar = nn.Parameter(w.data)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">del</span> self.module._parameters[self.name]</span><br><span class="line"></span><br><span class="line">        self.module.register_parameter(self.name + <span class="string">"_u"</span>, u)</span><br><span class="line">        self.module.register_parameter(self.name + <span class="string">"_v"</span>, v)</span><br><span class="line">        self.module.register_parameter(self.name + <span class="string">"_bar"</span>, w_bar)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        self._update_u_v()</span><br><span class="line">        <span class="keyword">return</span> self.module.forward(*args)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NLayerSetDiscriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_nc, ndf=<span class="number">64</span>, n_layers=<span class="number">3</span>, norm_layer=nn.BatchNorm2d, use_sigmoid=False)</span>:</span></span><br><span class="line">        super(NLayerSetDiscriminator, self).__init__()</span><br><span class="line">        self.input_nc = input_nc</span><br><span class="line">        <span class="keyword">if</span> type(norm_layer) == functools.partial:</span><br><span class="line">            use_bias = norm_layer.func == nn.InstanceNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_bias = norm_layer == nn.InstanceNorm2d</span><br><span class="line"></span><br><span class="line">        kw = <span class="number">4</span></span><br><span class="line">        padw = <span class="number">1</span></span><br><span class="line">        self.feature_img = self.get_feature_extractor(input_nc, ndf, n_layers, kw, padw, norm_layer, use_bias)</span><br><span class="line">        self.feature_seg = self.get_feature_extractor(<span class="number">1</span>, ndf, n_layers, kw, padw, norm_layer, use_bias)</span><br><span class="line">        self.classifier = self.get_classifier(<span class="number">2</span> * ndf, n_layers, kw, padw, norm_layer, use_sigmoid)  <span class="comment"># 2*ndf</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_feature_extractor</span><span class="params">(self, input_nc, ndf, n_layers, kw, padw, norm_layer, use_bias)</span>:</span></span><br><span class="line">        model = [</span><br><span class="line">            <span class="comment"># Use spectral normalization</span></span><br><span class="line">            SpectralNorm(nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=<span class="number">2</span>, padding=padw)),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">        ]</span><br><span class="line">        nf_mult = <span class="number">1</span></span><br><span class="line">        nf_mult_prev = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">            nf_mult_prev = nf_mult</span><br><span class="line">            nf_mult = min(<span class="number">2</span> ** n, <span class="number">8</span>)</span><br><span class="line">            model += [</span><br><span class="line">                <span class="comment"># Use spectral normalization</span></span><br><span class="line">                SpectralNorm(nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=<span class="number">2</span>, padding=padw, bias=use_bias)),</span><br><span class="line">                norm_layer(ndf * nf_mult),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">            ]</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_classifier</span><span class="params">(self, ndf, n_layers, kw, padw, norm_layer, use_sigmoid)</span>:</span></span><br><span class="line">        nf_mult_prev = min(<span class="number">2</span> ** (n_layers<span class="number">-1</span>), <span class="number">8</span>)</span><br><span class="line">        nf_mult = min(<span class="number">2</span> ** n_layers, <span class="number">8</span>)</span><br><span class="line">        model = [</span><br><span class="line">            <span class="comment"># Use spectral normalization</span></span><br><span class="line">            SpectralNorm(nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=<span class="number">1</span>, padding=padw)),</span><br><span class="line">            norm_layer(ndf * nf_mult),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">        ]</span><br><span class="line">        <span class="comment"># Use spectral normalization</span></span><br><span class="line">        model += [SpectralNorm(nn.Conv2d(ndf * nf_mult, <span class="number">1</span>, kernel_size=kw, stride=<span class="number">1</span>, padding=padw))]</span><br><span class="line">        <span class="keyword">if</span> use_sigmoid:</span><br><span class="line">            model += [nn.Sigmoid()]</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inp)</span>:</span></span><br><span class="line">        <span class="comment"># split data</span></span><br><span class="line">        img = inp[:, :self.input_nc, :, :]  <span class="comment"># (B, CX, W, H)</span></span><br><span class="line">        segs = inp[:, self.input_nc:, :, :]  <span class="comment"># (B, CA, W, H)</span></span><br><span class="line">        mean = (segs + <span class="number">1</span>).mean(<span class="number">0</span>).mean(<span class="number">-1</span>).mean(<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">if</span> mean.sum() == <span class="number">0</span>:</span><br><span class="line">            mean[<span class="number">0</span>] = <span class="number">1</span>  <span class="comment"># forward at least one segmentation</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># run feature extractor</span></span><br><span class="line">        feat_img = self.feature_img(img)</span><br><span class="line">        feat_segs = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(segs.size(<span class="number">1</span>)):  <span class="comment"># 第i个instance</span></span><br><span class="line">            <span class="keyword">if</span> mean[i] &gt; <span class="number">0</span>:  <span class="comment"># skip empty segmentation</span></span><br><span class="line">                seg = segs[:, i, :, :].unsqueeze(<span class="number">1</span>)</span><br><span class="line">                feat_segs.append(self.feature_seg(seg))</span><br><span class="line">        feat_segs_sum = torch.sum(torch.stack(feat_segs), dim=<span class="number">0</span>)  <span class="comment"># aggregated set feature</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># run classifier</span></span><br><span class="line">        feat = torch.cat([feat_img, feat_segs_sum], dim=<span class="number">1</span>)</span><br><span class="line">        out = self.classifier(feat)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="5-5-model-的输入"><a href="#5-5-model-的输入" class="headerlink" title="5.5 model 的输入"></a>5.5 model 的输入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_input</span><span class="params">(self, input)</span>:</span></span><br><span class="line">    AtoB = self.opt.direction == <span class="string">'AtoB'</span></span><br><span class="line">    self.real_A_img = input[<span class="string">'A'</span> <span class="keyword">if</span> AtoB <span class="keyword">else</span> <span class="string">'B'</span>].to(self.device)</span><br><span class="line">    self.real_B_img = input[<span class="string">'B'</span> <span class="keyword">if</span> AtoB <span class="keyword">else</span> <span class="string">'A'</span>].to(self.device)</span><br><span class="line">    real_A_segs = input[<span class="string">'A_segs'</span> <span class="keyword">if</span> AtoB <span class="keyword">else</span> <span class="string">'B_segs'</span>]</span><br><span class="line">    real_B_segs = input[<span class="string">'B_segs'</span> <span class="keyword">if</span> AtoB <span class="keyword">else</span> <span class="string">'A_segs'</span>]</span><br><span class="line">    self.real_A_segs = self.select_masks(real_A_segs).to(self.device)  <span class="comment"># shape:(1,4,240,160)</span></span><br><span class="line">    self.real_B_segs = self.select_masks(real_B_segs).to(self.device)</span><br><span class="line">    self.real_A = torch.cat([self.real_A_img, self.real_A_segs], dim=<span class="number">1</span>) <span class="comment"># shape:(1,7,240,160)</span></span><br><span class="line">    self.real_B = torch.cat([self.real_B_img, self.real_B_segs], dim=<span class="number">1</span>)</span><br><span class="line">    self.real_A_seg = self.merge_masks(self.real_A_segs)  <span class="comment"># merged mask</span></span><br><span class="line">    self.real_B_seg = self.merge_masks(self.real_B_segs)  <span class="comment"># merged mask</span></span><br><span class="line">    self.image_paths = input[<span class="string">'A_paths'</span> <span class="keyword">if</span> AtoB <span class="keyword">else</span> <span class="string">'B_paths'</span>]</span><br></pre></td></tr></table></figure><p>前面说过，每次都生成20个mask，不足用-1补充，在输入网络时，只取面积最大的4个mask，然后对这4个进行或者从高到低排序或者随机排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ins_max = 4</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_masks_random</span><span class="params">(self, segs_batch)</span>:</span></span><br><span class="line">  <span class="string">"""Select masks in random order"""</span></span><br><span class="line">  ret = list()</span><br><span class="line">  <span class="keyword">for</span> segs <span class="keyword">in</span> segs_batch:</span><br><span class="line">    mean = (segs + <span class="number">1</span>).mean(<span class="number">-1</span>).mean(<span class="number">-1</span>)</span><br><span class="line">    m, i = mean.topk(self.opt.ins_max)</span><br><span class="line">    num = min(len(mean.nonzero()), self.opt.ins_max)</span><br><span class="line">    reorder = np.concatenate((np.random.permutation(num), np.arange(num, self.opt.ins_max)))</span><br><span class="line">    ret.append(segs[i[reorder], :, :])</span><br><span class="line">  <span class="keyword">return</span> torch.stack(ret)</span><br></pre></td></tr></table></figure><p>这里的mask的合并没有太看懂，是为了去除(-1,1)之外的数字吗？</p><p>跑了代码,觉得是的,或许是担心有其他干扰因素吧,反正剩下的都是-1~1之间的数字.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_masks</span><span class="params">(self, segs)</span>:</span></span><br><span class="line">  <span class="string">"""Merge masks (B, N, W, H) -&gt; (B, 1, W, H)"""</span></span><br><span class="line">  ret = torch.sum((segs + <span class="number">1</span>)/<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)  <span class="comment"># (B, 1, W, H)</span></span><br><span class="line">  <span class="keyword">return</span> ret.clamp(max=<span class="number">1</span>, min=<span class="number">0</span>) * <span class="number">2</span> - <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li>[ ] 这一步的意义是什么？？</li></ul><p>理解了，如果图片中没有instance，那么就不用进行下一步的转换了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.forward_A = (self.real_A_seg_sng + <span class="number">1</span>).sum() &gt; <span class="number">0</span>  <span class="comment"># check if there are remaining instances</span></span><br><span class="line">self.forward_B = (self.real_B_seg_sng + <span class="number">1</span>).sum() &gt; <span class="number">0</span>  <span class="comment"># check if there are remaining instances</span></span><br></pre></td></tr></table></figure><ul><li>[x] fake_B_mul的意义是什么？</li></ul><p>因为在sequential mini-batch translation中，GAN_loss是全局的，所以每次需要把之前的fake_B_seg_sng保存起来一起计算，因此每次的临时的self.fake_B_mul，而self.fake_B_seg_list保存是mini-batch计算得到的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.forward_A:</span><br><span class="line">    self.real_A_sng = torch.cat([self.real_A_img_sng, self.real_A_seg_sng], dim=<span class="number">1</span>)</span><br><span class="line">    self.fake_B_sng = self.netG_A(self.real_A_sng)</span><br><span class="line">    self.rec_A_sng = self.netG_B(self.fake_B_sng)</span><br><span class="line"></span><br><span class="line">    self.fake_B_img_sng, self.fake_B_seg_sng = self.split(self.fake_B_sng)</span><br><span class="line">    self.rec_A_img_sng, self.rec_A_seg_sng = self.split(self.rec_A_sng)</span><br><span class="line">    fake_B_seg_list = self.fake_B_seg_list + [self.fake_B_seg_sng]  <span class="comment"># not detach</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.ins_iter - idx - <span class="number">1</span>):</span><br><span class="line">        fake_B_seg_list.append(empty)</span><br><span class="line"></span><br><span class="line">    self.fake_B_seg_mul = torch.cat(fake_B_seg_list, dim=<span class="number">1</span>)</span><br><span class="line">    self.fake_B_mul = torch.cat([self.fake_B_img_sng, self.fake_B_seg_mul], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>[x] 怎么选取的背景</li></ul><p>只要在A中且在B中都是背景的则都算是背景，否则只要有instance的区域不为背景。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_masks</span><span class="params">(self, segs)</span>:</span></span><br><span class="line">    <span class="string">"""Merge masks (B, N, W, H) -&gt; (B, 1, W, H)"""</span></span><br><span class="line">    <span class="comment"># segs: shape(1,4,240,260)， 取值(-1~1) 训练集A中有两个instance，训练集B中有两个instance，</span></span><br><span class="line">    ret = torch.sum((segs + <span class="number">1</span>)/<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)  <span class="comment"># (B, 1, W, H)</span></span><br><span class="line">    <span class="keyword">return</span> ret.clamp(max=<span class="number">1</span>, min=<span class="number">0</span>) * <span class="number">2</span> - <span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_masks</span><span class="params">(self, segs)</span>:</span></span><br><span class="line">    <span class="string">"""Merge masks (B, N, W, H) -&gt; (B, 1, W, H)"""</span></span><br><span class="line">    ret = torch.sum((segs + <span class="number">1</span>)/<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)  <span class="comment"># (B, 1, W, H)</span></span><br><span class="line">    <span class="keyword">return</span> ret.clamp(max=<span class="number">1</span>, min=<span class="number">0</span>) * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight_for_ctx</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    <span class="string">"""Get weight for context preserving loss"""</span></span><br><span class="line">    z = self.merge_masks(torch.cat([x, y], dim=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> - z) / <span class="number">2</span>  <span class="comment"># [-1,1] -&gt; [1,0]</span></span><br></pre></td></tr></table></figure><ul><li>[ ] 这里的empty的作用是什么</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">empty = -torch.ones(self.real_A_seg_sng.size()).to(self.device)</span><br></pre></td></tr></table></figure><ul><li><p>[ ] pix2pix 是怎么预测mask的，需要提前训练吗，数据集怎么提供？如果可以直接用，那么是否可以直接实现行人重识别的换人？</p></li><li><p>[x] 论文+代码，共4天</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;实例转换&lt;/p&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>deep-learning-network</title>
    <link href="http://yoursite.com/2019/01/03/deep-learning-network/"/>
    <id>http://yoursite.com/2019/01/03/deep-learning-network/</id>
    <published>2019-01-03T02:43:38.000Z</published>
    <updated>2019-01-04T01:57:14.995Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>最近看了一些代码，发现大家的代码风格互相不一样，且没有一个统一的风格，所以抽象一个大致流程，用于更快地熟悉代码。</p><a id="more"></a><h1 id="1-神经网络"><a href="#1-神经网络" class="headerlink" title="1. 神经网络"></a>1. 神经网络</h1><p>从pytorch的代码中看，可以发现发现这么几个阶段。</p><h2 id="1-1-数据dataset"><a href="#1-1-数据dataset" class="headerlink" title="1.1 数据dataset"></a>1.1 数据dataset</h2><p>这一阶段根据具体需求可以分为离线预处理、在线预处理，主要是对数据预处理，包括不限于图片的处理，名字的处理等等。</p><p>代表代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">xxx</span><span class="params">()</span>:</span></span><br><span class="line">    __getitem__(index):</span><br><span class="line"></span><br><span class="line">data = data.Dataloader()</span><br></pre></td></tr></table></figure><h2 id="1-2-神经网络"><a href="#1-2-神经网络" class="headerlink" title="1.2 神经网络"></a>1.2 神经网络</h2><p>主要分为：</p><ul><li>搭建网络</li><li>输入</li><li>损失函数</li><li>反向传播</li></ul><p>其中关键的是搭建网络和损失函数。</p><p>有些代码会对神经网络的输入再次处理，有些代码会将输入、损失函数、反向传播重新写个方法或者类，类似trainer。</p><h2 id="1-3-可视化"><a href="#1-3-可视化" class="headerlink" title="1.3 可视化"></a>1.3 可视化</h2><p>可视化方法包括不限于visdom、tensorboard、html、输出重定向。</p><p>loss的获取，用一个dict的loss记录。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;最近看了一些代码，发现大家的代码风格互相不一样，且没有一个统一的风格，所以抽象一个大致流程，用于更快地熟悉代码。&lt;/p&gt;
    
    </summary>
    
      <category term="deep-learning" scheme="http://yoursite.com/categories/deep-learning/"/>
    
    
      <category term="deep-learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="network" scheme="http://yoursite.com/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>SEIGAN</title>
    <link href="http://yoursite.com/2018/12/27/SEIGAN/"/>
    <id>http://yoursite.com/2018/12/27/SEIGAN/</id>
    <published>2018-12-27T08:57:16.000Z</published>
    <updated>2018-12-28T09:11:52.292Z</updated>
    
    <content type="html"><![CDATA[<p>论文分享</p><a id="more"></a><h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p><a href="https://arxiv.org/abs/1811.07630" target="_blank" rel="noopener">SEIGAN: Towards Compositional Image Generation by Simultaneously Learning to Segment, Enhance, and Inpaint.</a></p><p>Pavel Ostyakov, Roman Suvorov, Elizaveta Logacheva1 Oleg Khomenko, Sergey I. Nikolenko</p><p>一作Pavel Ostyakov是莫斯科三星AI中心的人员，在 Kaggle Cdiscount’s Image Classification Challenge 比赛中获得第一名，<a href="https://www.youtube.com/watch?v=Mw2vdYv4ups" target="_blank" rel="noopener">YouTube 视频</a>，github:<a href="https://github.com/PavelOstyakov?tab=repositories" target="_blank" rel="noopener">https://github.com/PavelOstyakov?tab=repositories</a>，他参加的 Kaggle 比赛多一点。</p><p>论文代码还没有公布。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p><img src="./SEIGAN/SEIGAN1.png" alt="实验效果图"><br><img src="/2018/12/27/SEIGAN/SEIGAN1.png" title="实验效果图"></p><p>要解决的问题是图像合成，一共三件事：<strong>剪切、贴图、修复</strong>。即如何把一张图片的物体剪切下来，然后贴到另一张背景图上，并且补全被剪掉的区域。</p><p>并且实验证明，结果不仅合成的新图片逼真，而且分割也做得很好。</p><blockquote><p>This process consists of three basic operations: (1) cut, extracting an object from image; (2) paste and enhance, making the pasted object appear natural in the new context; (3) inpaint, restoring the background after cutting out an object</p></blockquote><p><strong>关键词</strong>：语义分割、贴图、去目标、补全、修复、GAN、image generation、图像合成</p><p>备注：训练过程不需要成对图片。</p><p>相似工作：<a href="https://arxiv.org/abs/1803.06414" target="_blank" rel="noopener">Learning to segment via cut-and-paste</a></p><p>通过作者的论述，思想相同的地方很多。</p><h1 id="2-Methods"><a href="#2-Methods" class="headerlink" title="2. Methods"></a>2. Methods</h1><h2 id="2-1-Problem-Setting-and-General-Pipeline"><a href="#2-1-Problem-Setting-and-General-Pipeline" class="headerlink" title="2.1 Problem Setting and General Pipeline"></a>2.1 Problem Setting and General Pipeline</h2><p><strong>字符表示</strong>：</p><div class="table-container"><table><thead><tr><th style="text-align:center">字符</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">$x=<o,b_x>$</o,b_x></td><td style="text-align:center">图片$x$，由目标$O$和背景$B_x$组成</td></tr><tr><td style="text-align:center">$y=&lt;\emptyset,B_y&gt;$</td><td style="text-align:center">图片$y$，包含背景$B_y$，没有目标</td></tr><tr><td style="text-align:center">$Y={&lt;\emptyset,B_y&gt;}_{y\in Y}$</td><td style="text-align:center">背景数据集</td></tr><tr><td style="text-align:center">$X={<o_x,b_x>}_{x\in X}$</o_x,b_x></td><td style="text-align:center">不同物体在不同背景的图片</td></tr><tr><td style="text-align:center">$m$</td><td style="text-align:center">目标$O$在图片$x$上的mask $m$</td></tr><tr><td style="text-align:center">$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$</td><td style="text-align:center">去除目标只剩背景$B_x$并修复后的图片</td></tr><tr><td style="text-align:center">$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$</td><td style="text-align:center">将目标粘贴到背景$B_y$并增强后的图片</td></tr></tbody></table></div><p><strong>问题定义</strong>：</p><p>对于任意一组图片$x=<o,b_x>$和$y=&lt;\emptyset,B_y&gt;$，利用模型将其变成$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$和$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$，其中$\hat{B}_x\approx B_x$，$\hat{B}_y\approx B_y$和$\hat{O}\approx O$.</o,b_x></p><p><strong>步骤分解</strong>：</p><ul><li>分割(segmentation)：预测图片$x=<o,b_x>$的分割掩码$m=Mask(x)$，并且做简单的粘贴$z=m\odot x+(1-m)\odot y$，$\odot$我的理解是对应元素的乘法。</o,b_x></li></ul><blockquote><p>$\odot$ denotes componentwise multiplication;</p></blockquote><ul><li><p>增强(enhancement)：对于$z$，做进一步图片增强，使其更自然，得到$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$</p></li><li><p>修复(inpainting)：对于去除目标后的图片$(1-m)\odot x$，修复成$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$</p></li></ul><p><img src="./SEIGAN/SEIGAN2.png" alt="网络架构"><br><img src="/2018/12/27/SEIGAN/SEIGAN2.png" title="网络架构"></p><p>其中 swap network 就是 segmentation network 和 enhancement network 的组合。</p><ul><li>类似CycleGAN，网络架构应用两次，第一次得到$\hat{x}$和$\hat{y}$，第二次得到$\hat{\hat{x}}$和$\hat{\hat{y}}$</li><li>$G_{seg}$：Fig.3.right，输入$x=<o,b_x>$，输出$m=Mask(x)$</o,b_x></li><li>$G_{inp}$：Fig.3.left，输入$(1-m)\odot x$，输出$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$</li><li>$G_{enh}$：Fig.3.left，输入$z=m\odot x+(1-m)\odot y$和噪声，输出$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$</li><li>$D_{bg}$：背景判别器，判断真背景($y$)和假背景($\hat{x}$、$\hat{\hat{y}}$)，真为1，假为0</li><li>$D_{obj}$：目标判别器，判断背景和目标是一体的($x$)和背景和目标是合成的($z$,$\hat{y}$)，一体的为1，合成的为0.</li></ul><h2 id="2-2-The-inpainting-Network-G-inp"><a href="#2-2-The-inpainting-Network-G-inp" class="headerlink" title="2.2 The inpainting Network $G_{inp}$"></a>2.2 The inpainting Network $G_{inp}$</h2><p><strong>背景对抗损失函数</strong>:</p><script type="math/tex; mode=display">l^{GAN}\_{inp}=(1-D_{bg}(\hat{x})),l^{GAN}\_{inp2}=(1-D_{bg}(\hat{\hat{y}}))</script><script type="math/tex; mode=display">l^{GAN}_{bg}=\lambda_1 l^{GAN}\_{inp} + \lambda_2 l^{GAN}\_{inp2}</script><p><strong>背景重构损失函数</strong>：</p><script type="math/tex; mode=display">l^{texture}_{bg}=|Gram(VGG_1(y))-Gram(VGG_1(\hat{\hat{y}}))|</script><script type="math/tex; mode=display">l^{perc}_{bg}=|VGG_2(y)-VGG_2(\hat{\hat{y}})|</script><script type="math/tex; mode=display">l^{MAE}\_{bg}=|y-\hat{\hat{y}}|</script><script type="math/tex; mode=display">l^{rec}_{bg}=\lambda_3 l^{texture}_{bg}+\lambda_4 l^{perc}_{bg}+\lambda_5l^{MAE}_{bg}</script><p>其中，$VGG_1(y)$表示VGG19的前五层，$VGG_2(y)$表示VGG19的后五层</p><p><strong>$G_{inp}$ 的网络架构</strong>:</p><p><img src="./SEIGAN/SEIGAN3.png" alt="$G_{inp}$ 的网络架构"><br><img src="/2018/12/27/SEIGAN/SEIGAN3.png" title="$G_{inp}$ 的网络架构"></p><p>其实没太看懂这个网络，可能还是需要有代码。</p><h2 id="2-3-The-Swap-Network"><a href="#2-3-The-Swap-Network" class="headerlink" title="2.3 The Swap Network"></a>2.3 The Swap Network</h2><p>The swap network = the segmentation network + the enhancement network</p><p><strong>目标重构损失函数</strong>：</p><script type="math/tex; mode=display">l^{perc}_{obj}=|VGG_2(x)-VGG_2(\hat{\hat{x}})|</script><script type="math/tex; mode=display">l^{MAE}\_{obj}=|x-\hat{\hat{x}}|</script><script type="math/tex; mode=display">l^{rec}_{obj}=\lambda_6 l^{perc}_{obj}+\lambda_7 l^{MAE}_{obj}</script><p><strong>目标对抗损失函数</strong>：</p><script type="math/tex; mode=display">l^{GAN}\_{coarse}=(1-D_{obj}(z))^2</script><script type="math/tex; mode=display">l^{GAN}\_{enh}=(1-D_{obj}(\hat{y}))^2</script><script type="math/tex; mode=display">l^{GAN}_{obj}=\lambda_8 l^{GAN}_{coarse} + \lambda_9 l^{GAN}_{enh}</script><p><strong>identity loss</strong>：</p><script type="math/tex; mode=display">l^{id}\_{bg}=|G_{inp}((1-G_{seg}(y))\odot y)-y|</script><script type="math/tex; mode=display">l^{id}\_{obj}=|G_{enh}(x)-x|</script><script type="math/tex; mode=display">l^{id}=\lambda_{10}l^{id}\_{obj} + \lambda_{11}l^{id}_{bg}</script><h2 id="2-4-Total-Loss-Funcion-Remarks-and-Network-Architectures"><a href="#2-4-Total-Loss-Funcion-Remarks-and-Network-Architectures" class="headerlink" title="2.4 Total Loss Funcion, Remarks, and Network Architectures"></a>2.4 Total Loss Funcion, Remarks, and Network Architectures</h2><p><strong>The Generator Loss</strong>:</p><script type="math/tex; mode=display">\begin{split}l=\lambda_1 l^{GAN}\_{inp} + \lambda_2 l^{GAN}\_{inp2} + \lambda_4 l^{perc}_{bg} + \lambda_5l^{MAE}_{bg} + \lambda_6 l^{perc}_{obj} + \\\lambda_7 l^{MAE}_{obj} + \lambda_8 l^{GAN}_{coarse} + \lambda_9 l^{GAN}\_{enh} + \lambda_{10}l^{id}\_{obj} + \lambda_{11}l^{id}_{bg}\end{split}</script><p><strong>The Discriminator Loss</strong>:</p><script type="math/tex; mode=display">l^{disc}\_{bg}=(1-D_{bg}(y))^2+\frac{1}{2}D_{bg}(\hat{x})^2+\frac{1}{2}D_{bg}(\hat{\hat{y}})^2</script><script type="math/tex; mode=display">l^{disc}\_{obj}=(1-D_{obj}(x))^2+\frac{1}{2}D_{obj}(\hat{y})^2+\frac{1}{2}D_{obj}(z)^2</script><p><strong>Remarks</strong>：</p><ol><li>a pool of fake images，类似Cycle GAN也有。</li><li>对于不同大小和比例的图片$x,y$，作者使用了增强网络</li><li>texture loss $l^{rec}_{bg}$ 比 threshold 作用在 mask m 上的效果更好</li></ol><blockquote><p>In our setup this problem is addressed by a separate enhancement network, so we have fewer limitations when looking for appropriate training data.</p></blockquote><h1 id="3-Experimental-evaluation"><a href="#3-Experimental-evaluation" class="headerlink" title="3. Experimental evaluation"></a>3. Experimental evaluation</h1><p>实验性能主要分为两个：</p><ol><li>生成图片的主观真实性</li><li>生成分割掩码的准确性</li></ol><p><strong>生成图片的主观真实性</strong>：<br><img src="./SEIGAN/SEIGAN4.png" alt="生成图片的主观真实性"><br><img src="/2018/12/27/SEIGAN/SEIGAN4.png" title="生成图片的主观真实性"></p><p>从Fig.4.left可以简单地看出，Full的效果还是很明显的。</p><p><strong>生成分割掩码的准确性</strong>：<br><img src="./SEIGAN/SEIGAN5.png" alt="生成分割掩码的准确性"><br><img src="/2018/12/27/SEIGAN/SEIGAN5.png" title="生成分割掩码的准确性"></p><p><strong>实验效果</strong>：<br><img src="./SEIGAN/SEIGAN6.png" alt="实验效果"><br><img src="/2018/12/27/SEIGAN/SEIGAN6.png" title="实验效果"></p><h1 id="4-Future"><a href="#4-Future" class="headerlink" title="4. Future"></a>4. Future</h1><p>这个方法是否可以用在两个图片的目标对换呢？如果用在跨数据集的行人重识别上，那么是否可以将源数据集的行人粘贴在目标数据集上呢？好像有一个GAN就是类似的.</p><p><a href="https://arxiv.org/abs/1711.08565" target="_blank" rel="noopener">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文分享&lt;/p&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>GPU</title>
    <link href="http://yoursite.com/2018/12/26/GPU/"/>
    <id>http://yoursite.com/2018/12/26/GPU/</id>
    <published>2018-12-26T07:14:08.000Z</published>
    <updated>2019-03-02T03:49:26.703Z</updated>
    
    <content type="html"><![CDATA[<p>主要记录从安装显卡驱动到cudnn的过程。</p><a id="more"></a><h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>这篇博客主要记录自己安装显卡驱动到cudnn的过程，只能当作工作记录，不可以作为参考手册。</p><p>整个过程主要分为：</p><ul><li>显卡驱动</li><li>cuda</li><li>cudnn</li><li>ananconda</li><li>pytorch</li><li>tensorflow</li><li>matlab</li><li>分辨率</li><li>cuda、cudnn卸载升级</li></ul><h1 id="1-显卡驱动"><a href="#1-显卡驱动" class="headerlink" title="1. 显卡驱动"></a>1. 显卡驱动</h1><p>显卡驱动下载:<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">https://www.nvidia.cn/Download/index.aspx?lang=cn</a></p><p>官方参考手册:</p><p><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4RD7GVh1d" target="_blank" rel="noopener">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4RD7GVh1d</a></p><h2 id="1-1-删除已经安装的nvidia驱动-对应官方参考手册2-7"><a href="#1-1-删除已经安装的nvidia驱动-对应官方参考手册2-7" class="headerlink" title="1.1 删除已经安装的nvidia驱动(对应官方参考手册2.7)"></a>1.1 删除已经安装的nvidia驱动(对应官方参考手册2.7)</h2><p>如果已经安装过nvidia驱动没有成功或者接下来的过程中发生驱动安装错误的情况，则删除已经安装的nvidia驱动(对应官方参考手册2.7)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get --purge remove nvidia-*</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">sudo apt-get autoremove --purge nvidia-*</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><h2 id="1-2-禁止自带的-nouveau-nvidia-驱动-官方参考手册4-3"><a href="#1-2-禁止自带的-nouveau-nvidia-驱动-官方参考手册4-3" class="headerlink" title="1.2 禁止自带的 nouveau nvidia 驱动(官方参考手册4.3)"></a>1.2 禁止自带的 nouveau nvidia 驱动(官方参考手册4.3)</h2><h3 id="1-2-1-判断-nouveau-nvidia-驱动是否被禁止"><a href="#1-2-1-判断-nouveau-nvidia-驱动是否被禁止" class="headerlink" title="1.2.1 判断 nouveau nvidia 驱动是否被禁止"></a>1.2.1 判断 nouveau nvidia 驱动是否被禁止</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure><p>有输出表示没有禁止，需要执行1.2.2；没有输出表示禁止成功，可以跳过1.2.2，执行1.3。</p><h3 id="1-2-2-禁止-nouveau-nvidia-驱动"><a href="#1-2-2-禁止-nouveau-nvidia-驱动" class="headerlink" title="1.2.2 禁止 nouveau nvidia 驱动"></a>1.2.2 禁止 nouveau nvidia 驱动</h3><p>创建文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/modprobe.d/blacklist-nouveau.conf</span><br></pre></td></tr></table></figure><p>在文件中写入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=<span class="number">0</span></span><br></pre></td></tr></table></figure><p>更新</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-initramfs -u</span><br></pre></td></tr></table></figure><p>判断</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure><p>有输出表示没有禁止，需要重新执行1.2.2；没有输出表示禁止成功，重启电脑。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><h2 id="1-3-安装-nvidia-驱动"><a href="#1-3-安装-nvidia-驱动" class="headerlink" title="1.3 安装 nvidia 驱动"></a>1.3 安装 nvidia 驱动</h2><p>根据机器型号下载相应的驱动，名称如：NVIDIA-Linux-x86_64-410.78.run。</p><p>重启电脑后或者在桌面环境后，切换到tty1文本模式:ctrl+alt+F1</p><h3 id="1-3-1-关闭-x-windows-服务"><a href="#1-3-1-关闭-x-windows-服务" class="headerlink" title="1.3.1 关闭 x-windows 服务"></a>1.3.1 关闭 x-windows 服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm stop</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">sudo /etc/init.d/lightdm stop</span><br></pre></td></tr></table></figure><h3 id="1-3-2-安装驱动"><a href="#1-3-2-安装驱动" class="headerlink" title="1.3.2 安装驱动"></a>1.3.2 安装驱动</h3><p>先按照 1.3.2.a 安装驱动，在安装驱动的过程中，如果出现循环登陆的情况，按照 1.3.2.b 安装驱动。</p><h4 id="1-3-2-a-正常安装驱动"><a href="#1-3-2-a-正常安装驱动" class="headerlink" title="1.3.2.a 正常安装驱动"></a>1.3.2.a 正常安装驱动</h4><p>1.3.2.a.1 安装并测试</p><p><strong>安装</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo sh ./NVIDIA-Linux-x86_64-410.78.run</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">sudo bash ./NVIDIA-Linux-x86_64-410.78.run</span><br></pre></td></tr></table></figure><p>在弹出来的选项中，选择默认的即可。<br>好像是32-bit那个选否，core为否。</p><p>执行 1.3.3。</p><h4 id="1-3-2-b-循环登陆时安装驱动"><a href="#1-3-2-b-循环登陆时安装驱动" class="headerlink" title="1.3.2.b 循环登陆时安装驱动"></a>1.3.2.b 循环登陆时安装驱动</h4><p>一般而言我们安装的ubuntu 的显示器并没有接到nvidia的显卡上，而是使用了intel的集显。我们安装驱动其实只是想将我们运算的显卡的驱动更新，结果都给搞了，所以产生了冲突。当然，也可能是opengl产生的冲突。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get --purge remove nvidia-*</span><br><span class="line"><span class="meta">#</span> ctrl+alt+F1</span><br><span class="line"><span class="meta">#</span> 可以考虑适当关机</span><br><span class="line">sudo service lightdm stop</span><br><span class="line">sudo sh ./NVIDIA-Linux-x86_64-410.78.run –no-x-check –no-nouveau-check –no-opengl-files</span><br></pre></td></tr></table></figure><ul><li>–no-x-check 安装驱动时关闭X服务</li><li>–no-nouveau-check 安装驱动时禁用nouveau</li><li>–no-opengl-files 只安装驱动文件，不安装OpenGL文件</li></ul><p>在弹出来的选项中，选择默认的即可。</p><p>执行1.3.3</p><h3 id="1-3-3-测试"><a href="#1-3-3-测试" class="headerlink" title="1.3.3 测试"></a>1.3.3 测试</h3><p><strong>测试:</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-smi # 有输出表示成功，没有输出表示不成功，重新下载安装。</span><br></pre></td></tr></table></figure><p>有输出表示成功，执行 启动 <strong>x-windows 服务</strong> 及其之后的命令。 </p><p>没有输出表示<strong>失败</strong>，重新执行 1.1 、1.3.1 和 1.3.2.a.1.即：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get --purge remove nvidia-*</span><br><span class="line"><span class="meta">#</span> ctrl+alt+F1</span><br><span class="line">sudo service lightdm stop</span><br><span class="line">sudo sh ./NVIDIA-Linux-x86_64-410.78.run</span><br></pre></td></tr></table></figure><p>如果几次都不成功，则验证下载的文件是否完整</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo md5sum sudo service lightdm stop</span><br><span class="line"><span class="meta">#</span> 有输出表示完整，没有输出表示不完整</span><br></pre></td></tr></table></figure><p><strong>启动 x-windows 服务</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm start</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">sudo /etc/init.d/lightdm start</span><br></pre></td></tr></table></figure><p>进入图形桌面: ctrl+alt+F7</p><p>如果正常登陆，继续下一个命令，如果出现重复登陆，则按照 1.3.2.b 安装驱动。</p><p><strong>二次测试驱动(可以不需要)</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-smi # 有输出表示成功，没有输出表示不成功</span><br><span class="line">sudo nvidia-settings # 有输出或者没有输出但是smi有输出都表示成功，反之表示不成功</span><br></pre></td></tr></table></figure><h1 id="2-CUDA"><a href="#2-CUDA" class="headerlink" title="2. CUDA"></a>2. CUDA</h1><h2 id="2-1-安装文件"><a href="#2-1-安装文件" class="headerlink" title="2.1 安装文件"></a>2.1 安装文件</h2><p><strong>提前安装某些文件</strong>:</p><p>gcc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 验证gcc</span><br><span class="line">gcc --version</span><br><span class="line"><span class="meta">#</span> 如果有输出，表示gcc安装成功，进行下一步，如果没有输出，需要安装gcc</span><br><span class="line"><span class="meta">#</span> 安装gcc命令</span><br><span class="line">sudo apt-get install gcc</span><br></pre></td></tr></table></figure><p>内核(官方文档2.4)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install linux-headers-$(uname -r)</span><br></pre></td></tr></table></figure><p><strong>安装CUDA</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sh cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure><p>协议一路回车到底，然后会出现几个选项。</p><ul><li>EULA: accept</li><li>NVIDIA Acceerated Graphics Driver: n</li><li>CUDA Toolkit: y</li><li>Tookkit: 回车</li><li>symbolic link: y</li><li>Samples: y</li></ul><p>安装后，如果出现提示信息，表示缺少几个库，安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev</span><br></pre></td></tr></table></figure><p>重新运行安装命令。</p><p>如果有补丁，则依次安装补丁，选择默认即可。</p><h2 id="2-2-设置环境变量-官方文档7-1-1"><a href="#2-2-设置环境变量-官方文档7-1-1" class="headerlink" title="2.2 设置环境变量(官方文档7.1.1)"></a>2.2 设置环境变量(官方文档7.1.1)</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit ~/.bashrc</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">sudo gedit /etc/profile</span><br></pre></td></tr></table></figure><p>写入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> CUDA-9.0</span><br><span class="line">export CUDA_HOME=/usr/local/cuda-9.0</span><br><span class="line">export PATH=$PATH:$CUDA_HOME/bin</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 如果有多个cuda，那么可以写成</span><br><span class="line">export CUDA_HOME=/usr/local/cuda-9.0:$CUDA_HOME</span><br><span class="line">export PATH=/usr/local/cuda-9.0/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64$:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><p>刷新</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="2-3-验证"><a href="#2-3-验证" class="headerlink" title="2.3 验证"></a>2.3 验证</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/cuda/samples/1_Utilities/deviceQuery</span><br><span class="line">sudo make</span><br><span class="line">./deviceQuery # 有输出表示成功，反之表示失败，重新安装cuda</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -V # 有输出表示成功，反之表示失败，重新安装cuda</span><br></pre></td></tr></table></figure><h1 id="3-cuDNN"><a href="#3-cuDNN" class="headerlink" title="3. cuDNN"></a>3. cuDNN</h1><p><strong>注册下载对应版本的cudnn</strong>:</p><p><strong>解压</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cudnn-9.0-linux-x86-v7.1.tgz</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">cp  cudnn-9.0-linux-x86-v7.1.solitairetheme8 cudnn-9.0-linux-x86-v7.1.tgz</span><br><span class="line">tar -zxvf cudnn-9.0-linux-x86-v7.1.tgz</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line"><span class="meta">#</span> 直接右键解压缩</span><br></pre></td></tr></table></figure><p><strong>移动</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda-9.0/include/</span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda-9.0/lib64/</span><br><span class="line">sudo chmod a+r /usr/local/cuda-9.0/include/cudnn.h</span><br><span class="line">sudo chmod a+r /usr/local/cuda-9.0/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p><strong>建立软连接</strong>(可选)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod +r libcudnn.so.7.0.5</span><br><span class="line">sudo ln -sf libcudnn.so.7.0.5 libcudnn.so.7  </span><br><span class="line">sudo ln -sf libcudnn.so.7 libcudnn.so</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><p><strong>查看cudnn版本</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure><h1 id="4-ananconda"><a href="#4-ananconda" class="headerlink" title="4. ananconda"></a>4. ananconda</h1><p><strong>下载</strong>: Anaconda3-5.2.0-Linux-x86_64.sh</p><p><strong>安装</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sh ./Anaconda3-5.2.0-Linux-x86_64.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 路径</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p>默认或者同意</p><p><strong>清华源</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure><p><strong>测试</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">which python</span><br><span class="line">conda install numpy</span><br></pre></td></tr></table></figure><h1 id="5-pytorch"><a href="#5-pytorch" class="headerlink" title="5. pytorch"></a>5. pytorch</h1><p><strong>安装</strong>：在官网找到安装命令即可，如果速度太慢，可以 ctrl+c 之后复制链接到其他电脑上手动下载并安装离线包。</p><p>安装离线包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install xxx</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">pip install xxx</span><br></pre></td></tr></table></figure><p><strong>GPU测试</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipython</span><br><span class="line">import torch as t</span><br><span class="line">t.cuda.is_available()</span><br><span class="line"><span class="meta">#</span> True</span><br></pre></td></tr></table></figure><h1 id="6-Tensorflow"><a href="#6-Tensorflow" class="headerlink" title="6. Tensorflow"></a>6. Tensorflow</h1><p><strong>安装</strong>：在官网找到安装命令即可，如果速度太慢，可以 ctrl+c 之后复制链接到其他电脑上手动下载并安装离线包。</p><p>安装离线包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install xxx</span><br><span class="line"><span class="meta">#</span> or</span><br><span class="line">pip install xxx</span><br></pre></td></tr></table></figure><p><strong>测试</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipython</span><br><span class="line">import tensorflow as tf</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span><br><span class="line"><span class="meta">#</span> 如果输出有GPU，成功，没有的话重新开始装吧，我帮不了你，因为我成功了。</span><br></pre></td></tr></table></figure><p><strong>备注</strong>：<br>tensorflow与cuda、cudnn的版本需要严格一致。可以通过建立虚拟环境的形式或者tensorflow会自动安装对应版本的cuda、cudnn来解决。参见官网:<br><a href="https://tensorflow.google.cn/install/source" target="_blank" rel="noopener">https://tensorflow.google.cn/install/source</a></p><h1 id="7-matlab"><a href="#7-matlab" class="headerlink" title="7. matlab"></a>7. matlab</h1><p>我使用的是分成两部分的matlab，参考<br><a href="https://blog.csdn.net/qq_36982160/article/details/78397514" target="_blank" rel="noopener">https://blog.csdn.net/qq_36982160/article/details/78397514</a></p><p>matlab要比前面复杂一些，需要要有耐心。</p><p><strong>挂载</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /media/matlab</span><br><span class="line">sudo mount -t auto -o loop R2016b_glnxa64_dvd1.iso matlab/</span><br></pre></td></tr></table></figure><p><strong>安装</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /media/matlab</span><br><span class="line">sudo ./matlab/install</span><br></pre></td></tr></table></figure><ul><li>选项：第二个，使用文件安装密钥，不需要Internet连接</li><li>选项：是(y)</li><li>密钥：crack的readme的第一个数字串</li><li>文件路径：默认</li><li>下一步</li><li>安装</li></ul><p>安装进行到80%左右的时候，会弹出一个提示框，说请插入dvd2，这时候我们需要重新开一个终端，把dvd2挂载到matlab文件夹中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t auto -o loop R2016b_glnxa64_dvd2.iso /media/matlab/</span><br></pre></td></tr></table></figure><p>点击OK继续</p><p><strong>激活</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/MATLAB/R2016b/bin</span><br><span class="line">./matlab</span><br></pre></td></tr></table></figure><ul><li>在弹出的界面中选择: Activate manually without the internet，点击next</li><li>文件路径选择：./license_standalone.lic</li></ul><p>把Crack文件夹中R2016b/Linux/R2016b/bin/glnxa64四个文件，复制到/usr/local/MATLAB/R2016b/bin/glnxa64目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp Crack/R2016b/bin/glnxa64/lib* /usr/local/MATLAB/R2016b/bin/glnxa64</span><br></pre></td></tr></table></figure><p>至此，Matlab已经安装并激活。</p><p><strong>环境变量</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit ~./bashrc</span><br><span class="line"><span class="meta">#</span> 输入</span><br><span class="line"><span class="meta">#</span> matlab</span><br><span class="line">export PATH=/usr/local/MATLAB/R2016b/bin:$PATH</span><br><span class="line"><span class="meta">#</span> 以后直接在命令框中输入matlab即可启动</span><br></pre></td></tr></table></figure><p><strong>创建快捷方式</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /usr/share/applications/Matlab.desktop</span><br><span class="line"><span class="meta">#</span> 写入</span><br><span class="line">[Desktop Entry]</span><br><span class="line">Type=Application</span><br><span class="line">Name=Matlab</span><br><span class="line">GenericName=Matlab 2016b</span><br><span class="line">Comment=Matlab:The Language of Technical Computing</span><br><span class="line">Exec=sh /usr/local/MATLAB/R2016b/bin/matlab -desktop</span><br><span class="line">Icon=/usr/local/MATLAB/Matlab.png</span><br><span class="line">Terminal=false</span><br><span class="line">Categories=Development;</span><br></pre></td></tr></table></figure><ul><li>Exec代表应用程序的位置</li><li>Icon代表应用程序图标的位置</li><li>Terminal为false表示启动时不启动命令行窗口，为true表示启动命令行窗口</li></ul><p>此时会在/usr/share/applications中看到matlab（和文件Name对应）的快捷方式</p><h1 id="8-分辨率"><a href="#8-分辨率" class="headerlink" title="8. 分辨率"></a>8. 分辨率</h1><p><strong>查看分辨率</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xrandr</span><br><span class="line"><span class="meta">#</span> VGA1</span><br></pre></td></tr></table></figure><p><strong>自定义分辨率</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cvt 1920 1080</span><br><span class="line"><span class="meta">#</span> "1920x1080_60.00"  173.00  1920 2048 2248 2576  1080 1083 1088 1120 -hsync +vsync</span><br></pre></td></tr></table></figure><p><strong>设置分辨率</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/profile</span><br><span class="line"><span class="meta">#</span> 末尾加入</span><br><span class="line">xrandr --newmode "1920x1080_60.00" 173.00 1920 2048 2248 2576 1080 1083 1088 1120 -hsync +vsync</span><br><span class="line">xrandr --addmode VGA1 "1920x1080_60.00"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 刷新</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>以后在分辨率选项里就有1920x1080，同时开机显示率也自动变成1920x1080.</p><h1 id="9-cuda、cudnn的卸载"><a href="#9-cuda、cudnn的卸载" class="headerlink" title="9. cuda、cudnn的卸载"></a>9. cuda、cudnn的卸载</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 卸载cuda</span><br><span class="line">sudo /usr/local/cuda-9.0/bin/uninstall_cuda_9.0.pl</span><br><span class="line"><span class="meta">#</span> 删除cuda</span><br><span class="line">sudo rm -rf /usr/local/cuda-9.0</span><br><span class="line"><span class="meta">#</span> 卸载cudnn</span><br><span class="line">sudo rm -rf /usr/local/cuda/include/cudnn.h</span><br><span class="line">sudo rm -rf /usr/local/cuda/lib64/libcudnn</span><br></pre></td></tr></table></figure><h1 id="10-teamviewer-for-Linux-的安装与卸载"><a href="#10-teamviewer-for-Linux-的安装与卸载" class="headerlink" title="10. teamviewer for Linux 的安装与卸载"></a>10. teamviewer for Linux 的安装与卸载</h1><p>主要针对teamviewer出现商业用途时使用。</p><p>参考: <a href="https://www.cnblogs.com/fxust/p/8040706.html" target="_blank" rel="noopener">https://www.cnblogs.com/fxust/p/8040706.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 1. 卸载</span></span><br><span class="line">dpkg -l |grep xxx</span><br><span class="line"><span class="comment">#xxx就是安装的软件名</span></span><br><span class="line">sudo dpkg --purge xxxx</span><br><span class="line">rm -rf ~/.local/share/xxxx</span><br><span class="line"><span class="comment"># 然后把所有teamviewer的文件，尤其是log文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 安装</span></span><br><span class="line">sudo dpkg -i teamviewer_13<span class="number">.0</span><span class="number">.5693</span>_amd64.deb</span><br><span class="line"><span class="comment"># 2.1 如果出现错误</span></span><br><span class="line"><span class="comment">#添加架构依赖</span></span><br><span class="line">sudo dpkg --add-architecture i386</span><br><span class="line"><span class="comment">#更新软件库</span></span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment">#执行强制安装</span></span><br><span class="line">sudo apt-get -f install</span><br><span class="line"><span class="comment">#再安装</span></span><br><span class="line">sudo dpkg -i teamviewer_13<span class="number">.0</span><span class="number">.5693</span>_amd64.deb</span><br><span class="line"><span class="comment"># 配置文件可以修改，可以不改</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;主要记录从安装显卡驱动到cudnn的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="GPU" scheme="http://yoursite.com/categories/GPU/"/>
    
    
      <category term="driver" scheme="http://yoursite.com/tags/driver/"/>
    
      <category term="cuda" scheme="http://yoursite.com/tags/cuda/"/>
    
      <category term="cudnn" scheme="http://yoursite.com/tags/cudnn/"/>
    
  </entry>
  
  <entry>
    <title>starGAN</title>
    <link href="http://yoursite.com/2018/12/19/starGAN/"/>
    <id>http://yoursite.com/2018/12/19/starGAN/</id>
    <published>2018-12-19T01:44:59.000Z</published>
    <updated>2018-12-23T15:11:34.619Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>因为在person-reid论文<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.pdf" target="_blank" rel="noopener">HHL</a>中涉及到了starGAN，所以做一个StarGAN的阅读记录，并比较与CycleGAN的区别。</p><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf" target="_blank" rel="noopener">StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</a></p><p>Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo</p><a id="more"></a><p>code-pytorch-official: <a href="https://github.com/yunjey/stargan" target="_blank" rel="noopener">https://github.com/yunjey/stargan</a><br>code-tensorflow: &lt;<a href="https://github.com/taki0112/StarGAN-Tensorflow" target="_blank" rel="noopener">https://github.com/taki0112/StarGAN-Tensorflow</a> &gt;</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>解决多域之间图像转换一对多的问题，本文主要针对人脸进行改变。</p><p><strong>关键词</strong>：multi-domain image-image translation</p><p><strong>效果</strong>：转换效果如图所示</p><p><img src="./starGAN/starGAN1.png" alt="转换效果"><br><img src="/2018/12/19/starGAN/starGAN1.png" title="转换效果"></p><p><strong>网络模型</strong>：CycleGAN和StarGAN模型对比</p><p>starGAN有一个生成器G，两个判别器。</p><p><img src="./starGAN/starGAN2.png" alt="CycleGAN和StarGAN模型对比"><br><img src="/2018/12/19/starGAN/starGAN2.png" title="CycleGAN和StarGAN模型对比"></p><p><strong>备注</strong>：</p><p>multi-domain：单数据集的不同属性作为了一个domain</p><p>multi-datasets：不同数据集的不同属性</p><p>starGAN 分为multi-domain和multi-dataset两种。</p><h1 id="2-Star-Generative-Adversarial-Networks"><a href="#2-Star-Generative-Adversarial-Networks" class="headerlink" title="2. Star Generative Adversarial Networks"></a>2. Star Generative Adversarial Networks</h1><h2 id="2-1-Multi-Domain-Image-to-Image-Translation"><a href="#2-1-Multi-Domain-Image-to-Image-Translation" class="headerlink" title="2.1  Multi-Domain Image-to-Image Translation"></a>2.1  Multi-Domain Image-to-Image Translation</h2><p><strong>starGAN</strong>: starGAN的训练模型</p><p><img src="./starGAN/starGAN3.png" alt="starGAN in mutli domain"><br><img src="/2018/12/19/starGAN/starGAN3.png" title="starGAN in mutli domain"></p><blockquote><p>To achieve this, we train G to translate an input image x into an output image y conditioned on the target domain label c, <strong>G(x; c) -&gt; y</strong>. We randomly generate the target domain label c so that G learns to flexibly translate the input image. We also introduce an auxiliary classifier [22] that allows a single discriminator to control multiple domains. That is, our discriminator produces probability distributions over both sources and domain labels, <strong>D:x-&gt;{$D_{src}$(x); $D_{cls}$(x)}</strong></p></blockquote><p><strong>符号说明</strong>：符号表</p><div class="table-container"><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">x</td><td style="text-align:center">input image</td></tr><tr><td style="text-align:center">c</td><td style="text-align:center">target domain label</td></tr><tr><td style="text-align:center">c’</td><td style="text-align:center">source domain label</td></tr><tr><td style="text-align:center">y</td><td style="text-align:center">generate image</td></tr></tbody></table></div><p><strong>Loss</strong>: training loss</p><p><strong>Adversarial Loss</strong>:(CycleGAN也有)对抗损失</p><script type="math/tex; mode=display">L_{adv}=E_x[log D_{src}(x)]+E_{x,c}[log (1-D_{src}(G(x,c)))] \tag{1}</script><p><strong>Domain Classification Loss</strong>:(特有)分类损失</p><blockquote><p>That is, we decompose the objective into two terms: a domain classification loss of real images used to optimize D, and a domain classification loss of fake images used to optimize G.</p></blockquote><p><em>优化 D</em>:</p><script type="math/tex; mode=display">L_{cls}^r=E_{x,c'}[-log D_{cls}(c'|x)] \tag{2}</script><blockquote><p>By minimizing this objective, D learns to classify a real image x to its corresponding original domain c’.</p></blockquote><p><em>优化 G</em>:</p><script type="math/tex; mode=display">L_{cls}^f=E_{x,c}[-log D_{cls}(c|G(x,c))] \tag{3}</script><blockquote><p>G tries to minimize this objective to generate images that can be classified as the target domain c.</p></blockquote><p><strong>Reconstruction Loss</strong>: (共有)重构损失</p><script type="math/tex; mode=display">L_{rec}=E_{x,c,c'}[\parallel x-G(G(x,c),c') \parallel _1]  \tag{4}</script><p><strong>Full Objective</strong>: 共有</p><script type="math/tex; mode=display">L_D=-L_{adv}+\lambda_{cls} L_{cls}^r</script><script type="math/tex; mode=display">L_G=L_{adv}+\lambda_{cls} L_{cls}^f+\lambda_{rec}L_{rec} \tag{5}</script><script type="math/tex; mode=display">\lambda_{cls}=1, \lambda_{rec}=10</script><h2 id="2-2-Training-with-Multiple-Datasets"><a href="#2-2-Training-with-Multiple-Datasets" class="headerlink" title="2.2. Training with Multiple Datasets"></a>2.2. Training with Multiple Datasets</h2><p><img src="./starGAN/starGAN5.png" alt="starGAN in multi datasets"><br><img src="/2018/12/19/starGAN/starGAN5.png" title="starGAN in multi datasets"></p><p>StarGAN也适用于多数据集间的转换，上述过程中的重构损失要求数据集之间的标签一致(？？？)。针对这个问题，作者引入Mask Vector.</p><p><strong>Mask Vector</strong>: 修改真值。</p><script type="math/tex; mode=display">\tilde{c} = [c_1, ..., c_n, m]</script><blockquote><p>$c_i$ represents a vector for the labels of the i-th dataset. The vector of the known label $c_i$ can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n−1 unknown labels we simply assign zero values.</p></blockquote><p>这样的话，所有的c都需要变成$\tilde{c}$</p><h1 id="3-Implementation"><a href="#3-Implementation" class="headerlink" title="3. Implementation"></a>3. Implementation</h1><p><strong>Improved GAN training</strong>: 为了稳定训练过程，替代方程1.</p><script type="math/tex; mode=display">L_{adv}=E_x[log D_{src}(x)]-E_{x,c}[log (D_{src}(G(x,c)))]-\lambda_{gp}E_{\hat{x}}[(\parallel \nabla_{\hat{x}}  D_{src}(\hat{x}) \parallel_2-1)^2] \tag{6}</script><script type="math/tex; mode=display">\lambda_{gp}=10</script><p><strong>Network Architecture</strong>: 类似CycleGAN。</p><p>G: Leaky ReLU: 0.01</p><p><img src="./starGAN/starGAN6.png" alt="G"><br><img src="/2018/12/19/starGAN/starGAN6.png" title="G"></p><p>D: PatchGAN</p><p>现在网络架构可以看到的是作者使用的不是70x70的patchGAN，通过patchGAN的论文，也没有看到这种结构。</p><p><img src="./starGAN/starGAN7.png" alt="D"><br><img src="/2018/12/19/starGAN/starGAN7.png"></p><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><h2 id="4-1-Baseline-Models"><a href="#4-1-Baseline-Models" class="headerlink" title="4.1 Baseline Models"></a>4.1 Baseline Models</h2><p><img src="./starGAN/starGAN4.png" alt="baseline models"><br><img src="/2018/12/19/starGAN/starGAN4.png" title="baseline models"></p><p>通过结果可以看出，在Gender这个属性，ICGAN的转换效果要更好一些，但是损失了ID信息。</p><h2 id="4-2-Training"><a href="#4-2-Training" class="headerlink" title="4.2 Training"></a>4.2 Training</h2><ul><li>Adam: $\beta_1=0.5, \beta_2=0.999$</li><li>Updates: one generator update after five discriminator updates</li><li>lr: For CelebA, 0.0001 for the first 100000 epochs, and linearly decay the lr to 0 over the next 100000 epochs. For the RaFD, 0.0001 for the first 100000 epochs, and linearly decay the lr to 0 over the next 100000 epochs.作者在论文写的是10和100，但是代码显示的是100000</li><li>batch: 16</li><li>input: For CelebA, crop: 178, resize: 128; For RaFD, </li></ul><h2 id="4-3-Results"><a href="#4-3-Results" class="headerlink" title="4.3 Results"></a>4.3 Results</h2><p>作者通过人脸的转换实验，不仅说明了StarGAN在单数据集的不同domian中效果好，而且在多数据集的不同domian中效果也好。</p><h1 id="5-代码"><a href="#5-代码" class="headerlink" title="5. 代码"></a>5. 代码</h1><p>在这里分析pytorch的代码，并对其中关键的代码进行解读。</p><p>如果不说明，则假设讨论单数据集的多域。</p><h2 id="5-1-Model-G-and-D"><a href="#5-1-Model-G-and-D" class="headerlink" title="5.1 Model: G and D"></a>5.1 Model: G and D</h2><p><strong>Generator</strong>:<br>生成器Generator，结构与前面提到的网络架构一致，这里需要注意两点：</p><ul><li>当训练集是单数据集的多domain时，label需要扩充成图片大小，一起输入网络(这里有个疑问：网络真得能知道后面的通道是label吗)</li><li>当训练集是多数据集的多domain时，label的维度是c+c2+2，因为有mask，同样需要广播成图片大小，一起输入网络</li></ul><p><strong>Discriminator</strong>:<br>判别器Discriminator，有个疑问是关于是感受野和计算损失的。</p><p><img src="./starGAN/starGAN8.png" alt="感受野"><br><img src="/2018/12/19/starGAN/starGAN8.png" title="感受野"></p><p>下面会提及到计算损失的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Residual Block with instance normalization."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim_in, dim_out)</span>:</span></span><br><span class="line">        super(ResidualBlock, self).__init__()</span><br><span class="line">        self.main = nn.Sequential(</span><br><span class="line">            nn.Conv2d(dim_in, dim_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.InstanceNorm2d(dim_out, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Conv2d(dim_out, dim_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.InstanceNorm2d(dim_out, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x + self.main(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Generator network."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conv_dim=<span class="number">64</span>, c_dim=<span class="number">5</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">3</span>+c_dim, conv_dim, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.InstanceNorm2d(conv_dim, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>))</span><br><span class="line">        layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Down-sampling layers.</span></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            layers.append(nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">            layers.append(nn.InstanceNorm2d(curr_dim*<span class="number">2</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>))</span><br><span class="line">            layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Bottleneck layers.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(repeat_num):</span><br><span class="line">            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Up-sampling layers.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">            layers.append(nn.InstanceNorm2d(curr_dim//<span class="number">2</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>))</span><br><span class="line">            layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">            curr_dim = curr_dim // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        layers.append(nn.Conv2d(curr_dim, <span class="number">3</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>))</span><br><span class="line">        layers.append(nn.Tanh())</span><br><span class="line">        self.main = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, c)</span>:</span></span><br><span class="line">        <span class="comment"># Replicate spatially and concatenate domain information.</span></span><br><span class="line">        <span class="comment"># c: N*c_dim</span></span><br><span class="line">        <span class="comment"># 生成器直接将目标域c在通道维度进行拼接</span></span><br><span class="line">        c = c.view(c.size(<span class="number">0</span>), c.size(<span class="number">1</span>), <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        c = c.repeat(<span class="number">1</span>, <span class="number">1</span>, x.size(<span class="number">2</span>), x.size(<span class="number">3</span>))</span><br><span class="line">        x = torch.cat([x, c], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.main(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Discriminator network with PatchGAN."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_size=<span class="number">128</span>, conv_dim=<span class="number">64</span>, c_dim=<span class="number">5</span>, repeat_num=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">3</span>, conv_dim, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.LeakyReLU(<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">        curr_dim = conv_dim</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, repeat_num):</span><br><span class="line">            layers.append(nn.Conv2d(curr_dim, curr_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">            layers.append(nn.LeakyReLU(<span class="number">0.01</span>))</span><br><span class="line">            curr_dim = curr_dim * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        kernel_size = int(image_size / np.power(<span class="number">2</span>, repeat_num))</span><br><span class="line">        self.main = nn.Sequential(*layers)</span><br><span class="line">        self.conv1 = nn.Conv2d(curr_dim, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        h = self.main(x)</span><br><span class="line">        <span class="comment"># True or False</span></span><br><span class="line">        out_src = self.conv1(h)</span><br><span class="line">        <span class="comment"># classes onehot</span></span><br><span class="line">        out_cls = self.conv2(h)</span><br><span class="line">        <span class="keyword">return</span> out_src, out_cls.view(out_cls.size(<span class="number">0</span>), out_cls.size(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="5-2-input"><a href="#5-2-input" class="headerlink" title="5.2 input"></a>5.2 input</h2><p>对于任一张图片，其target label是随机取其他图片的label，而没有刻意去指定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># label_org和label_trg可以认为是单个图片的真实label，形式可以是[0,1,1,0]或者4，根据不同的数据集形式进行处理，前者是多分类label，后者是单分类label，用于计算损失</span></span><br><span class="line"><span class="comment"># c_org，c_trg是与图片一起输入网络的&#123;0,1&#125;向量，形式是[0,1,1,0]或者是[0,0,0,1]的形式，用于网络的输入</span></span><br><span class="line">x_real, label_org = next(data_iter)</span><br><span class="line">rand_idx = torch.randperm(label_org.size(<span class="number">0</span>))</span><br><span class="line">label_trg = label_org[rand_idx]</span><br><span class="line"><span class="keyword">if</span> self.dataset == <span class="string">'CelebA'</span>:</span><br><span class="line">    c_org = label_org.clone()</span><br><span class="line">    c_trg = label_trg.clone()</span><br><span class="line"><span class="keyword">elif</span> self.dataset == <span class="string">'RaFD'</span>:</span><br><span class="line">    c_org = self.label2onehot(label_org, self.c_dim)</span><br><span class="line">    c_trg = self.label2onehot(label_trg, self.c_dim)</span><br></pre></td></tr></table></figure><h2 id="5-3-train-G-and-D"><a href="#5-3-train-G-and-D" class="headerlink" title="5.3 train G and D"></a>5.3 train G and D</h2><h3 id="5-3-1-train-D"><a href="#5-3-1-train-D" class="headerlink" title="5.3.1 train D"></a>5.3.1 train D</h3><p>这里需要对上述提到的损失函数做进一步处理。</p><p><strong>判断图片真假损失</strong>:由方程6得：</p><script type="math/tex; mode=display">L_{adv}=-D_{src}(x)+D_{src}(G(x,c))+\lambda_{gp}(\parallel \nabla_{\hat{x}}  D_{src}(\hat{x}) \parallel_2-1)^2 \tag{7}</script><script type="math/tex; mode=display">\lambda_{gp}=10</script><p><strong>判断原图片属性正确</strong>:由方程2得：</p><script type="math/tex; mode=display">L_{cls}^r=D_{cls}(c'|x) \tag{8}</script><p><strong>总损失</strong>：</p><script type="math/tex; mode=display">L_D=-L_{adv}+\lambda_{cls} L_{cls}^r</script><script type="math/tex; mode=display">\lambda_{cls}=1</script><p><strong>备注</strong>：</p><ul><li>在计算真假损失的时候，是直接求输出的均值，这一点不是很理解。</li><li>方程7的第三项的计算见gradient_penalty，对整个图片的梯度求和。</li><li>方程8的的求解见classification_loss，就是一个简单的分类损失。</li><li>不理解方程2为什么要加个符号？方程7也是符号正好相反？</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"><span class="comment">#                             2. Train the discriminator                              #</span></span><br><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss with real images.</span></span><br><span class="line">out_src, out_cls = self.D(x_real) <span class="comment"># out_src：N,1,2,2; out_cls: N,c_dim</span></span><br><span class="line">d_loss_real = - torch.mean(out_src) <span class="comment"># 方程7的第一项</span></span><br><span class="line">d_loss_cls = self.classification_loss(out_cls, label_org, self.dataset) <span class="comment"># 方程8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss with fake images.</span></span><br><span class="line">x_fake = self.G(x_real, c_trg)</span><br><span class="line">out_src, out_cls = self.D(x_fake.detach())</span><br><span class="line">d_loss_fake = torch.mean(out_src) <span class="comment"># 方程7的第二项</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss for gradient penalty.</span></span><br><span class="line">alpha = torch.rand(x_real.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).to(self.device)</span><br><span class="line">x_hat = (alpha * x_real.data + (<span class="number">1</span> - alpha) * x_fake.data).requires_grad_(<span class="keyword">True</span>)</span><br><span class="line">out_src, _ = self.D(x_hat)</span><br><span class="line">d_loss_gp = self.gradient_penalty(out_src, x_hat) <span class="comment"># 方程7的第三项</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Backward and optimize.</span></span><br><span class="line">d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp <span class="comment"># 总损失</span></span><br><span class="line">self.reset_grad()</span><br><span class="line">d_loss.backward()</span><br><span class="line">self.d_optimizer.step()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_penalty</span><span class="params">(self, y, x)</span>:</span></span><br><span class="line">    <span class="string">"""Compute gradient penalty: (L2_norm(dy/dx) - 1)**2."""</span></span><br><span class="line">    weight = torch.ones(y.size()).to(self.device)</span><br><span class="line">    dydx = torch.autograd.grad(outputs=y,</span><br><span class="line">                                inputs=x,</span><br><span class="line">                                grad_outputs=weight,</span><br><span class="line">                                retain_graph=<span class="keyword">True</span>,</span><br><span class="line">                                create_graph=<span class="keyword">True</span>,</span><br><span class="line">                                only_inputs=<span class="keyword">True</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    dydx = dydx.view(dydx.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    dydx_l2norm = torch.sqrt(torch.sum(dydx**<span class="number">2</span>, dim=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> torch.mean((dydx_l2norm<span class="number">-1</span>)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classification_loss</span><span class="params">(self, logit, target, dataset=<span class="string">'CelebA'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Compute binary or softmax cross entropy loss."""</span></span><br><span class="line">    <span class="keyword">if</span> dataset == <span class="string">'CelebA'</span>:</span><br><span class="line">        <span class="keyword">return</span> F.binary_cross_entropy_with_logits(logit, target, size_average=<span class="keyword">False</span>) / logit.size(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> dataset == <span class="string">'RaFD'</span>:</span><br><span class="line">        <span class="keyword">return</span> F.cross_entropy(logit, target)</span><br></pre></td></tr></table></figure><h3 id="5-3-2-train-G"><a href="#5-3-2-train-G" class="headerlink" title="5.3.2 train G"></a>5.3.2 train G</h3><p>与以往训练几个G之后才训练D不同，这里是训练几个D之后才训练G。</p><p>同样对上述提到的损失做进一步处理。</p><p><strong>生成图片为真</strong>：由方程6得，与方程7正好相反：</p><script type="math/tex; mode=display">L_{adv}=-D_{src}(G(x,c)) \tag{9}</script><p><strong>生成图片的属性正确</strong>：由方程3得：</p><script type="math/tex; mode=display">L_{cls}^f=D_{cls}(c|G(x,c)) \tag{10}</script><p><strong>Reconstruction Loss</strong>: 重构损失</p><script type="math/tex; mode=display">L_{rec}=\parallel x-G(G(x,c),c') \parallel _1  \tag{4}</script><p><strong>总损失</strong></p><script type="math/tex; mode=display">L_G=L_{adv}+\lambda_{cls} L_{cls}^f+\lambda_{rec}L_{rec} \tag{5}</script><script type="math/tex; mode=display">\lambda_{cls}=1, \lambda_{rec}=10</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"><span class="comment">#                               3. Train the generator                                #</span></span><br><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (i+<span class="number">1</span>) % self.n_critic == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Original-to-target domain.</span></span><br><span class="line">    x_fake = self.G(x_real, c_trg)</span><br><span class="line">    out_src, out_cls = self.D(x_fake)</span><br><span class="line">    g_loss_fake = - torch.mean(out_src) <span class="comment"># 方程9</span></span><br><span class="line">    g_loss_cls = self.classification_loss(out_cls, label_trg, self.dataset) <span class="comment"># 方程10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Target-to-original domain.</span></span><br><span class="line">    x_reconst = self.G(x_fake, c_org)</span><br><span class="line">    g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward and optimize.</span></span><br><span class="line">    g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls</span><br><span class="line">    self.reset_grad()</span><br><span class="line">    g_loss.backward()</span><br><span class="line">    self.g_optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="5-4-val"><a href="#5-4-val" class="headerlink" title="5.4 val"></a>5.4 val</h2><p><strong>CelebA数据集：</strong>这里制作target domain label的方法分为头发属性(互相排斥)和其他属性(不排斥):对于选中的头发属性’Black_Hair’, ‘Blond_Hair’, ‘Brown_Hair’,则把5张图片的’Black_Hair’全部设为1,’Blond_Hair’, ‘Brown_Hair’设为0,作为第一个target domain label, 再把5张图片的’Blond_Hair’全部设为1,’Black_Hair’, ‘Brown_Hair’设为0,作为第二个target domain label, 再把5张图片的’Brown_Hair’全部设为1,’Black_Hair’,’Blond_Hair’ 设为0,作为第三个target domain label,对于其他属性,则直接取相反数做为第三个target domain label和第四个target domain label.</p><p><strong>RaFD数据集</strong>:属于排斥属性，也和头发类似，对某一列全部设为1，其余设为0.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">c_org</span><br><span class="line">Out[<span class="number">24</span>]: </span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line"></span><br><span class="line">c_trg_list</span><br><span class="line">Out[<span class="number">25</span>]: </span><br><span class="line">[tensor([[ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]], device=<span class="string">'cuda:0'</span>),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]], device=<span class="string">'cuda:0'</span>),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]], device=<span class="string">'cuda:0'</span>),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]], device=<span class="string">'cuda:0'</span>),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]], device=<span class="string">'cuda:0'</span>)]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_labels</span><span class="params">(self, c_org, c_dim=<span class="number">5</span>, dataset=<span class="string">'CelebA'</span>, selected_attrs=None)</span>:</span></span><br><span class="line">    <span class="string">"""Generate target domain labels for debugging and testing."""</span></span><br><span class="line">    <span class="comment"># Get hair color indices.</span></span><br><span class="line">    <span class="keyword">if</span> dataset == <span class="string">'CelebA'</span>:</span><br><span class="line">        hair_color_indices = []</span><br><span class="line">        <span class="keyword">for</span> i, attr_name <span class="keyword">in</span> enumerate(selected_attrs):</span><br><span class="line">            <span class="keyword">if</span> attr_name <span class="keyword">in</span> [<span class="string">'Black_Hair'</span>, <span class="string">'Blond_Hair'</span>, <span class="string">'Brown_Hair'</span>, <span class="string">'Gray_Hair'</span>]:</span><br><span class="line">                hair_color_indices.append(i)</span><br><span class="line"></span><br><span class="line">    c_trg_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(c_dim):</span><br><span class="line">        <span class="keyword">if</span> dataset == <span class="string">'CelebA'</span>:</span><br><span class="line">            c_trg = c_org.clone()</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> hair_color_indices:  <span class="comment"># Set one hair color to 1 and the rest to 0.</span></span><br><span class="line">                c_trg[:, i] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> hair_color_indices:</span><br><span class="line">                    <span class="keyword">if</span> j != i:</span><br><span class="line">                        c_trg[:, j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                c_trg[:, i] = (c_trg[:, i] == <span class="number">0</span>)  <span class="comment"># Reverse attribute value.</span></span><br><span class="line">        <span class="keyword">elif</span> dataset == <span class="string">'RaFD'</span>:</span><br><span class="line">            c_trg = self.label2onehot(torch.ones(c_org.size(<span class="number">0</span>))*i, c_dim)</span><br><span class="line"></span><br><span class="line">        c_trg_list.append(c_trg.to(self.device))</span><br><span class="line">    <span class="keyword">return</span> c_trg_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fetch fixed inputs for debugging.</span></span><br><span class="line">data_iter = iter(data_loader)</span><br><span class="line">x_fixed, c_org = next(data_iter)</span><br><span class="line">x_fixed = x_fixed.to(self.device)</span><br><span class="line">c_fixed_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs)</span><br></pre></td></tr></table></figure><h2 id="5-5-多数据集"><a href="#5-5-多数据集" class="headerlink" title="5.5 多数据集"></a>5.5 多数据集</h2><p>在多数据集的情况下，损失函数大体不变，略微不同。</p><h3 id="5-5-1-input"><a href="#5-5-1-input" class="headerlink" title="5.5.1 input"></a>5.5.1 input</h3><p>多数据集顺序输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> [<span class="string">'CelebA'</span>, <span class="string">'RaFD'</span>]:</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">celeba_iter = iter(self.celeba_loader)</span><br><span class="line">x_real, label_org = next(celeba_iter)</span><br><span class="line">rand_idx = torch.randperm(label_org.size(<span class="number">0</span>))</span><br><span class="line">label_trg = label_org[rand_idx]</span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">'CelebA'</span>:</span><br><span class="line">    c_org = label_org.clone()</span><br><span class="line">    c_trg = label_trg.clone()</span><br><span class="line">    zero = torch.zeros(x_real.size(<span class="number">0</span>), self.c2_dim)</span><br><span class="line">    mask = self.label2onehot(torch.zeros(x_real.size(<span class="number">0</span>)), <span class="number">2</span>)</span><br><span class="line">    c_org = torch.cat([c_org, zero, mask], dim=<span class="number">1</span>)</span><br><span class="line">    c_trg = torch.cat([c_trg, zero, mask], dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">elif</span> dataset == <span class="string">'RaFD'</span>:</span><br><span class="line">    c_org = self.label2onehot(label_org, self.c2_dim)</span><br><span class="line">    c_trg = self.label2onehot(label_trg, self.c2_dim)</span><br><span class="line">    zero = torch.zeros(x_real.size(<span class="number">0</span>), self.c_dim)</span><br><span class="line">    mask = self.label2onehot(torch.ones(x_real.size(<span class="number">0</span>)), <span class="number">2</span>)</span><br><span class="line">    c_org = torch.cat([zero, c_org, mask], dim=<span class="number">1</span>)</span><br><span class="line">    c_trg = torch.cat([zero, c_trg, mask], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="5-5-2-train-D-and-G"><a href="#5-5-2-train-D-and-G" class="headerlink" title="5.5.2 train D and G"></a>5.5.2 train D and G</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"><span class="comment">#                             2. Train the discriminator                              #</span></span><br><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss with real images.</span></span><br><span class="line">out_src, out_cls = self.D(x_real) </span><br><span class="line">out_cls = out_cls[:, :self.c_dim] <span class="keyword">if</span> dataset == <span class="string">'CelebA'</span> <span class="keyword">else</span> out_cls[:, self.c_dim:] <span class="comment"># 属性损失只考虑一半</span></span><br><span class="line">d_loss_real = - torch.mean(out_src) <span class="comment"># 方程7的第一项</span></span><br><span class="line">d_loss_cls = self.classification_loss(out_cls, label_org, dataset) <span class="comment"># 方程8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss with fake images.</span></span><br><span class="line">x_fake = self.G(x_real, c_trg)</span><br><span class="line">out_src, _ = self.D(x_fake.detach())</span><br><span class="line">d_loss_fake = torch.mean(out_src) <span class="comment"># 方程7的第二项</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss for gradient penalty.</span></span><br><span class="line">alpha = torch.rand(x_real.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).to(self.device)</span><br><span class="line">x_hat = (alpha * x_real.data + (<span class="number">1</span> - alpha) * x_fake.data).requires_grad_(<span class="keyword">True</span>)</span><br><span class="line">out_src, _ = self.D(x_hat)</span><br><span class="line">d_loss_gp = self.gradient_penalty(out_src, x_hat) <span class="comment"># 方程7的第三项</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Backward and optimize.</span></span><br><span class="line">d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp</span><br><span class="line">self.reset_grad()</span><br><span class="line">d_loss.backward()</span><br><span class="line">self.d_optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Logging.</span></span><br><span class="line">loss = &#123;&#125;</span><br><span class="line">loss[<span class="string">'D/loss_real'</span>] = d_loss_real.item()</span><br><span class="line">loss[<span class="string">'D/loss_fake'</span>] = d_loss_fake.item()</span><br><span class="line">loss[<span class="string">'D/loss_cls'</span>] = d_loss_cls.item()</span><br><span class="line">loss[<span class="string">'D/loss_gp'</span>] = d_loss_gp.item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"><span class="comment">#                               3. Train the generator                                #</span></span><br><span class="line"><span class="comment"># =================================================================================== #</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (i+<span class="number">1</span>) % self.n_critic == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Original-to-target domain.</span></span><br><span class="line">    x_fake = self.G(x_real, c_trg)</span><br><span class="line">    out_src, out_cls = self.D(x_fake)</span><br><span class="line">    out_cls = out_cls[:, :self.c_dim] <span class="keyword">if</span> dataset == <span class="string">'CelebA'</span> <span class="keyword">else</span> out_cls[:, self.c_dim:] <span class="comment"># 生成图片的属性只考虑一半</span></span><br><span class="line">    g_loss_fake = - torch.mean(out_src)</span><br><span class="line">    g_loss_cls = self.classification_loss(out_cls, label_trg, dataset)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Target-to-original domain.</span></span><br><span class="line">    x_reconst = self.G(x_fake, c_org)</span><br><span class="line">    g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward and optimize.</span></span><br><span class="line">    g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls</span><br><span class="line">    self.reset_grad()</span><br><span class="line">    g_loss.backward()</span><br><span class="line">    self.g_optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Logging.</span></span><br><span class="line">    loss[<span class="string">'G/loss_fake'</span>] = g_loss_fake.item()</span><br><span class="line">    loss[<span class="string">'G/loss_rec'</span>] = g_loss_rec.item()</span><br><span class="line">    loss[<span class="string">'G/loss_cls'</span>] = g_loss_cls.item()</span><br></pre></td></tr></table></figure><h3 id="5-5-3-val-and-test"><a href="#5-5-3-val-and-test" class="headerlink" title="5.5.3 val and test"></a>5.5.3 val and test</h3><p>对当前图片生成两个数据集下不同属性的图片，也就是说，具有跨数据集生成图片的能力。</p><p><strong>val</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (i+<span class="number">1</span>) % self.sample_step == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        x_fake_list = [x_fixed]</span><br><span class="line">        <span class="keyword">for</span> c_fixed <span class="keyword">in</span> c_celeba_list:</span><br><span class="line">            c_trg = torch.cat([c_fixed, zero_rafd, mask_celeba], dim=<span class="number">1</span>)</span><br><span class="line">            x_fake_list.append(self.G(x_fixed, c_trg))</span><br><span class="line">        <span class="keyword">for</span> c_fixed <span class="keyword">in</span> c_rafd_list:</span><br><span class="line">            c_trg = torch.cat([zero_celeba, c_fixed, mask_rafd], dim=<span class="number">1</span>)</span><br><span class="line">            x_fake_list.append(self.G(x_fixed, c_trg))</span><br></pre></td></tr></table></figure><p><strong>test</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (x_real, c_org) <span class="keyword">in</span> enumerate(self.celeba_loader):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prepare input images and target domain labels.</span></span><br><span class="line">    x_real = x_real.to(self.device)</span><br><span class="line">    c_celeba_list = self.create_labels(c_org, self.c_dim, <span class="string">'CelebA'</span>, self.selected_attrs)</span><br><span class="line">    c_rafd_list = self.create_labels(c_org, self.c2_dim, <span class="string">'RaFD'</span>)</span><br><span class="line">    zero_celeba = torch.zeros(x_real.size(<span class="number">0</span>), self.c_dim).to(self.device)            <span class="comment"># Zero vector for CelebA.</span></span><br><span class="line">    zero_rafd = torch.zeros(x_real.size(<span class="number">0</span>), self.c2_dim).to(self.device)             <span class="comment"># Zero vector for RaFD.</span></span><br><span class="line">    mask_celeba = self.label2onehot(torch.zeros(x_real.size(<span class="number">0</span>)), <span class="number">2</span>).to(self.device)  <span class="comment"># Mask vector: [1, 0].</span></span><br><span class="line">    mask_rafd = self.label2onehot(torch.ones(x_real.size(<span class="number">0</span>)), <span class="number">2</span>).to(self.device)     <span class="comment"># Mask vector: [0, 1].</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Translate images.</span></span><br><span class="line">    x_fake_list = [x_real]</span><br><span class="line">    <span class="keyword">for</span> c_celeba <span class="keyword">in</span> c_celeba_list:</span><br><span class="line">        c_trg = torch.cat([c_celeba, zero_rafd, mask_celeba], dim=<span class="number">1</span>)</span><br><span class="line">        x_fake_list.append(self.G(x_real, c_trg))</span><br><span class="line">    <span class="keyword">for</span> c_rafd <span class="keyword">in</span> c_rafd_list:</span><br><span class="line">        c_trg = torch.cat([zero_celeba, c_rafd, mask_rafd], dim=<span class="number">1</span>)</span><br><span class="line">        x_fake_list.append(self.G(x_real, c_trg))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save the translated images.</span></span><br><span class="line">    x_concat = torch.cat(x_fake_list, dim=<span class="number">3</span>)</span><br><span class="line">    result_path = os.path.join(self.result_dir, <span class="string">'&#123;&#125;-images.jpg'</span>.format(i+<span class="number">1</span>))</span><br><span class="line">    save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">'Saved real and fake images into &#123;&#125;...'</span>.format(result_path))</span><br></pre></td></tr></table></figure><h1 id="6-其他"><a href="#6-其他" class="headerlink" title="6. 其他"></a>6. 其他</h1><p>通过代码,我们可以猜出,对于starGAN,每一个domain都是一个二值属性,这些属性可以是互相排斥的,例如头发颜色,可以是不互相排斥的,并且这里和CycleGAN还是有一些区别的,CycleGAN的domain是数据集,source domain 和 target domain是风马牛不相及的,source domain和target domain有自己的风格,例如map数据集,是没有真值的,有的只是深度网络提取出的特征和70*70patchGAN.但是starGAN中,生成的图片和原始图片是一个数据集的,并且这两张图片不是要求风格一样,感觉这能应用到person-reid中也是神奇.</p><p>在图片真假的分类损失中，之前的GAN都是使用True和False来表示，这次换了一个新公式直接mean，还有点难理解。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h1&gt;&lt;p&gt;因为在person-reid论文&lt;a href=&quot;http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HHL&lt;/a&gt;中涉及到了starGAN，所以做一个StarGAN的阅读记录，并比较与CycleGAN的区别。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo&lt;/p&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="starGAN" scheme="http://yoursite.com/tags/starGAN/"/>
    
  </entry>
  
  <entry>
    <title>CASN</title>
    <link href="http://yoursite.com/2018/12/17/CASN/"/>
    <id>http://yoursite.com/2018/12/17/CASN/</id>
    <published>2018-12-17T03:42:25.000Z</published>
    <updated>2018-12-18T07:59:48.469Z</updated>
    
    <content type="html"><![CDATA[<p>CASN: <a href="https://arxiv.org/abs/1811.07487" target="_blank" rel="noopener">Re-Identification with Consistent Attentive Siamese Networks</a></p><p>Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J. Radke</p><a id="more"></a><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>这篇论文；</p><ul><li>采用了Grad-CAM作为attention机制</li><li>attention consistency: 采用Siamese-Net来使同一个人的attention位置是一样的</li></ul><h1 id="2-The-Consistent-Attentive-Siamese-Network"><a href="#2-The-Consistent-Attentive-Siamese-Network" class="headerlink" title="2. The Consistent Attentive Siamese Network"></a>2. The Consistent Attentive Siamese Network</h1><p>整体网络架构如图所示：</p><p><img src="./CASN/CASN1.png" alt="CASN的整体网络架构"><br><img src="/2018/12/17/CASN/CASN1.png" title="CASN的整体网络架构"></p><ul><li>整体网络架构以IDE为基准网络，分为两部分:</li><li><ul><li>Identification Module</li></ul></li><li><ul><li>Siamese Module</li></ul></li><li>Identification 和 Siamese 的特征提取网络共享，不同的只是fc层</li></ul><h2 id="2-1-The-Identification-Module"><a href="#2-1-The-Identification-Module" class="headerlink" title="2.1 The Identification Module"></a>2.1 The Identification Module</h2><p>通过<a href="https://tjjtjjtjj.github.io/2018/12/14/Grad-CAM/#more" target="_blank" rel="noopener">Grad-CAM</a>的学习，已经知道了Grad-CAM的作用。</p><p><strong>Identification loss:</strong></p><script type="math/tex; mode=display">L_{ide}=-\sum_{n=1}^N log \frac{exp(y_{c_n})}{\sum_j exp(y_j)}</script><p>Identification loss 更偏向于不同行人之间的判别信息。</p><p><strong>Identification attention loss:</strong></p><script type="math/tex; mode=display">L_{ia}=\overline{y_{c_n}}</script><p>其中，给定一张图片$I_n$和类别$c_n$，Grad-CAM得到attention map $M_n$，做归一化操作，令$\Sigma(M_n)=sigmoid(\alpha(M_n-\beta))$，从而得到去掉attention区域的新图片$\overline{I_n}=I_n*(1-\Sigma(M_n))$，$\overline{y_{c_n}}$是$\overline{I_n}$的预测值。</p><p>Identification attention loss 更偏向于行人的全部信息。</p><p>我的理解是$\overline{I_n}$中尽可能包含少的ID信息，所以预测出是$c_n$的概率更小，得到的attention区域尽可能地包括全部信息。</p><p><strong>两种loss的效果对比图</strong><br><img src="./CASN/CASN2.png" alt="两种loss的效果对比图"><br><img src="/2018/12/17/CASN/CASN2.png" title="两种loss的效果对比图"></p><h2 id="2-2-The-Siamese-Module"><a href="#2-2-The-Siamese-Module" class="headerlink" title="2.2 The Siamese Module"></a>2.2 The Siamese Module</h2><p>Siamese Module 结构图<br><img src="./CASN/CASN3.png" alt="Siamese Module 结构图"><br><img src="/2018/12/17/CASN/CASN3.png" title="Siamese Module 结构图"></p><p><strong>Siamese loss</strong></p><script type="math/tex; mode=display">L_{bce}=-\sum_p log(\frac{exp(z_{c_p})}{exp(z_0)+exp(z_1)})</script><p><strong>Siamese attention loss</strong></p><script type="math/tex; mode=display">\alpha_i=\begin{cases}1, \mbox{if} f_i^->0 \\0, otherwise\end{cases}</script><script type="math/tex; mode=display">s_1=<\alpha, f_1>, s_2=<\alpha, f_2>(dot products)</script><script type="math/tex; mode=display">\alpha_1^k=GAP(\frac{\partial s_1}{\partial A_1}), \alpha_2^k=GAP(\frac{\partial s_2}{\partial A_2}) \tag{5}</script><script type="math/tex; mode=display">M_1=ReLU(\sum_k \alpha_1^k A_1^k),M_2=ReLU(\sum_k \alpha_2^k A_2^k)</script><script type="math/tex; mode=display">L_{sa}=L_{bce}+\alpha \parallel M_{m1}^{resize}-M_{m2}^{resize} \parallel _2</script><script type="math/tex; mode=display">\alpha=0.2</script><p>其中，$M_{m1}$是$M_1$中超过阈值t的元素，$M_{m1}^{resize}$是$M_{m1}$resize成相同大小的元素，主要是为了解决对齐问题。</p><p>通过与作者的沟通，作者认为$s_1$表示了$f_1$中对 BCE prediction 有用的信息。但是我没有见过这么表示对预测有用的信息的方法，之前只见过通过类别进行反向传播的(Grad-CAM)，但是作者这么坚持，说明应该是有效的。</p><blockquote><ol><li>Sorry I didn’t get your first question. By finding neurons in fi which are larger than zero, we find features in fi which have positive influence on BCE prediction.</li><li>s1 is not I1’s class. We here call s1 the importance score, which collect scores for every neuron which contributes to BCE prediction.</li></ol></blockquote><h2 id="2-3-Overall-Design-of-the-CASN"><a href="#2-3-Overall-Design-of-the-CASN" class="headerlink" title="2.3 Overall Design of the CASN"></a>2.3 Overall Design of the CASN</h2><p>CASN的整体架构<br><img src="./CASN/CASN4.png" alt="CASN的整体架构"><br><img src="/2018/12/17/CASN/CASN4.png" title="CASN的整体架构"></p><p><strong>The overall loss</strong></p><script type="math/tex; mode=display">L=L_{ide}+\lambda_1 L_{ia}+\lambda_2 L_{sa}</script><h3 id="3-Experiments-and-Results"><a href="#3-Experiments-and-Results" class="headerlink" title="3. Experiments and Results"></a>3. Experiments and Results</h3><h4 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h4><ul><li>input: 288x144</li><li>SGD: momentum=0.9</li><li>lr=0.03</li><li>epoch=40</li><li>lr decay=0.1 after 30</li><li>baseline: IDE and PCB( input: 384x128)</li><li>batch=16</li><li>test:  we send the query and gallery as pair inputs to obtain attention maps $\parallel M_{m1}^{resize}-M_{m2}^{resize} \parallel _2$</li></ul><h4 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h4><p><img src="./CASN/CASN5.png" alt="Results"><br><img src="/2018/12/17/CASN/CASN5.png" title="Results"></p><p><img src="./CASN/CASN6.png" alt="Ablation Study"><br><img src="/2018/12/17/CASN/CASN6.png" title="Ablation Study"></p><p>通过 Ablation Study , 对比CASN(IDE)、PCB，可以看出IA或者SA的作用和简单地分成6块达到的效果是类似的，这是不是说明了这种attention机制没有很大的作用，或者说分成6块就已经是一种很好的attention机制。</p><p>另外，+IA、+SA、CASN的对比，感觉IA或者SA一种机制就已经足够了，两者达到的效果是一样的，只使用一种就可以了。</p><h3 id="4-Others"><a href="#4-Others" class="headerlink" title="4. Others"></a>4. Others</h3><p>这篇论文不懂的地方：</p><ul><li>$L_{ia}$为什么可以直接这么写，不需要经过softmax之类的，或者不应该是每类的概率差不多么，</li><li>$L_{ia}$还是经过相同的网络得到的吗？反向求导要怎么写？</li><li>根据Grad-CAM的以类别反向求导，方程5给我的感觉更像是$f$的特征和作为输入图片的分类预测值，合理性站不住脚。</li><li>在测试时，需要每次输入一对图片，是不是太慢了。</li><li>如果实验结果可以复现的话，那么IA我觉得还是很有用的，解释性也强。</li></ul><p>参考：</p><ul><li>对于 Identification attention loss 的流程，需要参考<a href="https://tjjtjjtjj.github.io/2018/12/14/Grad-CAM/#more" target="_blank" rel="noopener">Grad-CAM</a>和<a href="https://arxiv.org/pdf/1802.10171.pdf" target="_blank" rel="noopener">GAIN</a>, GAIN也可以在<a href="https://tjjtjjtjj.github.io/2018/12/14/Grad-CAM/#more" target="_blank" rel="noopener">Grad-CAM</a>中找到详解。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CASN: &lt;a href=&quot;https://arxiv.org/abs/1811.07487&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Re-Identification with Consistent Attentive Siamese Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J. Radke&lt;/p&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="person-reid" scheme="http://yoursite.com/tags/person-reid/"/>
    
      <category term="Grad-CAM" scheme="http://yoursite.com/tags/Grad-CAM/"/>
    
  </entry>
  
</feed>
