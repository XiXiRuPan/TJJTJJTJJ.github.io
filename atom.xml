<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>田佳杰</title>
  <icon>https://www.gravatar.com/avatar/cbd320e406f4c9571bb798e8810c4d18</icon>
  <subtitle>记录一些学习到的东西和论文记录</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-13T14:32:44.801Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jiajie Tian</name>
    <email>18810906582@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2018/09/13/test2/"/>
    <id>http://yoursite.com/2018/09/13/test2/</id>
    <published>2018-09-13T14:31:32.607Z</published>
    <updated>2018-09-13T14:32:44.801Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: test2<br>mathjax: false<br>date: 2018-09-13 22:31:32<br>tags:<br>categories:</p><hr><p>kkjgk</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;p&gt;title: test2&lt;br&gt;mathjax: false&lt;br&gt;date: 2018-09-13 22:31:32&lt;br&gt;tags:&lt;br&gt;categories:&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;kkjgk&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>test</title>
    <link href="http://yoursite.com/2018/09/13/test/"/>
    <id>http://yoursite.com/2018/09/13/test/</id>
    <published>2018-09-13T14:30:24.000Z</published>
    <updated>2018-09-13T14:30:24.982Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>numpy</title>
    <link href="http://yoursite.com/2018/09/13/numpy/"/>
    <id>http://yoursite.com/2018/09/13/numpy/</id>
    <published>2018-09-13T14:27:14.000Z</published>
    <updated>2018-09-13T14:28:36.851Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>neural style</title>
    <link href="http://yoursite.com/2018/09/11/neural-style/"/>
    <id>http://yoursite.com/2018/09/11/neural-style/</id>
    <published>2018-09-11T03:19:25.000Z</published>
    <updated>2018-09-12T10:43:57.537Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>本文主要是针对<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">陈云的PyTorch入门与实践</a>的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。<br>这是我的<a href="https://github.com/TJJTJJTJJ/pytorch__learn" target="_blank" rel="noopener">代码</a><br>大神链接：(<a href="https://github.com/anishathalye/neural-style" target="_blank" rel="noopener">https://github.com/anishathalye/neural-style</a>)<br>这是论文作者写的</p><hr><a id="more"></a><h1 id="2-问题及其解决"><a href="#2-问题及其解决" class="headerlink" title="2 问题及其解决"></a>2 问题及其解决</h1><p>我在第六章和第七章的时候还是基于pytorch 0.4.0，而第八章的时候我开始基于pytorch 0.4.1，所以以下的内容介绍都是基于0.4.1</p><h2 id="2-1-文件组织形式"><a href="#2-1-文件组织形式" class="headerlink" title="2.1 文件组织形式"></a>2.1 文件组织形式</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">├─checkpoints/</span><br><span class="line">├─content_img/</span><br><span class="line">│  ├─<span class="selector-tag">input</span>.jpg</span><br><span class="line">│  ├─output.jpg</span><br><span class="line">│  └─style.jpg</span><br><span class="line">├─data/</span><br><span class="line">│  ├─coco/<span class="selector-tag">a</span>.jpg</span><br><span class="line"></span><br><span class="line">├─dataset/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─dataset.py</span><br><span class="line">├─models/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─PackedVGG.py</span><br><span class="line">│  └─transformer_net.py</span><br><span class="line">└─utils/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─utils.py</span><br><span class="line">│  └─visualize.py</span><br><span class="line">├─config.py</span><br><span class="line">└─main.py</span><br></pre></td></tr></table></figure><p>其中，上半部分是对数据和模型的保存组织形式，我们只需要能对应起来即可，其中，checkpoints是为了保存模型，content_img中的style.jpg是训练时候的风格图片，input.jpg是测试的输入，output.jpg是测试的输出，data中的数据是训练数据，主要是因为这个训练数据太整齐，是用ImageFolder读取的，为了避免麻烦，也为了在测试的时候方便观察图片，所以style.jpg我们暂时放在了content中。<br>下半部分是重点，我们需要写的代码，每次都是先从dataset.py和models开始写起，然后导入visualize.py，这个文件基本不会发生改变，然后同时写main.py和config.py，边写边扩展utils中的其他文件，例如main中用到的函数等等。</p><h2 id="2-2-models"><a href="#2-2-models" class="headerlink" title="2.2 models"></a>2.2 models</h2><h3 id="PackedVGG-py"><a href="#PackedVGG-py" class="headerlink" title="PackedVGG.py"></a>PackedVGG.py</h3><p>这里我们主要是取已有的网络，得到中间层的输出<br><strong>models.named_parameters()</strong>:返回的是一个生成器，每次返回一个参数的关键字和值<br><strong>models.state_dict()</strong>:返回的是一个字典，记录了参数的关键字和值<br><strong>models.parameters()</strong>:返回的是变量，没有名字，可以在requires_grad中用到<br>models.features返回的是相对应的模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">7</span>]: <span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: models = vgg16(pretrained=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: model = models.features[:<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: model</span><br><span class="line">Out[<span class="number">10</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: models.parameters()</span><br><span class="line">Out[<span class="number">11</span>]: &lt;generator object Module.parameters at <span class="number">0x7f8fad26b3b8</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: models.named_parameters()</span><br><span class="line">Out[<span class="number">12</span>]: &lt;generator object Module.named_parameters at <span class="number">0x7f8f29e99d58</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: model.named_parameters()</span><br><span class="line">Out[<span class="number">13</span>]: &lt;generator object Module.named_parameters at <span class="number">0x7f8fad26b2b0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: model.parameters()</span><br><span class="line">Out[<span class="number">14</span>]: &lt;generator object Module.parameters at <span class="number">0x7f8fad26b4c0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: model.state_dict()</span><br><span class="line">Out[<span class="number">15</span>]: </span><br><span class="line">OrderedDict([(<span class="string">'0.weight'</span>, tensor([[[[<span class="number">-0.5537</span>,  <span class="number">0.1427</span>,  <span class="number">0.5290</span>],</span><br><span class="line">                        [<span class="number">-0.5831</span>,  <span class="number">0.3566</span>,  <span class="number">0.7657</span>],</span><br><span class="line">                        [<span class="number">-0.6902</span>, <span class="number">-0.0480</span>,  <span class="number">0.4841</span>]],</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16</span><br><span class="line">models = vgg16(pretarined = <span class="keyword">True</span>)</span><br><span class="line">In [<span class="number">19</span>]: models</span><br><span class="line">Out[<span class="number">19</span>]: </span><br><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">3</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">6</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">8</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">11</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">13</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">15</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">18</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">20</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">22</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">25</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">27</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">29</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">    (<span class="number">3</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">    (<span class="number">4</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: models.features</span><br><span class="line">Out[<span class="number">20</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">3</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">6</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">8</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">11</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">13</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">15</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">18</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">20</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">22</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">25</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">27</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">29</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: models.features[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">21</span>]: ReLU(inplace)</span><br><span class="line"><span class="comment"># list</span></span><br><span class="line">In [<span class="number">27</span>]: models4 = models2[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: models4</span><br><span class="line">Out[<span class="number">28</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: models4list</span><br><span class="line">Out[<span class="number">32</span>]: </span><br><span class="line">[Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line"> ReLU(inplace)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">36</span>]: models4list[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">36</span>]: ReLU(inplace)</span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: models4list[<span class="number">1</span>].named_parameters</span><br><span class="line">Out[<span class="number">37</span>]: &lt;bound method Module.named_parameters of ReLU(inplace)&gt;</span><br></pre></td></tr></table></figure><p>sequencial是支持索引操作的<br>list(module)会变成一个list，可以通过索引来获取层，注意，nn.ModuleList, nn.Sequential, nn.Conv等都是Module,都可以通过named_parameters来获取参数。<br>为了能够提取出中间层的输出，作者换了一个方法，用的nn.ModuleList,nn.ModuleList和nn.Sequential的区别在此才真正显现，nn.Sequential更有利于直接把输入传给Module，计算是一个整体，写起来更方便，而nn.Modulist则不能直接把输入传给Module，需要用循环传输入，更有利于在层中做一些保留，提取中间层的输出。后面我们会讲到hook。或者说提取中间层的输出我们可以选择在定义网络的forward中进行，另外，就是需要注意的是，这里的输入是一个batch_size大小的矩阵，所以即便像作者这样，用一个列表保存输出，但实际输出的列表中的元素都是(b,n,h,w)大小的。后面我会验证。</p><p>提取中间层的输出有两种方法：<br>第二种方法参考链接：<a href="https://www.jianshu.com/p/0a23db1df55a" target="_blank" rel="noopener">https://www.jianshu.com/p/0a23db1df55a</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种方法，这种方法是在前向网络中提取输出，好像也是在反向传播网络中，但这种提取中间层是永久性的，也适合用这些层的做其他运算，这些运算是计算在整体网络框架中的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> ii, model <span class="keyword">in</span> enumerate(self.features):</span><br><span class="line">        x = model(x)</span><br><span class="line">        <span class="keyword">if</span> ii <span class="keyword">in</span> &#123;<span class="number">3</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">22</span>&#125;:</span><br><span class="line">            results.append(x)</span><br><span class="line"></span><br><span class="line">    vgg_outputs = namedtuple(<span class="string">"VggOutputs"</span>, [<span class="string">'relu1_2'</span>, <span class="string">'relu2_2'</span>, <span class="string">'relu3_3'</span>, <span class="string">'relu4_3'</span>])</span><br><span class="line">    <span class="keyword">return</span> vgg_outputs(*results)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二种方法，适合在在不影响整体网络的情况下拿出一个分支进行单独计算，现在还不清楚这样子会不会影响backward，个人感觉会，因为也是相当于一个变量对其进行计算，导数为1。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    x= self.model(x)</span><br><span class="line">    self.fea = x</span><br><span class="line">   x = self.main(x)</span><br></pre></td></tr></table></figure><hr><h3 id="transformer-py"><a href="#transformer-py" class="headerlink" title="transformer.py"></a>transformer.py</h3><p>可参考<a href="https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py" target="_blank" rel="noopener">链接</a></p><ol><li>padding的操作是边界反射补充</li><li>放大方法是双线性插值，而不是ConvTransposed2d，即unsample或者说是interpolate， 但是其中的一个参数align_corners一直<strong>没有理解</strong>，既然是双线性插值，那结果就是固定的，怎么还会因为其他参数发生变化。</li><li><p>其中，写的时候必要的时候可以写写子网络<br>这里我对residualblock提出了疑问，事实上left+right后面可以没有relu层，这一点我们可以从以下链接找到说明。<br><a href="https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py" target="_blank" rel="noopener">https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py</a><br><a href="http://torch.ch/blog/2016/02/04/resnets.html" target="_blank" rel="noopener">http://torch.ch/blog/2016/02/04/resnets.html</a></p><blockquote><p>The above result seems to suggest that it’s important to avoid changing data that passes through identity connections only. We can take this philosophy one step further: should we remove the ReLU layers at the end of each residual block? ReLU layers also perturb data that flows through identity connections, but unlike batch normalization, ReLU’s idempotence means that it doesn’t matter if data passes through one ReLU or thirty ReLUs. When we remove ReLU layers at the end of each building block, we observe a small improvement in test performance compared to the paper’s suggested ReLU placement after the addition. However, the effect is fairly minor. More exploration is needed.</p></blockquote></li><li><p>对于其他的出现的网络架构，其实都是有理可循的，但暂时不是本篇的重点，所以只做一个记录。<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">上卷积</a>简单地看了看这篇论文，unsample要比ConvTransposed2D要好，但是没有看懂。留作后续。  </p></li></ol><hr><h3 id="dataset-py-amp-visualize-py"><a href="#dataset-py-amp-visualize-py" class="headerlink" title="dataset.py &amp; visualize.py"></a>dataset.py &amp; visualize.py</h3><p>因为加载数据是用的tv.datasets.ImageFolder，所以dataset.py不需要写，<br>visualize.py是第六章的时候写好的，这里只写几个改进的</p><ol><li>self.vis = Visdom(env=env,use_incoming_socket=False, **kwargs)，这里的use_incoming_socket是不需要从浏览器接受数据到软件中，如果没有的话会提示 ‘&gt;’ not supported between instances of ‘float’ and ‘NoneType’</li><li>在一个函数前提示输入的大小和类型是一件很重要的事情，必要的时候需要输入分布，</li><li>这里的plot用了一个很巧的方法，用字典记录不同的点<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.index = &#123;&#125;</span><br><span class="line">x = self.index.get(win,<span class="number">0</span>)</span><br><span class="line">self.index[win] = x+<span class="number">1</span></span><br></pre></td></tr></table></figure></li></ol><p>其他的细节可以看代码中的记录，应该比较清晰了。</p><hr><h3 id="main-py-amp-utils-py-amp-config-py"><a href="#main-py-amp-utils-py-amp-config-py" class="headerlink" title="main.py &amp; utils.py &amp;  config.py"></a>main.py &amp; utils.py &amp;  config.py</h3><p>其中utils主要为main提供一些用到的函数，config提供参数，<br>main作为主函数，里面主要就是train(),val(),test(),help(),下面记录一些写main函数的一些疑问。</p><h4 id="cuda"><a href="#cuda" class="headerlink" title="cuda"></a>cuda</h4><p>这里写几种怎么从cpu到gpu的方法以及应用场景。<br><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line">models.<span class="keyword">to</span>(device)</span><br><span class="line">tensor = tensor.<span class="keyword">to</span>(device)</span><br><span class="line">此时使用默认的cuda，一般是cuda:<span class="number">0</span>，适用于全局</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">torch.cuda.current_device() <span class="comment"># 查询当前GPU</span></span><br><span class="line">torch.cuda.set_device(<span class="number">1</span>)</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">models.<span class="keyword">to</span>(device)</span><br><span class="line">此时用的是cuda:<span class="number">1</span>，使用于全局</span><br><span class="line"></span><br><span class="line"><span class="comment">#第三种</span></span><br><span class="line"><span class="comment">#上下文管理器</span></span><br><span class="line"><span class="keyword">with</span> torch.cuda.device(<span class="number">1</span>):</span><br><span class="line">    models.<span class="keyword">to</span>(device)</span><br><span class="line"><span class="comment">#第四种</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>]=<span class="string">"2"</span></span><br><span class="line">没用过</span><br></pre></td></tr></table></figure></p><h3 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a><a href="">tqdm</a></h3><p>进度条，但是只在jupyter和终端中用的时候效果很明显，在代码中用的效果没有那么好，tqdm试了试，用在enumerate()中时，需要写成这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">elements = (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>)</span><br><span class="line"><span class="keyword">for</span> count, ele <span class="keyword">in</span> tqdm(enumerate(elements)):</span><br><span class="line">    print(count, i)</span><br><span class="line"><span class="comment"># two arguments</span></span><br><span class="line"><span class="keyword">for</span> count, ele <span class="keyword">in</span> tqdm(enumerate(elements), total=len(train_ids)):</span><br><span class="line">    print(count, i)</span><br></pre></td></tr></table></figure></p><p>包括zip也是一样，因为他们返回的是一个生成器，并不知道长度。</p><h3 id="反向传播和梯度下降"><a href="#反向传播和梯度下降" class="headerlink" title="反向传播和梯度下降"></a>反向传播和梯度下降</h3><p>参考链接<a href="https://blog.csdn.net/qq_16234613/article/details/80025832" target="_blank" rel="noopener">https://blog.csdn.net/qq_16234613/article/details/80025832</a><br>这里主要是针对第七章和第八章出现的反向传播和梯度下降出现的问题进行记录。<br>在第七章，是这么实现分别训练的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">fake_img =  netg(noises).detach() </span><br><span class="line">fake_output = netd(fake_img)</span><br><span class="line">error_d_fake = criterion(fake_output, fake_labels)</span><br><span class="line">error_d_fake.backward()</span><br><span class="line">optimizer_d.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer_g.zero_grad()</span><br><span class="line">noises.data.copy_(t.randn(opt.batch_size, opt.nz, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">fake_img = netg(noises)</span><br><span class="line">output = netd(fake_img)</span><br><span class="line">error_g = criterion(output, true_labels)</span><br><span class="line">error_g.backward()</span><br><span class="line">optimizer_g.step()</span><br></pre></td></tr></table></figure></p><p>y = x.detach()：表示将生成一个新的叶子节点，值与当前节点的值相同，但是y.requires_grad = False, y.grad_fn=None，此时x和y共享内存，对y数据的操作也会影响x，可以理解为冻结了通过y进行反向传播的路。如果在网络的输出detach，即y= models(x).detach()，可以理解成，models只进行前向传播，grad=None。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">17</span>]: <span class="selector-tag">a</span> = torch.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: <span class="selector-tag">a</span>.requires_grad=True</span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: <span class="selector-tag">b</span> = a*<span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: <span class="selector-tag">b</span>.requires_grad</span><br><span class="line">Out[<span class="number">20</span>]: True</span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: <span class="selector-tag">b</span>.grad_fn</span><br><span class="line">Out[<span class="number">21</span>]: &lt;MulBackward at <span class="number">0</span>x7f8fac6e40f0&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: c = <span class="selector-tag">b</span>.detach()</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: c.requires_grad</span><br><span class="line">Out[<span class="number">23</span>]: False</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: print(c.grad_fn)</span><br><span class="line">None</span><br><span class="line">In [<span class="number">25</span>]: c.is_leaf</span><br></pre></td></tr></table></figure></p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">In <span class="comment">[2]</span>: a = torch.ones(3,3)</span><br><span class="line">In <span class="comment">[14]</span>: b</span><br><span class="line">Out<span class="comment">[14]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>, grad_fn=&lt;MulBackward&gt;)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[15]</span>: c =  b.detach()</span><br><span class="line"></span><br><span class="line">In <span class="comment">[16]</span>: c</span><br><span class="line">Out<span class="comment">[16]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[17]</span>: c<span class="comment">[0,0]</span>=1</span><br><span class="line"></span><br><span class="line">In <span class="comment">[18]</span>: c</span><br><span class="line">Out<span class="comment">[18]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[1., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[19]</span>: b</span><br><span class="line">Out<span class="comment">[19]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[1., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>, grad_fn=&lt;MulBackward&gt;)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[20]</span>: c.requires_grad</span><br><span class="line">Out<span class="comment">[20]</span>: False</span><br><span class="line"></span><br><span class="line">In <span class="comment">[21]</span>: b.grad_fn</span><br><span class="line">Out<span class="comment">[21]</span>: &lt;MulBackward at 0x7f764429ffd0&gt;</span><br><span class="line"></span><br><span class="line">In <span class="comment">[22]</span>: b.grad_fn.next_functions</span><br><span class="line">Out<span class="comment">[22]</span>: ((&lt;AccumulateGrad at 0x7f7644428358&gt;, 0),)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[23]</span>: a.grad_fn</span><br></pre></td></tr></table></figure><p>在第八章，是这么表示的<br><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">param</span> <span class="built_in">in</span> vgg16.parameters():</span><br><span class="line">    <span class="built_in">param</span>.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure></p><p>这种表示可以使得某一个网络不参与梯度下降这个过程，但是对于网络的输入和输出还是支持梯度下降的。<br>requires_grad只是表示当前的变量不再需要梯度下降，<br>综上所述，对于中间变量，需要使用x.detach()，使其变成默认的叶子节点，对于叶子节点，使用x.requires_grad。并且对于中间变量使用requires_grad会报错。</p><p>在第八章，还有一种表示方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    features = vgg16(style_img)</span><br><span class="line">    gram_style = [gram_matrix(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> features]</span><br><span class="line"></span><br><span class="line"><span class="meta">@t.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stylize</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p><p>这种方法会使得任何计算得到的结果都是requires_grad = False,暂时不清楚和detach()的区别。也是一种表示只前向传播的方法，不参与反向传播和梯度下降。</p><h3 id="train"><a href="#train" class="headerlink" title="train()"></a>train()</h3><p>图片分为两种：风格图片，只需要一张，内容图片，很多，用于训练，这一点没有暂时没有理解为什么这么设置。其中，对输入的图片进行了乘以255，我觉得是因为为了使模型的输出直接就是255，不需要再进行处理，没有验证。<br>ensor.item()<br> tensor.tolist()<br>content_image = tv.datasets.folder.default_loader(opt.content_path)<br>在训练过程中，会发现对于整个训练过程，不仅有神经网络，而且还有自己定义的函数，nn.functional，还有两个损失函数，这是之前没有预料到的。</p><h3 id="保存图片"><a href="#保存图片" class="headerlink" title="保存图片"></a>保存图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存图片的几种方法，第七章的是 </span></span><br><span class="line"><span class="comment"># 0-1</span></span><br><span class="line">tv.utils.save_image(fix_fake_imgs,<span class="string">'%s/%s.png'</span> % (opt.img_save_path, epoch),normalize=<span class="keyword">True</span>, range=(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># vis.save竟然没找到  我的神   </span></span><br><span class="line"><span class="comment"># 0-1</span></span><br><span class="line">vis.img(<span class="string">'input'</span>)</span><br><span class="line">vis.save([opt.env])</span><br></pre></td></tr></table></figure><h3 id="utils-py"><a href="#utils-py" class="headerlink" title="utils.py"></a>utils.py</h3><p>这里的疑问是得到gram矩阵的时候，为什么要除以c*h*w,而不是h*w，虽然源码都是这么写的。</p><p>写到这里也还是还要很多疑问，暂时保留。<br>昨天发现训练的过程不对，今天在对比代码的过程中，发现了自己写代码的一些漏洞，主要有</p><ol><li>命名不规范：表示同一个东西出现了两个命名，导致了自己在写代码的过程中传参出现了问题，或者是一类东西没有一个规则进行命名，导致自己在写代码的过程中用到之前的变量的时候必须返回去去查找这个变量，效率低且容易出错。</li><li>对源码的修改不是很恰当，导致在写上卷积层的输出和源码完全不一致，这个是自己之前没有遇到的。</li><li>visdom的运用，我用不同的environment导致结果也不一样，default是之前一直用的，这次换成了test1之后显示的结果就对了。这个暂时还不清楚原因，如果是会保留信息的话，但是plot是重新开始画的，等会测试测试vis的问题。是网络的问题。但是vis.save()的介绍是序列化信息，暂时还没有理解。</li></ol><p>贴两个成果图看看效果。<br><img src="/pictures/neural-style/pic1.png" alt="训练过程中的图片"></p><h2 id="遗留的问题"><a href="#遗留的问题" class="headerlink" title="遗留的问题"></a>遗留的问题</h2><p>Gram矩阵为什么可以代表图片风格，这里有个解释(<a href="https://arxiv.org/pdf/1701.01036.pdf)，还没来得及看。" target="_blank" rel="noopener">https://arxiv.org/pdf/1701.01036.pdf)，还没来得及看。</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1 前言&quot;&gt;&lt;/a&gt;1 前言&lt;/h1&gt;&lt;p&gt;本文主要是针对&lt;a href=&quot;https://github.com/chenyuntc/pytorch-book&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;陈云的PyTorch入门与实践&lt;/a&gt;的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。&lt;br&gt;这是我的&lt;a href=&quot;https://github.com/TJJTJJTJJ/pytorch__learn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;代码&lt;/a&gt;&lt;br&gt;大神链接：(&lt;a href=&quot;https://github.com/anishathalye/neural-style&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/anishathalye/neural-style&lt;/a&gt;)&lt;br&gt;这是论文作者写的&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="deep learning" scheme="http://yoursite.com/categories/deep-learning/"/>
    
    
      <category term="neural style transfer" scheme="http://yoursite.com/tags/neural-style-transfer/"/>
    
  </entry>
  
  <entry>
    <title>data</title>
    <link href="http://yoursite.com/2018/09/06/data/"/>
    <id>http://yoursite.com/2018/09/06/data/</id>
    <published>2018-09-06T15:06:10.000Z</published>
    <updated>2018-09-11T03:03:05.762Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Market-1501"><a href="#Market-1501" class="headerlink" title="Market-1501"></a>Market-1501</h1><ul><li>6个摄像头</li><li>1501个人，其中751个人、12936张图片用于训练，750个人、19732张图片用于测试，</li><li>3368张查询图片    </li></ul><hr><a id="more"></a><h2 id="目录说明"><a href="#目录说明" class="headerlink" title="目录说明"></a>目录说明</h2><ul><li>bounding_box_test <ul><li>19732张测试图片</li><li>0000_c1s1_000151_01.jpg</li><li>前缀为 0000 表示在提取这 750 人的过程中DPM检测错的图（可能与query是同一个人），-1 表示检测出来其他人的图（不在这 750 人中）</li><li>DPM检测出的</li><li>gallery样本</li></ul></li><li>bounding_box_train <ul><li>12936张训练图片</li><li>0002_c1s1_000451_03.jpg</li><li>train样本</li></ul></li><li>query <ul><li>3368张图片，与test的750人对应 但是是人工绘制的 与bounding_box_test中的图片略微有所不同 与gt_bbox中的图片是一样的</li><li>0001_c1s1_001051_00.jpg</li><li>为 750 人在每个摄像头中随机选择一张图像作为query，因此一个人的query最多有 6 个，共有 3,368 张图像</li><li>query样本</li></ul></li><li>gt_bbox <ul><li>25259张图片 手工绘制  包含1501个行人</li><li>0001_c1s1_001051_00.jpg </li><li>手工标注的bounding box，用于判断DPM检测的bounding box是不是一个好的box</li></ul></li><li>gt_query 是对3368张图片的查询图片的判定，好坏，<ul><li>0001_c1s1_001051_00_good.mat</li><li>matlab格式，用于判断一个query的哪些图片是好的匹配（同一个人不同摄像头的图像）和不好的匹配（同一个人同一个摄像头的图像或非同一个人的图像）</li></ul></li></ul><hr><h2 id="命名规则"><a href="#命名规则" class="headerlink" title="命名规则"></a>命名规则</h2><p>以 0001_c1s1_000151_01.jpg 为例<br>1） 0001 表示每个人的标签编号，从0001到1501；<br>2） c1 表示第一个摄像头(camera1)，共有6个摄像头；<br>3） s1 表示第一个录像片段(sequece1)，每个摄像机都有数个录像段；<br>4） 000151 表示 c1s1 的第000151帧图片，视频帧率25fps；<br>5） 01 表示 c1s1_001051 这一帧上的第1个检测框，由于采用DPM检测器，对于每一帧上的行人可能会框出好几个bbox。00 表示手工标注框 </p><h1 id="DukeMTMC-reID"><a href="#DukeMTMC-reID" class="headerlink" title="DukeMTMC-reID"></a>DukeMTMC-reID</h1><p>DukeMTMC是多目标多摄像机行人跟踪数据集，8个摄像头，2700多个人物，DukeMTMC-reID是DukeMTMC的行人重识别子集，并且提供了人工标注的bounding box。<br>从视频中每 120 帧采样一张图像，得到了 36,411 张图像。一共有 1,404 个人出现在大于两个摄像头下，有 408 个人 (distractor ID) 只出现在一个摄像头下</p><hr><h2 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h2><ul><li>bounding_box_test<ul><li>0002_c1_f0044158.jpg</li><li>用于测试集的 702 人</li><li>包含 17,661 张图像（随机采样，702 ID + 408 distractor ID）</li></ul></li><li>bounding_box_train<ul><li>0001_c2_f0046182.jpg</li><li>用于训练集的 702 人</li><li>包含 16,522 张图像（随机采样）</li></ul></li><li>query<ul><li>0005_c2_f0046985.jpg</li><li>为测试集中的 702 人在每个摄像头中随机选择一张图像作为 query，共有 2,228 张图像</li></ul></li></ul><p>。</p><hr><h2 id="命名规则-1"><a href="#命名规则-1" class="headerlink" title="命名规则"></a>命名规则</h2><p>0001_c2_f0046182.jpg<br>1） 0001 表示每个人的标签编号；<br>2） c2 表示来自第二个摄像头(camera2)，共有 8 个摄像头；<br>3） f0046182 表示来自第二个摄像头的第 46182 帧。</p><hr><h1 id="CUHK03"><a href="#CUHK03" class="headerlink" title="CUHK03"></a>CUHK03</h1><p>CUHK03是第一个足以进行深度学习的大规模行人重识别数据集，该数据集的图像采集于香港中文大学（CUHK）校园。数据以”cuhk-03.mat”的 MAT 文件格式存储，含有 1467 个不同的人物，由 5 对摄像头采集。</p><hr><h2 id="目录结构-1"><a href="#目录结构-1" class="headerlink" title="目录结构"></a>目录结构</h2><ul><li>detected - 5 * 1 cell  由机器标注，每个 cell 中包含一对摄像头组采集的照片，每个摄像头组由 M x 10 cells 组成，M 为行人索引，前 5 列和后 5 列分别来自同一组的不同摄像头。cell 内每个元素为一幅 H x W x 3 的行人框图像(uint8 数据类型)，个别图像可能空缺，为空集。<ul><li>843*10 cell 摄像头组pair 1</li><li>440*10 cell 摄像头组pair 2</li><li>77*10 cell 摄像头组pair 3</li><li>58*10 cell  摄像头组pair 4</li><li>49*10 cell摄像头组pair 5</li></ul></li><li>labeled  - 5 * 1 cell  行人框由人工标注，格式和内容和”detected”相同。<ul><li>843*10 cell</li><li>440*10 cell</li><li>77*10 cell</li><li>58*10 cell</li><li>49*10 cell</li></ul></li><li>testsets - 20*1 cell 测试协议，由 20 个 100 x 2 double 类型矩阵组成 (重复二十次)<ul><li>100*2 double matrix 100 行代表 100 个测试样本，第 1 列为摄像头 pair 索引，第 2 列为行人索引</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Market-1501&quot;&gt;&lt;a href=&quot;#Market-1501&quot; class=&quot;headerlink&quot; title=&quot;Market-1501&quot;&gt;&lt;/a&gt;Market-1501&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;6个摄像头&lt;/li&gt;
&lt;li&gt;1501个人，其中751个人、12936张图片用于训练，750个人、19732张图片用于测试，&lt;/li&gt;
&lt;li&gt;3368张查询图片    &lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>dataset</title>
    <link href="http://yoursite.com/2018/09/04/dataset/"/>
    <id>http://yoursite.com/2018/09/04/dataset/</id>
    <published>2018-09-04T11:28:20.000Z</published>
    <updated>2018-09-04T11:28:47.887Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://blog.fangchengjin.cn/reid-market-1501.html" target="_blank" rel="noopener">Person Re-identification数据集描述——Market-1501</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://blog.fangchengjin.cn/reid-market-1501.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Person Re-identification数据集描述——Market-1501&lt;/a&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Inception</title>
    <link href="http://yoursite.com/2018/09/04/Inception/"/>
    <id>http://yoursite.com/2018/09/04/Inception/</id>
    <published>2018-09-04T11:23:43.000Z</published>
    <updated>2018-09-04T11:31:01.245Z</updated>
    
    <content type="html"><![CDATA[<p>关于Inception的好的讲解<br><a href="https://blog.csdn.net/loveliuzz/article/details/79135583" target="_blank" rel="noopener">深度学习卷积神经网络——经典网络GoogLeNet(Inception V3)网络的搭建与实现</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于Inception的好的讲解&lt;br&gt;&lt;a href=&quot;https://blog.csdn.net/loveliuzz/article/details/79135583&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;深度学习卷积神经网络——经典网络Goo
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>deconv\dilated conv</title>
    <link href="http://yoursite.com/2018/09/04/deconv/"/>
    <id>http://yoursite.com/2018/09/04/deconv/</id>
    <published>2018-09-04T08:07:46.000Z</published>
    <updated>2018-09-12T03:09:02.924Z</updated>
    
    <content type="html"><![CDATA[<p>最近看的一片论文里看到了反卷积，找了一些材料，记录一下，留作纪念。<br>首先定义符号：    </p><ul><li>假设本文提到的图形和卷积核都是一维的线或者二维的正方形，$x$和$y$轴方向的padding和stride相同    </li><li>$i,o,k,p,s,i’,o’,k’,p’,s’$分别表示卷积/反卷积输入图片的大小input size, 输出图片的大小 output size，卷积/反卷积的核大小kernel size，padding，stride.<br><a href="https://github.com/vdumoulin/conv_arithmetic#convolution-arithmetic" target="_blank" rel="noopener">动图演示</a><br><a href="https://buptldy.github.io/2016/10/29/2016-10-29-deconv/" target="_blank" rel="noopener">详细解析</a>    <a id="more"></a>反卷积对应的是直接在原图上添加0<br>对应的公式如下：<script type="math/tex; mode=display">i'=o</script><script type="math/tex; mode=display">o'=i</script><script type="math/tex; mode=display">k'=k</script><script type="math/tex; mode=display">s'=1</script><script type="math/tex; mode=display">p'=k-1-p</script><script type="math/tex; mode=display">d'=1</script><script type="math/tex; mode=display">o' = (s-1)(i'-1)+i'-k'+2p'</script><script type="math/tex; mode=display">o' =   s(i'-1)+k-2p</script><script type="math/tex; mode=display">d' = 2</script><script type="math/tex; mode=display">k' = k + (d-1)*(k-1)</script><script type="math/tex; mode=display">p' = k-1-p+(d-1)*(k-1)</script><script type="math/tex; mode=display">o' = (s-1)(i'-1)+i'-k'+2p'</script><script type="math/tex; mode=display">o' = s(i'-1)+k+(d-1)*(k-1)-2p'</script>所以要保持图片图片大小不变，必须令s=1，<script type="math/tex">d = \frac{2}{k-1}*p</script><br>例如：k=3，d=p,</li></ul><p>以上只是先说明了对应尺寸的大小，具体的数学原理留作补充。</p><p>扩张卷积的计算公式与上面不一样<br>扩张卷积扩张的是卷积核的大小，在卷积核上添加0<br><a href="https://blog.csdn.net/Quincuntial/article/details/78743033" target="_blank" rel="noopener">扩张卷积</a></p><script type="math/tex; mode=display">o'=\frac{i'-k+2p-(k-1)*(d-1)}{s}+1</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近看的一片论文里看到了反卷积，找了一些材料，记录一下，留作纪念。&lt;br&gt;首先定义符号：    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假设本文提到的图形和卷积核都是一维的线或者二维的正方形，$x$和$y$轴方向的padding和stride相同    &lt;/li&gt;
&lt;li&gt;$i,o,k,p,s,i’,o’,k’,p’,s’$分别表示卷积/反卷积输入图片的大小input size, 输出图片的大小 output size，卷积/反卷积的核大小kernel size，padding，stride.&lt;br&gt;&lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic#convolution-arithmetic&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;动图演示&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://buptldy.github.io/2016/10/29/2016-10-29-deconv/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;详细解析&lt;/a&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="卷积\反卷积 扩张卷积" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF-%E5%8F%8D%E5%8D%B7%E7%A7%AF-%E6%89%A9%E5%BC%A0%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>pytorch</title>
    <link href="http://yoursite.com/2018/08/26/pytorch/"/>
    <id>http://yoursite.com/2018/08/26/pytorch/</id>
    <published>2018-08-26T13:38:25.000Z</published>
    <updated>2018-09-07T02:32:09.408Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><p>魔术方法: P23  </p><p>调试: P27<br><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ipdb</span><br><span class="line"><span class="title">ipdb</span>.set_trace()</span><br></pre></td></tr></table></figure></p><p>带下划线_的函数会修改Tensor本身，比如x.add_(y)和x.add(y)的区别  </p><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>Numpy与Tensor共享内存<br>b = a.numpy() # Tensor -&gt; Numpy<br><a id="more"></a><br>a = t.from_numpy(a)# Numpy -&gt; Tensor<br>x = x.cuda()<br>tensor的操作:torch.function,tensor.function.<br>普通索引共享内存<br>高级索引不共享内存<br>线性代数函数 P70<br>自动广播原则: unsqueese(view),expand(expand_as)<br>tensor=Tensor+Storage<br>持久化和加载: t.save(a,’a.pth’) b=t.load(‘a.pth’) P77<br>%timeit -n 10</p><hr><h2 id="Variable和autograd"><a href="#Variable和autograd" class="headerlink" title="Variable和autograd"></a>Variable和autograd</h2><p>from torch.autograd import Variable<br>三个属性<br>data: 对应Tensor<br>grad: 梯度，和data大小一样，也是Variable<br>grad_fn: 指向Function对象,用于构建计算图。用户创建对应叶子节点,grad_fn=None.记录的是它什么操作的输出。<br>Variable的构造函数的关键字参数:requires_grad(bool):是否需要求导; volatile(bool):True表示之上的计算图都不会求导<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(t.ones(2,2),requires_grad = True)</span><br><span class="line">y = x.sum()</span><br><span class="line">y.grad_fn</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br><span class="line">x.grad.data.zero_() <span class="comment"># 反向传播清零</span></span><br></pre></td></tr></table></figure></p><p>variable.backward(grad_variable=None, retain_graph=None, create_graph=None)<br>假设用户输入的数据是真实的不需要求导的。<br>数值在前向传导过程成会保存成buffer,计算梯度之后自动清空。多次反向求导可以使用关键字参数retain_graph=True<br>retain_graph=True 实现多次反向传播？？？？</p><p>反向传播过程中非叶子节点的导数在计算完之后就会清空，y=x*w,z=y.sum() 其中y.grad会清空。其对应的方法有两种，P92，t.autograd.grad(z,y)和hook<br>扩展Autograd Function：P95 自己实现前向和反向</p><hr><h2 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h2><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Net</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def \_\_init\_\_():</span></span><br><span class="line"><span class="class">super(<span class="type">Net</span>,<span class="title">self</span>).__init__()</span></span><br><span class="line"><span class="class">\# 有参数的层的定义</span></span><br><span class="line"><span class="class">def forward(<span class="title">self</span>,<span class="title">x</span>):</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">net = <span class="type">Net</span>()</span></span><br><span class="line"><span class="class">print(<span class="title">net</span>)</span></span><br><span class="line"><span class="class">list(<span class="title">net</span>.<span class="title">parameters</span>())</span></span><br><span class="line"><span class="class">for name,paramenters in net.named_parameters():</span></span><br><span class="line"><span class="class">print(<span class="title">name</span>,':',<span class="title">parameters</span>.<span class="title">size</span>())</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">out = net(<span class="title">input</span>)</span></span><br><span class="line"><span class="class">net.zero_grad()</span></span><br><span class="line"><span class="class">out.backwad(<span class="type">Variable</span>(<span class="title">t</span>.<span class="title">ones</span>(1,10)))</span></span><br><span class="line"><span class="class">&gt; ???</span></span><br></pre></td></tr></table></figure><p>nn.Sequential()<br>nn.ModuleList()<br>nn.ParameterList()<br>在优化器中为各层分别设置学习率<br>nn.functional对应nn.Module<br>参数初始化:<br>nn.Conv2d: nSamples<em>nChannels</em>Height*Widh<br>单样本: input.unsqueeze(0)<br>model.train()<br>model.eval()<br>前向或者后向注册钩子函数，P125，可以查看中间层。<br>获取网络的模块属性：getattr(module)P128<br>保存模型：t.save(net.state_dict(), ‘net.pth’)<br>加载模型：net2=Net() net2.load_state_dict(t.load(‘net.pth’))<br>多个GPU并行操作</p><hr><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = Variable(t.arrange(0,10))</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss</span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">net</span><span class="selector-class">.zero_grad</span>()</span><br><span class="line"><span class="selector-tag">print</span>(<span class="selector-tag">net</span><span class="selector-class">.con1</span><span class="selector-class">.bias</span><span class="selector-class">.grad</span>)</span><br><span class="line"><span class="selector-tag">loss</span><span class="selector-class">.backward</span>()</span><br><span class="line"><span class="selector-tag">print</span>(<span class="selector-tag">net</span><span class="selector-class">.conv1</span><span class="selector-class">.bias</span><span class="selector-class">.grad</span>)</span><br></pre></td></tr></table></figure><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate=0.01</span><br><span class="line">for f in net.parameters():</span><br><span class="line">　　f.data.sub_(f.grad.data*leaning_data)</span><br></pre></td></tr></table></figure><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line"><span class="comment"># 训练过程中 梯度清零</span></span><br><span class="line">optimizer.zero_grad() <span class="comment"># 等效于 net.zero_grad()</span></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">output=net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># 更新参数</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="CIFAR-10分类"><a href="#CIFAR-10分类" class="headerlink" title="CIFAR-10分类"></a>CIFAR-10分类</h2><ol><li>数据预处理:transform,trainset,trainloader,testset,testloader </li><li>定义网络:Net(nn.Module),super(Net,self).<strong>init</strong>(),forward()</li><li>定义损失函数和优化器</li><li>训练网络<ol><li>输入数据</li><li>梯度清零</li><li>前向传播+反向传播</li><li>更新参数  </li></ol></li></ol><hr><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>自定义的数据集需要继承Dataset类，并实现两个Python魔术方法<br>__getitem<strong>:返回一个样本。obj[index]=obj.</strong>getitem<strong>(index)<br>__len</strong>:返回样本数量。<br>transform=T.Compose()<br>trans=T.Lambda(lambda img:img.rotate(randonm()*360))<br>ImageFolder(root,transform,target_transform,loader)P139<br>self.class_to_idx了解label和文件夹名的映射关系<br>DataLoader()定义shuffle等P142<br>取样:P146<br>工具包:torchvision  P147</p><ul><li>models:训练好模型</li><li>dataset:数据集加载</li><li>transforms:数据预处理操作，主要针对Tensor和PIL Image对象的操作    </li></ul><p>make_grid和save_img<br>可视化工具 Tensorboard和visdom<br>tensor_board和TensorboardX<br>visdom:env pane</p><p>%env LS_COLORS=None<br>!tree —charset ascii data/dogcat<br>Tensor—numpy:np.array(Tensor) torch.Tensor(np.darray)<br>PIL.image—numpy:np.asarray(PIL.image) image.fromarray(numpy.ndarray)<br>PIL.image—Tensor:trans = transforms.Compose([transforms.ToTensor()]) tens = trans(img)  ToPILImage()  </p><h2 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h2><p>P158<br>with t.cuda.device(1):<br>t.cuda.set_device(1)<br>export CUDA_VISIVLE_DEVICES=1<br>b = t.load(‘a.pth’)<br>c = t.load(‘a.pth’,map_location=lamdba storage, loc: storage)<br>d = t.load(‘a.pth’,map_location={‘cuda:1’:’cuda:0’})<br>Module和Optimizer: state_dict  </p><h2 id="Dog-vs-Cat"><a href="#Dog-vs-Cat" class="headerlink" title="Dog.vs.Cat"></a>Dog.vs.Cat</h2><p>checkpoints/ 中间模型<br>data/</p><ul><li>__init__.py  </li><li>dataset.py  </li><li><ul><li>def <strong>init</strong>(self,root,transform=None, train=True, test=False):</li></ul></li><li><ul><li>def <strong>getitem</strong>(self,index):</li></ul></li><li><ul><li>def <strong>len</strong>(self):</li></ul></li><li>get_data.sh<br>models/</li><li>__init__.py</li><li>AlexNet.py</li><li>BasicModule.py</li><li>ResNet34.py<br>utils/</li><li>__init__.py</li><li>visualize.py<br>config.py<br>main.py<br>requirements.txt<br>README.md</li></ul><p>main.py</p><ul><li>def train(**kwargs):</li><li>def test(**kwargs):</li><li>def val(model,dataloader):</li><li>def help():</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__==<span class="string">'main'</span>:</span><br><span class="line">    <span class="keyword">import</span> fire</span><br><span class="line">    fire.Fire()</span><br></pre></td></tr></table></figure><h2 id="pytorch-中文文档"><a href="#pytorch-中文文档" class="headerlink" title="pytorch 中文文档"></a>pytorch 中文文档</h2><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/functional/" target="_blank" rel="noopener">https://pytorch-cn.readthedocs.io/zh/latest/package_references/functional/</a></p><h1 id="PyTorch实战指南-第六章-Dog-VS-Cat"><a href="#PyTorch实战指南-第六章-Dog-VS-Cat" class="headerlink" title="PyTorch实战指南 第六章  Dog.VS.Cat"></a>PyTorch实战指南 第六章  Dog.VS.Cat</h1><p>这是根据深度学习框架：PyTorch入门与实践这本书的第六章写的代码，是关于猫狗识别的，在这个过程中，一边看，一边写，刚开始是运行作者已经写好的代码，后来自己在jupyter上进行复制的复现，发现import无法导入ipynb文件，在使用了Ipynb_importer.py之后可以实现同一文件内导入ipynb模块，如果是在其他文件中进行导入，会有点费事，以下会记录Ipynb_importer.py的用法。因为费事，自己开始开始使用pycharm+jupyter的方式，直接自己根据作者提供的源码进行编写，在编写的过程中接受作者的思想。用pycharm的不方便的地方是无法直接运行测试，所以采取的是对自己不熟悉的模块或者方法，用jupyter进行测试，而直接编写则是pycharm。但是感觉pycharm还是没有那么好用，可能是自己用的少。我是按照data、model、util、main+config、requirement的顺序编写的。在编写函数的过程中，因为刚开始不理解各个模块是怎么组织起来的，所以都是从简单的开始，所以函数的位置和作者的不一样，其中对于model.save和model.load、vis.plot和vis.log的封装让我感觉很有意思，刚开始是编写的时候只能直接打上问号，因为不懂这么编写的意义，但在编写主函数main的时候才感觉到了这种编写的好处，基本把模型训练和对模型、结果的处理完全分离开，避免了耦合性很强的后果。</p><hr><h2 id="Ipynb-importer-py"><a href="#Ipynb-importer-py" class="headerlink" title="Ipynb_importer.py"></a>Ipynb_importer.py</h2><p>我通过几次测试发现，import Ipynb_importer 只需要放在你的当前要运行的文件中即可，然后在其他文件下的<strong>init</strong>.py 中导入所有的当前文件夹中的Module，就像这样<br>/first/second/models/<br>——-<strong>init</strong>.py<br>——————- None<br>——-BasicModule.ipynb<br>——-AlexNet.ipynb<br>—————from models.BasicModule import BasicModule</p><p>/first/main.py<br>import Ipynb_importer<br>from models import AlexNet    </p><p>之所以在AlexNet中写models.BasicModule是因为直接导入BasicModule会报错，我根据<strong>dict</strong>的输出发现有问题，这一点和<a href="https://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Importing%20Notebooks.html" target="_blank" rel="noopener">官网</a>介绍的有一点区别，我没有实现官网说明的跨文件夹导入。因为如果改文件夹导入的话，models.BasicModule要接着换成相应的名字，与我预想的不一致，我预想的是不管在哪里导入，已经导入的应该不受影响才对。</p><hr><h2 id="ipynb-py-sh"><a href="#ipynb-py-sh" class="headerlink" title="ipynb-py.sh"></a>ipynb-py.sh</h2><p>之后发现了这个神器，可以把ipynb转化成.py，还是挺好用的，转化之后也没问题。</p><hr><p>同时，借助这次实验，自己对python的掌握也更深了一点。</p><hr><p>不过对于网络的构成还是有一些问题，那就是网络为什么这么写，这应该属于理论的东西。还需要进一步加强。    </p><hr><p>这次实验一共用了三天才完全搞懂，可以说其中涉及到的函数的用法基本都明白了。<br>本意是记录自己，不过如果有任何问题，欢迎交流。     </p><hr><h1 id="PyTorch实战指南-第七章-DCGAN"><a href="#PyTorch实战指南-第七章-DCGAN" class="headerlink" title="PyTorch实战指南 第七章 DCGAN"></a>PyTorch实战指南 第七章 DCGAN</h1><p>这一次实现的也比较慢，用了小三天才做完，现在记录一下其中学到的几个东西。</p><h2 id="file"><a href="#file" class="headerlink" title="__file__:"></a>__file__:</h2><p>用来获取模块所在路径 可能是一个相对路径，可能是一个绝对路径，<br>如果当前文件包含在sys.path里面，那么，__file__返回一个相对路径！<br>也可以认为获取模块的名字<br>最后的落脚点一定是XX/XX.py<br>类没有这个属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy</span><br><span class="line">In [<span class="number">3</span>]: numpy.__file__                                                                                                 </span><br><span class="line">Out[<span class="number">3</span>]: <span class="string">'F:\\Programs\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py'</span></span><br><span class="line">In [<span class="number">6</span>]: numpy.random.__file__</span><br><span class="line">Out[<span class="number">6</span>]: <span class="string">'F:\\Programs\\Anaconda3\\lib\\site-packages\\numpy\\random\\__init__.py'</span></span><br><span class="line"></span><br><span class="line">$ python test.py <span class="comment">##print(__file__)</span></span><br><span class="line">test.py</span><br></pre></td></tr></table></figure><h2 id="name"><a href="#name" class="headerlink" title="__name__:"></a>__name__:</h2><p>__name__就是标识模块的名字的一个系统变量。这里分两种情况：假如当前模块是主模块（也就是调用其他模块的模块），那么此模块名字就是__main__，通过if判断这样就可以执行“__mian__:”后面的主函数内容；假如此模块是被import的，则此模块名字为文件名字（不加后面的.py），通过if判断这样就会跳过“__main__:”后面的内容。<br>这个模块可以是文件夹的名字，可以是类的名字，可以是__mian__”,XX的形式。<br>可以用于获取当前文件的文件名<br>通过上面方式，python就可以分清楚哪些是主函数，进入主函数执行；并且可以调用其他模块的各个函数等等。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> test.py</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># print(__file__)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># print(__name__)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> test2.py</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># import test</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># print(__file__)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># print(__name__)</span></span></span><br><span class="line">H:\GitHub\pytorch_learn\Chapter7\test.py</span><br><span class="line">test</span><br><span class="line">test2.py</span><br><span class="line">__main__</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># from test import ccc</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># print(ccc.__name__)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># print(__file__)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># print(__name__)</span></span></span><br><span class="line">ccc</span><br><span class="line">test2.py</span><br><span class="line">__main__</span><br></pre></td></tr></table></figure></p><h2 id="type"><a href="#type" class="headerlink" title="type():"></a>type():</h2><p>返回对象的类型<br>如果是module,则返回module<br>如果是类的实例，则返回类的名称，这个名称以XXX.XXX的形式返回，从import的第一个开始算起。<br>常用于判断数据类型，在pytorch中，用于返回模型名称，这个用法很巧妙，相当于返回了子类的类型名字<br>我觉得没有理解作者是怎么用的。在父类里的type(self) 返回的是子类的类名<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">In</span> [<span class="number">1</span>]: import numpy</span><br><span class="line"></span><br><span class="line"><span class="keyword">In</span> [<span class="number">2</span>]: <span class="keyword">type</span>(numpy)</span><br><span class="line"><span class="keyword">Out</span>[<span class="number">2</span>]: module</span><br><span class="line"></span><br><span class="line"><span class="keyword">In</span> [<span class="number">3</span>]: a = numpy.<span class="keyword">array</span>(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">In</span> [<span class="number">4</span>]: <span class="keyword">type</span>(a)</span><br><span class="line"><span class="keyword">Out</span>[<span class="number">4</span>]: numpy.ndarray</span><br><span class="line"></span><br><span class="line"><span class="keyword">In</span> [<span class="number">5</span>]: <span class="keyword">type</span>(numpy.<span class="keyword">array</span>)</span><br><span class="line"><span class="keyword">Out</span>[<span class="number">5</span>]: builtin_function_or_method</span><br></pre></td></tr></table></figure></p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    pass</span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="title">A</span>):</span></span><br><span class="line">    pass</span><br><span class="line">  </span><br><span class="line">isinstance(A(), A)    <span class="comment"># returns True</span></span><br><span class="line"><span class="keyword">type</span>(A()) == A        <span class="comment"># returns True</span></span><br><span class="line">isinstance(B(), A)    <span class="comment"># returns True</span></span><br><span class="line"><span class="keyword">type</span>(B()) == A        <span class="comment"># returns False</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>(<span class="title">object</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span></span>(<span class="keyword">self</span>):</span><br><span class="line">print(<span class="keyword">type</span>(<span class="keyword">self</span>))</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="title">A</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span></span>(<span class="keyword">self</span>):</span><br><span class="line"><span class="keyword">super</span>(B,<span class="keyword">self</span>).__init__()</span><br><span class="line">print(<span class="keyword">type</span>(<span class="keyword">self</span>))</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">import test</span><br><span class="line">test.B()</span><br><span class="line"></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">test</span>.<span class="title">B</span>'&gt;                                                                                                        </span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">test</span>.<span class="title">B</span>'&gt;</span></span><br></pre></td></tr></table></figure><h2 id="class"><a href="#class" class="headerlink" title="__class__:"></a>__class__:</h2><p>和type类似<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>(<span class="title">object</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span></span>(<span class="keyword">self</span>):</span><br><span class="line">print(<span class="keyword">type</span>(<span class="keyword">self</span>))</span><br><span class="line">print(<span class="keyword">self</span>.__class__)</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="title">A</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span></span>(<span class="keyword">self</span>):</span><br><span class="line"><span class="keyword">super</span>(B,<span class="keyword">self</span>).__init__()</span><br><span class="line">print(<span class="keyword">type</span>(<span class="keyword">self</span>))</span><br><span class="line">print(<span class="keyword">self</span>.__class__)</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">test</span>.<span class="title">B</span>'&gt;</span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">test</span>.<span class="title">B</span>'&gt;</span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">test</span>.<span class="title">B</span>'&gt;</span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">test</span>.<span class="title">B</span>'&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="获取config源码"><a href="#获取config源码" class="headerlink" title="获取config源码"></a>获取config源码</h2><p>打印参数，方便输入参数<br><a href="https://docs.python.org/3/library/inspect.html" target="_blank" rel="noopener">inspect.getsource</a><br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> <span class="keyword">inspect</span> <span class="keyword">import</span> getsource</span><br><span class="line"><span class="keyword">source</span> = getsource(opt.__class__)</span><br><span class="line"><span class="keyword">print</span>(<span class="keyword">source</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;基础知识&quot;&gt;&lt;a href=&quot;#基础知识&quot; class=&quot;headerlink&quot; title=&quot;基础知识&quot;&gt;&lt;/a&gt;基础知识&lt;/h1&gt;&lt;p&gt;魔术方法: P23  &lt;/p&gt;
&lt;p&gt;调试: P27&lt;br&gt;&lt;figure class=&quot;highlight elm&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; ipdb&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;title&quot;&gt;ipdb&lt;/span&gt;.set_trace()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;带下划线_的函数会修改Tensor本身，比如x.add_(y)和x.add(y)的区别  &lt;/p&gt;
&lt;h2 id=&quot;Tensor&quot;&gt;&lt;a href=&quot;#Tensor&quot; class=&quot;headerlink&quot; title=&quot;Tensor&quot;&gt;&lt;/a&gt;Tensor&lt;/h2&gt;&lt;p&gt;Numpy与Tensor共享内存&lt;br&gt;b = a.numpy() # Tensor -&amp;gt; Numpy&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch-learn chenyun" scheme="http://yoursite.com/tags/pytorch-learn-chenyun/"/>
    
  </entry>
  
  <entry>
    <title>python learning</title>
    <link href="http://yoursite.com/2018/08/26/python-learning/"/>
    <id>http://yoursite.com/2018/08/26/python-learning/</id>
    <published>2018-08-26T09:34:06.000Z</published>
    <updated>2018-09-05T01:43:22.096Z</updated>
    
    <content type="html"><![CDATA[<h1 id="python的学习过程："><a href="#python的学习过程：" class="headerlink" title="python的学习过程："></a>python的学习过程：</h1><ul><li>第一个是看着廖雪峰的网站，里面的内容基础，是关于数据结构等的十分基本的内容，适合小白入门</li><li>第二个是流畅的python<ul><li>这本书比python-codebook还深入，更适合当你实现了一个功能之后，还是想知道其具体怎么实现的时候查询。</li><li>第三个是python-codebook. </li><li>它的组织形式是任务式、问题式的，而且问题也相对而言比较高级，不是算法导论那种以解决某个实际问题，而是在编程上我想实现什么更好的功能那种问题，通过每一个问题，或者说通过每一个你想怎么更优的实现一个方法的思路，来引导如何更好地写代码，实现高级功能。这本书的前提是你已经入门，并且写了一段时间的python代码，在实际写的过程中已经遇到了类似的问题，也勉强实现了，只是苦于没有更好更顺心的方法实现。我现在是个小白，看这本书用了将近两周吧，主要看了第一二三四七八章，里面的代码翔实。其他的也是略微看了看，因为没有实际操作背景，有的时候不懂为什么那样做会更好，可以在以后的编程过程中，遇到这样的情况：这个我能勉强实现，但是感觉不太好，我想实现的更优美。那就应该来看看这本书，说不定这本书的实现能给自己一些思路。不适合为了读而读，因为不是入门。</li></ul></li><li>接下来可以考虑看看那种直接算法任务型的。刚刚看了看python算法教程，估计要跳着看了，因为里面的关于算法的内容已经熟悉了，可以扫描着看。</li><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;python的学习过程：&quot;&gt;&lt;a href=&quot;#python的学习过程：&quot; class=&quot;headerlink&quot; title=&quot;python的学习过程：&quot;&gt;&lt;/a&gt;python的学习过程：&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;第一个是看着廖雪峰的网站，里面的内容基础，是关于
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>MdEditor</title>
    <link href="http://yoursite.com/2018/08/22/MdEditor/"/>
    <id>http://yoursite.com/2018/08/22/MdEditor/</id>
    <published>2018-08-22T08:22:31.000Z</published>
    <updated>2018-08-22T08:58:57.983Z</updated>
    
    <content type="html"><![CDATA[<h1 id="欢迎使用-Markdown在线编辑器-MdEditor"><a href="#欢迎使用-Markdown在线编辑器-MdEditor" class="headerlink" title="欢迎使用 Markdown在线编辑器 MdEditor"></a>欢迎使用 Markdown在线编辑器 MdEditor</h1><p><strong>Markdown是一种轻量级的「标记语言」</strong></p><p><img src="http://www.mdeditor.com/images/logos/markdown.png" alt="markdown" title="markdown"><br><a id="more"></a></p><p>Markdown是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面，Markdown文件的后缀名便是“.md”</p><h2 id="MdEditor是一个在线编辑Markdown文档的编辑器"><a href="#MdEditor是一个在线编辑Markdown文档的编辑器" class="headerlink" title="MdEditor是一个在线编辑Markdown文档的编辑器"></a>MdEditor是一个在线编辑Markdown文档的编辑器</h2><p><em>MdEditor扩展了Markdown的功能（如表格、脚注、内嵌HTML等等），以使让Markdown转换成更多的格式，和更丰富的展示效果，这些功能原初的Markdown尚不具备。</em></p><blockquote><p>Markdown增强版中比较有名的有Markdown Extra、MultiMarkdown、 Maruku等。这些衍生版本要么基于工具，如<del>Pandoc</del>，Pandao；要么基于网站，如GitHub和Wikipedia，在语法上基本兼容，但在一些语法和渲染效果上有改动。</p></blockquote><p>MdEditor源于Pandao的JavaScript开源项目，开源地址<a href="https://github.com/pandao/editor.md" title="Editor.md" target="_blank" rel="noopener">Editor.md</a>，并在MIT开源协议的许可范围内进行了优化，以适应广大用户群体的需求。向优秀的markdown开源编辑器原作者Pandao致敬。</p><p><img src="https://pandao.github.io/editor.md/images/logos/editormd-logo-180x180.png" alt="Pandao editor.md" title="Pandao editor.md"></p><h2 id="MdEditor的功能列表演示"><a href="#MdEditor的功能列表演示" class="headerlink" title="MdEditor的功能列表演示"></a>MdEditor的功能列表演示</h2><h1 id="标题H1"><a href="#标题H1" class="headerlink" title="标题H1"></a>标题H1</h1><h2 id="标题H2"><a href="#标题H2" class="headerlink" title="标题H2"></a>标题H2</h2><h3 id="标题H3"><a href="#标题H3" class="headerlink" title="标题H3"></a>标题H3</h3><h4 id="标题H4"><a href="#标题H4" class="headerlink" title="标题H4"></a>标题H4</h4><h5 id="标题H5"><a href="#标题H5" class="headerlink" title="标题H5"></a>标题H5</h5><h6 id="标题H5-1"><a href="#标题H5-1" class="headerlink" title="标题H5"></a>标题H5</h6><h3 id="字符效果和横线等"><a href="#字符效果和横线等" class="headerlink" title="字符效果和横线等"></a>字符效果和横线等</h3><hr><p><del>删除线</del> <s>删除线（开启识别HTML标签时）</s></p><p><em>斜体字</em>      _斜体字_</p><p><strong>粗体</strong>  <strong>粗体</strong></p><p><strong><em>粗斜体</em></strong> <strong>_粗斜体_</strong></p><p>上标：X<sub>2</sub>，下标：O<sup>2</sup></p><p><strong>缩写(同HTML的abbr标签)</strong></p><blockquote><p>即更长的单词或短语的缩写形式，前提是开启识别HTML标签时，已默认开启</p></blockquote><p>The <abbr title="Hyper Text Markup Language">HTML</abbr> specification is maintained by the <abbr title="World Wide Web Consortium">W3C</abbr>.</p><h3 id="引用-Blockquotes"><a href="#引用-Blockquotes" class="headerlink" title="引用 Blockquotes"></a>引用 Blockquotes</h3><blockquote><p>引用文本 Blockquotes</p></blockquote><p>引用的行内混合 Blockquotes</p><blockquote><p>引用：如果想要插入空白换行<code>即&lt;br /&gt;标签</code>，在插入处先键入两个以上的空格然后回车即可，<a href="http://www.mdeditor.com/" target="_blank" rel="noopener">普通链接</a>。</p></blockquote><h3 id="锚点与链接-Links"><a href="#锚点与链接-Links" class="headerlink" title="锚点与链接 Links"></a>锚点与链接 Links</h3><p><a href="http://www.mdeditor.com/" target="_blank" rel="noopener">普通链接</a><br><a href="http://www.mdeditor.com/" title="普通链接带标题" target="_blank" rel="noopener">普通链接带标题</a><br>直接链接：<a href="http://www.mdeditor.com" target="_blank" rel="noopener">http://www.mdeditor.com</a><br><a href="http://www.mdeditor.com/" target="_blank" rel="noopener">锚点链接</a></p><p><a href="mailto:test.test@gmail.com" target="_blank" rel="noopener">mailto:test.test@gmail.com</a><br>GFM a-tail link @pandao<br>邮箱地址自动链接 test.test@gmail.com  www@vip.qq.com</p><blockquote><p>@pandao</p></blockquote><h3 id="多语言代码高亮-Codes"><a href="#多语言代码高亮-Codes" class="headerlink" title="多语言代码高亮 Codes"></a>多语言代码高亮 Codes</h3><h4 id="行内代码-Inline-code"><a href="#行内代码-Inline-code" class="headerlink" title="行内代码 Inline code"></a>行内代码 Inline code</h4><p>执行命令：<code>npm install marked</code></p><h4 id="缩进风格"><a href="#缩进风格" class="headerlink" title="缩进风格"></a>缩进风格</h4><p>即缩进四个空格，也做为实现类似 <code>&lt;pre&gt;</code> 预格式化文本 ( Preformatted Text ) 的功能。</p><pre><code>&lt;?php    echo &quot;Hello world!&quot;;?&gt;</code></pre><p>预格式化文本：</p><pre><code>| First Header  | Second Header || ------------- | ------------- || Content Cell  | Content Cell  || Content Cell  | Content Cell  |</code></pre><h4 id="JS代码"><a href="#JS代码" class="headerlink" title="JS代码"></a>JS代码</h4><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">test</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">"Hello world!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="HTML-代码-HTML-codes"><a href="#HTML-代码-HTML-codes" class="headerlink" title="HTML 代码 HTML codes"></a>HTML 代码 HTML codes</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mate</span> <span class="attr">charest</span>=<span class="string">"utf-8"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"keywords"</span> <span class="attr">content</span>=<span class="string">"Editor.md, Markdown, Editor"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>Hello world!<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">            body&#123;font-size:14px;color:#444;font-family: "Microsoft Yahei", Tahoma, "Hiragino Sans GB", Arial;background:#fff;&#125;</span></span><br><span class="line"><span class="undefined">            ul&#123;list-style: none;&#125;</span></span><br><span class="line"><span class="undefined">            img&#123;border:none;vertical-align: middle;&#125;</span></span><br><span class="line"><span class="undefined">        </span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">h1</span> <span class="attr">class</span>=<span class="string">"text-xxl"</span>&gt;</span>Hello world!<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"text-green"</span>&gt;</span>Plain text<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="图片-Images"><a href="#图片-Images" class="headerlink" title="图片 Images"></a>图片 Images</h3><p>图片加链接 (Image + Link)：</p><p><a href="http://www.mdeditor.com/images/logos/markdown.png" title="markdown" target="_blank" rel="noopener"><img src="http://www.mdeditor.com/images/logos/markdown.png" alt=""></a></p><blockquote><p>Follow your heart.</p></blockquote><hr><h3 id="列表-Lists"><a href="#列表-Lists" class="headerlink" title="列表 Lists"></a>列表 Lists</h3><h4 id="无序列表（减号）Unordered-Lists"><a href="#无序列表（减号）Unordered-Lists" class="headerlink" title="无序列表（减号）Unordered Lists (-)"></a>无序列表（减号）Unordered Lists (-)</h4><ul><li>列表一</li><li>列表二</li><li>列表三</li></ul><h4 id="无序列表（星号）Unordered-Lists"><a href="#无序列表（星号）Unordered-Lists" class="headerlink" title="无序列表（星号）Unordered Lists (*)"></a>无序列表（星号）Unordered Lists (*)</h4><ul><li>列表一</li><li>列表二</li><li>列表三</li></ul><h4 id="无序列表（加号和嵌套）Unordered-Lists"><a href="#无序列表（加号和嵌套）Unordered-Lists" class="headerlink" title="无序列表（加号和嵌套）Unordered Lists (+)"></a>无序列表（加号和嵌套）Unordered Lists (+)</h4><ul><li>列表一</li><li>列表二<ul><li>列表二-1</li><li>列表二-2</li><li>列表二-3</li></ul></li><li>列表三<ul><li>列表一</li><li>列表二</li><li>列表三</li></ul></li></ul><h4 id="有序列表-Ordered-Lists"><a href="#有序列表-Ordered-Lists" class="headerlink" title="有序列表 Ordered Lists (-)"></a>有序列表 Ordered Lists (-)</h4><ol><li>第一行</li><li>第二行</li><li>第三行</li></ol><h4 id="GFM-task-list"><a href="#GFM-task-list" class="headerlink" title="GFM task list"></a>GFM task list</h4><ul><li>[x] GFM task list 1</li><li>[x] GFM task list 2</li><li>[ ] GFM task list 3<ul><li>[ ] GFM task list 3-1</li><li>[ ] GFM task list 3-2</li><li>[ ] GFM task list 3-3</li></ul></li><li>[ ] GFM task list 4<ul><li>[ ] GFM task list 4-1</li><li>[ ] GFM task list 4-2</li></ul></li></ul><hr><h3 id="绘制表格-Tables"><a href="#绘制表格-Tables" class="headerlink" title="绘制表格 Tables"></a>绘制表格 Tables</h3><div class="table-container"><table><thead><tr><th>项目</th><th style="text-align:right">价格</th><th style="text-align:center">数量</th></tr></thead><tbody><tr><td>计算机</td><td style="text-align:right">$1600</td><td style="text-align:center">5</td></tr><tr><td>手机</td><td style="text-align:right">$12</td><td style="text-align:center">12</td></tr><tr><td>管线</td><td style="text-align:right">$1</td><td style="text-align:center">234</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>First Header</th><th>Second Header</th></tr></thead><tbody><tr><td>Content Cell</td><td>Content Cell</td></tr><tr><td>Content Cell</td><td>Content Cell</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>First Header</th><th>Second Header</th></tr></thead><tbody><tr><td>Content Cell</td><td>Content Cell</td></tr><tr><td>Content Cell</td><td>Content Cell</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>Function name</th><th>Description</th></tr></thead><tbody><tr><td><code>help()</code></td><td>Display the help window.</td></tr><tr><td><code>destroy()</code></td><td><strong>Destroy your computer!</strong></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">Left-Aligned</th><th style="text-align:center">Center Aligned</th><th style="text-align:right">Right Aligned</th></tr></thead><tbody><tr><td style="text-align:left">col 3 is</td><td style="text-align:center">some wordy text</td><td style="text-align:right">$1600</td></tr><tr><td style="text-align:left">col 2 is</td><td style="text-align:center">centered</td><td style="text-align:right">$12</td></tr><tr><td style="text-align:left">zebra stripes</td><td style="text-align:center">are neat</td><td style="text-align:right">$1</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>Item</th><th style="text-align:right">Value</th></tr></thead><tbody><tr><td>Computer</td><td style="text-align:right">$1600</td></tr><tr><td>Phone</td><td style="text-align:right">$12</td></tr><tr><td>Pipe</td><td style="text-align:right">$1</td></tr></tbody></table></div><hr><h4 id="特殊符号-HTML-Entities-Codes"><a href="#特殊符号-HTML-Entities-Codes" class="headerlink" title="特殊符号 HTML Entities Codes"></a>特殊符号 HTML Entities Codes</h4><p>&copy; &amp;  &uml; &trade; &iexcl; &pound;<br>&amp; &lt; &gt; &yen; &euro; &reg; &plusmn; &para; &sect; &brvbar; &macr; &laquo; &middot;</p><p>X&sup2; Y&sup3; &frac34; &frac14;  &times;  &divide;   &raquo;</p><p>18&ordm;C  &quot;  &apos;</p><p>[========]</p><h3 id="Emoji表情-smiley"><a href="#Emoji表情-smiley" class="headerlink" title="Emoji表情 :smiley:"></a>Emoji表情 :smiley:</h3><blockquote><p>Blockquotes :star:</p></blockquote><h4 id="GFM-task-lists-amp-Emoji-amp-fontAwesome-icon-emoji-amp-editormd-logo-emoji-editormd-logo-5x"><a href="#GFM-task-lists-amp-Emoji-amp-fontAwesome-icon-emoji-amp-editormd-logo-emoji-editormd-logo-5x" class="headerlink" title="GFM task lists &amp; Emoji &amp; fontAwesome icon emoji &amp; editormd logo emoji :editormd-logo-5x:"></a>GFM task lists &amp; Emoji &amp; fontAwesome icon emoji &amp; editormd logo emoji :editormd-logo-5x:</h4><ul><li>[x] :smiley: @mentions, :smiley: #refs, <a href="">links</a>, <strong>formatting</strong>, and <del>tags</del> supported :editormd-logo:;</li><li>[x] list syntax required (any unordered or ordered list supported) :editormd-logo-3x:;</li><li>[x] [ ] :smiley: this is a complete item :smiley:;</li><li>[ ] []this is an incomplete item <a href="#">test link</a> :fa-star: @pandao;</li><li>[ ] [ ]this is an incomplete item :fa-star: :fa-gear:;<ul><li>[ ] :smiley: this is an incomplete item <a href="#">test link</a> :fa-star: :fa-gear:;</li><li>[ ] :smiley: this is  :fa-star: :fa-gear: an incomplete item <a href="#">test link</a>;</li></ul></li></ul><h4 id="反斜杠-Escape"><a href="#反斜杠-Escape" class="headerlink" title="反斜杠 Escape"></a>反斜杠 Escape</h4><p>*literal asterisks*</p><p>[========]</p><h3 id="科学公式-TeX-KaTeX"><a href="#科学公式-TeX-KaTeX" class="headerlink" title="科学公式 TeX(KaTeX)"></a>科学公式 TeX(KaTeX)</h3><script type="math/tex; mode=display">E=mc^2</script><p>行内的公式<script type="math/tex">E=mc^2</script>行内的公式，行内的<script type="math/tex">E=mc^2</script>公式。</p><script type="math/tex; mode=display">x > y</script><script type="math/tex; mode=display">\(\sqrt{3x-1}+(1+x)^2\)</script><script type="math/tex; mode=display">\sin(\alpha)^{\theta}=\sum_{i=0}^{n}(x^i + \cos(f))</script><p>多行公式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\displaystyle</span><br><span class="line">\left( \sum\_&#123;k=1&#125;^n a\_k b\_k \right)^2</span><br><span class="line">\leq</span><br><span class="line">\left( \sum\_&#123;k=1&#125;^n a\_k^2 \right)</span><br><span class="line">\left( \sum\_&#123;k=1&#125;^n b\_k^2 \right)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">\displaystyle</span><br><span class="line">    \frac&#123;1&#125;&#123;</span><br><span class="line">        \Bigl(\sqrt&#123;\phi \sqrt&#123;5&#125;&#125;-\phi\Bigr) e^&#123;</span><br><span class="line">        \frac25 \pi&#125;&#125; = 1+\frac&#123;e^&#123;-2\pi&#125;&#125; &#123;1+\frac&#123;e^&#123;-4\pi&#125;&#125; &#123;</span><br><span class="line">        1+\frac&#123;e^&#123;-6\pi&#125;&#125;</span><br><span class="line">        &#123;1+\frac&#123;e^&#123;-8\pi&#125;&#125;</span><br><span class="line">         &#123;1+\cdots&#125; &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f(x) = \int_&#123;-\infty&#125;^\infty</span><br><span class="line">    \hat f(\xi)\,e^&#123;2 \pi i \xi x&#125;</span><br><span class="line">    \,d\xi</span><br></pre></td></tr></table></figure><h3 id="分页符-Page-break"><a href="#分页符-Page-break" class="headerlink" title="分页符 Page break"></a>分页符 Page break</h3><blockquote><p>Print Test: Ctrl + P</p></blockquote><p>[========]</p><h3 id="绘制流程图-Flowchart"><a href="#绘制流程图-Flowchart" class="headerlink" title="绘制流程图 Flowchart"></a>绘制流程图 Flowchart</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">st=&gt;start: 用户登陆</span><br><span class="line">op=&gt;operation: 登陆操作</span><br><span class="line">cond=&gt;condition: 登陆成功 Yes or No?</span><br><span class="line">e=&gt;end: 进入后台</span><br><span class="line"></span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure><p>[========]</p><h3 id="绘制序列图-Sequence-Diagram"><a href="#绘制序列图-Sequence-Diagram" class="headerlink" title="绘制序列图 Sequence Diagram"></a>绘制序列图 Sequence Diagram</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Andrew-&gt;China: Says Hello</span><br><span class="line">Note right of China: China thinks\nabout it</span><br><span class="line">China--&gt;Andrew: How are you?</span><br><span class="line">Andrew-&gt;&gt;China: I am good thanks!</span><br></pre></td></tr></table></figure><h3 id="End"><a href="#End" class="headerlink" title="End"></a>End</h3>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;欢迎使用-Markdown在线编辑器-MdEditor&quot;&gt;&lt;a href=&quot;#欢迎使用-Markdown在线编辑器-MdEditor&quot; class=&quot;headerlink&quot; title=&quot;欢迎使用 Markdown在线编辑器 MdEditor&quot;&gt;&lt;/a&gt;欢迎使用 Markdown在线编辑器 MdEditor&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Markdown是一种轻量级的「标记语言」&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://www.mdeditor.com/images/logos/markdown.png&quot; alt=&quot;markdown&quot; title=&quot;markdown&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>test1</title>
    <link href="http://yoursite.com/2018/08/22/test1/"/>
    <id>http://yoursite.com/2018/08/22/test1/</id>
    <published>2018-08-22T07:30:17.000Z</published>
    <updated>2018-08-22T10:24:08.816Z</updated>
    
    <content type="html"><![CDATA[<script type="math/tex; mode=display">E=mc^2</script><p>\begin{aligned}<br>\dot{x} &amp; = \sigma(y-x) \\\<br>\dot{y} &amp; = \rho x - y - xz \\\<br>\dot{z} &amp; = -\beta z + xy<br>\end{aligned}</p><p>基准测试<a href="http:www.baidu.com" target="_blank" rel="noopener">百度</a></p><p>是一个链接测试<a href="http://www.baidu.com" title="baid" target="_blank" rel="noopener">百度</a></p><p>第二个链接测试<a href="http://www.baidu.com" target="_blank" rel="noopener">bai du</a></p><p>第三个链接测试<a href="http://www.baidu.com" target="_blank" rel="noopener">badu</a></p><p>第四个测试<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p><p>第五个测试这是一个链接到谷歌的<sup><a href="#fn_脚注" id="reffn_脚注">脚注</a></sup>。</p><blockquote id="fn_脚注"><sup>脚注</sup>. <a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><a href="#reffn_脚注" title="Jump back to footnote [脚注] in the text."> &#8617;</a></blockquote><p>第六个测试 Here is a footnote reference,<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> and another.<sup><a href="#fn_longnote" id="reffn_longnote">longnote</a></sup></p><blockquote id="fn_longnote"><sup>longnote</sup>. Here’s one with multiple blocks.<a href="#reffn_longnote" title="Jump back to footnote [longnote] in the text."> &#8617;</a></blockquote><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">Here is the footnote.</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">http://www.baidu.com</span><a href="#fnref:2" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;E=mc^2&lt;/script&gt;&lt;p&gt;\begin{aligned}&lt;br&gt;\dot{x} &amp;amp; = \sigma(y-x) \\\&lt;br&gt;\dot{y} &amp;amp; = \rho x - y - x
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2018/08/21/%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8%20Cmd%20Markdown%20%E7%BC%96%E8%BE%91%E9%98%85%E8%AF%BB%E5%99%A8/"/>
    <id>http://yoursite.com/2018/08/21/欢迎使用 Cmd Markdown 编辑阅读器/</id>
    <published>2018-08-21T14:18:26.574Z</published>
    <updated>2018-08-22T08:59:09.145Z</updated>
    
    <content type="html"><![CDATA[<h1 id="欢迎使用-Cmd-Markdown-编辑阅读器"><a href="#欢迎使用-Cmd-Markdown-编辑阅读器" class="headerlink" title="欢迎使用 Cmd Markdown 编辑阅读器"></a>欢迎使用 Cmd Markdown 编辑阅读器</h1><hr><p>我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，<strong>Cmd Markdown</strong> 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown：</p><blockquote><ul><li>整理知识，学习笔记</li><li>发布日记，杂文，所见所想</li><li>撰写发布技术文稿（代码支持）</li><li>撰写发布学术论文（LaTeX 公式支持）<a id="more"></a><img src="https://www.zybuluo.com/static/img/logo.png" alt="cmd-markdown-logo"></li></ul></blockquote><p>除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载：</p><h3 id="Windows-Mac-Linux-全平台客户端"><a href="#Windows-Mac-Linux-全平台客户端" class="headerlink" title="Windows/Mac/Linux 全平台客户端"></a><a href="https://www.zybuluo.com/cmd/" target="_blank" rel="noopener">Windows/Mac/Linux 全平台客户端</a></h3><blockquote><p>请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 <i class="icon-file"></i> <strong>新文稿</strong> 或者使用快捷键 <code>Ctrl+Alt+N</code>。</p></blockquote><hr><h2 id="什么是-Markdown"><a href="#什么是-Markdown" class="headerlink" title="什么是 Markdown"></a>什么是 Markdown</h2><p>Markdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，<strong>粗体</strong> 或者 <em>斜体</em> 某些文字，更棒的是，它还可以</p><h3 id="1-制作一份待办事宜-Todo-列表"><a href="#1-制作一份待办事宜-Todo-列表" class="headerlink" title="1. 制作一份待办事宜 Todo 列表"></a>1. 制作一份待办事宜 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#13-待办事宜-todo-列表" target="_blank" rel="noopener">Todo 列表</a></h3><ul><li>[ ] 支持以 PDF 格式导出文稿</li><li>[ ] 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率</li><li>[x] 新增 Todo 列表功能</li><li>[x] 修复 LaTex 公式渲染问题</li><li>[x] 新增 LaTex 公式编号功能</li></ul><h3 id="2-书写一个质能守恒公式LaTeX"><a href="#2-书写一个质能守恒公式LaTeX" class="headerlink" title="2. 书写一个质能守恒公式LaTeX"></a>2. 书写一个质能守恒公式<sup><a href="#fn_LaTeX" id="reffn_LaTeX">LaTeX</a></sup></h3><script type="math/tex; mode=display">E=mc^2</script><script type="math/tex; mode=display">x = {-b \pm \sqrt{b^2-4ac} \over 2a}</script><h3 id="3-高亮一段代码code"><a href="#3-高亮一段代码code" class="headerlink" title="3. 高亮一段代码code"></a>3. 高亮一段代码<sup><a href="#fn_code" id="reffn_code">code</a></sup></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@requires_authorization</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomeClass</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># A comment</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'hello world'</span></span><br></pre></td></tr></table></figure><h3 id="4-高效绘制-流程图"><a href="#4-高效绘制-流程图" class="headerlink" title="4. 高效绘制 流程图"></a>4. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#7-流程图" target="_blank" rel="noopener">流程图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">st=&gt;start: Start</span><br><span class="line">op=&gt;operation: Your Operation</span><br><span class="line">cond=&gt;condition: Yes or No?</span><br><span class="line">e=&gt;end</span><br><span class="line"></span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure><h3 id="5-高效绘制-序列图"><a href="#5-高效绘制-序列图" class="headerlink" title="5. 高效绘制 序列图"></a>5. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#8-序列图" target="_blank" rel="noopener">序列图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Alice-&gt;Bob: Hello Bob, how are you?</span><br><span class="line">Note right of Bob: Bob thinks</span><br><span class="line">Bob--&gt;Alice: I am good thanks!</span><br></pre></td></tr></table></figure><h3 id="6-高效绘制-甘特图"><a href="#6-高效绘制-甘特图" class="headerlink" title="6. 高效绘制 甘特图"></a>6. 高效绘制 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#9-甘特图" target="_blank" rel="noopener">甘特图</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">title 项目开发流程</span><br><span class="line">section 项目确定</span><br><span class="line">    需求分析       :a1, 2016-06-22, 3d</span><br><span class="line">    可行性报告     :after a1, 5d</span><br><span class="line">    概念验证       : 5d</span><br><span class="line">section 项目实施</span><br><span class="line">    概要设计      :2016-07-05  , 5d</span><br><span class="line">    详细设计      :2016-07-08, 10d</span><br><span class="line">    编码          :2016-07-15, 10d</span><br><span class="line">    测试          :2016-07-22, 5d</span><br><span class="line">section 发布验收</span><br><span class="line">    发布: 2d</span><br><span class="line">    验收: 3d</span><br></pre></td></tr></table></figure><h3 id="7-绘制表格"><a href="#7-绘制表格" class="headerlink" title="7. 绘制表格"></a>7. 绘制表格</h3><div class="table-container"><table><thead><tr><th>项目</th><th style="text-align:right">价格</th><th style="text-align:center">数量</th></tr></thead><tbody><tr><td>计算机</td><td style="text-align:right">$1600</td><td style="text-align:center">5</td></tr><tr><td>手机</td><td style="text-align:right">$12</td><td style="text-align:center">12</td></tr><tr><td>管线</td><td style="text-align:right">$1</td><td style="text-align:center">234</td></tr></tbody></table></div><h3 id="8-更详细语法说明"><a href="#8-更详细语法说明" class="headerlink" title="8. 更详细语法说明"></a>8. 更详细语法说明</h3><p>想要查看更详细的语法说明，可以参考我们准备的 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown" target="_blank" rel="noopener">Cmd Markdown 简明语法手册</a>，进阶用户可以参考 <a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown#cmd-markdown-高阶语法手册" target="_blank" rel="noopener">Cmd Markdown 高阶语法手册</a> 了解更多高级功能。</p><p>总而言之，不同于其它 <em>所见即所得</em> 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。<strong>Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。</strong> 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。</p><hr><h2 id="什么是-Cmd-Markdown"><a href="#什么是-Cmd-Markdown" class="headerlink" title="什么是 Cmd Markdown"></a>什么是 Cmd Markdown</h2><p>您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 <strong>编辑/发布/阅读</strong> Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。</p><h3 id="1-实时同步预览"><a href="#1-实时同步预览" class="headerlink" title="1. 实时同步预览"></a>1. 实时同步预览</h3><p>我们将 Cmd Markdown 的主界面一分为二，左边为<strong>编辑区</strong>，右边为<strong>预览区</strong>，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！</p><h3 id="2-编辑工具栏"><a href="#2-编辑工具栏" class="headerlink" title="2. 编辑工具栏"></a>2. 编辑工具栏</h3><p>也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 <strong>编辑区</strong> 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。</p><p><img src="https://www.zybuluo.com/static/img/toolbar-editor.png" alt="tool-editor"></p><h3 id="3-编辑模式"><a href="#3-编辑模式" class="headerlink" title="3. 编辑模式"></a>3. 编辑模式</h3><p>完全心无旁骛的方式编辑文字：点击 <strong>编辑工具栏</strong> 最右侧的拉伸按钮或者按下 <code>Ctrl + M</code>，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！</p><h3 id="4-实时的云端文稿"><a href="#4-实时的云端文稿" class="headerlink" title="4. 实时的云端文稿"></a>4. 实时的云端文稿</h3><p>为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 <strong>编辑工具栏</strong> 的最右侧提示 <code>已保存</code> 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。</p><h3 id="5-离线模式"><a href="#5-离线模式" class="headerlink" title="5. 离线模式"></a>5. 离线模式</h3><p>在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。</p><h3 id="6-管理工具栏"><a href="#6-管理工具栏" class="headerlink" title="6. 管理工具栏"></a>6. 管理工具栏</h3><p>为了便于管理您的文稿，在 <strong>预览区</strong> 的顶部放置了如下所示的 <strong>管理工具栏</strong>：</p><p><img src="https://www.zybuluo.com/static/img/toolbar-manager.jpg" alt="tool-manager"></p><p>通过管理工具栏可以：</p><p><i class="icon-share">&lt;/i&gt; 发布：将当前的文稿生成固定链接，在网络上发布，分享<br><i class="icon-file"></i> 新建：开始撰写一篇新的文稿<br><i class="icon-trash">&lt;/i&gt; 删除：删除当前的文稿<br><i class="icon-cloud"></i> 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地<br><i class="icon-reorder">&lt;/i&gt; 列表：所有新增和过往的文稿都可以在这里查看、操作<br><i class="icon-pencil"></i> 模式：切换 普通/Vim/Emacs 编辑模式</i></i></i></p><h3 id="7-阅读工具栏"><a href="#7-阅读工具栏" class="headerlink" title="7. 阅读工具栏"></a>7. 阅读工具栏</h3><p><img src="https://www.zybuluo.com/static/img/toolbar-reader.jpg" alt="tool-manager"></p><p>通过 <strong>预览区</strong> 右上角的 <strong>阅读工具栏</strong>，可以查看当前文稿的目录并增强阅读体验。</p><p>工具栏上的五个图标依次为：</p><p><i class="icon-list">&lt;/i&gt; 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落<br><i class="icon-chevron-sign-left"></i> 视图：互换左边编辑区和右边预览区的位置<br><i class="icon-adjust">&lt;/i&gt; 主题：内置了黑白两种模式的主题，试试 <strong>黑色主题</strong>，超炫！<br><i class="icon-desktop"></i> 阅读：心无旁骛的阅读模式提供超一流的阅读体验<br><i class="icon-fullscreen"></i> 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境</i></i></p><h3 id="8-阅读模式"><a href="#8-阅读模式" class="headerlink" title="8. 阅读模式"></a>8. 阅读模式</h3><p>在 <strong>阅读工具栏</strong> 点击 <i class="icon-desktop"></i> 或者按下 <code>Ctrl+Alt+M</code> 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。</p><h3 id="9-标签、分类和搜索"><a href="#9-标签、分类和搜索" class="headerlink" title="9. 标签、分类和搜索"></a>9. 标签、分类和搜索</h3><p>在编辑区任意行首位置输入以下格式的文字可以标签当前文档：</p><p>标签： 未分类</p><p>标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示：</p><p><img src="https://www.zybuluo.com/static/img/file-list.png" alt="file-list"></p><h3 id="10-文稿发布和分享"><a href="#10-文稿发布和分享" class="headerlink" title="10. 文稿发布和分享"></a>10. 文稿发布和分享</h3><p>在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 <i class="icon-share"></i> (Ctrl+Alt+P) 发布这份文档给好友吧！</p><hr><p>再一次感谢您花费时间阅读这份欢迎稿，点击 <i class="icon-file"></i> (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！</p><p>作者 <a href="http://weibo.com/ghosert" target="_blank" rel="noopener">@ghosert</a><br>2016 年 07月 07日    </p><blockquote id="fn_LaTeX"><sup>LaTeX</sup>. 支持 <strong>LaTeX</strong> 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 <a href="http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener">MathJax</a> 参考更多使用方法。<a href="#reffn_LaTeX" title="Jump back to footnote [LaTeX] in the text."> &#8617;</a></blockquote><blockquote id="fn_code"><sup>code</sup>. 代码高亮功能支持包括 Java, Python, JavaScript 在内的，<strong>四十一</strong>种主流编程语言。<a href="#reffn_code" title="Jump back to footnote [code] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;欢迎使用-Cmd-Markdown-编辑阅读器&quot;&gt;&lt;a href=&quot;#欢迎使用-Cmd-Markdown-编辑阅读器&quot; class=&quot;headerlink&quot; title=&quot;欢迎使用 Cmd Markdown 编辑阅读器&quot;&gt;&lt;/a&gt;欢迎使用 Cmd Markdown 编辑阅读器&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，&lt;strong&gt;Cmd Markdown&lt;/strong&gt; 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;整理知识，学习笔记&lt;/li&gt;
&lt;li&gt;发布日记，杂文，所见所想&lt;/li&gt;
&lt;li&gt;撰写发布技术文稿（代码支持）&lt;/li&gt;
&lt;li&gt;撰写发布学术论文（LaTeX 公式支持）
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>markdown</title>
    <link href="http://yoursite.com/2018/08/20/markdown/"/>
    <id>http://yoursite.com/2018/08/20/markdown/</id>
    <published>2018-08-20T14:19:47.000Z</published>
    <updated>2018-09-11T06:14:53.443Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://superuser.com/questions/531592/how-do-i-add-the-tree-command-to-git-bash-on-windows/1141489#1141489?newreg=b92bd8700f4d4918b9230564f65e56d0" target="_blank" rel="noopener">利用git bash生成目录树</a><br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmd</span><span class="bash"> //c tree</span></span><br></pre></td></tr></table></figure></p><p>用下载软件的方法生成的目录树很丑，所以还是继续上述的命令。<br>在linux下 tree是可以生成到文件的。</p><h1 id="主标题"><a href="#主标题" class="headerlink" title="主标题"></a>主标题</h1><h2 id="副标题"><a href="#副标题" class="headerlink" title="副标题"></a>副标题</h2><p>[TOC]</p><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题大"><a href="#四级标题大" class="headerlink" title="四级标题大"></a>四级标题大</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><a id="more"></a><blockquote><p>引用</p><blockquote><p>二级引用</p></blockquote></blockquote><p><del>删除</del><br><em>斜体</em>         <strong>粗体</strong>   <strong><em>粗斜体</em></strong>   下划</p><p>教程2(<a href="https://www.cnblogs.com/Jimmy1988/p/7053875.html" target="_blank" rel="noopener">https://www.cnblogs.com/Jimmy1988/p/7053875.html</a>)</p><p>教程2 <a href="https://www.cnblogs.com/Jimmy1988/p/7053875.html" target="_blank" rel="noopener">https://www.cnblogs.com/Jimmy1988/p/7053875.html</a></p><p><img src="pic\3.jpg" alt="图片"></p><ul><li>无序</li></ul><ul><li>列</li></ul><ul><li>表</li></ul><ol><li>有序</li><li>列表</li></ol><div class="table-container"><table><thead><tr><th style="text-align:left">姓名</th><th style="text-align:center">张三</th><th style="text-align:right">李四</th></tr></thead><tbody><tr><td style="text-align:left">性别</td><td style="text-align:center">男</td><td style="text-align:right">女</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">one</th><th style="text-align:right">two</th><th style="text-align:center">three</th><th>four</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:right">2</td><td style="text-align:center">3</td><td>4</td></tr></tbody></table></div><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">print</span>(<span class="string">"I am studying"</span>);</span><br><span class="line"><span class="builtin-name">print</span>(<span class="string">"yes or no"</span>);</span><br></pre></td></tr></table></figure><p>代码是<code>hello world</code></p><p><a href="www.baidu.com" title="baidu">樱桃的简书</a></p><p>行内链接和参考链接</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://superuser.com/questions/531592/how-do-i-add-the-tree-command-to-git-bash-on-windows/1141489#1141489?newreg=b92bd8700f4d4918b9230564f65e56d0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;利用git bash生成目录树&lt;/a&gt;&lt;br&gt;&lt;figure class=&quot;highlight dockerfile&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; //c tree&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;用下载软件的方法生成的目录树很丑，所以还是继续上述的命令。&lt;br&gt;在linux下 tree是可以生成到文件的。&lt;/p&gt;
&lt;h1 id=&quot;主标题&quot;&gt;&lt;a href=&quot;#主标题&quot; class=&quot;headerlink&quot; title=&quot;主标题&quot;&gt;&lt;/a&gt;主标题&lt;/h1&gt;&lt;h2 id=&quot;副标题&quot;&gt;&lt;a href=&quot;#副标题&quot; class=&quot;headerlink&quot; title=&quot;副标题&quot;&gt;&lt;/a&gt;副标题&lt;/h2&gt;&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;一级标题&quot;&gt;&lt;a href=&quot;#一级标题&quot; class=&quot;headerlink&quot; title=&quot;一级标题&quot;&gt;&lt;/a&gt;一级标题&lt;/h1&gt;&lt;h2 id=&quot;二级标题&quot;&gt;&lt;a href=&quot;#二级标题&quot; class=&quot;headerlink&quot; title=&quot;二级标题&quot;&gt;&lt;/a&gt;二级标题&lt;/h2&gt;&lt;h3 id=&quot;三级标题&quot;&gt;&lt;a href=&quot;#三级标题&quot; class=&quot;headerlink&quot; title=&quot;三级标题&quot;&gt;&lt;/a&gt;三级标题&lt;/h3&gt;&lt;h4 id=&quot;四级标题大&quot;&gt;&lt;a href=&quot;#四级标题大&quot; class=&quot;headerlink&quot; title=&quot;四级标题大&quot;&gt;&lt;/a&gt;四级标题大&lt;/h4&gt;&lt;h5 id=&quot;五级标题&quot;&gt;&lt;a href=&quot;#五级标题&quot; class=&quot;headerlink&quot; title=&quot;五级标题&quot;&gt;&lt;/a&gt;五级标题&lt;/h5&gt;&lt;h6 id=&quot;六级标题&quot;&gt;&lt;a href=&quot;#六级标题&quot; class=&quot;headerlink&quot; title=&quot;六级标题&quot;&gt;&lt;/a&gt;六级标题&lt;/h6&gt;
    
    </summary>
    
    
      <category term="markdown教程" scheme="http://yoursite.com/tags/markdown%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>hello hexo </title>
    <link href="http://yoursite.com/2018/08/17/hello-hexo/"/>
    <id>http://yoursite.com/2018/08/17/hello-hexo/</id>
    <published>2018-08-17T13:00:33.000Z</published>
    <updated>2018-09-13T05:13:43.364Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常用命令-cmd"><a href="#常用命令-cmd" class="headerlink" title="常用命令(cmd)"></a>常用命令(cmd)</h2><p>hexo n “postName”<br>hexo clean<br>hexo g 本地生成<br>hexo d 同步到github<br>hexo d -g<br>hexo new page aboutme<br>hexo s 本地服务器预览<br>hexo s -p 4100 换端口<br><a id="more"></a></p><h2 id="搭建Github-hexo-博客的过程"><a href="#搭建Github-hexo-博客的过程" class="headerlink" title="搭建Github+hexo 博客的过程"></a>搭建Github+hexo 博客的过程</h2><p>参考链接如下<br><a href="https://www.cnblogs.com/fengxiongZz/p/7707219.html" target="_blank" rel="noopener">使用Hexo+Github一步步搭建属于自己的博客（基础）</a><br><a href="https://www.cnblogs.com/fengxiongZz/p/7707568.html" target="_blank" rel="noopener">使用Hexo+Github一步步搭建属于自己的博客（进阶）</a><br><a href="http://blog.haoji.me/build-blog-website-by-hexo-github.html?from=xa" target="_blank" rel="noopener">1</a><br><a href="https://www.cnblogs.com/fengxiongZz/p/7707568.html" target="_blank" rel="noopener">2</a><br><a href="https://www.jianshu.com/p/84a8384be1ae" target="_blank" rel="noopener">3</a><br><a href="http://tengj.top/2016/02/22/hexo1/" target="_blank" rel="noopener">4</a><br>$ npm install -g hexo-cli<br>安装Node.js →安装Hexo → 安装主题 →  本地测试运行 → 注册给github与coding并创建pages仓库 → 部署<br>安装Git                                        →<br><a href="https://blog.csdn.net/heshuaicsdn/article/details/78923476" target="_blank" rel="noopener">node.js的解释是高并发</a><br>npm是模块的包管理器,与node.js一起安装的。<br>npm install hexo -g 全局安装<a href="https://www.runoob.com/nodejs/nodejs-npm.html" target="_blank" rel="noopener"></a><br>hexo是基于node.js的静态博客，所以我们才需要安装node.js<br>Git是为了让其他人也可以看到你的博客，把本地的内容提交到github上面去<br>常用命令<a href="https://blog.csdn.net/qq_26975307/article/details/62447489" target="_blank" rel="noopener"></a><br>hexo g 生成 generate<br>hexo s 启动服务器预览 server<br>hexo d 部署 deploy<br>hexo clean 清除缓存<br>hexo server -p 4100<br>hexo generate —deploy 完成后部署<br>hexo deploy —generate 完成后部<br>hexo new “postName”</p><p>node_modules 依赖包<br>public 生成的页面<br>scaffolds 模板文件夹 post draft page<br>source 用户资源的地方</p><p><a href="https://www.jianshu.com/p/3a8dba06856a" target="_blank" rel="noopener">hexo解释</a><br><a href="https://blog.csdn.net/kingice1014/article/details/52924523" target="_blank" rel="noopener">hexo中_config.yml</a><br><a href="https://www.jianshu.com/p/56d99a3049a5" target="_blank" rel="noopener">markdown写博客</a><br><a href="https://blog.csdn.net/o_mario_o/article/details/80347863" target="_blank" rel="noopener">hexo中的配置信息</a><br><a href="https://www.jianshu.com/p/84a8384be1ae" target="_blank" rel="noopener">域名绑定</a><br><a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">hexo渲染MathJax数学公式</a><br>markdownpad渲染数学公式只能F6浏览器预览，并且不能实时预览，所以考虑换个markdown编辑器<br>cmd markdown对本地文件支持不友好，并且不能加载本地图片<br><a href="https://hyxxsfwy.github.io/2016/01/15/Hexo-Markdown-%E7%AE%80%E6%98%8E%E8%AF%AD%E6%B3%95%E6%89%8B%E5%86%8C/" target="_blank" rel="noopener">Hexo Markdown 简明语法手册</a><br><a href="https://github.com/LouisBarranqueiro/hexo-footnotes" target="_blank" rel="noopener">hexo的脚注问题</a><br>只能实现数字的脚注</p><hr><p>2018-09-12</p><h2 id="实现评论功能"><a href="#实现评论功能" class="headerlink" title="实现评论功能"></a>实现评论功能</h2><p>此次评论功能使用disqus，理由：同学推荐<br>参考链接：<br><a href="https://www.jianshu.com/p/2671b28b79e5" target="_blank" rel="noopener">Hexo折腾记之科学使用Disqus与Next的集成</a><br><a href="https://blog.fooleap.org/use-disqus-correctly.html" target="_blank" rel="noopener">科学使用 Disqus</a><br><a href="https://github.com/fooleap/disqus-php-api" target="_blank" rel="noopener">Disqus PHP API</a><br><a href="http://smk17.cn/posts/104/" target="_blank" rel="noopener">基于disqus-php-api在Hexo博客中使用Disqus</a><br><a href="https://ycwalker.com/2017/06/01/diqus-proxy-config/" target="_blank" rel="noopener">Disqus-Proxy 配置说明</a><br><a href="https://blog.csdn.net/u010053344/article/details/50701191" target="_blank" rel="noopener">Github 搭建 hexo （四）——更换主题，disqus，RSS</a></p><h2 id="添加rss功能"><a href="#添加rss功能" class="headerlink" title="添加rss功能"></a>添加rss功能</h2><p>不知道是干嘛的，好像是为了实现订阅的。暂时不是很清楚。<br>参考链接：<br><a href="https://blog.csdn.net/tx874828503/article/details/51577815" target="_blank" rel="noopener">最简便的方法搭建Hexo+Github博客,基于Next主题</a></p><h2 id="添加site-map功能"><a href="#添加site-map功能" class="headerlink" title="添加site-map功能"></a>添加site-map功能</h2><p>参考链接<br>不知道是干嘛的<br><a href="https://blog.csdn.net/u010053344/article/details/50706790" target="_blank" rel="noopener">Github 搭建 hexo （五）- 站点地图（sitemap.xml）</a><br>站点地图还挺高级，以后再说。</p><h2 id="百度自动推送"><a href="#百度自动推送" class="headerlink" title="百度自动推送"></a>百度自动推送</h2><p>参考链接<br><a href="https://blog.csdn.net/hosea1008/article/details/53384382" target="_blank" rel="noopener">Hexo+Next主题博客提交百度谷歌收录</a></p><h2 id="添加公益404界面"><a href="#添加公益404界面" class="headerlink" title="添加公益404界面"></a>添加公益404界面</h2><p>参考链接<br><a href="https://blog.csdn.net/liu1340308350/article/details/81744824" target="_blank" rel="noopener">hexo添加404公益界面</a><br><a href="https://blog.csdn.net/tx874828503/article/details/51577815" target="_blank" rel="noopener">最简便的方法搭建Hexo+Github博客,基于Next主题</a></p><h2 id="添加搜索"><a href="#添加搜索" class="headerlink" title="添加搜索"></a>添加搜索</h2><p>参考链接<br><a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener">hexo-genarator-search</a></p><h2 id="高级教程"><a href="#高级教程" class="headerlink" title="高级教程"></a>高级教程</h2><p>以后再说<br><a href="https://blog.csdn.net/sunshine940326/article/details/52552283" target="_blank" rel="noopener">利用Gitpage+hexo开发自己的博客</a><br><a href="https://blog.csdn.net/linshuhe1/article/details/52424573" target="_blank" rel="noopener">Hexo个人免费博客(三) next主题、评论、阅读量统计和站内搜索</a><br><a href="https://hexo.io/plugins/" target="_blank" rel="noopener">官网插件</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;常用命令-cmd&quot;&gt;&lt;a href=&quot;#常用命令-cmd&quot; class=&quot;headerlink&quot; title=&quot;常用命令(cmd)&quot;&gt;&lt;/a&gt;常用命令(cmd)&lt;/h2&gt;&lt;p&gt;hexo n “postName”&lt;br&gt;hexo clean&lt;br&gt;hexo g 本地生成&lt;br&gt;hexo d 同步到github&lt;br&gt;hexo d -g&lt;br&gt;hexo new page aboutme&lt;br&gt;hexo s 本地服务器预览&lt;br&gt;hexo s -p 4100 换端口&lt;br&gt;
    
    </summary>
    
      <category term="搭建博客" scheme="http://yoursite.com/categories/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="github" scheme="http://yoursite.com/tags/github/"/>
    
      <category term="npm" scheme="http://yoursite.com/tags/npm/"/>
    
      <category term="基础" scheme="http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>hello world</title>
    <link href="http://yoursite.com/2018/08/17/hello-world-1/"/>
    <id>http://yoursite.com/2018/08/17/hello-world-1/</id>
    <published>2018-08-17T12:59:52.000Z</published>
    <updated>2018-08-17T12:59:52.387Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/08/17/hello-world/"/>
    <id>http://yoursite.com/2018/08/17/hello-world/</id>
    <published>2018-08-17T12:52:19.357Z</published>
    <updated>2018-08-22T10:04:07.521Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><p>[TOC]</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#Create-a-new-post&quot; class=&quot;headerlink&quot; title=&quot;Create a new post&quot;&gt;&lt;/a&gt;Create a new post&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hexo new &lt;span class=&quot;string&quot;&gt;&quot;My New Post&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
