<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>田佳杰</title>
  <icon>https://www.gravatar.com/avatar/cbd320e406f4c9571bb798e8810c4d18</icon>
  <subtitle>记录一些学习到的东西和论文记录</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-12-03T03:22:05.493Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jiajie Tian</name>
    <email>18810906582@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>github-markdown-mathjax</title>
    <link href="http://yoursite.com/2018/12/03/github-markdown-mathjax/"/>
    <id>http://yoursite.com/2018/12/03/github-markdown-mathjax/</id>
    <published>2018-12-03T02:59:51.000Z</published>
    <updated>2018-12-03T03:22:05.493Z</updated>
    
    <content type="html"><![CDATA[<p>前言：在github上传自己的论文记录后发现，github上的显示和在vscode上的显示不同，主要体现在公式在github上不能正确地显示，并且换行也不能正确地显示。<br><a id="more"></a></p><p>参考链接：<br><a href="https://blog.csdn.net/phdsky/article/details/81431279" target="_blank" rel="noopener">https://blog.csdn.net/phdsky/article/details/81431279</a></p><p>搜索之后发现github的markdown不支持mathjax的渲染。<br><a href="https://github.com/github/markup/issues/897" target="_blank" rel="noopener">Github issue - github’s markdown mathjax rending</a><br><a href="https://stackoverflow.com/questions/11256433/how-to-show-math-equations-in-general-githubs-markdownnot-githubs-blog" target="_blank" rel="noopener">Stackoverflow - How to show math equations in general github’s markdown</a></p><p>解决方案或者是公式转图片，或者是使用github内嵌的公式编辑器，或者是适用于chrome的github with MathJax插件。</p><p>我最后采用的是github with MathJax插件。<br><a href="https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima" target="_blank" rel="noopener">GitHub with MathJax 插件</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前言：在github上传自己的论文记录后发现，github上的显示和在vscode上的显示不同，主要体现在公式在github上不能正确地显示，并且换行也不能正确地显示。&lt;br&gt;
    
    </summary>
    
      <category term="github-markdown" scheme="http://yoursite.com/categories/github-markdown/"/>
    
    
      <category term="github" scheme="http://yoursite.com/tags/github/"/>
    
      <category term="markdown" scheme="http://yoursite.com/tags/markdown/"/>
    
      <category term="mathjax" scheme="http://yoursite.com/tags/mathjax/"/>
    
  </entry>
  
  <entry>
    <title>markdown-math</title>
    <link href="http://yoursite.com/2018/12/03/markdown-math/"/>
    <id>http://yoursite.com/2018/12/03/markdown-math/</id>
    <published>2018-12-03T02:17:02.000Z</published>
    <updated>2018-12-04T03:08:39.223Z</updated>
    
    <content type="html"><![CDATA[<p>前言: 因为最近经常用到markdown写数学公式，每次都查感觉有点啰嗦，所以做个简单小结，把平常用的做个记录。这个博客根据平时自己常用到的进行动态增加。</p><a id="more"></a><p>参考链接：<br><a href="https://blog.csdn.net/deepinC/article/details/81103326" target="_blank" rel="noopener">https://blog.csdn.net/deepinC/article/details/81103326</a></p><p><a href="https://blog.csdn.net/HaleyPKU/article/details/80341932" target="_blank" rel="noopener">https://blog.csdn.net/HaleyPKU/article/details/80341932</a></p><p><a href="https://blog.csdn.net/qq_39599067/article/details/81184139?utm_source=blogxgwz6" target="_blank" rel="noopener">https://blog.csdn.net/qq_39599067/article/details/81184139?utm_source=blogxgwz6</a></p><p><a href="https://www.zybuluo.com/fyywy520/note/82980" target="_blank" rel="noopener">https://www.zybuluo.com/fyywy520/note/82980</a></p><h1 id="公式使用参考"><a href="#公式使用参考" class="headerlink" title="公式使用参考"></a>公式使用参考</h1><h2 id="插入公式"><a href="#插入公式" class="headerlink" title="插入公式"></a>插入公式</h2><p>插入公式分为行中公式，独立公式和自动编号公式</p><p>1.行中公式 $ a=b $</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> 数学公式 $</span></span><br></pre></td></tr></table></figure><p>2.独立公式 $$ a=b $$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$ 数学公式 $$</span></span><br></pre></td></tr></table></figure><p>3.编号公式<br>$$ a=b \tag {1} $$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$ 数学公式 \tag &#123;1&#125; $$</span></span><br></pre></td></tr></table></figure><p>由公式$(1)$可以得出结论</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由公式$(1)$可以得出结论</span><br></pre></td></tr></table></figure><p>4.自动编号公式<br>自动编号公式在github上显示不出来，原则上是可以的，推荐使用手动编号。<br>$$<br>\begin{equation}<br>x^n+y^n=z^n<br>\label{eq:afa}<br>\end{equation}<br>$$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">数学公式</span><br><span class="line">\label&#123;eq:当前公式名&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure><p>5.自动编号公式的引用方法</p><p>在公式 $$\eqref{eq:wwqr}$$ 中，我们看到了这个被自动编号的公式。<br>貌似没有成功</p><p>6.单个公式换行</p><p>单个公式很长的时候需要换行，但仅允许生成一个编号时，可以用split标签包围公式代码，在需要转行的地方使用\，每行需要使用1个&amp;来标识对齐的位置，结束后可使用\tag{…}标签编号。</p><p>$$<br>\begin{split}<br>a &amp;= b \<br>c &amp;= d \<br>e &amp;= f<br>\end{split}\tag{1.2}<br>$$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">a &amp;= b \\</span><br><span class="line">c &amp;= d \\</span><br><span class="line">e &amp;= f </span><br><span class="line">\end&#123;split&#125;\tag&#123;1.3&#125;</span><br><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br></pre></td></tr></table></figure><p>7.多行的独立公式</p><p>有时候需要罗列多个公式，可以用eqnarray*标签包围公式代码，在需要转行的地方使用\，每行需要使用2个&amp;来标识对齐位置，两个&amp;…&amp;号之间的是公式间对齐的位置，每行公式后可使用\tag{…}标签编号：</p><p>github貌似对于多行公式显示不出来。</p><p>$$\begin{eqnarray<em>}<br>x^n+y^n &amp;=&amp; z^n \tag{1.4} \<br>x+y &amp;=&amp; z \tag{1.5}<br>\end{eqnarray</em>}$$</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br><span class="line">\begin&#123;eqnarray*&#125;</span><br><span class="line">x^n+y^n &amp;=&amp; z^n \tag&#123;1.4&#125; \\</span><br><span class="line">x+y &amp;=&amp; z \tag&#123;1.5&#125;</span><br><span class="line">\end&#123;eqnarray*&#125;</span><br><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br></pre></td></tr></table></figure><h2 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h2><table><thead><tr><th style="text-align:center">输入</th><th style="text-align:center">显示</th><th style="text-align:center">输入</th><th style="text-align:center">显示</th></tr></thead><tbody><tr><td style="text-align:center">x^y</td><td style="text-align:center">$x^y$</td><td style="text-align:center">x_y</td><td style="text-align:center">$x_y$</td></tr><tr><td style="text-align:center">\sideset{\^1_2}{\^3_4}\bigotimes</td><td style="text-align:center">$\sideset{^1_2}{^3_4}\bigotimes$</td><td style="text-align:center">\langle</td><td style="text-align:center">&lt;</td></tr><tr><td style="text-align:center">\lceil</td><td style="text-align:center">$\lceil$</td><td style="text-align:center">\rceil</td><td style="text-align:center">$\rceil$</td></tr><tr><td style="text-align:center">\lfloor</td><td style="text-align:center">$\lfloor$</td><td style="text-align:center">\frac{a}{b}</td><td style="text-align:center">$\frac{a}{b}$</td></tr><tr><td style="text-align:center">\sqrt{2}</td><td style="text-align:center">$\sqrt{2}$</td><td style="text-align:center">\alpha,\gamma</td><td style="text-align:center">$\alpha$ $\gamma$</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前言: 因为最近经常用到markdown写数学公式，每次都查感觉有点啰嗦，所以做个简单小结，把平常用的做个记录。这个博客根据平时自己常用到的进行动态增加。&lt;/p&gt;
    
    </summary>
    
      <category term="markdown" scheme="http://yoursite.com/categories/markdown/"/>
    
    
      <category term="markdown" scheme="http://yoursite.com/tags/markdown/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>person-reid-transfer-learning</title>
    <link href="http://yoursite.com/2018/11/29/person-reid-transfer-learning/"/>
    <id>http://yoursite.com/2018/11/29/person-reid-transfer-learning/</id>
    <published>2018-11-29T15:29:52.000Z</published>
    <updated>2018-12-10T02:45:17.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="transfer-learning"><a href="#transfer-learning" class="headerlink" title="transfer learning"></a>transfer learning</h1><a id="more"></a><p>这个博客主要是因为最近看了几篇关于无监督迁移学习在行人重识别领域的论文，发现隔了几天，自己对论文就忘记得差不多了，所以对论文的关键内容做个简单记录。</p><p>参考链接: <a href="https://github.com/layumi/DukeMTMC-reID_evaluation/blob/master/State-of-the-art/README.md" target="_blank" rel="noopener">Transfer Learning</a></p><p>因为在某些情况下，图片或者公式无法正常显示，所以，我基本会同步到我的博客<br><a href="https://tjjtjjtjj.github.io/2018/11/29/person-reid-transfer-learning/#more" target="_blank" rel="noopener">https://tjjtjjtjj.github.io/2018/11/29/person-reid-transfer-learning/#more</a></p><p>现有方法在transfer learning方向的性能对比</p><p><img src="./pic/transfer/transfer.png" alt="transfer learning"><br><img src="/2018/11/29/person-reid-transfer-learning/transfer.png" title="transfer learning"></p><hr><h2 id="1-ARN"><a href="#1-ARN" class="headerlink" title="1. ARN"></a>1. ARN</h2><p><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w6/Li_Adaptation_and_Re-Identification_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification</a></p><p>Yu-Jhe Li, Fu-En Yang, Yen-Cheng Liu, Yu-Ying Yeh, Xiaofei Du, and Yu-Chiang Frank Wang, CVPR 2018 Workshop</p><p>这篇论文主要分离了数据集的特有特征和行人特征，从而使不同数据集的行人特征投射到统一特征空间中。</p><p>作者是台湾人，没有公布代码。有其他人复现了<a href="https://github.com/huanghoujing/ARN" target="_blank" rel="noopener">代码</a>，但是效果很差。</p><p>我下一步也会尝试复现一下。</p><h3 id="1-1-网络架构"><a href="#1-1-网络架构" class="headerlink" title="1.1 网络架构"></a>1.1 网络架构</h3><p><img src="./pic/ARN/ARN.png" alt="ARN的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/ARN.png" title="ARN的网络架构"></p><p>根据作者的描述，</p><ul><li>$E_I$是resnet50的前四个layer,输入是3X256X256,输出$X^s$是2048X7X7</li><li>$E_T,E_C,E_S$,是相同的网络架构，来自FCN的三层，通过查阅FCN的网络设置，初步猜想是FCN的conv6，conv7，conv8，相应的Decoder暂时按照反卷积来设置。这一部分还需要参考FCN的网络设置。</li><li>$E_T,E_C,E_S$ conv6:7X7X2048,relu6,drop6(0.5),conv7:1X1X2048,relu6,drop6(0.5),conv8:1X1X2048,至于conv6,7的bn和conv8的bn，relu要不要，还需要实验的验证</li><li>在FCN中，逆卷积的使用方式是 deconv(k=64, s=32, p=0)+crop(offset=19)，参考资料:<a href="https://zhuanlan.zhihu.com/p/22976342?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">FCN学习:Semantic Segmentation</a>,<a href="https://blog.csdn.net/zlrai5895/article/details/80473814" target="_blank" rel="noopener">经典网络复现系列（一）：FCN</a></li><li>反卷积的时候一般都是k=2n, s=n,</li><li>参考FCN和pytorch的入门与实践第六章的生成器，我们的Decoder使用deconv(k=1,s=1), deconv(k=1, s=1), deconv(k=7, s=1)</li><li>encoder和decoder都使用bn和relu</li><li>分类层有dropout</li><li>学习率，$E_I=10^{-7}, E_T E_C E_S D_C = 10^{-3}, C_S = 2*10^{-3}  $，并且在前几个epoch只更新$E_I$</li><li>优化器：SGD</li></ul><h3 id="1-2-损失函数"><a href="#1-2-损失函数" class="headerlink" title="1.2 损失函数"></a>1.2 损失函数</h3><p>分类损失</p><script type="math/tex; mode=display">L_{class}=-\sum_{i=1}^{N_s}y_i^s.log\hat{y}_i^s \tag {1}</script><p>对比损失</p><script type="math/tex; mode=display">L_{ctrs}=\sum_{i,j}{\lambda}(e_{c,i}^s-e_{c,j}^s)^2+ ({1-\lambda}) [max(0, m-(e_{c,i}^s-e_{c,j}^s))]^2 \tag {2}</script><p>重构误差</p><script type="math/tex; mode=display">L_{rec} = \sum_{i=1}^{N_s} ||X_i^s-\hat{X_i^s}||^2 + \sum_{i=1}^{N_t} ||X_i^t-\hat{X_i^t}||^2 \tag 3</script><p>差别损失</p><script type="math/tex; mode=display">L_{diff} = || {H_c^s}^T H_p^s ||_F^2 + || {H_c^t}^T H_p^t ||_F^2 \tag 4</script><p>总损失</p><script type="math/tex; mode=display">L_{total} = L_{class} + \alpha L_{ctrs} + \beta L_{rec} + \gamma L_{diff} \tag {5}</script><p>其中</p><script type="math/tex; mode=display">\alpha=0.01, \beta= 2.0, \gamma=1500</script><h3 id="1-3-模块分析"><a href="#1-3-模块分析" class="headerlink" title="1.3 模块分析"></a>1.3 模块分析</h3><p>三个模块:</p><ol><li><strong>$ L_{rec} $</strong></li><li><strong>$ L_{class} $和$ L_{ctrs} $</strong></li><li><strong>$ E_T$和$E_S$</strong></li></ol><h4 id="1-3-1-半监督-L-rec"><a href="#1-3-1-半监督-L-rec" class="headerlink" title="1.3.1 半监督$ L_{rec} $"></a>1.3.1 半监督$ L_{rec} $</h4><p>这里不是很懂这个重构误差损失函数的作用，下面的这个解释也不行。重构损失是半监督损失函数。暂时理解成重构损失保证在获取特征的过程中尽可能减少信息损失。或者说，类似PCA，保留主成分，这个主成分只能保证尽可能地把样本分开。至于这个主成分是否重要，是否有利于分类，不得而知。</p><p>参考链接：<a href="https://blog.csdn.net/hijack00/article/details/52238549" target="_blank" rel="noopener">深度学习中的“重构”</a></p><p>作者在这里提示，当只有重构损失函数的时候，应该保持$E_I$不更新，只更新$E_C$.</p><p>S: Market, T: Duke; S: Duke, T: Market</p><div class="table-container"><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th></tr></thead><tbody><tr><td style="text-align:center">$L_{rec}$</td><td style="text-align:center">44.5</td><td style="text-align:center">20.3</td><td style="text-align:center">31.2</td><td style="text-align:center">18.4</td></tr></tbody></table></div><h4 id="1-3-2-监督-L-rec-L-class-和-L-ctrs"><a href="#1-3-2-监督-L-rec-L-class-和-L-ctrs" class="headerlink" title="1.3.2 监督$ L_{rec} $, $ L_{class} $和$ L_{ctrs} $"></a>1.3.2 监督$ L_{rec} $, $ L_{class} $和$ L_{ctrs} $</h4><p>半监督和监督</p><p>监督损失使得共享空间捕获到行人语义信息。</p><p>S: Market, T: Duke; S: Duke, T: Market</p><div class="table-container"><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th></tr></thead><tbody><tr><td style="text-align:center">w/o $ L_{class} $, $ L_{ctrs} $</td><td style="text-align:center">52.2</td><td style="text-align:center">23.7</td><td style="text-align:center">36.7</td><td style="text-align:center">19.6</td></tr><tr><td style="text-align:center">w $ L_{class} $, $ L_{ctrs} $</td><td style="text-align:center">70.3</td><td style="text-align:center">39.4</td><td style="text-align:center">60.2</td><td style="text-align:center">33.4</td></tr><tr><td style="text-align:center">$L_{rec}$</td><td style="text-align:center">44.5</td><td style="text-align:center">20.3</td><td style="text-align:center">31.2</td><td style="text-align:center">18.4</td></tr><tr><td style="text-align:center">$L_{rec}$, $ L_{class} $和$ L_{ctrs} $</td><td style="text-align:center">60.5</td><td style="text-align:center">28.7</td><td style="text-align:center">48.4</td><td style="text-align:center">26.8</td></tr></tbody></table></div><h4 id="1-3-3-无监督-L-rec-E-T-和-E-S"><a href="#1-3-3-无监督-L-rec-E-T-和-E-S" class="headerlink" title="1.3.3 无监督$ L_{rec} $, $ E_T $和$ E_S $"></a>1.3.3 无监督$ L_{rec} $, $ E_T $和$ E_S $</h4><p>特有特征的提取是为了去除共享空间的噪声。</p><p>假设共享空间存在，且特有特征空间存在，如果没有特有特征的提取，那么得到的行人特征或多或少地都会包含特征空间的基向量。</p><p>当然，这里也隐含了一些假设，共享空间和特有空间一定是线性无关的。空间的基向量是2048维。</p><p>S: Market, T: Duke; S: Duke, T: Market</p><div class="table-container"><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th><th style="text-align:center">rank-1</th><th style="text-align:center">mAP</th></tr></thead><tbody><tr><td style="text-align:center">w/o  $ E_T $, $ E_S $</td><td style="text-align:center">60.5</td><td style="text-align:center">28.7</td><td style="text-align:center">48.4</td><td style="text-align:center">26.8</td></tr><tr><td style="text-align:center">w $ L_{class} $, $ L_{ctrs} $</td><td style="text-align:center">70.3</td><td style="text-align:center">39.4</td><td style="text-align:center">60.2</td><td style="text-align:center">33.4</td></tr><tr><td style="text-align:center">$L_{rec}$</td><td style="text-align:center">44.5</td><td style="text-align:center">20.3</td><td style="text-align:center">31.2</td><td style="text-align:center">18.4</td></tr><tr><td style="text-align:center">$ L_{rec} $, $ E_T $和$ E_S $</td><td style="text-align:center">52.2</td><td style="text-align:center">23.7</td><td style="text-align:center">36.7</td><td style="text-align:center">19.6</td></tr></tbody></table></div><hr><h2 id="2-HHL"><a href="#2-HHL" class="headerlink" title="2. HHL"></a>2. HHL</h2><p><a href="https://github.com/zhunzhong07/zhunzhong07.github.io/blob/master/paper/HHL.pdf" target="_blank" rel="noopener">Generalizing A Person Retrieval Model Hetero- and Homogeneously</a></p><p>Zhun Zhong, Liang Zheng, Shaozi Li, Yi Yang, ECCV 2018</p><p>code: <a href="https://github.com/zhunzhong07/HHL" target="_blank" rel="noopener">https://github.com/zhunzhong07/HHL</a></p><p>web: <a href="http://zhunzhong.site/paper/HHL.pdf" target="_blank" rel="noopener">http://zhunzhong.site/paper/HHL.pdf</a></p><p>中文: <a href="http://www.cnblogs.com/Thinker-pcw/p/9787440.html" target="_blank" rel="noopener">http://www.cnblogs.com/Thinker-pcw/p/9787440.html</a></p><p>preson-reid中主要面临的问题：</p><ol><li>数据集之间的差异</li><li>数据集内部摄像头的差异</li></ol><p>解决方法：</p><ol><li>相机差异：利用StarGAN进行风格转化</li><li>数据集差异：将源域/目标域图片视为负匹配</li></ol><p>数据集之间的三元组损失有把不同数据集的行人特征映射到同一特征空间的效果。</p><p>创新点在于使用straGAN和复杂的三元组损失。</p><h3 id="2-1-网络架构"><a href="#2-1-网络架构" class="headerlink" title="2.1 网络架构"></a>2.1 网络架构</h3><p><img src="./pic/HHL/HHL.png" alt="HHL的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL.png" title="HHL的网络架构"></p><p>网络的简要介绍</p><ul><li>CNN是resnet50，网络包括两个分支，一个计算源数据集的分类损失，一个计算相似度学习的triplet损失。</li><li>FC-2014的组成：linear(2048，1024)—&gt;bn(1024)—&gt;relu—&gt;dropout(0.5),相当于一个embedding。</li><li>FC-#ID是linear(1024,751), FC-128是linear(1024, 128), 两个分支的具体情况是：</li><li><ul><li>x1—&gt;linear(2048, 1024)—&gt;x2—&gt;bn(1024)—&gt;x3—&gt;relu—&gt;x4—&gt;dropout(0.5)—&gt;x5—&gt;linear(102, 751)—&gt;x6</li></ul></li><li><ul><li>x1—&gt;linear(2048, 1024)—&gt;x2—&gt;bn(1024)—&gt;x3—&gt;relu—&gt;x4—&gt;linear(1024, 128)</li></ul></li><li>网络的triplet损失是Batch Hard Triplet Loss</li><li>网络的输入设置：在每一个batch中，对于分类损失，source domain随机选取batchsize=128张图片，对于triplet损失，source domain随机选取8个人的共batchsize=64张图片，其中连续的8张图片属于同一个人，target domain随机选取batchsize=16个人的共16X9=144张图片，假设这16个人都是不同的人。实验发现，当source domain的分类损失的图片比较少的时候，无法实现预期效果，其他情况下没有测试。当batchsize是这样的配比时，可以达到作者的效果。理由未知．</li><li>starGAN是离线训练</li><li>学习率设置：base：$10^{-1}$，其他：$10^{-2}$，并且每过40个epoch，学习率阶梯性地乘以0.1.一共训练60个epoch就可以达到预期效果，这部分设置和PCB很类似。不知道是经验还是怎么。</li><li>关于StarGAN待自己复现之后再做进一步解释，现在只复现过StyleGAN。</li><li>triplet损失的margin=0.3</li></ul><h3 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2 损失函数"></a>2.2 损失函数</h3><p>分类损失</p><script type="math/tex; mode=display">L_{cross}=-\sum_{i=1}^{N_s}y_i^s.log\hat{y}_i^s</script><p>triplet损失</p><script type="math/tex; mode=display">L_T=\sum_{x_a, x_p, x_n}[D_{x_a, x_p}+m-D_{x_a, x_n}]_+</script><p>相机不变性的triplet损失</p><p>目标域中一张原始图片作为anchor，StarGAN图片为positive，其他图片为negative</p><script type="math/tex; mode=display">L_C=L_T((x_t^i)^{n_t}\bigcup(x_{t^\*}^i)^{n_t^*})</script><p>域不变性的triplet损失</p><p>源域中一张图片为anchor，同一id的其他图片作为positive，目标域的任一图片为negative</p><script type="math/tex; mode=display">L_D=L_T((x_s^i)^{n_s}\bigcup(x_t^i)^{n_t})</script><p>相机不变性和域不变性的triplet损失</p><p>是将相机不变性和域不变性合为一体，源域的positive不变，negative为源域的其他图片和目标域的图片，目标域的positive不变，negative为源域的图片和目标域的其他行人图片</p><script type="math/tex; mode=display">L_{CD}=L_T((x_s^i)^{n_s}\bigcup(x_t^i)^{n_t}\bigcup(x_{t*}^i)^{n_t^*})</script><p>总损失：</p><script type="math/tex; mode=display">L_{HHL}=L_{cross}+\beta*L_{CD}</script><p>其中：</p><script type="math/tex; mode=display">\beta=0.5</script><h3 id="2-3-模块分析"><a href="#2-3-模块分析" class="headerlink" title="2.3 模块分析"></a>2.3 模块分析</h3><ol><li><strong>starGAN</strong></li><li><strong>sample方法</strong></li></ol><h4 id="2-3-1-starGAN"><a href="#2-3-1-starGAN" class="headerlink" title="2.3.1 starGAN"></a>2.3.1 starGAN</h4><p>在源数据集上训练，在目标数据集上测试不同图像增强方法下的图片距离，通过表格可以得出，预训练的模型对于目标数据集的随机翻转等等有很好的鲁棒性，但是，对于不同摄像头的同一个人，其距离还是很大。因此，利用StarGAN和相机不变性的triplet损失来减少由于摄像头带来的偏差。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Source</th><th style="text-align:center">Target</th><th style="text-align:center">Random Crop</th><th style="text-align:center">Random Flip</th><th style="text-align:center">CamStyle Transfer</th></tr></thead><tbody><tr><td style="text-align:center">Duke</td><td style="text-align:center">Market</td><td style="text-align:center">0.049</td><td style="text-align:center">0.034</td><td style="text-align:center">0.485</td></tr><tr><td style="text-align:center">Market</td><td style="text-align:center">Duke</td><td style="text-align:center">0.059</td><td style="text-align:center">0.044</td><td style="text-align:center">0.614</td></tr></tbody></table></div><h4 id="2-3-2-sample方法"><a href="#2-3-2-sample方法" class="headerlink" title="2.3.2 sample方法"></a>2.3.2 sample方法</h4><p>对于目标域的取样方法，对比了三种方法的性能，分别是随机取样、聚类取样、有监督取样，通过下图可以看出，这三种方法的性能是一样的，最后，作者给的代码是随机取样。</p><p><img src="./pic/HHL/HHL2.png" alt="sample"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL2.png" title="sample"></p><h3 id="2-4-实验设置"><a href="#2-4-实验设置" class="headerlink" title="2.4 实验设置"></a>2.4 实验设置</h3><h4 id="2-4-1-Camera-style-transfer-model：StarGAN"><a href="#2-4-1-Camera-style-transfer-model：StarGAN" class="headerlink" title="2.4.1 Camera style transfer model：StarGAN"></a>2.4.1 Camera style transfer model：StarGAN</h4><p>使用StarGAN进行对于摄像头风格转化。</p><ul><li>2 conv + 6 residual + 2 transposed</li><li>input 128X64</li><li>Adam $\beta_1=0.5, \beta_2=0.999$</li><li>数据初始化:随机翻转和随机裁剪</li><li>学习率：前100个epoch为0.0001，后100个epoch线性衰减到0</li></ul><h4 id="2-4-2-Re-ID-model-training"><a href="#2-4-2-Re-ID-model-training" class="headerlink" title="2.4.2 Re-ID model training"></a>2.4.2 Re-ID model training</h4><ul><li>设置可以参考Zhong, Z., Zheng, L., Zheng, Z., Li, S., Yang, Y.: Camera style adaptation for person re-identification</li><li>input 256*128</li><li>数据初始化：随机裁剪和随机翻转</li><li>dropout=0.5</li><li>学习率：新增的层：0.1，base：0.01，每隔40个epoch乘以0.1</li><li>mini-batch：源域上对于IDE为128，对于tripletloss是64.目标域上对于triplet loss是16.</li><li>epoch=60</li><li>测试：2048-dim计算欧式距离</li></ul><h3 id="2-5-超参数设置"><a href="#2-5-超参数设置" class="headerlink" title="2.5 超参数设置"></a>2.5 超参数设置</h3><ul><li>triplet loss的权重$\beta$</li><li>一个batch中目标域上$n_t$的个数</li></ul><h4 id="2-5-1-参数的设置-beta"><a href="#2-5-1-参数的设置-beta" class="headerlink" title="2.5.1 参数的设置$\beta$"></a>2.5.1 参数的设置$\beta$</h4><p><img src="./pic/HHL/HHL3.png" alt="$\beta$参数的设置"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL3.png" title="$\beta$参数的设置"></p><p>$\beta$应该设置成0.4-0.8</p><h4 id="2-5-2-参数的设置-n-t"><a href="#2-5-2-参数的设置-n-t" class="headerlink" title="2.5.2 参数的设置$n_t$"></a>2.5.2 参数的设置$n_t$</h4><p><img src="./pic/HHL/HHL4.png" alt="$n_t$参数的设置"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL4.png" title="$n_t$参数的设置"></p><p>$n_t$在当前设置(源域上对于IDE为128，对于tripletloss是64)下，应该$n_t&gt;16$</p><p>通过上述参数的设置，结合自己实验时的错误，不妨这么理解，在固定mini-batch=128的情况下</p><ul><li>首先引入源域的triplet_loss，并调整batch和$\beta$，使效果达到最优，,batch的选取2倍数的等间隔，$\beta$可以取等间隔，最后batch=64，即128/2=64，$\beta$则可以先固定成某个值.</li><li>然后引入目标域的triplet_loss，并且要先考虑只有目标域的性能，再考虑结合的性能，每次都需要重新考虑$\beta$和batch的大小</li><li>这么一想，这篇论文做的实验还是很多的。</li></ul><h3 id="2-6-实验结果"><a href="#2-6-实验结果" class="headerlink" title="2.6 实验结果"></a>2.6 实验结果</h3><p><img src="./pic/HHL/HHL5.png" alt="实验结果"><br><img src="/2018/11/29/person-reid-transfer-learning/HHL5.png" title="实验结果"></p><p>通过结果我们看出来，其实提升的效果主要来源于$L_C$，说明预训练的模型对于目标域不同摄像头的图片鲁棒性很差。</p><p>是否说明预训练的模型只学习到了源数据集的跨摄像头的不变行人特征，而对于目标域的摄像头下的不同风格很敏感，而对目标域的同一摄像头下的行人特征很鲁棒。</p><p>$L_T$的提升效果很小是否可以说明目标数据集与源数据集的行人特征空间本身就已经很好地重合了，假如tripl_loss真得具有将不同数据集的行人特征映射到同一特征空间的效果的话。</p><p>通过这篇论文，我们能学到的东西很多，比如对比实验，参数设置实验，想法验证实验等等。</p><h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><h4 id="triplet-loss"><a href="#triplet-loss" class="headerlink" title="triplet_loss"></a>triplet_loss</h4><p>发现triplet_loss很厉害的样子，不妨看看是个什么情况。</p><p>参考链接：<br><a href="https://omoindrot.github.io/triplet-loss" target="_blank" rel="noopener">Triplet Loss and Online Triplet Mining in TensorFlow</a></p><p><a href="http://www.itkeyword.com/doc/2025902251705572502/re-id-with-triplet-loss" target="_blank" rel="noopener">Re-ID with Triplet Loss</a></p><p><a href="https://arxiv.org/pdf/1703.07737.pdf" target="_blank" rel="noopener">In Defense of the Triplet Loss for Person Re-Identification</a></p><p><a href="https://github.com/VisualComputingInstitute/triplet-reid" target="_blank" rel="noopener">code</a></p><p><a href="https://omoindrot.github.io/triplet-loss" target="_blank" rel="noopener">Triplet Loss and Online Triplet Mining in TensorFlow</a>这个博客讲述了triplet_loss的起源、发展和具体使用的几种形式。最后的结论是应该使用在线的batch hard策略。</p><p><a href="http://www.itkeyword.com/doc/2025902251705572502/re-id-with-triplet-loss" target="_blank" rel="noopener">Re-ID with Triplet Loss</a>这篇博客则逻辑性地介绍了各种triplet_loss的变体。最后的结论是batch hard+soft margin效果更好。</p><p>也有提及到，triplet_loss总是不如分类损失强。</p><h4 id="下一步工作"><a href="#下一步工作" class="headerlink" title="下一步工作"></a>下一步工作</h4><p>已经理解源代码</p><hr><h2 id="3-SPGAN"><a href="#3-SPGAN" class="headerlink" title="3. SPGAN"></a>3. SPGAN</h2><p><a href="https://arxiv.org/pdf/1711.07027.pdf" target="_blank" rel="noopener">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</a></p><p>Weijian Deng, Liang Zheng, Guoliang Kang, Yi Yang, Qixiang Ye, Jianbin Jiao, CVPR 2018</p><p>这篇论文主要是构建”Learning via Translation”的框架来进行迁移学习，利用SPGAN(CycleGAN+Simaese net)从源数据集迁移到目标数据集，然后在目标数据集上训练。</p><p>论文的重点是怎么改进CycleGAN。</p><p>web:<a href="http://www.sohu.com/a/208231404_642762" target="_blank" rel="noopener">http://www.sohu.com/a/208231404_642762</a></p><p>code:<a href="https://github.com/Simon4Yan/Learning-via-Translation" target="_blank" rel="noopener">https://github.com/Simon4Yan/Learning-via-Translation</a></p><p>CycleGAN</p><p><a href="https://arxiv.org/pdf/1703.10593.pdf" target="_blank" rel="noopener">Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</a></p><p>code:<a href="https://github.com/zhunzhong07/CamStyle" target="_blank" rel="noopener">https://github.com/zhunzhong07/CamStyle</a></p><p>自己对代码的分析<a href="https://tjjtjjtjj.github.io/2018/11/19/cycleGAN/#more" target="_blank" rel="noopener">https://tjjtjjtjj.github.io/2018/11/19/cycleGAN/#more</a></p><h3 id="3-1-前言"><a href="#3-1-前言" class="headerlink" title="3.1 前言"></a>3.1 前言</h3><p>一般的无监督迁移方法都是假设源域和目标域上有相同ID的图片，不太适用于跨数据集的行人重识别。</p><h3 id="3-2-网络架构"><a href="#3-2-网络架构" class="headerlink" title="3.2 网络架构"></a>3.2 网络架构</h3><p>GAN网络</p><p><img src="./pic/SPGAN/SPGAN1.png" alt="SPGAN"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN1.png" title="SPGAN"></p><p>LMP网络</p><p><img src="./pic/SPGAN/SPGAN3.png" alt="LMP"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN3.png" title="LMP"></p><p>行人重识别整体网络</p><p><img src="./pic/SPGAN/SPGAN2.png" alt="SPGAN"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN2.png" title="SPGAN"></p><p>网络的简要介绍</p><ul><li>整理网络由两部分组成，第一部分是SPGAN，第二部分是常见的行人重识别网络的修改版LMP，重点是第一部分。</li><li>整个网络是用Caffe搭建。</li><li>因为自己没有仔细看caffe的代码，后期有需要的还是要看看超参数设置的。</li><li>SPGAN基本沿用了CycleGAN的设置，epoch=5，更多的epoch没有用。</li><li>SPGAN的正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$,负样本是$G(x_S)$和$x_T$或者$F(x_T)$和$x_S$，作者给的代码中用的正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$，负样本是$x_S$和$x_T$。</li><li>SPGAN的训练分为生成器、判别器、SiaNet。</li><li>$L_{ide}$可以保持转换前后图片颜色保持一致。</li><li>LMP网络直接generated domain上训练。</li><li>在论文的tabel2的注释中，可以看到是分成了7份，与PCB的6份差不多。</li></ul><h3 id="3-3-损失函数"><a href="#3-3-损失函数" class="headerlink" title="3.3 损失函数"></a>3.3 损失函数</h3><h4 id="3-3-1-CycleGAN"><a href="#3-3-1-CycleGAN" class="headerlink" title="3.3.1 CycleGAN"></a>3.3.1 CycleGAN</h4><script type="math/tex; mode=display">L_{T_{adv}}(G,D_T,p_x,p_y)=E_{y\sim p_y}[(D_T(y)-1)^2]+E_{x\sim p_x}[(D_T(G(x))-1)^2]</script><script type="math/tex; mode=display">L_{S_{adv}}(F,D_S,p_x,p_y)=E_{x\sim p_x}[(D_S(x)-1)^2]+E_{y\sim p_y}[(D_S(F(y)))^2]</script><script type="math/tex; mode=display">L_{cyc}(G,F)=E_{x\sim p_x}\parallel F(G(x))-x \parallel_1+E_{y\sim p_y}\parallel G(F(y))-y\parallel_1</script><script type="math/tex; mode=display">L_{ide}(G,F,p_x,p_y)=E_{x\sim p_x}\parallel F(x)-x\parallel_1+E_{y\sim p_y}\parallel G(y)-y\parallel_1</script><h4 id="3-3-2-SPGAN"><a href="#3-3-2-SPGAN" class="headerlink" title="3.3.2 SPGAN"></a>3.3.2 SPGAN</h4><p>Siameses Net:</p><script type="math/tex; mode=display">L_{con}(i,x_1,x_2)=(1-i)(max(0,m-d))^2+id^2</script><p>其中，$m\in [0,2]$，$d=1-cos(\theta)\in [0,2]$表示归一化后的欧式距离.正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$,负样本是$G(x_S)$和$x_T$或者$F(x_T)$和$x_S$。</p><p>Overall objective loss:</p><script type="math/tex; mode=display">L_{sp}=L_{T_{adv}}+L_{S_{adv}}+\lambda_1 L_{cyc}+\lambda_2 L_{ide}+\lambda_3 L_{con}</script><p>其中，$\lambda_1=10，\lambda_2=5，\lambda_3=2, m=2$</p><h4 id="3-3-3-行人重识别网络"><a href="#3-3-3-行人重识别网络" class="headerlink" title="3.3.3 行人重识别网络"></a>3.3.3 行人重识别网络</h4><p>以resnet50为基础网络，和PCB类似，分割成两块。</p><h3 id="3-4-实验设置"><a href="#3-4-实验设置" class="headerlink" title="3.4 实验设置"></a>3.4 实验设置</h3><h4 id="3-4-1-SPGAN"><a href="#3-4-1-SPGAN" class="headerlink" title="3.4.1 SPGAN"></a>3.4.1 SPGAN</h4><p>SPGAN的整体训练过程与CycleGAN基本是一致的，建议先参考CycleGAN，再学习SPGAN。</p><p>$\lambda_1=10，\lambda_2=5，\lambda_3=2, m=2$，学习率为0.0002，batch=1，total_epoch=5</p><p><strong>SiaNet:</strong> </p><p>4个conv+4个max pool+1个FC。</p><p>x(3,256,256)-&gt;conv(3,64,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)</p><p>-&gt;conv(64,128,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)</p><p>-&gt;conv(128,256,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)</p><p>-&gt;conv(256,512,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)(1,1,512)</p><p>-&gt;FC(512, 128)-&gt;leak_relu(0.2)-&gt;dropout(0.5)-&gt;FC(128,64)</p><p>输入预处理：随机左右翻转、resize(286)、crop(256)、img/127.5-1。</p><p>激活函数全部使用leak_relu(0.2)，没有使用bn</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">metric_net</span><span class="params">(img, scope, df_dim=<span class="number">64</span>, reuse=False, train=True)</span>:</span></span><br><span class="line"></span><br><span class="line">    bn = functools.partial(slim.batch_norm, scale=<span class="keyword">True</span>, is_training=train,</span><br><span class="line">                           decay=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, updates_collections=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope + <span class="string">'_discriminator'</span>, reuse=reuse):</span><br><span class="line">        h0 = lrelu(conv(img, df_dim, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h0_conv'</span>))    <span class="comment"># h0 is (128 x 128 x df_dim)</span></span><br><span class="line">        pool1 = Mpool(h0, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        h1 = lrelu(conv(pool1, df_dim * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h1_conv'</span>))  <span class="comment"># h1 is (32 x 32 x df_dim*2)</span></span><br><span class="line">        pool2 = Mpool(h1, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        h2 = lrelu(conv(pool2, df_dim * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h2_conv'</span>))  <span class="comment"># h2 is (8 x 8 x df_dim*4)</span></span><br><span class="line">        pool3 = Mpool(h2, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        h3 = lrelu(conv(pool3, df_dim * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, scope=<span class="string">'h3_conv'</span>))  <span class="comment"># h3 is (2 x 2 x df_dim*4)</span></span><br><span class="line">        pool4 = Mpool(h3, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">        shape = pool4.get_shape()</span><br><span class="line">        flatten_shape = shape[<span class="number">1</span>].value * shape[<span class="number">2</span>].value * shape[<span class="number">3</span>].value</span><br><span class="line">        h3_reshape = tf.reshape(pool4, [<span class="number">-1</span>, flatten_shape], name = <span class="string">'h3_reshape'</span>)</span><br><span class="line"></span><br><span class="line">        fc1 = lrelu(FC(h3_reshape, df_dim*<span class="number">2</span>, scope=<span class="string">'fc1'</span>))</span><br><span class="line">        dropout_fc1 = slim.dropout(fc1, <span class="number">0.5</span>, scope=<span class="string">'dropout_fc1'</span>)  </span><br><span class="line">        net = FC(dropout_fc1, df_dim, scope=<span class="string">'fc2'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#print_activations(net)</span></span><br><span class="line">        <span class="comment">#print_activations(pool4)</span></span><br><span class="line">        <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><h4 id="3-4-2-LMP"><a href="#3-4-2-LMP" class="headerlink" title="3.4.2 LMP"></a>3.4.2 LMP</h4><p>batch_size=16, total_epoch=50, SGD, momentum=0.9, gamma=0.1, lr_ini=0.001, decay to 0.0001 after 40 epochs.</p><p>这部分的设置和IDE基本类似。</p><h3 id="3-5-对比实验"><a href="#3-5-对比实验" class="headerlink" title="3.5 对比实验"></a>3.5 对比实验</h3><h4 id="模块的对比实验"><a href="#模块的对比实验" class="headerlink" title="模块的对比实验"></a>模块的对比实验</h4><p><img src="./pic/SPGAN/SPGAN4.png" alt="对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN4.png" title="对比实验"></p><p><img src="./pic/SPGAN/SPGAN8.png" alt="生成效果"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN8.png" title="生成效果"></p><p>通过对比实验可以看到，以mAP为指标，CycleGAN增加了3个点，SiaNet(m=2)增加了3个点，LMP增加了4个点。说明作者尝试的3个模块都在一定程度上起到了作用。但是个人感觉还是差点什么。比如，为什么会有效？</p><p>假设目标都是为了使源域与目标域的行人特征映射到同一特征空间。这里的CycleGAN做到了这一点。LMP可以认为是加在哪里都有效的一种方式。那SiaNet其实更像是在保证生成的图片不仅要保留源图片的内容，更要保留源图片的行人特征。这种保留是以一种隐空间的形式在保存，而不是明显的分类损失这样子。</p><p>$\lambda_3 $对比实验</p><p><img src="./pic/SPGAN/SPGAN5.png" alt="$\lambda_3 $对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN5.png" title="$\lambda_3 $对比实验"></p><p>pool 和 part的对比实验</p><p><img src="./pic/SPGAN/SPGAN6.png" alt="pool 和 part的对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN6.png" title="pool 和 part的对比实验"></p><p>也就是说，pool的方式和parts的取法是实验得到的，不是凭空想出来的。</p><p>通过上述实验超参数的设置对比实验，与HHL论文比较，都是固定其他，变化一个参数，然后选取最优的参数，是基于局部最优就是全局最优的思想。感觉到作者的实验做得很足。</p><p>不同base model的对比实验</p><p><img src="./pic/SPGAN/SPGAN7.png" alt="不同base model的对比实验"><br><img src="/2018/11/29/person-reid-transfer-learning/SPGAN7.png" title="不同base model的对比实验"></p><h3 id="附录-1"><a href="#附录-1" class="headerlink" title="附录"></a>附录</h3><h4 id="IDE-and-IDE"><a href="#IDE-and-IDE" class="headerlink" title="IDE and $IDE^+$"></a>IDE and $IDE^+$</h4><p><a href="https://arxiv.org/pdf/1604.02531.pdf" target="_blank" rel="noopener">IDE</a></p><p><a href="https://github.com/zhunzhong07/IDE-baseline-Market-1501" target="_blank" rel="noopener">https://github.com/zhunzhong07/IDE-baseline-Market-1501</a></p><blockquote><p>We name the descriptor as ID-discriminative Embedding (IDE).<br>感觉还是没有很好地理解IDE。</p></blockquote><p>对于IDE+没有找到对应的原文，因为不是重点，暂且跳过。</p><p>IDE的pytorch代码</p><p><a href="https://github.com/Simon4Yan/Person_reID_baseline_pytorch" target="_blank" rel="noopener">https://github.com/Simon4Yan/Person_reID_baseline_pytorch</a></p><p>IDE和$IDE^+$的网络模型是一样的：</p><p>resnet50(layer4)+avgpool+Linear(2048,512)+bn1d(512)+LeakReLU(0.1)+Dropout(0.5)+Linear(512, num_class)</p><p>区别在于训练时bn层是否更新：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model.model = resnet50(layer4)+avgpool+Linear(2048,512)+bn1d(512)+LeakReLU(0.1)+Dropout(0.5)</span></span><br><span class="line"><span class="comment"># model.classifier = Linear(512, num_class)</span></span><br><span class="line"><span class="comment"># IDE</span></span><br><span class="line"><span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        model.train(<span class="keyword">True</span>)  <span class="comment"># Set model to training mode</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.train(<span class="keyword">False</span>)  <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"><span class="comment"># IDE+</span></span><br><span class="line"><span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        model.eval()  <span class="comment"># Fix BN of ResNet50</span></span><br><span class="line">        model.model.fc.train(<span class="keyword">True</span>)</span><br><span class="line">        model.classifier.train(<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.train(<span class="keyword">False</span>)  <span class="comment"># Set model to evaluate mode</span></span><br></pre></td></tr></table></figure><h4 id="Caffe-and-pytorch"><a href="#Caffe-and-pytorch" class="headerlink" title="Caffe and pytorch"></a>Caffe and pytorch</h4><p><a href="https://github.com/Simon4Yan/Learning-via-Translation/issues/1" target="_blank" rel="noopener">Caffe和pytorch中的bn层的计算方式不一样。</a></p><p>在caffe中，bn层在训练时是eval状态，也是只使用Imagenet的mean和variance</p><blockquote><p>The eval mode for BN layer during training, corresponding to Caffe’s batch_norm_param {use_global_stats: true}, means using ImageNet BN mean and variance during training.</p></blockquote><p>在pytorch中，bn层在训练时如果设置成eval装填，才可以达到caffe的精度。</p><h4 id="疑惑"><a href="#疑惑" class="headerlink" title="疑惑"></a>疑惑</h4><p>IDE和IDE+的效果区别为什么会这么大?</p><h3 id="下一步工作-1"><a href="#下一步工作-1" class="headerlink" title="下一步工作"></a>下一步工作</h3><ul><li>[x] 已经理解源代码</li></ul><p>尝试在pytorch上复现结果，现在根据作者提供的代码，感觉并不是很难。主要是SPGAN。</p><hr><h2 id="4-基于GAN的类似论文"><a href="#4-基于GAN的类似论文" class="headerlink" title="4. 基于GAN的类似论文"></a>4. 基于GAN的类似论文</h2><p>类似的采取GAN做person-reid方向的论文还有好多，上面两篇是现在最新的，下面就简单地介绍几篇类似的文章，其中涉及到的原理和前文提到的GAN的方法类似。</p><h3 id="4-1-PTGAN"><a href="#4-1-PTGAN" class="headerlink" title="4.1 PTGAN"></a>4.1 PTGAN</h3><p><a href="https://arxiv.org/pdf/1711.08565.pdf" target="_blank" rel="noopener">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</a></p><p>Longhui Wei1, Shiliang Zhang1, Wen Gao1, Qi Tian</p><p>这篇论文对Cycle-GAN进行了改进，保留ID信息的损失函数如下：</p><script type="math/tex; mode=display">L_{ID}=E_{a \sim p_{data}(a)} [||(G(a)-a) \odot M(a)||_2] + E_{b \sim p_{data}(b)} [||(F(b)-b) \odot M(b)||_2]</script><p>其中，$M(b)$表示使用PSPNet分割后的结果。</p><p>转化效果如下图所示</p><p><img src="./pic/PTGAN/PTGAN.png" alt="PTGAN的转化效果"><br><img src="/2018/11/29/person-reid-transfer-learning/PTGAN.png" title="PTGAN的转化效果"></p><p>这里的Cycle-Gan生成图片的效果和SPGAN生成的效果还是有一些区别的，不是很理解。</p><p>其他的不是本次的重点，不做介绍。</p><h3 id="4-2-DCGAN-CNN"><a href="#4-2-DCGAN-CNN" class="headerlink" title="4.2 DCGAN+CNN"></a>4.2 DCGAN+CNN</h3><p><a href="https://arxiv.org/pdf/1701.07717.pdf" target="_blank" rel="noopener">Unlabeled Samples Generated by GAN Improve the Person Re-Identification Baseline in Vitro</a></p><p>Zhedong Zheng Liang Zheng Yi Yang</p><p>这篇论文主要是利用DCGAN生成新的数据集进行数据集扩充。</p><p>网络架构如图所示：</p><p><img src="./pic/DCGAN+CNN/DCGAN+CNN.png" alt="DCGAN+CNN的网络结构"><br><img src="/2018/11/29/person-reid-transfer-learning/DCGAN+CNN.png" title="DCGAN+CNN的网络结构"></p><p>生成效果图</p><p><img src="./pic/DCGAN+CNN/DCGAN+CNN2.png" alt="生成效果图"><br><img src="/2018/11/29/person-reid-transfer-learning/DCGAN+CNN2.png" title="生成效果图"></p><p>生成图片的标签LSRO</p><script type="math/tex; mode=display">q_{LSR}=\begin{cases} \frac{\epsilon}{K},&k\neq y\\\\                        1-\epsilon+\frac{\epsilon}{K},&k=y \end{cases}</script><script type="math/tex; mode=display">l_{LSR}=-(1-\epsilon)log(p(y))-\frac{\epsilon}{K}\sum_{k=1}^{K}log(p(k))</script><script type="math/tex; mode=display">q_{LSRO}=\frac{1}{K}</script><script type="math/tex; mode=display">l_{LSRO}=-(1-Z)log(p(y))-\frac{Z}{K}\sum_{k=1}^Klog(p(k))</script><p>其中，真实图片的Z=0，生成图片的Z=1.</p><h2 id="5-MMFA"><a href="#5-MMFA" class="headerlink" title="5. MMFA"></a>5. MMFA</h2><p><a href="https://arxiv.org/pdf/1807.01440.pdf" target="_blank" rel="noopener">Multi-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification</a></p><p>Shan Lin, Haoliang Li, Chang-Tsun Li, Alex Chichung Kot, BMVC 2018</p><h3 id="5-1-前言"><a href="#5-1-前言" class="headerlink" title="5.1 前言"></a>5.1 前言</h3><p>其想法也是将源域与目标域映射到同一特征空间。创新点是：</p><ul><li>利用MMD缩小源域与目标域的分布差异</li><li>考虑了属性</li></ul><p><a href="https://blog.csdn.net/a529975125/article/details/81176029" target="_blank" rel="noopener">MMD的参考代码</a></p><h3 id="5-2-网络架构"><a href="#5-2-网络架构" class="headerlink" title="5.2 网络架构"></a>5.2 网络架构</h3><p><img src="./pic/MMFA/MMFA.png" alt="MMFA的网络架构"><br><img src="/2018/11/29/person-reid-transfer-learning/MMFA.png" title="MMFA的网络架构"></p><p>网络架构的说明:</p><ul><li>每一个batch中包括$n_s$张源域图片，$n_t$张目标域图片。batch=32</li><li>backbone是resnet50，并且修改resnet50的avg_pool为max_pool</li><li>$H_S$是pool层的输出向量，$H_S^{id}$是ID-FC层的输出相邻，$H_S^{attr_m}$Attr-FC-m的输出向量。</li><li>input (256,128,3)</li><li>FC=fc+bn+dropout(0.5)+leaky ReLU+fc</li><li>SGD:momentum=0.9,weight decay=5x10e-4</li><li>lr=0.01,每20个epoch乘以0.1</li><li>测试使用max pool的2048维向量的欧式距离</li><li><a href="http://www.liangzheng.org/Project/project_reid.html" target="_blank" rel="noopener">Market</a>有27个<a href="https://github.com/vana77/Market-1501_Attribute" target="_blank" rel="noopener">属性</a>，<a href="http://vision.cs.duke.edu/DukeMTMC/" target="_blank" rel="noopener">Duke</a>有23个<a href="https://github.com/vana77/DukeMTMC-attribute" target="_blank" rel="noopener">属性</a></li></ul><h3 id="5-3-损失函数"><a href="#5-3-损失函数" class="headerlink" title="5.3 损失函数"></a>5.3 损失函数</h3><p>Identity Loss:</p><script type="math/tex; mode=display">L_{id}=-\frac{1}{n_s}\sum_{i=1}^{n_S}log(p_{id}(h_{S,i}^{id},y_{S,i}))</script><p>Attribute Loss:</p><script type="math/tex; mode=display">L_{attr}=-\frac{1}{M}\frac{1}{n_S}\sum_{m=1}^{M}\sum_{i=1}^{n_S}(a_{S,i}^{m}\cdot log(p_{attr}(h_{S,i}^{attr_m}, m)) - (1-a_{S,i}^{m})\cdot log(1-p_{attr}(h_{S,i}^{attr_m}, m)))</script><p>Attribute Feature Adaptation</p><script type="math/tex; mode=display">L_{AAL}=\frac{1}{M}\sum_{m=1}^{M}MMD(H_{S}^{attr_m}, H_{T}^{attr_m})^2\\         =\frac{1}{M}\sum_{m=1}^{M}\parallel \frac{1}{n_S}\sum_{i=1}^{n_S}\phi(h_{S,i}^{attr_m}) - \frac{1}{n_T}\sum_{i=1}^{n_T}\phi(h_{T,j}^{attr_m}) \parallel _{H}^2 \\         =\frac{1}{M}\sum_{m=1}^{M}[ \frac{1}{(n_S)^2}\sum_{i=1}^{n_S}\sum_{i'=1}^{n_S}k(h_{S,i}^{attr_m}, h_{S,i'}^{attr_m})+\frac{1}{(n_T)^2}\sum_{i=1}^{n_T}\sum_{i'=1}^{n_T}k(h_{T,i}^{attr_m}, h_{T,i'}^{attr_m})-\frac{2}{n_S\cdot n_T}\sum_{i=1}^{n_S}\sum_{j=1}^{n_T}k(h_{S,i}^{attr_m}, h_{T,j}^{attr_m})  ]</script><script type="math/tex; mode=display">k(h_{S,i}^{attr_m}, h_{T,j}^{attr_m})=exp(-\frac{1}{2\alpha}\parallel  h_{S,i}^{attr_m} - h_{T,j}^{attr_m}\parallel ^2)</script><script type="math/tex; mode=display">\alpha=1,5,10</script><p>Mid-level Deep Feature Adaptation</p><script type="math/tex; mode=display">L_{MDAL}=MMD(H_S,H_T)^2</script><p>Overall loss</p><script type="math/tex; mode=display">L_{all}=L_{id}+\lambda_1 L_{attr}+\lambda_2 L_{AAL}+\lambda_3 L_{MDAL}</script><script type="math/tex; mode=display">\lambda_1=0.1,\lambda_2=1,\lambda_3=1</script><h3 id="5-4-实验分析"><a href="#5-4-实验分析" class="headerlink" title="5.4 实验分析"></a>5.4 实验分析</h3><h4 id="5-4-1-实验结果"><a href="#5-4-1-实验结果" class="headerlink" title="5.4.1 实验结果"></a>5.4.1 实验结果</h4><p><img src="./pic/MMFA/MMFA2.png" alt="实验结果"><br><img src="/2018/11/29/person-reid-transfer-learning/MMFA2.png" title="实验结果"></p><h3 id="5-4-2-实验模块"><a href="#5-4-2-实验模块" class="headerlink" title="5.4.2 实验模块"></a>5.4.2 实验模块</h3><p>实验模块对比实验结果<br><img src="./pic/MMFA/MMFA3.png" alt="实验模块对比实验结果"><br><img src="/2018/11/29/person-reid-transfer-learning/MMFA3.png" title="实验模块对比实验结果"></p><h3 id="5-5-附录"><a href="#5-5-附录" class="headerlink" title="5.5 附录"></a>5.5 附录</h3><p>通过实验结果可以看出，在MMFA模型中，ID+Mid-level Deep Feature Adaptation的贡献最大。</p><p>下一步可以尝试考虑Mid-level Deep Feature Adaptation。</p><p>作者把avg pool 换成max pool。</p><h2 id="6-TJ-AIDL"><a href="#6-TJ-AIDL" class="headerlink" title="6. TJ-AIDL"></a>6. TJ-AIDL</h2><p><a href="http://www.eecs.qmul.ac.uk/~xiatian/papers/WangEtAl_CVPR2018.pdf" target="_blank" rel="noopener">Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification</a></p><p>Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li, ECCV 2018</p><h3 id="6-1-前言"><a href="#6-1-前言" class="headerlink" title="6.1 前言"></a>6.1 前言</h3><p>这篇论文的创新点在于：</p><ul><li>根据属性和id的关系，提出了Identity Inferred Attribute Space。</li></ul><h3 id="6-2-网络架构"><a href="#6-2-网络架构" class="headerlink" title="6.2 网络架构"></a>6.2 网络架构</h3><p>Attribute-Identity Transferable Joint Learning</p><p><img src="./pic/TJ-AIDL/TJ-AIDL.png" alt="TJ-AIDL的网络架构 "><br><img src="/2018/11/29/person-reid-transfer-learning/TJ-AIDL.png" title="TJ-AIDL的网络架构"></p><p>Unsupervised Target Domain Adaptation</p><p><img src="./pic/TJ-AIDL/TJ-AIDL2.png" alt="IJ-AIDL的部分网络架构详解"><br><img src="/2018/11/29/person-reid-transfer-learning/TJ-AIDL2.png" title="IJ-AIDL的部分网络架构详解"></p><p>网络架构的简要说明：</p><ul><li>(a) Identity Branch</li><li>(b) Attribute Branch</li><li>(c) Identity Inferred Attribute (IIA) space</li><li>训练过程分为两步:</li><li><ul><li>(I) 源域训练: Attribute-Identity Transferable Joint Learning</li></ul></li><li><ul><li>(II) 目标域微调: Unsupervised Target Domain Adaptation</li></ul></li><li>一般情况下Identity Branch和Attribute Branch是共享网络，但是本论文中特意分成两个非共享网络</li><li>重点在于对$e_{IIA}$的处理</li><li>IIA-encoder 是3个fc层，512/128/m，decoder是encoder的镜像。</li><li>基准网络是MobileNet</li><li>Adam优化器，lr=0.002，mementum$\beta_1=0.5, \beta_2=0.999$</li><li>batch size=8<blockquote><p>We started with training the identity branch by 100,000 iterations on the source identity labels and then the whole model by 20,000 iterations for both transferable joint learning on the labelled source data and unsupervised domain adaptation on the unlabelled target data</p></blockquote></li></ul><h3 id="6-3-损失函数"><a href="#6-3-损失函数" class="headerlink" title="6.3 损失函数"></a>6.3 损失函数</h3><p>Identity Branch (a) softmax</p><script type="math/tex; mode=display">L_{id}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}log(p_{id}(I_i^S,y_i^S)) \tag{1}</script><p>Attribute Branch(b) sigmoid</p><script type="math/tex; mode=display">L_{att}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}\sum_{j=1}^{m}(a_{i,j}log(p_{att}(I_i,j))+(1-a_{i,j})log(1-p_{att}(I_i,j))) \tag{2}</script><p>Identity Inferred Attribute (IIA) space (c)</p><script type="math/tex; mode=display">L_{rec}=\parallel x_{id}-f_{IIA}(x_{id}) \parallel ^2 \tag{3}</script><script type="math/tex; mode=display">L_{ID-transfer}=\parallel e_{IIA}-\tilde{p}_{att} \parallel ^2 \tag{4}</script><script type="math/tex; mode=display">L_{att,IIA}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}\sum_{j=1}^{m}(a_{i,j}log(p_{IIA}(I_i,j))+(1-a_{i,j})log(1-p_{IIA}(I_i,j))) \tag{5}</script><script type="math/tex; mode=display">L_{IIA}=L_{att,IIA}+\lambda_1 L_{rec}+\lambda_2 L_{ID-transfer} \tag{6}</script><script type="math/tex; mode=display">\lambda_1=10, \lambda_2=10</script><p>Impact of IIA on Identity and Attribute Branches</p><script type="math/tex; mode=display">L_{att-total}=L_{att}+\lambda_2 L_{ID-transfer} \tag{7}</script><h3 id="6-4-训练与部署流程"><a href="#6-4-训练与部署流程" class="headerlink" title="6.4 训练与部署流程"></a>6.4 训练与部署流程</h3><p><img src="./pic/TJ-AIDL/TJ-AIDL3.png" alt="IJ-AIDL的训练与部署流程"><br></p><h3 id="6-5-模块分析"><a href="#6-5-模块分析" class="headerlink" title="6.5 模块分析"></a>6.5 模块分析</h3><h4 id="6-5-1-ID和Attribute模块分析"><a href="#6-5-1-ID和Attribute模块分析" class="headerlink" title="6.5.1 ID和Attribute模块分析"></a>6.5.1 ID和Attribute模块分析</h4><p><img src="./pic/TJ-AIDL/TJ-AIDL4.png" alt="IJ-AIDL的ID和Attribute模块分析"><br></p><p>通过ID only的mAP和HHL的baseline，可以看出MobileNet和Resnet50对mAP的影响不受很大。</p><p>另外，可以看出，依然是ID占据了很大比重。</p><h4 id="6-5-2-Adapation的作用"><a href="#6-5-2-Adapation的作用" class="headerlink" title="6.5.2 Adapation的作用"></a>6.5.2 Adapation的作用</h4><p><img src="./pic/TJ-AIDL/TJ-AIDL5.png" alt="IJ-AIDL的Adapation的作用"><br><img src="/2018/11/29/person-reid-transfer-learning/TJ-AIDL5.png" title="IJ-AIDL的Adapation的作用"></p><p>从表格中可以看出，Adaptation的作用很小。说明，预训练的模型已经很好地能保持属性的一致性，即不同角度得到的属性是一样的。</p><h3 id="6-6-补充"><a href="#6-6-补充" class="headerlink" title="6.6 补充"></a>6.6 补充</h3><p>还是难以理解作者这么做的出发点，感觉有点凭空就设计出这么多损失函数，可能是哪里还缺点什么东西。</p><p>训练更新的时候方程(7)的出现原因是什么？更新(6)的时候应该就已经对attr进行了影响吧？</p><p>在step(II)中，是怎么更新方程(6)的。</p><p>Identity Inferred Attribute Space的合理性是怎么体现的？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;transfer-learning&quot;&gt;&lt;a href=&quot;#transfer-learning&quot; class=&quot;headerlink&quot; title=&quot;transfer learning&quot;&gt;&lt;/a&gt;transfer learning&lt;/h1&gt;
    
    </summary>
    
      <category term="ind1" scheme="http://yoursite.com/categories/ind1/"/>
    
    
      <category term="transfer learning" scheme="http://yoursite.com/tags/transfer-learning/"/>
    
      <category term="person-reid" scheme="http://yoursite.com/tags/person-reid/"/>
    
  </entry>
  
  <entry>
    <title>cycleGAN</title>
    <link href="http://yoursite.com/2018/11/19/cycleGAN/"/>
    <id>http://yoursite.com/2018/11/19/cycleGAN/</id>
    <published>2018-11-19T01:11:24.000Z</published>
    <updated>2018-11-19T05:44:34.982Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博客主要记录在跟随cycleGAN作者的代码复现学到的东西。<br>title: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(ICCV2017)</p><p>paper: <a href="https://arxiv.org/abs/1703.10593" target="_blank" rel="noopener">https://arxiv.org/abs/1703.10593</a><br>code: <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank" rel="noopener">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a><br>mycode: <a href="https://github.com/TJJTJJTJJ/pytorch_cycleGAN" target="_blank" rel="noopener">https://github.com/TJJTJJTJJ/pytorch_cycleGAN</a><br>cycle_gan的整体框架写得很漂亮，frame可以参考github的frame<br><a id="more"></a></p><h1 id="1-动态导入模块以及文件内的类"><a href="#1-动态导入模块以及文件内的类" class="headerlink" title="1.动态导入模块以及文件内的类"></a>1.动态导入模块以及文件内的类</h1><p>类似这种文件结构<br>.models<br>|– <strong>init</strong>.py<br>|– base_model.py<br>|– cycle_gan_model.py<br>|– networks.py<br>|– pix2pix_model.py<br>`– test_model.py</p><p>在<strong>init</strong>.py这样写两个函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_model_using_name</span><span class="params">(model_name)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    根据model_name导入具体模型'models/model_name_model.py'</span></span><br><span class="line"><span class="string">    :param model_name: eg. cycle_gan</span></span><br><span class="line"><span class="string">    :return: mdoel class eg.cycle_gan_model.CycleGANModle</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># step1 import 'models/model_name_model'</span></span><br><span class="line">    model_filename = <span class="string">'models.'</span>+model_name+<span class="string">'_model'</span></span><br><span class="line">    modellib = importlib.import_module(model_filename)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># step2 get model_name</span></span><br><span class="line">    model = <span class="keyword">None</span></span><br><span class="line">    target_model_name = model_name.replace(<span class="string">'_'</span>,<span class="string">''</span>)+<span class="string">'model'</span></span><br><span class="line">    <span class="keyword">for</span> name, cls <span class="keyword">in</span> modellib.__dict__.items():</span><br><span class="line">        <span class="keyword">if</span> name.lower() == target_model_name.lower() \</span><br><span class="line">                <span class="keyword">and</span> issubclass(cls, BaseModel):</span><br><span class="line">            model = cls</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> model <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        print_str = <span class="string">"In &#123;model_filename&#125;.py, there should be a subclass of BaseModel with class name "</span> \</span><br><span class="line">              <span class="string">"that matches &#123;target_model_name&#125; in lowercase."</span>.format(model_filename=model_filename, \</span><br><span class="line">                                                                      target_model_name=target_model_name)</span><br><span class="line">        print(print_str)</span><br><span class="line">        exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">modellib.__dict__ == vars(modellib)</span><br><span class="line">vars().keys() == dir()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line">modellib = importlib.import_module(model_filename)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> dir(modellib):</span><br><span class="line">    print(k)</span><br><span class="line">CycleGANModel</span><br><span class="line">__builtins__</span><br><span class="line">__cached__</span><br><span class="line">__doc__</span><br><span class="line">__file__</span><br><span class="line">__loader__</span><br><span class="line">__name__</span><br><span class="line">__package__</span><br><span class="line">__spec__</span><br><span class="line"></span><br><span class="line">print(modellib.__dict__)</span><br><span class="line">&#123;<span class="string">'__name__'</span>: <span class="string">'cycle_gan_model'</span>,</span><br><span class="line">...</span><br><span class="line"><span class="string">'CycleGANModel'</span>: cycle_gan_model.CycleGANModel&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">exit(<span class="number">0</span>)无错误退出</span><br><span class="line">exit(<span class="number">1</span>)有错误退出</span><br></pre></td></tr></table></figure><h1 id="2-学习率直线下降"><a href="#2-学习率直线下降" class="headerlink" title="2.学习率直线下降"></a>2.学习率直线下降</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.epoch_count, opt.niter + opt.niter_decay + <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lambda_rule</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    lr_l = <span class="number">1.0</span> - max(<span class="number">0</span>, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> lr_l</span><br><span class="line">scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)</span><br></pre></td></tr></table></figure><h1 id="3-NotImplemented-amp-amp-NotImplementedError"><a href="#3-NotImplemented-amp-amp-NotImplementedError" class="headerlink" title="3.NotImplemented &amp;&amp; NotImplementedError"></a>3.NotImplemented &amp;&amp; NotImplementedError</h1><p>参考:<br><a href="http://www.php.cn/python-tutorials-160083.html" target="_blank" rel="noopener">http://www.php.cn/python-tutorials-160083.html</a><br><a href="https://stackoverflow.com/questions/1062096/python-notimplemented-constant" target="_blank" rel="noopener">https://stackoverflow.com/questions/1062096/python-notimplemented-constant</a></p><p>return NotImplemented<br>raise NotImplementedError(‘initialization method [%s] is not implemented’ % init_type)</p><h1 id="4-parser的修改"><a href="#4-parser的修改" class="headerlink" title="4.parser的修改"></a>4.parser的修改</h1><p>这里既有外界传入的参数,也有自己的参数isTrain,在主函数里调用的时候调用方式是一致的,只是一个可以通过外界传参,一个不能通过外界传参</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainOptions</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(self)</span>:</span></span><br><span class="line">        parser = argparse.ArgumentParser(</span><br><span class="line">                formatter_class=argparse.ArgumentDefaultsHelpFormatter</span><br><span class="line">            )</span><br><span class="line">        parser.add_argument(<span class="string">'--batch_size'</span>, type=int, default=<span class="number">1</span>, help=<span class="string">'input batch size'</span>)</span><br><span class="line">        parser.set_defaults(dataset_mode=<span class="string">'single'</span>)</span><br><span class="line">        opt, _ = parser.parse_known_args()</span><br><span class="line"></span><br><span class="line">        self.isTrain = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">return</span> opt</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self)</span>:</span></span><br><span class="line">        opt = self.initialize()</span><br><span class="line">        opt.isTrain = self.isTrain</span><br><span class="line"></span><br><span class="line">opt = TrainOptions().parse()</span><br></pre></td></tr></table></figure><h1 id="5-eval-和test-函数的结合"><a href="#5-eval-和test-函数的结合" class="headerlink" title="5.eval()和test()函数的结合"></a>5.eval()和test()函数的结合</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    make models eval mode during test time</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> self.model_names:</span><br><span class="line">        <span class="keyword">if</span> isinstance(name, str):</span><br><span class="line">            net = getattr(self, <span class="string">'net'</span>+name)</span><br><span class="line">            net.eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    don't need backprop during test time</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        self.forward()</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">model.test()</span><br></pre></td></tr></table></figure><h1 id="6-多GPU"><a href="#6-多GPU" class="headerlink" title="6.多GPU"></a>6.多GPU</h1><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">modelb = torch.nn.DataParallel(modela, device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="comment"># save</span></span><br><span class="line">torch.save(modelb.module.cpu().state_dict(),path)</span><br><span class="line">modelb.cuda(gpu_ids[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># load</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_networks</span><span class="params">(self, epoch)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> self.model_names:</span><br><span class="line">        <span class="keyword">if</span> isinstance(name, str):</span><br><span class="line">            load_filename = <span class="string">'%s_net_%s.pth'</span> % (epoch, name)</span><br><span class="line">            load_path = os.path.join(self.save_dir, load_filename)</span><br><span class="line">            net = getattr(self, <span class="string">'net'</span> + name)</span><br><span class="line">            <span class="keyword">if</span> isinstance(net, torch.nn.DataParallel):</span><br><span class="line">                net = net.module</span><br><span class="line">            print(<span class="string">'loading the model from %s'</span> % load_path)</span><br><span class="line">            <span class="comment"># if you are using PyTorch newer than 0.4 (e.g., built from</span></span><br><span class="line">            <span class="comment"># GitHub source), you can remove str() on self.device</span></span><br><span class="line">            state_dict = torch.load(load_path, map_location=str(self.device))</span><br><span class="line">            <span class="keyword">if</span> hasattr(state_dict, <span class="string">'_metadata'</span>):</span><br><span class="line">                <span class="keyword">del</span> state_dict._metadata</span><br><span class="line"></span><br><span class="line">            <span class="comment"># patch InstanceNorm checkpoints prior to 0.4</span></span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> list(state_dict.keys()):  <span class="comment"># need to copy keys here because we mutate in loop</span></span><br><span class="line">                self.__patch_instance_norm_state_dict(state_dict, net, key.split(<span class="string">'.'</span>))</span><br><span class="line">            net.load_state_dict(state_dict)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">torch.nn.DataParallel加载预训练模型</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelA</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ModelA, self).__init__()</span><br><span class="line">        self.base1 = torch.nn.Conv2d(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">aa = ModelA()</span><br><span class="line">bb = torch.nn.DataParallel(aa, device_ids=[<span class="number">0</span>])</span><br><span class="line">bb.module.load_state_dict(torch.load(<span class="string">'aa.pth'</span>))</span><br></pre></td></tr></table></figure><p>对于单gpu和Module<br>对于普通的model.cuda,在保存模型会自动变成cpu,需要再次cuda一次<br>对于DataParallel,在保存模型会自动变成cpu,需要再次cuda一次<br>通过源码可以得知,DataParallel的device_ids初始化就已经确定,所以不用担心cuda到第一个GPU上而导致DataParallel忘记自己可以复制到哪些GPU上,会自动复制的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelA</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ModelA, self).__init__()</span><br><span class="line">        self.base = torch.nn.Conv2d(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">aa = ModelA()</span><br><span class="line">print(aa)</span><br><span class="line">ModelA(</span><br><span class="line">  (base): Conv2d(<span class="number">2</span>, <span class="number">2</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">)</span><br><span class="line">print(aa.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]])), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>]))])</span><br><span class="line">aa.cuda()</span><br><span class="line">print(aa.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]], device=<span class="string">'cuda:0'</span>)), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>], device=<span class="string">'cuda:0'</span>))]</span><br><span class="line"></span><br><span class="line">print(aa.cpu().state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.1570</span>, <span class="number">-0.2992</span>],</span><br><span class="line">                        [<span class="number">-0.2927</span>, <span class="number">-0.2748</span>]],</span><br><span class="line">                       [[<span class="number">-0.0097</span>,  <span class="number">0.0346</span>],</span><br><span class="line">                        [<span class="number">-0.3125</span>,  <span class="number">0.2615</span>]]],</span><br><span class="line">                      [[[<span class="number">-0.2506</span>, <span class="number">-0.2632</span>],</span><br><span class="line">                        [ <span class="number">0.1302</span>, <span class="number">-0.2223</span>]],</span><br><span class="line">                       [[ <span class="number">0.1422</span>,  <span class="number">0.0427</span>],</span><br><span class="line">                        [ <span class="number">0.3453</span>,  <span class="number">0.0219</span>]]]])),</span><br><span class="line">             (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1974</span>, <span class="number">-0.1549</span>]))])</span><br><span class="line"></span><br><span class="line">print(aa.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.1570</span>, <span class="number">-0.2992</span>],</span><br><span class="line">          [<span class="number">-0.2927</span>, <span class="number">-0.2748</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">-0.0097</span>,  <span class="number">0.0346</span>],</span><br><span class="line">          [<span class="number">-0.3125</span>,  <span class="number">0.2615</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2506</span>, <span class="number">-0.2632</span>],</span><br><span class="line">          [ <span class="number">0.1302</span>, <span class="number">-0.2223</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.1422</span>,  <span class="number">0.0427</span>],</span><br><span class="line">          [ <span class="number">0.3453</span>,  <span class="number">0.0219</span>]]]])), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1974</span>, <span class="number">-0.1549</span>]))])</span><br><span class="line"></span><br><span class="line">bb = torch.nn.DataParallel(aa, device_ids=[<span class="number">0</span>])</span><br><span class="line">print(bb)</span><br><span class="line">DataParallel(</span><br><span class="line">  (module): ModelA(</span><br><span class="line">    (base): Conv2d(<span class="number">2</span>, <span class="number">2</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">print(bb.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'module.base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]], device=<span class="string">'cuda:0'</span>)), (<span class="string">'module.base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>], device=<span class="string">'cuda:0'</span>))])</span><br><span class="line">print(bb.module.cpu().state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.1570</span>, <span class="number">-0.2992</span>],</span><br><span class="line">                        [<span class="number">-0.2927</span>, <span class="number">-0.2748</span>]],</span><br><span class="line">                       [[<span class="number">-0.0097</span>,  <span class="number">0.0346</span>],</span><br><span class="line">                        [<span class="number">-0.3125</span>,  <span class="number">0.2615</span>]]],</span><br><span class="line"></span><br><span class="line">                      [[[<span class="number">-0.2506</span>, <span class="number">-0.2632</span>],</span><br><span class="line">                        [ <span class="number">0.1302</span>, <span class="number">-0.2223</span>]],</span><br><span class="line">                       [[ <span class="number">0.1422</span>,  <span class="number">0.0427</span>],</span><br><span class="line">                        [ <span class="number">0.3453</span>,  <span class="number">0.0219</span>]]]])),</span><br><span class="line">             (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1974</span>, <span class="number">-0.1549</span>]))])</span><br><span class="line">print(bb)</span><br><span class="line">DataParallel(</span><br><span class="line">  (module): ModelA(</span><br><span class="line">    (base): Conv2d(<span class="number">2</span>, <span class="number">2</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">print(bb.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]])), (<span class="string">'base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>]))])</span><br><span class="line">bb.cuda(gpu_ids[<span class="number">0</span>])</span><br><span class="line">print(bb.state_dict())</span><br><span class="line">OrderedDict([(<span class="string">'module.base.weight'</span>, tensor([[[[ <span class="number">0.0119</span>,  <span class="number">0.2522</span>],</span><br><span class="line">          [<span class="number">-0.0682</span>,  <span class="number">0.2366</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2013</span>,  <span class="number">0.2106</span>],</span><br><span class="line">          [ <span class="number">0.2242</span>,  <span class="number">0.1711</span>]]],</span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2777</span>,  <span class="number">0.2446</span>],</span><br><span class="line">          [ <span class="number">0.3494</span>,  <span class="number">0.1552</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.0270</span>,  <span class="number">0.1272</span>],</span><br><span class="line">          [<span class="number">-0.1878</span>, <span class="number">-0.3501</span>]]]], device=<span class="string">'cuda:0'</span>)), (<span class="string">'module.base.bias'</span>, tensor([ <span class="number">0.1433</span>,  <span class="number">0.1061</span>], device=<span class="string">'cuda:0'</span>))])</span><br></pre></td></tr></table></figure><h1 id="7-Norm"><a href="#7-Norm" class="headerlink" title="7.Norm"></a>7.Norm</h1><p>参考：<br><a href="https://blog.csdn.net/liuxiao214/article/details/81037416" target="_blank" rel="noopener">https://blog.csdn.net/liuxiao214/article/details/81037416</a></p><p>输入图像：[N,C,H,W]<br>BatchNorm: [1,C,1,1]<br>InstanceNorm: [N,C,1,1]</p><p>经过实验,instanceNorm层的weight, bias, running_mean, running_var总是None<br>代码中加载模型的时候对instanceNorm层进行了删除操作,为什么<br>对于pytorch之前的版本instanceNorm层是有running_mean和running_var的,之后的版本修正了之后,就不再需要了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__patch_instance_norm_state_dict</span><span class="params">(self, state_dict, module, keys, i=<span class="number">0</span>)</span>:</span></span><br><span class="line">    key = keys[i]</span><br><span class="line">    <span class="keyword">if</span> i + <span class="number">1</span> == len(keys):  <span class="comment"># at the end, pointing to a parameter/buffer</span></span><br><span class="line">        <span class="keyword">if</span> module.__class__.__name__.startswith(<span class="string">'InstanceNorm'</span>) <span class="keyword">and</span> \</span><br><span class="line">                (key == <span class="string">'running_mean'</span> <span class="keyword">or</span> key == <span class="string">'running_var'</span>):</span><br><span class="line">            <span class="keyword">if</span> getattr(module, key) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                state_dict.pop(<span class="string">'.'</span>.join(keys))</span><br><span class="line">        <span class="keyword">if</span> module.__class__.__name__.startswith(<span class="string">'InstanceNorm'</span>) <span class="keyword">and</span> \</span><br><span class="line">           (key == <span class="string">'num_batches_tracked'</span>):</span><br><span class="line">            state_dict.pop(<span class="string">'.'</span>.join(keys))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="8-functools"><a href="#8-functools" class="headerlink" title="8.functools"></a>8.functools</h1><p>偏函数：适合为多个调用函数提供一致的函数接口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(m,n,p)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> m*n*p</span><br><span class="line">re=partial(f,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">print(re(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 60</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> type(norm_layer) == functools.partial:</span><br><span class="line">    use_bias = norm_layer.func == nn.InstanceNorm2d</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    use_bias = norm_layer == nn.InstanceNorm2d</span><br></pre></td></tr></table></figure><h1 id="9-论文与代码"><a href="#9-论文与代码" class="headerlink" title="9.论文与代码"></a>9.论文与代码</h1><h2 id="ndf"><a href="#ndf" class="headerlink" title="ndf"></a>ndf</h2><p>模型的定义与论文有一个地方不一致,论文写的第一个conv之后通道数是32,但实现是64.<br>与作者沟通得知,第一层不是32,而是64,剩下的也依次递增.</p><p>下采样的时候没有使用reflect进行补充,而是使用了0填充.<br>与作者沟通后，提出的是都可以尝试一下</p><h2 id="unet-model"><a href="#unet-model" class="headerlink" title="unet model"></a>unet model</h2><p>Unet model<br>与网上的不是很一致<br>3-&gt;<em>1-&gt;</em>2-&gt;<em>4-&gt;</em>8-&gt;<em>8-&gt;</em>8<br>3&lt;-<em>2&lt;-</em>4&lt;-<em>8&lt;-</em>16&lt;-<em>16&lt;-</em>16</p><h2 id="参数-no-lsgan"><a href="#参数-no-lsgan" class="headerlink" title="参数 no_lsgan"></a>参数 no_lsgan</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">self.criterionGAN = networks.GANLoss(use_lsgan=<span class="keyword">not</span> opt.no_lsgan).to(self.device)</span><br><span class="line"><span class="comment"># GAN loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GANLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_lsgan=True, target_real_label=<span class="number">1.0</span>, target_fake_label=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(GANLoss, self).__init__()</span><br><span class="line">        self.register_buffer(<span class="string">'real_label'</span>, torch.tensor(target_real_label))</span><br><span class="line">        self.register_buffer(<span class="string">'fake_label'</span>, torch.tensor(target_fake_label))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_lsgan:</span><br><span class="line">            self.loss = nn.MSELoss()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">use_sigmoid = opt.no_lsgan</span><br><span class="line">self.netD_A = define_D(opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm,</span><br><span class="line">                    use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)</span><br></pre></td></tr></table></figure><p>也就是<br>opt.no_lsgan为True时, netD使用sigmoid, GANloss使用BCELoss()<br>opt.no_lsgan为False时, netD不使用sigmoid, GANloss使用MSELoss()</p><p>MSELoss:均方误差 (x-y)*<em>2<br>BCELoss:二分类的交叉熵:使用前需要使用sigmoid函数,input和target的输入维度是一样的.(N,</em>)</p><p>根据作者提供的运行代码,猜测作者使用的是opt.no_lsgan为False,均方误差</p><p>L1loss: |x-y|</p><h2 id="G-and-D-的反向传播过程"><a href="#G-and-D-的反向传播过程" class="headerlink" title="G and D 的反向传播过程"></a>G and D 的反向传播过程</h2><p>回顾一下G和D的反向传播<br><strong>train G</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set_requires_grad(D, <span class="keyword">False</span>)</span><br><span class="line">fake_A = G(real_A)</span><br><span class="line">loss = criterion(D(fake_A), <span class="keyword">True</span>)</span><br><span class="line">optimizers_G.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizers_G.step()</span><br></pre></td></tr></table></figure><p><strong>train D</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set_requires_grad(D, <span class="keyword">False</span>)</span><br><span class="line"><span class="comment">#fake_A = fake_A.detach() # 取消G的grad</span></span><br><span class="line">loss1 = criterion(fake_A.detach(), <span class="keyword">False</span>)</span><br><span class="line">loss2 = criterion(realA, <span class="keyword">True</span>)</span><br><span class="line">loss = loss1 + loss2</span><br><span class="line">optimizers_D.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizers_D.step()</span><br></pre></td></tr></table></figure><h1 id="10-ConTransposed的计算方法"><a href="#10-ConTransposed的计算方法" class="headerlink" title="10.ConTransposed的计算方法"></a>10.ConTransposed的计算方法</h1><p>逆卷积后的图像大小和之前的能对应上，需要output_padding</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nn.ConvTranspose2d(ngf*mult, int(ngf*mult/<span class="number">2</span>), kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>, bias=use_bias)]</span><br><span class="line">-k+<span class="number">2</span>p+s-out_padding是s的整数</span><br><span class="line">k=<span class="number">3</span>,s=<span class="number">2</span>,p=<span class="number">1</span>,则out_padding=<span class="number">1</span></span><br><span class="line">k=<span class="number">3</span>,s=<span class="number">4</span>,p=<span class="number">1</span>,则out_padding=<span class="number">3</span></span><br></pre></td></tr></table></figure><h1 id="11-初始化参数"><a href="#11-初始化参数" class="headerlink" title="11.初始化参数"></a>11.初始化参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(net, init_type=<span class="string">'normal'</span>, gain=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_func</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="comment"># conv, contranspose ,linear, bn</span></span><br><span class="line">        <span class="comment"># type(m) == nn.Conv2d</span></span><br><span class="line">        classname = m.__class__.__name__</span><br><span class="line">        <span class="keyword">if</span> hasattr(m, <span class="string">'weight'</span>) <span class="keyword">and</span> (classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span> <span class="keyword">or</span> classname.find(<span class="string">'Linear'</span>) != <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> init_type == <span class="string">'normal'</span>:</span><br><span class="line">                init.normal_(m.weight.data, <span class="number">0.0</span>, gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'xavier'</span>:</span><br><span class="line">                init.xavier_normal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'kaiming'</span>:</span><br><span class="line">                init.kaiming_normal_(m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_in'</span>)</span><br><span class="line">            <span class="keyword">elif</span> init_type == <span class="string">'orthogonal'</span>:</span><br><span class="line">                init.orthogonal_(m.weight.data, gain=gain)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> NotImplementedError(<span class="string">'initialization method [%s] is not implemented'</span> % init_type)</span><br><span class="line">            <span class="keyword">if</span> hasattr(m, <span class="string">'bias'</span>) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm2d'</span>) != <span class="number">-1</span>:</span><br><span class="line">            init.normal_(m.weight.data, <span class="number">1.0</span>, gain)</span><br><span class="line">            init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'initialize network with %s'</span> % init_type)</span><br><span class="line">    net.apply(init_func)</span><br></pre></td></tr></table></figure><h1 id="12-disriminator-PatchGAN-and-GANLoss"><a href="#12-disriminator-PatchGAN-and-GANLoss" class="headerlink" title="12.disriminator PatchGAN and GANLoss"></a>12.disriminator PatchGAN and GANLoss</h1><p>PatchGAN的kernel是4.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GANLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_lsgan=True, target_real_label=<span class="number">1.0</span>, target_fake_label=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(GANLoss, self).__init__()</span><br><span class="line">        self.register_buffer(<span class="string">'real_label'</span>, torch.tensor(target_real_label))</span><br><span class="line">        self.register_buffer(<span class="string">'fake_label'</span>, torch.tensor(target_fake_label))</span><br><span class="line">        <span class="keyword">if</span> use_lsgan:</span><br><span class="line">            self.loss = nn.MSELoss()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_target_tensor</span><span class="params">(self, input, target_is_real)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> target_is_real:</span><br><span class="line">            target_tensor = self.real_label</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target_tensor = self.fake_label</span><br><span class="line">        <span class="keyword">return</span> target_tensor.expand_as(input)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, input, target_is_real)</span>:</span></span><br><span class="line">        target_tensor = self.get_target_tensor(input, target_is_real)</span><br><span class="line">        <span class="keyword">return</span> self.loss(input, target_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defines the PatchGAN discriminator with the specified arguments.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NLayerDiscriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_nc, ndf=<span class="number">64</span>, n_layers=<span class="number">3</span>, norm_layer=nn.BatchNorm2d, use_sigmoid=False)</span>:</span></span><br><span class="line">        super(NLayerDiscriminator, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> type(norm_layer) == functools.partial:</span><br><span class="line">            use_bias = norm_layer.func == nn.InstanceNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_bias = norm_layer == nn.InstanceNorm2d</span><br><span class="line"></span><br><span class="line">        kw = <span class="number">4</span></span><br><span class="line">        padw = <span class="number">1</span></span><br><span class="line">        sequence = [</span><br><span class="line">            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=<span class="number">2</span>, padding=padw),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        nf_mult = <span class="number">1</span></span><br><span class="line">        nf_mult_prev = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">            nf_mult_prev = nf_mult</span><br><span class="line">            nf_mult = min(<span class="number">2</span>**n, <span class="number">8</span>)</span><br><span class="line">            sequence += [</span><br><span class="line">                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,</span><br><span class="line">                          kernel_size=kw, stride=<span class="number">2</span>, padding=padw, bias=use_bias),</span><br><span class="line">                norm_layer(ndf * nf_mult),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">            ]</span><br><span class="line"></span><br><span class="line">        nf_mult_prev = nf_mult</span><br><span class="line">        nf_mult = min(<span class="number">2</span>**n_layers, <span class="number">8</span>)</span><br><span class="line">        sequence += [</span><br><span class="line">            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,</span><br><span class="line">                      kernel_size=kw, stride=<span class="number">1</span>, padding=padw, bias=use_bias),</span><br><span class="line">            norm_layer(ndf * nf_mult),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        sequence += [nn.Conv2d(ndf * nf_mult, <span class="number">1</span>, kernel_size=kw, stride=<span class="number">1</span>, padding=padw)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_sigmoid:</span><br><span class="line">            sequence += [nn.Sigmoid()]</span><br><span class="line"></span><br><span class="line">        self.model = nn.Sequential(*sequence)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.model(input)</span><br><span class="line"></span><br><span class="line">pred_real = netD(real)</span><br><span class="line">loss_D_real = self.criterionGAN(pred_real, <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="GANLoss的备注"><a href="#GANLoss的备注" class="headerlink" title="GANLoss的备注"></a>GANLoss的备注</h2><p>使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.<strong>call</strong>(input)，在<strong>call</strong>函数中，主要调用的是 layer.forward(x)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。</p><h1 id="13-PatchGAN的感受野"><a href="#13-PatchGAN的感受野" class="headerlink" title="13.PatchGAN的感受野"></a>13.PatchGAN的感受野</h1><p>论文使用的是70X70 PatchGAN<br>PatchGAN:<br>paper:<br>Image-to-Image Translation with Conditional Adversarial Networks<br><a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="noopener">https://arxiv.org/abs/1611.07004</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">感受野的计算规则</span><br><span class="line">对于第m层,m=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,...,N. hm表示第m层应该有的视野,假设mN=<span class="number">1</span></span><br><span class="line">km, sm, pm,表示第m<span class="number">-1</span>层到第m层的conv的kernel</span><br><span class="line">第一层对于第<span class="number">0</span>层的感受野</span><br><span class="line">h1 = <span class="number">1</span>, </span><br><span class="line">(h0-k1)/s1+<span class="number">1</span>=h1</span><br><span class="line">第二层对于第<span class="number">0</span>层的感受野</span><br><span class="line">h2 = <span class="number">1</span></span><br><span class="line">(h1-k2)/s2+<span class="number">1</span>=h2</span><br><span class="line">(h0-k1)/s1+<span class="number">1</span>=h1</span><br><span class="line">依次类推</span><br><span class="line">反之 https://fomoro.com/tools/receptive-fields/<span class="comment">#</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(output_size, ksize, stride)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (output_size - <span class="number">1</span>) * stride + ksize</span><br><span class="line"></span><br><span class="line">last_layer = f(output_size=<span class="number">1</span>, ksize=<span class="number">4</span>, stride=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Receptive field: 4</span></span><br><span class="line">fourth_layer = f(output_size=last_layer, ksize=<span class="number">4</span>, stride=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Receptive field: 7</span></span><br><span class="line">third_layer = f(output_size=fourth_layer, ksize=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Receptive field: 16</span></span><br><span class="line">second_layer = f(output_size=third_layer, ksize=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Receptive field: 34</span></span><br><span class="line">first_layer = f(output_size=second_layer, ksize=<span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Receptive field: 70</span></span><br><span class="line"></span><br><span class="line">print(first_layer)</span><br></pre></td></tr></table></figure><h1 id="14-torch-tensor-clone"><a href="#14-torch-tensor-clone" class="headerlink" title="14.torch.tensor.clone()"></a>14.torch.tensor.clone()</h1><p>clone()<br>梯度受影响,clone之后的新的tensor的梯度也会影响到原tensor,但是新tensor本身是没有梯度的.<br>clone之后的新tensor的改变不会影响原有的tensor<br>应该这么理解,clone也是计算图中的一个操作,这样的话就可以解释通了.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input = torch.ones(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">input.requires_grad = <span class="keyword">True</span></span><br><span class="line">input2 = input.clone()</span><br><span class="line">print(input2.requires_grad)</span><br><span class="line">y = input.sum()</span><br><span class="line">y.backward()</span><br><span class="line">print(input.grad)</span><br><span class="line"><span class="comment"># 1,1,1...</span></span><br><span class="line">print(input2.grad)</span><br><span class="line"><span class="comment"># None</span></span><br><span class="line">y = input2.sum()</span><br><span class="line">y.backward()</span><br><span class="line">print(input.grad)</span><br><span class="line"><span class="comment"># 2,2,2...</span></span><br><span class="line">print(input2.grad)</span><br><span class="line"><span class="comment"># None</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input = torch.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">input.requires_grad = <span class="keyword">True</span></span><br><span class="line">input2 = input.clone()</span><br><span class="line">input2[<span class="number">1</span>,<span class="number">1</span>] = <span class="number">6</span></span><br><span class="line">print(input2)</span><br><span class="line">print(input)</span><br><span class="line">tensor([[ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">6.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line">tensor([[ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line">input2[<span class="number">1</span>,<span class="number">1</span>] = <span class="number">6</span></span><br><span class="line">y = (input2*input2).sum()</span><br><span class="line">y.backward()</span><br><span class="line">print(input.grad)</span><br><span class="line">print(input2.grad)</span><br><span class="line">tensor([[ <span class="number">8.</span>,  <span class="number">8.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">8.</span>,  <span class="number">8.</span>]])</span><br><span class="line"><span class="keyword">None</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(input2.grad_fn)</span><br><span class="line">&lt;CopySlices object at <span class="number">0x7fe89dc841d0</span>&gt;</span><br></pre></td></tr></table></figure><p>clone的用法<br>tensor保留梯度的交换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tmp = tensor1.clone()</span><br><span class="line">tensor2 = tmp</span><br><span class="line">tensor1 = tensor3</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tensor1, tensor2 = tensor3, tensor1.clone()</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tmp = self.images[random_id].clone()</span><br><span class="line">self.images[random_id] = image</span><br><span class="line">return_images.append(tmp)</span><br></pre></td></tr></table></figure><h1 id="15-from-XX-import"><a href="#15-from-XX-import" class="headerlink" title="15.from XX import"></a>15.from XX import</h1><p>这里还有一些不太对的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .base_model <span class="keyword">import</span> BaseModel <span class="comment"># 同一个文件夹</span></span><br><span class="line"><span class="keyword">from</span> util.image_pool <span class="keyword">import</span> <span class="comment"># 父级文件夹</span></span><br><span class="line"><span class="comment"># 建议</span></span><br><span class="line"><span class="keyword">from</span> .base_model <span class="keyword">import</span> BaseModel <span class="comment"># 同一个文件夹</span></span><br><span class="line"><span class="keyword">from</span> ..util.image_pool <span class="keyword">import</span> <span class="comment"># 父级文件夹</span></span><br></pre></td></tr></table></figure><h1 id="16-register-buffer"><a href="#16-register-buffer" class="headerlink" title="16.register_buffer"></a>16.register_buffer</h1><p>register_buffer<br>self.register_buffer可以将tensor注册成buffer，在forward中使用self.mybuffer, 而不是self.mybuffer_tmp.<br>定义Parameter和buffer都只需要传入Tensor即可。也不需要将其转成gpu。这是因为，当网络进行.cuda()时候，会自动将里面的层的参数,buffer等转换成相应的GPU上。<br>网络存储时也会将buffer存下，当网络load模型时，会将存储的模型的buffer也进行赋值。<br>buffer的更新在forward中，optim.step只能更新nn.Parameter类型的参数。<br>用法<br>self.register_buffer(‘running_mean’, torch.zeros(num_features))</p><h1 id="17-itertools"><a href="#17-itertools" class="headerlink" title="17. itertools"></a>17. itertools</h1><p>无限迭代器<br>itertools，用于创建高效迭代器的函数,<br>itertools.chain 连接多个列表或者迭代器。<br>将多个网络写在一起,使用一个优化器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),</span><br><span class="line">                                    lr=opt.lr, betas=(opt.beta1, <span class="number">0.999</span>))</span><br><span class="line">self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()),</span><br><span class="line">                                    lr=opt.lr, betas=(opt.beta1, <span class="number">0.999</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自然数无限迭代器</span></span><br><span class="line"><span class="comment"># itertools.count</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> itertools</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>natuals = itertools.count(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> natuals:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 序列无限重复</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> itertools</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cs = itertools.cycle(<span class="string">'ABC'</span>) <span class="comment"># 注意字符串也是序列的一种</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> c <span class="keyword">in</span> cs:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> c</span><br><span class="line">...</span><br><span class="line"><span class="string">'A'</span></span><br><span class="line"><span class="string">'B'</span></span><br><span class="line"><span class="string">'C'</span></span><br><span class="line"><span class="string">'A'</span></span><br><span class="line"><span class="string">'B'</span></span><br><span class="line"><span class="string">'C'</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单元素无限重复</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ns = itertools.repeat(<span class="string">'A'</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> ns:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line">打印<span class="number">10</span>次<span class="string">'A'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 无限迭代器中截取有限序列</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>natuals = itertools.count(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ns = itertools.takewhile(<span class="keyword">lambda</span> x: x &lt;= <span class="number">10</span>, natuals)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> ns:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line">打印出<span class="number">1</span>到<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代对象的串联</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> itertools.chain(<span class="string">'ABC'</span>, <span class="string">'XYZ'</span>):</span><br><span class="line">    <span class="keyword">print</span> c</span><br><span class="line"><span class="comment"># 迭代效果：'A' 'B' 'C' 'X' 'Y' 'Z'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代器中相邻的重复元素挑出来放在一起</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> key, group <span class="keyword">in</span> itertools.groupby(<span class="string">'AAABBBCCAAA'</span>):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> key, list(group) <span class="comment"># 为什么这里要用list()函数呢？</span></span><br><span class="line">...</span><br><span class="line">A [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'A'</span>]</span><br><span class="line">B [<span class="string">'B'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">C [<span class="string">'C'</span>, <span class="string">'C'</span>]</span><br><span class="line">A [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'A'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># imap, ifilter</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> x <span class="keyword">in</span> itertools.imap(<span class="keyword">lambda</span> x, y: x * y, [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>], itertools.count(<span class="number">1</span>)):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> x</span><br><span class="line">...</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">40</span></span><br><span class="line"><span class="number">90</span></span><br></pre></td></tr></table></figure><h1 id="18-visdom"><a href="#18-visdom" class="headerlink" title="18.visdom"></a>18.visdom</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.vis = visdom.Visdom(server=opt.display_server, port=opt.display_port, </span><br><span class="line">                         env=opt.display_env, raise_exceptions=<span class="keyword">True</span>, use_incoming_socket=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># env根据_自动分层。e.g. cycle_gan--&gt;cycle,cycle_gan</span></span><br></pre></td></tr></table></figure><h1 id="19-三引号"><a href="#19-三引号" class="headerlink" title="19.三引号"></a>19.三引号</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">三引号的作用</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>str1 = <span class="string">"""List of name:</span></span><br><span class="line"><span class="string"><span class="meta">... </span>Hua Li</span></span><br><span class="line"><span class="string"><span class="meta">... </span>Chao Deng</span></span><br><span class="line"><span class="string"><span class="meta">... </span>&#123;&#125;</span></span><br><span class="line"><span class="string"><span class="meta">... </span>"""</span>.format(<span class="string">'hhh'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(str1)</span><br><span class="line">List of name:</span><br><span class="line">Hua Li</span><br><span class="line">Chao Deng</span><br></pre></td></tr></table></figure><h1 id="20-异常"><a href="#20-异常" class="headerlink" title="20.异常"></a>20.异常</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.触发异常</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mye</span><span class="params">( level )</span>:</span></span><br><span class="line">    <span class="keyword">if</span> level &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"Invalid level!"</span>)</span><br><span class="line">        <span class="comment"># 触发异常后，后面的代码就不会再执行</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    mye(<span class="number">0</span>)            <span class="comment"># 触发异常</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> err:</span><br><span class="line">    print(<span class="number">1</span>,err)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">1</span> Invalid level!</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.自定义异常</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyException</span><span class="params">(Exception)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,message)</span>:</span></span><br><span class="line">        Exception.__init__(self)</span><br><span class="line">        self.message=message </span><br><span class="line">        print(<span class="string">'This is MyException'</span>)</span><br><span class="line">a=<span class="number">7</span></span><br><span class="line"><span class="keyword">if</span> a&lt;<span class="number">10</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">raise</span> MyException(<span class="string">"my excepition is raised "</span>)</span><br><span class="line">    <span class="keyword">except</span> MyException <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">'*****************'</span>)</span><br><span class="line">        print(e.message)    </span><br><span class="line">This <span class="keyword">is</span> MyException</span><br><span class="line">my excepition <span class="keyword">is</span> raised</span><br></pre></td></tr></table></figure><h1 id="21-自定义类的iter"><a href="#21-自定义类的iter" class="headerlink" title="21.自定义类的iter"></a>21.自定义类的iter</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义类的iter</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cl1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.N = <span class="number">10</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            print(i)</span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="number">5</span>:</span><br><span class="line">                <span class="keyword">yield</span> i</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">cc = cl1()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> cc:</span><br><span class="line">    print(<span class="string">'hhh'</span>,i)</span><br><span class="line"><span class="number">0</span></span><br><span class="line">hhh <span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line">hhh <span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">hhh <span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">hhh <span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line">hhh <span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇博客主要记录在跟随cycleGAN作者的代码复现学到的东西。&lt;br&gt;title: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(ICCV2017)&lt;/p&gt;
&lt;p&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1703.10593&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1703.10593&lt;/a&gt;&lt;br&gt;code: &lt;a href=&quot;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&lt;/a&gt;&lt;br&gt;mycode: &lt;a href=&quot;https://github.com/TJJTJJTJJ/pytorch_cycleGAN&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/TJJTJJTJJ/pytorch_cycleGAN&lt;/a&gt;&lt;br&gt;cycle_gan的整体框架写得很漂亮，frame可以参考github的frame&lt;br&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://yoursite.com/categories/DeepLearning/"/>
    
    
      <category term="cycleGAN" scheme="http://yoursite.com/tags/cycleGAN/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow</title>
    <link href="http://yoursite.com/2018/11/05/tensorflow/"/>
    <id>http://yoursite.com/2018/11/05/tensorflow/</id>
    <published>2018-11-05T14:41:03.000Z</published>
    <updated>2018-11-05T15:10:47.797Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇关于tensorflow的博客，这里面很多东西都是很杂碎的，不在此做处理，等积累的多了，理解才能正确。<br><a id="more"></a></p><p>TensorFlow入门教程</p><p>1.TensorFlow深度学习应用实践<br>评价不好</p><ol start="2"><li><p>TensorFlow：实战Google深度学习框架（第2版）<br>8.6分，可以用来实践</p></li><li><p>Tensorflow：实战Google深度学习框架<br>8.4分</p></li></ol><p>4.莫烦的tensorlfow教程<br><a href="https://github.com/MorvanZhou" target="_blank" rel="noopener">https://github.com/MorvanZhou</a><br>适合实践</p><p>5.某个网友的自己实现的教程<br><a href="https://www.jianshu.com/p/27a2fb320934" target="_blank" rel="noopener">https://www.jianshu.com/p/27a2fb320934</a><br><a href="https://github.com/zhaozhengcoder/Machine-Learning/tree/master/tensorflow_tutorials" target="_blank" rel="noopener">https://github.com/zhaozhengcoder/Machine-Learning/tree/master/tensorflow_tutorials</a></p><p>6.官网API<br><a href="https://tensorflow.google.cn/api_docs/python/tf" target="_blank" rel="noopener">https://tensorflow.google.cn/api_docs/python/tf</a></p><p>7.深度学习之TensorFlow入门、原理与进阶实战<br>7.6分<br>22章，内容更加详实，偏向理论，可以用来只看不实践</p><p>8.TensorFlow实战<br>7.3分<br>适合看看，内容不深，实践性不强，理论也很浅<br>在github上也没有代码</p><p>不应该总是要求全部，所以应该这样的顺序来学习<br>先学：TensorFlow：实战Google深度学习框架（第2版）<br>再学：莫烦：<a href="https://github.com/MorvanZhou+网页的教程" target="_blank" rel="noopener">https://github.com/MorvanZhou+网页的教程</a><br>基本就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">print(a.graph == tf.get_default_graph())</span><br><span class="line">tf.get_variable() name 必须双引号</span><br><span class="line"></span><br><span class="line">name的作用  https://blog.csdn.net/xiaohuihui1994/article/details/<span class="number">81022043</span></span><br><span class="line"></span><br><span class="line">可以理解成sess需要指定，不能自动加入</span><br><span class="line">.run,.eval能执行的两种方式</span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g1) <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">或者</span><br><span class="line">sess = tf.Session()</span><br><span class="line">tf.global_variables_initializer().run(session=sess)</span><br><span class="line">或者</span><br><span class="line">sess = tf.InteractiveSession() <span class="comment"># 会自动注册为默认会话</span></span><br><span class="line">result.eval()</span><br><span class="line">或者</span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    result.eval()</span><br><span class="line"></span><br><span class="line"><span class="comment">####</span></span><br><span class="line">初始化</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">w1.initializer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">『TensorFlow』使用集合collection控制variables</span><br><span class="line">https://www.cnblogs.com/hellcat/p/<span class="number">9006904.</span>html</span><br><span class="line"></span><br><span class="line">collection</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>], initializer = tf.zeros_initializer()) <span class="comment"># 设置初始值为0</span></span><br><span class="line">    gv= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES <span class="comment"># gv = tf.global_variables()</span></span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> gv:</span><br><span class="line">        print(var)</span><br><span class="line"></span><br><span class="line">现在有了几个概念需要理清楚：</span><br><span class="line">计算图： 不同计算图中的变量是独立的</span><br><span class="line">collection： 不同类型的variable放在不同的collection中，主要是tf.GraphKeys.GLOBAL_VARIABLES和tf.GraphKeys.TRAINABLE_VARIABLES</span><br><span class="line">会话： 会话需要与计算图相连接，完成相应计算图的执行，一个会话对应一个计算图及其执行结果</span><br><span class="line"></span><br><span class="line">tf.add_to_collection</span><br><span class="line">https://www.jianshu.com/p/<span class="number">6612</span>f368e8f4</span><br><span class="line"></span><br><span class="line">这样就不需要传入weighs和biases，这里的reuse实现了定义和使用的一体化，不需要专门对weights定义和调用。</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, reuse=False)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1'</span>, reuse=reuse):</span><br><span class="line">        weights = tf.get_variable(<span class="string">"weights"</span>)</span><br><span class="line">        biases = tf.get_variable(<span class="string">"biases"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer2'</span>, reuse=reuse):</span><br><span class="line">        weights = tf.get_variable(<span class="string">"weights"</span>)</span><br><span class="line">        biases = tf.get_variable(<span class="string">"biases"</span>)</span><br><span class="line"></span><br><span class="line">TFRecord数据格式</span><br><span class="line">https://blog.csdn.net/u012759136/article/details/<span class="number">52232266</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一篇关于tensorflow的博客，这里面很多东西都是很杂碎的，不在此做处理，等积累的多了，理解才能正确。&lt;br&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-cuda</title>
    <link href="http://yoursite.com/2018/11/05/pytorch-cuda/"/>
    <id>http://yoursite.com/2018/11/05/pytorch-cuda/</id>
    <published>2018-11-05T13:04:26.000Z</published>
    <updated>2018-11-05T15:12:27.757Z</updated>
    
    <content type="html"><![CDATA[<p>关于pytorch中模型的多GPU<br><a id="more"></a></p><h1 id="1-cudnn-benchmark-True"><a href="#1-cudnn-benchmark-True" class="headerlink" title="1.cudnn.benchmark = True"></a>1.cudnn.benchmark = True</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.backends <span class="keyword">import</span> cudnn</span><br><span class="line">cudnn.benchmark = <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>而且大家都说这样可以增加程序的运行效率。那到底有没有这样的效果，或者什么情况下应该这样做呢？<br>总的来说，大部分情况下，设置这个 flag 可以让内置的 cuDNN 的 auto-tuner 自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。</p><p>一般来讲，应该遵循以下准则：</p><ul><li>如果网络的输入数据维度或类型上变化不大，设置  torch.backends.cudnn.benchmark = true  可以增加运行效率；</li><li>如果网络的输入数据在每次 iteration 都变化的话，会导致 cnDNN 每次都会去寻找一遍最优配置，这样反而会降低运行效率。</li></ul><p>这下就清晰明了很多了。</p><p>其实看完这段还是很蒙蔽，不知道具体什么情况下使用，暂且算加速过程好了。</p><h1 id="2-nn-DataParallel"><a href="#2-nn-DataParallel" class="headerlink" title="2. nn.DataParallel"></a>2. nn.DataParallel</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">model2 = nn.DataParallel(model1)</span><br><span class="line">model2.cuda()</span><br><span class="line"></span><br><span class="line">`</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span> <span class="comment"># Our model </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, output_size)</span>:</span> </span><br><span class="line">        super(Model, self).__init__() </span><br><span class="line">        self.fc = nn.Linear(input_size, output_size) </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span> </span><br><span class="line">        output = self.fc(input) </span><br><span class="line">        print(<span class="string">" In Model: input size"</span>, input.size(), <span class="string">"output size"</span>, output.size()) </span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">----</span><br><span class="line">model1 = Model(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">print(model1)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> model1.parameters():</span><br><span class="line">    print(var)</span><br><span class="line"></span><br><span class="line">Model(</span><br><span class="line">  (fc): Linear(in_features=<span class="number">3</span>, out_features=<span class="number">4</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.1964</span>,  <span class="number">0.4389</span>, <span class="number">-0.2216</span>],</span><br><span class="line">        [<span class="number">-0.1046</span>, <span class="number">-0.2055</span>, <span class="number">-0.5383</span>],</span><br><span class="line">        [ <span class="number">0.0673</span>,  <span class="number">0.0949</span>,  <span class="number">0.5205</span>],</span><br><span class="line">        [ <span class="number">0.5473</span>, <span class="number">-0.3700</span>, <span class="number">-0.4179</span>]])</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2416</span>,  <span class="number">0.4188</span>, <span class="number">-0.0096</span>,  <span class="number">0.1569</span>])</span><br><span class="line">----</span><br><span class="line">model2 = nn.DataParallel(model1)</span><br><span class="line">print(model2)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> model2.parameters():</span><br><span class="line">    print(var)</span><br><span class="line"></span><br><span class="line">DataParallel(</span><br><span class="line">  (module): Model(</span><br><span class="line">    (fc): Linear(in_features=<span class="number">3</span>, out_features=<span class="number">4</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.1964</span>,  <span class="number">0.4389</span>, <span class="number">-0.2216</span>],</span><br><span class="line">        [<span class="number">-0.1046</span>, <span class="number">-0.2055</span>, <span class="number">-0.5383</span>],</span><br><span class="line">        [ <span class="number">0.0673</span>,  <span class="number">0.0949</span>,  <span class="number">0.5205</span>],</span><br><span class="line">        [ <span class="number">0.5473</span>, <span class="number">-0.3700</span>, <span class="number">-0.4179</span>]], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2416</span>,  <span class="number">0.4188</span>, <span class="number">-0.0096</span>,  <span class="number">0.1569</span>], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">----</span><br><span class="line">model2.cuda()</span><br><span class="line">print(model2)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> model2.parameters():</span><br><span class="line">    print(var)</span><br><span class="line"></span><br><span class="line">DataParallel(</span><br><span class="line">  (module): Model(</span><br><span class="line">    (fc): Linear(in_features=<span class="number">3</span>, out_features=<span class="number">4</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.1964</span>,  <span class="number">0.4389</span>, <span class="number">-0.2216</span>],</span><br><span class="line">        [<span class="number">-0.1046</span>, <span class="number">-0.2055</span>, <span class="number">-0.5383</span>],</span><br><span class="line">        [ <span class="number">0.0673</span>,  <span class="number">0.0949</span>,  <span class="number">0.5205</span>],</span><br><span class="line">        [ <span class="number">0.5473</span>, <span class="number">-0.3700</span>, <span class="number">-0.4179</span>]], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2416</span>,  <span class="number">0.4188</span>, <span class="number">-0.0096</span>,  <span class="number">0.1569</span>], device=<span class="string">'cuda:0'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于pytorch中模型的多GPU&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>beyond-part-models</title>
    <link href="http://yoursite.com/2018/11/05/beyond-part-models/"/>
    <id>http://yoursite.com/2018/11/05/beyond-part-models/</id>
    <published>2018-11-05T12:59:51.000Z</published>
    <updated>2018-11-05T15:14:46.007Z</updated>
    
    <content type="html"><![CDATA[<p>PCB:<br><a href="https://github.com/huanghoujing/beyond-part-models" target="_blank" rel="noopener">https://github.com/huanghoujing/beyond-part-models</a><br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── bpm</span><br><span class="line">│   ├── dataset</span><br><span class="line">│   │   ├── Dataset.py</span><br><span class="line">│   │   ├── __init__.py</span><br><span class="line">│   │   ├── Prefetcher.py</span><br><span class="line">│   │   ├── PreProcessImage.py</span><br><span class="line">│   │   ├── TestSet.py</span><br><span class="line">│   │   └── TrainSet.py</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── model</span><br><span class="line">│   │   ├── __init__.py</span><br><span class="line">│   │   ├── PCBModel.py</span><br><span class="line">│   │   └── resnet.py</span><br><span class="line">│   └── utils</span><br><span class="line">│       ├── dataset_utils.py</span><br><span class="line">│       ├── distance.py</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       ├── metric.py</span><br><span class="line">│       ├── re_ranking.py</span><br><span class="line">│       ├── utils.py</span><br><span class="line">│       └── visualization.py</span><br><span class="line">├── example_rank_lists_on_Market1501</span><br><span class="line">│</span><br><span class="line">└── script</span><br><span class="line">    ├── dataset</span><br><span class="line">    │   ├── combine_trainval_sets.py</span><br><span class="line">    │   ├── mapping_im_names_duke.py</span><br><span class="line">    │   ├── mapping_im_names_market1501.py</span><br><span class="line">    │   ├── transform_cuhk03.py</span><br><span class="line">    │   ├── transform_duke.py</span><br><span class="line">    │   └── transform_market1501.py</span><br><span class="line">    └── experiment</span><br><span class="line">        ├── train_pcb.py</span><br><span class="line">        └── visualize_rank_list.py</span><br><span class="line"></span><br><span class="line">bpm:正式的模型训练</span><br><span class="line">script:主要用于数据的预处理和训练的对外借口</span><br></pre></td></tr></table></figure></p><p>以market1501为例</p><h1 id="数据的预处理"><a href="#数据的预处理" class="headerlink" title="数据的预处理"></a>数据的预处理</h1><p>第一个表示id，第二个表示cam，第三个表示同id同cam的第几张图片，对zip_file中的*.jpg移动到save_dir+images中，并且重命名，将所有图片重命名保存到save_dir+images<br>  分为<br>  —-trainval  name+label<br>      |—-train name+label<br>      |—-val   name+mask<br>           |—-query   0<br>           |—-gallery 1<br>  —-test name+mask<br>      |—-query        0<br>      |—-multi-query  2<br>      |—-gallery      1</p><p>  保存到save_dir, ‘partitions.pkl’中</p><p>  trainval提取val的时候，val中的id只提取100个id，并且会自动跳过只在一个cam下的id。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">partitions: dict</span><br><span class="line">            &#123;<span class="string">'trainval_im_names'</span>: train_test_split[<span class="string">'trainval_im_names'</span>],</span><br><span class="line">            <span class="string">'trainval_ids2labels'</span>: trainval_ids2labels,</span><br><span class="line">            <span class="string">'train_im_names'</span>: train_im_names,</span><br><span class="line">            <span class="string">'train_ids2labels'</span>: train_ids2labels,</span><br><span class="line">            <span class="string">'val_im_names'</span>: val_im_names,</span><br><span class="line">            <span class="string">'val_marks'</span>: val_marks,</span><br><span class="line">            <span class="string">'test_im_names'</span>: test_im_names,</span><br><span class="line">            <span class="string">'test_marks'</span>: test_marks&#125;</span><br></pre></td></tr></table></figure></p><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p>对于数据集是怎么加载、转化的，还是没有头绪，写法之前没有遇到过，这一部分有待提高</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>模型是一致的</p><h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><p>test好像是re-ranking了<br>val没有re-ranking</p><p>lr: 0.1 0.01<br>factor: 0.1<br>epochs: 60<br>staircase_decay_at_epochs: 41</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCB:&lt;br&gt;&lt;a href=&quot;https://github.com/huanghoujing/beyond-part-models&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/huanghoujing/beyond-part-models&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper" scheme="http://yoursite.com/categories/paper/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>python-2.0</title>
    <link href="http://yoursite.com/2018/11/05/python-2.0/"/>
    <id>http://yoursite.com/2018/11/05/python-2.0/</id>
    <published>2018-11-05T09:27:03.000Z</published>
    <updated>2018-12-03T05:22:46.972Z</updated>
    
    <content type="html"><![CDATA[<p>这也是关于python的一些记录，与其他python不矛盾，也不一定相互独立。<br><a id="more"></a></p><h1 id="1-python-matlab"><a href="#1-python-matlab" class="headerlink" title="1.python+matlab"></a>1.python+matlab</h1><p>python和matlab关于.mat数据的交换<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scipy.io.loadmat(file_name, mdict=<span class="keyword">None</span>, appendmat=<span class="keyword">True</span>, **kwargs)</span><br><span class="line">scipy.io.savemat(file_name, mdict, appendmat=<span class="keyword">True</span>, format=<span class="string">'5'</span>, </span><br><span class="line">                 long_field_names=<span class="keyword">False</span>, do_compression=<span class="keyword">False</span>, oned_as=<span class="string">'row'</span>)</span><br></pre></td></tr></table></figure></p><h1 id="2-python与文件IO"><a href="#2-python与文件IO" class="headerlink" title="2.python与文件IO"></a>2.python与文件IO</h1><p>主要参考：python cookbook</p><h2 id="2-1-文本-txt"><a href="#2-1-文本-txt" class="headerlink" title="2.1 文本.txt"></a>2.1 文本.txt</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t:rt模式下，python在读取文本时会自动把\r\n转换成\n.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># read</span></span><br><span class="line"><span class="comment"># read the entire file as a single string</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.txt'</span>, <span class="string">'rt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># read lines</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.txt'</span>, <span class="string">'rt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line"></span><br><span class="line"><span class="comment">#write</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.txt'</span>, <span class="string">'wt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(text1)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.txt'</span>, <span class="string">'wt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    print(line1, file=f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 换行模式，默认情况下，python会自动识别，或者传入newline,</span></span><br><span class="line"><span class="comment"># newline可以取的值有None, \n, \r, ‘\r\n'，用于区分换行符，但是这个参数只对文本模式有效；</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.txt'</span>, <span class="string">'rt'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码错误问题 errors: replace, ignore</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'sample.txt'</span>, <span class="string">'rt'</span>, encoding=<span class="string">'ascii'</span>, errors=<span class="string">'replace'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="2-2-print的分隔符与行尾符"><a href="#2-2-print的分隔符与行尾符" class="headerlink" title="2.2 print的分隔符与行尾符"></a>2.2 print的分隔符与行尾符</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print seq end</span></span><br><span class="line">print(<span class="string">'ACME'</span>, <span class="number">50</span>, seq=<span class="string">','</span>, end=<span class="string">'!!\n'</span>)</span><br><span class="line">ACME,<span class="number">50</span>!!</span><br><span class="line"></span><br><span class="line">row = (<span class="string">'ACME'</span>, <span class="number">50</span>)</span><br><span class="line">print(*row, seq=<span class="string">','</span>, end=<span class="string">'!!\n'</span>)</span><br><span class="line">ACME,<span class="number">50</span>!!</span><br></pre></td></tr></table></figure><h2 id="2-3-二进制数据"><a href="#2-3-二进制数据" class="headerlink" title="2.3 二进制数据"></a>2.3 二进制数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二进制数据，比如图片、声音等</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.bin'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'some.bin'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">b'hello'</span>)</span><br></pre></td></tr></table></figure><h2 id="2-4-模拟普通文件"><a href="#2-4-模拟普通文件" class="headerlink" title="2.4 模拟普通文件"></a>2.4 模拟普通文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟文本文件</span></span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line">s = io.StringIO()</span><br><span class="line"><span class="comment"># s = io.StringIO('hello world\n')</span></span><br><span class="line">s.write(<span class="string">'hello world\n'</span>)</span><br><span class="line">print(<span class="string">'this is a test'</span>, file=s)</span><br><span class="line">s.getvalue()</span><br><span class="line"><span class="string">'hello world\nthis is a test'</span></span><br><span class="line">s.read(<span class="number">4</span>)</span><br><span class="line">s.read()</span><br><span class="line"><span class="comment"># 模拟二进制文件</span></span><br><span class="line">s = io.BytesIO()</span><br><span class="line">s.write(<span class="string">b'binary data'</span>)</span><br><span class="line">s.getvalue()</span><br><span class="line"><span class="string">b'binary data'</span></span><br></pre></td></tr></table></figure><h2 id="2-5-压缩文件"><a href="#2-5-压缩文件" class="headerlink" title="2.5 压缩文件"></a>2.5 压缩文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gzip,bz2</span></span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">with</span> gzip.open(<span class="string">'some.gz'</span>, <span class="string">'rt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br><span class="line"><span class="keyword">with</span> gzip.open(<span class="string">'some.gz'</span>, <span class="string">'wt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(text)</span><br><span class="line">-------------------------------------</span><br><span class="line"><span class="keyword">import</span> bz2</span><br><span class="line"><span class="keyword">with</span> bz2.open(<span class="string">'some.bz2'</span>, <span class="string">'rt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br><span class="line"><span class="keyword">with</span> bz2.open(<span class="string">'some.bz2'</span>, <span class="string">'wt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(text)</span><br><span class="line">------------------------------------</span><br><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br><span class="line"><span class="keyword">with</span> ZipFile(zip_file) <span class="keyword">as</span> z:</span><br><span class="line">    z.extractall(path=save_dir)</span><br></pre></td></tr></table></figure><h2 id="2-6-csv"><a href="#2-6-csv" class="headerlink" title="2.6 csv"></a>2.6 csv</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># csv</span></span><br><span class="line"><span class="comment"># 其实namedtuple继承自OrderedDict有序字典</span></span><br><span class="line"><span class="comment"># read</span></span><br><span class="line">------</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stock.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="comment"># 第一种：row是列表，访问：row[0]</span></span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    headers = next(f_csv)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二种：命名元组，访问：Row.Symbol</span></span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    headers = next(f_csv)</span><br><span class="line">    <span class="comment"># headers = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) ]</span></span><br><span class="line">    Row = namedtuple(<span class="string">'Row'</span>, headers)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> f_csv:</span><br><span class="line">        row = Row(*r)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第三种：字典序列，访问：row['Sysbol']</span></span><br><span class="line">    f_csv = csv.DictReader(f)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># write</span></span><br><span class="line">-------</span><br><span class="line"><span class="comment"># 第一种：类表</span></span><br><span class="line">headers = [<span class="string">'Symbol'</span>,<span class="string">'Price'</span>,<span class="string">'Date'</span>,<span class="string">'Time'</span>,<span class="string">'Change'</span>,<span class="string">'Volume'</span>]</span><br><span class="line">rows = [(<span class="string">'AA'</span>, <span class="number">39.48</span>, <span class="string">'6/11/2007'</span>, <span class="string">'9:36am'</span>, <span class="number">-0.18</span>, <span class="number">181800</span>),</span><br><span class="line">        (<span class="string">'AIG'</span>, <span class="number">71.38</span>, <span class="string">'6/11/2007'</span>, <span class="string">'9:36am'</span>, <span class="number">-0.15</span>, <span class="number">195500</span>),</span><br><span class="line">        (<span class="string">'AXP'</span>, <span class="number">62.58</span>, <span class="string">'6/11/2007'</span>, <span class="string">'9:36am'</span>, <span class="number">-0.46</span>, <span class="number">935000</span>),</span><br><span class="line">        ]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stock.csv'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.writer(f)</span><br><span class="line">    f_csv.writerow(headers)</span><br><span class="line">    f_csv.writerows(rows)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种：字典</span></span><br><span class="line">headers = [<span class="string">'Symbol'</span>, <span class="string">'Price'</span>, <span class="string">'Date'</span>, <span class="string">'Time'</span>, <span class="string">'Change'</span>, <span class="string">'Volume'</span>]</span><br><span class="line">rows = [&#123;<span class="string">'Symbol'</span>:<span class="string">'AA'</span>, <span class="string">'Price'</span>:<span class="number">39.48</span>, <span class="string">'Date'</span>:<span class="string">'6/11/2007'</span>,</span><br><span class="line">         <span class="string">'Time'</span>:<span class="string">'9:36am'</span>, <span class="string">'Change'</span>:<span class="number">-0.18</span>, <span class="string">'Volume'</span>:<span class="number">181800</span>&#125;,</span><br><span class="line">        &#123;<span class="string">'Symbol'</span>:<span class="string">'AIG'</span>, <span class="string">'Price'</span>: <span class="number">71.38</span>, <span class="string">'Date'</span>:<span class="string">'6/11/2007'</span>,</span><br><span class="line">         <span class="string">'Time'</span>:<span class="string">'9:36am'</span>, <span class="string">'Change'</span>:<span class="number">-0.15</span>, <span class="string">'Volume'</span>: <span class="number">195500</span>&#125;,</span><br><span class="line">        &#123;<span class="string">'Symbol'</span>:<span class="string">'AXP'</span>, <span class="string">'Price'</span>: <span class="number">62.58</span>, <span class="string">'Date'</span>:<span class="string">'6/11/2007'</span>,</span><br><span class="line">         <span class="string">'Time'</span>:<span class="string">'9:36am'</span>, <span class="string">'Change'</span>:<span class="number">-0.46</span>, <span class="string">'Volume'</span>: <span class="number">935000</span>&#125;,</span><br><span class="line">        ]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stocks.csv'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.DictWriter(f, headers)</span><br><span class="line">    f_csv.writeheader()</span><br><span class="line">    f_csv.writerows(rows)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以tab分割</span></span><br><span class="line">-----</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stock.tsv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_tsv = csv.reader(f, delimiter=<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_tsv:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 类型转换</span></span><br><span class="line">---------</span><br><span class="line"><span class="comment"># 第一种：tuple</span></span><br><span class="line">col_types = [str, float, str, str, float, int]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stocks.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    headers = next(f_csv)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">        <span class="comment"># Apply conversions to the row items</span></span><br><span class="line">        row = tuple(convert(value) <span class="keyword">for</span> convert, value <span class="keyword">in</span> zip(col_types, row))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种：dict</span></span><br><span class="line">print(<span class="string">'Reading as dicts with type conversion'</span>)</span><br><span class="line">field_types = [ (<span class="string">'Price'</span>, float),</span><br><span class="line">                (<span class="string">'Change'</span>, float),</span><br><span class="line">                (<span class="string">'Volume'</span>, int) ]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stocks.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> csv.DictReader(f):</span><br><span class="line">        row.update((key, conversion(row[key]))</span><br><span class="line">            <span class="keyword">for</span> key, conversion <span class="keyword">in</span> field_types)</span><br><span class="line">        print(row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 高级操作</span></span><br><span class="line">--------</span><br><span class="line"><span class="comment"># pandas.read_csv()</span></span><br></pre></td></tr></table></figure><h2 id="2-7-json"><a href="#2-7-json" class="headerlink" title="2.7 json"></a>2.7 json</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># json</span></span><br><span class="line"><span class="comment"># write &amp; read</span></span><br><span class="line">--------------</span><br><span class="line"><span class="comment"># 第一种 dict----str</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">data = &#123;</span><br><span class="line">        <span class="string">'name'</span> : <span class="string">'ACME'</span>,<span class="string">'shares'</span> : <span class="number">100</span>,</span><br><span class="line">        <span class="string">'price'</span> : <span class="number">542.23</span></span><br><span class="line">        &#125;</span><br><span class="line">json_str = json.dumps(data)</span><br><span class="line">daa = json.loads(json_str)</span><br><span class="line"><span class="comment"># 第二种 dict----file</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.json'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(data, f)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.json'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = json.load(f)</span><br></pre></td></tr></table></figure><hr><h1 id="3-sys-path"><a href="#3-sys-path" class="headerlink" title="3. sys.path"></a>3. sys.path</h1><p>sys.path:动态地改变Python搜索路径<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(’引用模块的地址<span class="string">')</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#或者</span></span><br><span class="line"><span class="string">import sys</span></span><br><span class="line"><span class="string">sys.path.insert(0, '</span>引用模块的地址<span class="string">')</span></span><br></pre></td></tr></table></figure></p><h1 id="4-os-path"><a href="#4-os-path" class="headerlink" title="4. os.path"></a>4. os.path</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">os.path.abspath(path) <span class="comment">#返回绝对路径</span></span><br><span class="line">os.path.basename(path) <span class="comment">#返回文件名</span></span><br><span class="line">os.path.exists(path)  <span class="comment">#路径存在则返回True,路径损坏返回False</span></span><br><span class="line">os.path.dirname(path) <span class="comment">#返回文件路径</span></span><br><span class="line">os.path.expanduser(path)  <span class="comment">#把path中包含的"~"和"~user"转换成用户目录</span></span><br><span class="line">os.path.isabs(path)  <span class="comment">#判断是否为绝对路径</span></span><br><span class="line">os.path.isfile(path)  <span class="comment">#判断路径是否为文件</span></span><br><span class="line">os.path.isdir(path)  <span class="comment">#判断路径是否为目录</span></span><br><span class="line">os.path.join(path1[, path2[, ...]])  <span class="comment">#把目录和文件名合成一个路径</span></span><br><span class="line">os.path.samefile(path1, path2)  <span class="comment">#判断目录或文件是否相同</span></span><br><span class="line">os.path.split(path)  <span class="comment">#把路径分割成dirname和basename，返回一个元组</span></span><br><span class="line">os.path.splitext(path)  <span class="comment">#分割路径，返回路径名和文件扩展名的元组</span></span><br><span class="line">os.path.walk(path, visit, arg)</span><br></pre></td></tr></table></figure><hr><h1 id="5-glob-glob"><a href="#5-glob-glob" class="headerlink" title="5. glob.glob"></a>5. glob.glob</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line">listglob = []</span><br><span class="line">listglob = glob.glob(<span class="string">r"/home/xxx/picture/*.png"</span>)</span><br><span class="line">listglob.sort()</span><br><span class="line">print(listglob)</span><br></pre></td></tr></table></figure><h1 id="6-argparse"><a href="#6-argparse" class="headerlink" title="6. argparse"></a>6. argparse</h1><p>参考链接：<a href="http://lib.csdn.net/article/python/49052" target="_blank" rel="noopener">http://lib.csdn.net/article/python/49052</a><br><a href="https://blog.csdn.net/u010895119/article/details/78960740" target="_blank" rel="noopener">https://blog.csdn.net/u010895119/article/details/78960740</a><br><a href="https://www.jianshu.com/p/a50aead61319" target="_blank" rel="noopener">https://www.jianshu.com/p/a50aead61319</a><br><a href="https://blog.csdn.net/guoyajie1990/article/details/76739977" target="_blank" rel="noopener">https://blog.csdn.net/guoyajie1990/article/details/76739977</a></p><p>不是很适合交互式调试 命令行参数<br>分为位置参数和选项参数</p><h2 id="位置参数"><a href="#位置参数" class="headerlink" title="位置参数"></a>位置参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">"say something about this application !!"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'name'</span>, type=int,  help=<span class="string">"i can tell you how to set a name argument"</span>)</span><br><span class="line"></span><br><span class="line">result = parser.parse_args()</span><br><span class="line"></span><br><span class="line">print(result.name)</span><br><span class="line"></span><br><span class="line">$python main.py taylor</span><br><span class="line">taylor</span><br></pre></td></tr></table></figure><h2 id="选项参数"><a href="#选项参数" class="headerlink" title="选项参数"></a>选项参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">"say something about this application !!"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-a"</span>,<span class="string">"--age"</span>, help=<span class="string">"this is an optional argument"</span>)</span><br><span class="line">result = parser.parse_args()</span><br><span class="line">print(result.age)</span><br><span class="line"></span><br><span class="line">$python main.py  --age <span class="number">888</span></span><br><span class="line"><span class="number">888</span></span><br><span class="line">$python main.py  --age=<span class="number">888</span></span><br><span class="line"><span class="number">888</span></span><br></pre></td></tr></table></figure><h2 id="特殊的选项参数"><a href="#特殊的选项参数" class="headerlink" title="特殊的选项参数"></a>特殊的选项参数</h2><p>起着开关的作用<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line">parser = argparse.ArgumentParser(<span class="attribute">description</span>=<span class="string">"say something about this application !!"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-a"</span>, <span class="string">"--age"</span>, <span class="attribute">help</span>=<span class="string">"this is an optional argument"</span>, <span class="attribute">action</span>=<span class="string">"store_true"</span>)</span><br><span class="line">result = parser.parse_args()</span><br><span class="line"><span class="builtin-name">print</span>(result.age)</span><br><span class="line"></span><br><span class="line"><span class="variable">$python</span> main.py  -a</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure></p><p>指定选项<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">"say something about this application !!"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-a"</span>, <span class="string">"--age"</span>, help=<span class="string">"this is an optional argument"</span>, type=int, choices=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">result = parser.parse_args()</span><br><span class="line">print(result.age)</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>nargs<br>nargs=N(N是int类型)，nargs=’*’, nargs=’?’某个参数接受的值，nargs定义了值的个数，加了nargs后，接受的值会变成一个list，’?’代表一个值，’*’代表一个或多个值，举例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'capital'</span>, default=<span class="string">'hello'</span>, nargs=<span class="number">1</span>, help=<span class="string">'将首字母大写'</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">print(args)</span><br><span class="line">print(args.capital)</span><br></pre></td></tr></table></figure></p><p>计数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">"-v"</span>, <span class="string">"--verbosity"</span>,</span><br><span class="line">       action=<span class="string">"count"</span>, default=<span class="number">0</span>, help=<span class="string">"increase output verbosity"</span>)</span><br></pre></td></tr></table></figure></p><h1 id="7-defaultdict"><a href="#7-defaultdict" class="headerlink" title="7. defaultdict"></a>7. defaultdict</h1><p>遍历生成字典<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">counts = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> strings:</span><br><span class="line">    counts[k]=counts.setdault(k, <span class="number">0</span>)+<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">dd = defaultdict(int)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> strings:</span><br><span class="line">    counts[k]=counts[k]+<span class="number">1</span></span><br></pre></td></tr></table></figure></p><h1 id="8-shuti"><a href="#8-shuti" class="headerlink" title="8. shuti"></a>8. shuti</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">shutil.copy 复制文件</span><br><span class="line">shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉</span><br><span class="line">shutil.move( src, dst)  移动文件或重命名</span><br><span class="line">shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的</span><br><span class="line">shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间</span><br><span class="line">shutil.copy( src, dst)  复制一个文件到一个文件或一个目录</span><br><span class="line">shutil.copy2( src, dst)  在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，类似于cp –p的东西</span><br><span class="line">shutil.copy2( src, dst)  如果两个位置的文件系统是一样的话相当于是rename操作，只是改名；如果是不在相同的文件系统的话就是做move操作</span><br><span class="line">shutil.copytree( olddir, newdir, <span class="keyword">True</span>/Flase)</span><br><span class="line">把olddir拷贝一份newdir，如果第<span class="number">3</span>个参数是<span class="keyword">True</span>，则复制目录时将保持文件夹下的符号连接，如果第<span class="number">3</span>个参数是<span class="keyword">False</span>，则将在复制的目录下生成物理副本来替代符号连接</span><br><span class="line">shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容</span><br></pre></td></tr></table></figure><h1 id="9-pickle"><a href="#9-pickle" class="headerlink" title="9. pickle"></a>9. pickle</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pickle</span><br><span class="line">python codebook</span><br></pre></td></tr></table></figure><p>注释：序列化对象，将对象obj保存到文件file中去。参数protocol是序列化模式，默认是0（ASCII协议，表示以文本的形式进行序列化），protocol的值还可以是1和2（1和2表示以二进制的形式进行序列化。其中，1是老式的二进制协议；2是新二进制协议）。file表示保存到的类文件对象，file必须有write()接口，file可以是一个以’w’打开的文件或者是一个StringIO对象，也可以是任何可以实现write()接口的对象。</p><h1 id="10-重定向"><a href="#10-重定向" class="headerlink" title="10.重定向"></a>10.重定向</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Logger</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, fpath=None)</span>:</span></span><br><span class="line">        self.console = sys.stdout</span><br><span class="line">        self.file = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> fpath <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            mkdir_if_missing(os.path.dirname(fpath))</span><br><span class="line">            self.file = open(fpath, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(self, msg)</span>:</span></span><br><span class="line">        self.console.write(msg)</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.write(msg)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">flush</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.console.flush()</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.flush()</span><br><span class="line">            os.fsync(self.file.fileno())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.console.close()</span><br><span class="line">        <span class="keyword">if</span> self.file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.file.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkdir_if_missing</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">        os.makedirs(path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    sys.stdout = Logger(fpath=<span class="string">'./log.txt'</span>)</span><br><span class="line">    print(<span class="string">'2222222222'</span>)</span><br></pre></td></tr></table></figure><h1 id="其他"><a href="#其他" class="headerlink" title=". 其他"></a>. 其他</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># id2labels</span></span><br><span class="line">trainval_ids2labels = dict(zip(trainval_ids, range(len(trainval_ids))))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这也是关于python的一些记录，与其他python不矛盾，也不一定相互独立。&lt;br&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-optim</title>
    <link href="http://yoursite.com/2018/11/05/pytorch-optim/"/>
    <id>http://yoursite.com/2018/11/05/pytorch-optim/</id>
    <published>2018-11-05T09:13:57.000Z</published>
    <updated>2018-11-05T15:15:49.854Z</updated>
    
    <content type="html"><![CDATA[<p>pytorch-optim<br><a id="more"></a></p><h1 id="optim的学习率设置问题"><a href="#optim的学习率设置问题" class="headerlink" title="optim的学习率设置问题"></a>optim的学习率设置问题</h1><h2 id="1-不同的学习率"><a href="#1-不同的学习率" class="headerlink" title="1.不同的学习率"></a>1.不同的学习率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">optimizer = t.optim.Adam(model.parameters(), lr = <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">optim_group = [&#123;<span class="string">'params'</span>:model.net1.parameters(),<span class="string">'lr'</span>:<span class="number">0.4</span>&#125;,</span><br><span class="line">       &#123;<span class="string">'params'</span>:model.net2.parameters(),<span class="string">'lr'</span>:<span class="number">0.1</span>&#125;]</span><br><span class="line">optimizer = t.optim.Adam(optim_group,lr=<span class="number">0.04</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三种</span></span><br><span class="line">ignored_params = list(map(id, model.model.fc.parameters() )) </span><br><span class="line">               + list(map(id, model.classifier.parameters() ))</span><br><span class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> ignored_params, model.parameters())</span><br><span class="line">optimizer_ft = optim.SGD([</span><br><span class="line">     &#123;<span class="string">'params'</span>: base_params, <span class="string">'lr'</span>: <span class="number">0.01</span>&#125;,</span><br><span class="line">     &#123;<span class="string">'params'</span>: model.model.fc.parameters(), <span class="string">'lr'</span>: <span class="number">0.1</span>&#125;,</span><br><span class="line">     &#123;<span class="string">'params'</span>: model.classifier.parameters(), <span class="string">'lr'</span>: <span class="number">0.1</span>&#125;</span><br><span class="line"> ], weight_decay=<span class="number">5e-4</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><h2 id="2-学习率衰减"><a href="#2-学习率衰减" class="headerlink" title="2. 学习率衰减"></a>2. 学习率衰减</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=<span class="number">40</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, scheduler, num_epochs=<span class="number">25</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">            optimizer.zero_grad()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;pytorch-optim&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-init</title>
    <link href="http://yoursite.com/2018/11/05/pytorch-init/"/>
    <id>http://yoursite.com/2018/11/05/pytorch-init/</id>
    <published>2018-11-05T08:55:36.000Z</published>
    <updated>2018-11-05T15:16:03.014Z</updated>
    
    <content type="html"><![CDATA[<p>pytorch-init<br><a id="more"></a></p><h1 id="pytorch模型的初始化"><a href="#pytorch模型的初始化" class="headerlink" title="pytorch模型的初始化"></a>pytorch模型的初始化</h1><p>pytorch模型的初始化的常用方法。</p><h2 id="1-apply-type"><a href="#1-apply-type" class="headerlink" title="1.apply+type"></a>1.apply+type</h2><p>apply可以理解成从children开始遍历<br>可以用于<strong>init</strong>，可以用于model定义之后，与type配合。</p><blockquote><p>Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init).</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># define model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        self.pre = nn.Sequential(nn.Linear(<span class="number">2</span>,<span class="number">2</span>), nn.Conv2d(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">        self.two = nn.Sequential(nn.Linear(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">        self.apply(init_weights)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    print(m)</span><br><span class="line">    print(type(m))</span><br><span class="line">    print(nn.Linear)</span><br><span class="line">    print(m.__class__)</span><br><span class="line">    print(m.__class__.__name__)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">        m.weight.data.fill_(<span class="number">0.0</span>)</span><br><span class="line">        print(m.weight.data)</span><br><span class="line">    print(<span class="string">"_______________________"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net2 = Net()</span><br><span class="line"></span><br><span class="line">Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Linear</span></span></span><br><span class="line"><span class="class"><span class="title">tensor</span><span class="params">([[ <span class="number">0.</span>,  <span class="number">0.</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">        [ <span class="number">0.</span>,  <span class="number">0.</span>]])</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br><span class="line"><span class="class"><span class="title">Conv2d</span><span class="params">(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="params">(<span class="number">3</span>, <span class="number">3</span>)</span>, stride=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">conv</span>.<span class="title">Conv2d</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">conv</span>.<span class="title">Conv2d</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Conv2d</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br><span class="line"><span class="class"><span class="title">Sequential</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">  <span class="params">(<span class="number">0</span>)</span>: Linear<span class="params">(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=True)</span></span></span></span><br><span class="line"><span class="class"><span class="params">  <span class="params">(<span class="number">1</span>)</span>: Conv2d<span class="params">(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="params">(<span class="number">3</span>, <span class="number">3</span>)</span>, stride=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span></span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Sequential</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br><span class="line"><span class="class"><span class="title">Linear</span><span class="params">(in_features=<span class="number">3</span>, out_features=<span class="number">3</span>, bias=True)</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Linear</span></span></span><br><span class="line"><span class="class"><span class="title">tensor</span><span class="params">([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br><span class="line"><span class="class"><span class="title">Sequential</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">  <span class="params">(<span class="number">0</span>)</span>: Linear<span class="params">(in_features=<span class="number">3</span>, out_features=<span class="number">3</span>, bias=True)</span></span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Sequential</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br><span class="line"><span class="class"><span class="title">Net</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">  <span class="params">(pre)</span>: Sequential<span class="params">(</span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">    <span class="params">(<span class="number">0</span>)</span>: Linear<span class="params">(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=True)</span></span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">    <span class="params">(<span class="number">1</span>)</span>: Conv2d<span class="params">(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="params">(<span class="number">3</span>, <span class="number">3</span>)</span>, stride=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span></span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">  )</span></span></span></span><br><span class="line"><span class="class"><span class="params">  <span class="params">(two)</span>: Sequential<span class="params">(</span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">    <span class="params">(<span class="number">0</span>)</span>: Linear<span class="params">(in_features=<span class="number">3</span>, out_features=<span class="number">3</span>, bias=True)</span></span></span></span></span><br><span class="line"><span class="class"><span class="params"><span class="params">  )</span></span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">__main__</span>.<span class="title">Net</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">__main__</span>.<span class="title">Net</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Net</span></span></span><br><span class="line"><span class="class"><span class="title">_______________________</span></span></span><br></pre></td></tr></table></figure><h1 id="2-apply-m-class-name"><a href="#2-apply-m-class-name" class="headerlink" title="2.apply+m.class.name"></a>2.apply+m.<strong>class</strong>.<strong>name</strong></h1><p>weights_init_kaiming<br>还要一种初始化函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init_kaiming</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="comment"># print(classname)</span></span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        init.kaiming_normal(m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_in'</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">'Linear'</span>) != <span class="number">-1</span>:</span><br><span class="line">        init.kaiming_normal(m.weight.data, a=<span class="number">0</span>, mode=<span class="string">'fan_out'</span>)</span><br><span class="line">        init.constant(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm1d'</span>) != <span class="number">-1</span>:</span><br><span class="line">        init.normal(m.weight.data, <span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        init.constant(m.bias.data, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init_classifier</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Linear'</span>) != <span class="number">-1</span>:</span><br><span class="line">        init.normal(m.weight.data, std=<span class="number">0.001</span>)</span><br><span class="line">        init.constant(m.bias.data, <span class="number">0.0</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;pytorch-init&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>cuda安装教程</title>
    <link href="http://yoursite.com/2018/11/05/cuda/"/>
    <id>http://yoursite.com/2018/11/05/cuda/</id>
    <published>2018-11-05T08:49:29.000Z</published>
    <updated>2018-11-05T08:53:08.588Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>安装教程参考链接<br><a href="https://www.jianshu.com/p/35c7fde85968" target="_blank" rel="noopener">https://www.jianshu.com/p/35c7fde85968</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;安装教程参考链接&lt;br&gt;&lt;a href=&quot;https://www.jianshu.com/p/35c7fde85968&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/3
      
    
    </summary>
    
      <category term="cuda" scheme="http://yoursite.com/categories/cuda/"/>
    
    
      <category term="cuda" scheme="http://yoursite.com/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>python_reptilian</title>
    <link href="http://yoursite.com/2018/10/05/python-reptilian/"/>
    <id>http://yoursite.com/2018/10/05/python-reptilian/</id>
    <published>2018-10-05T03:13:00.000Z</published>
    <updated>2018-10-05T14:39:27.400Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这是第一次爬虫，以理论为主，以实现为辅。因为是到处看的，所以不是很有逻辑性。<br>参考链接：<br><a href="https://www.cnblogs.com/sss4/p/7809821.html" target="_blank" rel="noopener">python爬虫原理</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&amp;mid=2247487461&amp;idx=2&amp;sn=9cf9910f44808e429f32731fb5214380&amp;chksm=ec5ed098db29598e9ce3a99a1ed6005fe552c9fde8533639c561e3ca7c44d71f6f053c68de8e&amp;mpshare=1&amp;scene=1&amp;srcid=0827ZJWXHEkP54zWy7t6FfGV#rd" target="_blank" rel="noopener">Python爬虫的两套解析方法和四种爬虫实现</a><br><a href="http://www.cnblogs.com/linhaifeng/articles/7773496.html" target="_blank" rel="noopener">爬虫基本原理</a><br><a id="more"></a></p><h1 id="一、工具"><a href="#一、工具" class="headerlink" title="一、工具"></a>一、工具</h1><ul><li>两个解析库：BeautifulSoup, lxml</li><li>两个请求库：urllib, requests</li><li>法法</li></ul><hr><h1 id="二、爬虫流程"><a href="#二、爬虫流程" class="headerlink" title="二、爬虫流程"></a>二、爬虫流程</h1><p>用户获取网络数据的方式<br>方式1:浏览器提交请求—-&gt;下载网页代码—-&gt;解析成页面<br>方式2:模拟浏览器发送请求(获取网页代码)—-&gt;提取有用数据—-&gt;存放在数据库或者文件中<br>爬虫就是指方式2.</p><h2 id="1-发起请求"><a href="#1-发起请求" class="headerlink" title="1.发起请求"></a>1.发起请求</h2><p>使用http库向目标站点发送请求，即发送一个Request。<br>Request包含：请求头，请求体等。<br>Request模块缺点：不能执行JS和CSS代码。</p><h2 id="2-获取响应内容"><a href="#2-获取响应内容" class="headerlink" title="2.获取响应内容"></a>2.获取响应内容</h2><p>服务器正常响应，得到一个Response。<br>Response包含：html，json，图片，视频等。</p><h2 id="3-解析内容"><a href="#3-解析内容" class="headerlink" title="3.解析内容"></a>3.解析内容</h2><p>解析html数据：正则表达式(RE模块)，第三方解析库如BeautifulSoup,pyquery等<br>解析json数据：json模块<br>解析二进制数据：以wb形式写入文件</p><h2 id="4-保存数据"><a href="#4-保存数据" class="headerlink" title="4.保存数据"></a>4.保存数据</h2><p>数据库（MySQL, Mongdb, Redis)<br>文件</p><hr><h1 id="三、Request-amp-Response"><a href="#三、Request-amp-Response" class="headerlink" title="三、Request&amp;Response"></a>三、Request&amp;Response</h1><h2 id="1-Request"><a href="#1-Request" class="headerlink" title="1.Request"></a>1.Request</h2><h3 id="1-1-请求方式"><a href="#1-1-请求方式" class="headerlink" title="1.1.请求方式"></a>1.1.请求方式</h3><p>常见的有：GET/POST</p><h3 id="1-2-请求的URL"><a href="#1-2-请求的URL" class="headerlink" title="1.2.请求的URL"></a>1.2.请求的URL</h3><p>url是全球容易资源定位符，用来丁意思互联网上一个唯一的资源，例如：一张图片、一个文件、一段视频。</p><h3 id="1-3-请求头"><a href="#1-3-请求头" class="headerlink" title="1.3.请求头"></a>1.3.请求头</h3><p>User-agen：访问的浏览器请求头没有user-agent客户端配置，会被当成非法用户host<br>cookies：cookie用来保存登录信息<br>Referrer：访问源至哪里来</p><h3 id="1-4-请求体"><a href="#1-4-请求体" class="headerlink" title="1.4.请求体"></a>1.4.请求体</h3><p>get：请求体没有内容<br>post：请求体是format data</p><h2 id="2-Response"><a href="#2-Response" class="headerlink" title="2.Response"></a>2.Response</h2><h3 id="2-1-响应状态码"><a href="#2-1-响应状态码" class="headerlink" title="2.1 响应状态码"></a>2.1 响应状态码</h3><p>200：代表成功<br>301：代表调转<br>404：文件不存在<br>403：无权限访问<br>502：服务器错误</p><h3 id="2-2-响应头"><a href="#2-2-响应头" class="headerlink" title="2.2 响应头"></a>2.2 响应头</h3><p>Set-Cookie:BDSVRTM=0; path=/：可能有多个，是来告诉浏览器，把cookie保存下来<br>Content-Location：服务端响应头中包含Location返回浏览器之后，浏览器就会重新访问另一个页面</p><h3 id="2-3preview"><a href="#2-3preview" class="headerlink" title="2.3preview"></a>2.3preview</h3><p>网页源代码，包括：<br>Json数据、html、图片、二进制数据</p><h1 id="接下来开始尝试写一些基本的爬虫代码，并做记录"><a href="#接下来开始尝试写一些基本的爬虫代码，并做记录" class="headerlink" title="接下来开始尝试写一些基本的爬虫代码，并做记录"></a>接下来开始尝试写一些基本的爬虫代码，并做记录</h1><p><code>`</code>python</p><h1 id="发起请求，并获取请求内容"><a href="#发起请求，并获取请求内容" class="headerlink" title="发起请求，并获取请求内容"></a>发起请求，并获取请求内容</h1><p>from urllib import request<br>resp = request.urlopen(‘<a href="https://movie.douban.com/nowplaying/hangzhou/&#39;" target="_blank" rel="noopener">https://movie.douban.com/nowplaying/hangzhou/&#39;</a>) # http.client.HTTPResponse<br>html_data = resp.read().decode(‘utf-8’) # str 这里的print是最好看的</p><h1 id="解析内容"><a href="#解析内容" class="headerlink" title="解析内容"></a>解析内容</h1><p>from bs4 import BeautifulSoup as bs<br>soup = bs(html_data, ‘html.parser’)  # bs4.BeautifulSoup<br>nowplaying_movie = soup.find_all(‘div’, id=’nowplaying’) # bs4.element.ResultSet list的形式，可以暂时看成是多个组成的list，需要先[0]的进行访问。<br>tmp = nowplaying_movie[0] # bs4.element.Tag<br>nowplaying_movie_list = nowplaying_movie[0].find_all(‘li’, class_=’list-item’) # bs4.element.ResultSet  list形式， bs4.element.Tag<br>nowplaying_list = []  # 此时就是直接获取数据了，find_all是对相应片段的截取<br>for item in nowplaying_movie_list:<br>    nowplaying_dict = {}<br>    nowplaying_dict[‘id’] = item[‘data-subject’]<br>    for tag_img_item in item.find_all(‘img’):<br>        nowplaying_dict[‘name’] = tag_img_item[‘alt’]<br>        nowplaying_list.append(nowplaying_dict)</p><p>requrl = ‘<a href="https://movie.douban.com/subject/&#39;+nowplaying_list[0][&#39;id&#39;]" target="_blank" rel="noopener">https://movie.douban.com/subject/&#39;+nowplaying_list[0][&#39;id&#39;]</a> + ‘/comments’ +’?’ +’start=0’ + ‘&amp;limit=20’</p><h1 id="三句一体"><a href="#三句一体" class="headerlink" title="三句一体"></a>三句一体</h1><p>resp = request.urlopen(requrl)<br>html_data = resp.read().decode(‘utf-8’)<br>soup = bs(html_data, ‘html.parser’)</p><p>comment_div_lists[0].find_all(‘span’, class_=”short”)[0].string # .string 可以暂时理解成中间的字符串</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;这是第一次爬虫，以理论为主，以实现为辅。因为是到处看的，所以不是很有逻辑性。&lt;br&gt;参考链接：&lt;br&gt;&lt;a href=&quot;https://www.cnblogs.com/sss4/p/7809821.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;python爬虫原理&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&amp;amp;mid=2247487461&amp;amp;idx=2&amp;amp;sn=9cf9910f44808e429f32731fb5214380&amp;amp;chksm=ec5ed098db29598e9ce3a99a1ed6005fe552c9fde8533639c561e3ca7c44d71f6f053c68de8e&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0827ZJWXHEkP54zWy7t6FfGV#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Python爬虫的两套解析方法和四种爬虫实现&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.cnblogs.com/linhaifeng/articles/7773496.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;爬虫基本原理&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>git</title>
    <link href="http://yoursite.com/2018/10/04/git/"/>
    <id>http://yoursite.com/2018/10/04/git/</id>
    <published>2018-10-04T08:06:55.000Z</published>
    <updated>2018-11-19T05:03:22.267Z</updated>
    
    <content type="html"><![CDATA[<h1 id="learning-git"><a href="#learning-git" class="headerlink" title="learning git"></a>learning git</h1><a id="more"></a><p>自己之前已经学过一次git了，但是最近在用的时候，仍然感觉不顺手，所以今天趁这个机会，再学一遍，这一次，以命令为主，以原理为辅。</p><h2 id="初始化仓库"><a href="#初始化仓库" class="headerlink" title="初始化仓库"></a>初始化仓库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br></pre></td></tr></table></figure><h2 id="文件到Git仓库"><a href="#文件到Git仓库" class="headerlink" title="文件到Git仓库"></a>文件到Git仓库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git add readme.txt</span><br><span class="line">$ git commit -m <span class="string">"wrote a readme file"</span></span><br></pre></td></tr></table></figure><h2 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><h2 id="查看修改内容"><a href="#查看修改内容" class="headerlink" title="查看修改内容"></a>查看修改内容</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff readme.txt</span><br></pre></td></tr></table></figure><h2 id="查看提交日志"><a href="#查看提交日志" class="headerlink" title="查看提交日志"></a>查看提交日志</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span></span><br></pre></td></tr></table></figure><h2 id="版本回到过去和将来"><a href="#版本回到过去和将来" class="headerlink" title="版本回到过去和将来"></a>版本回到过去和将来</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回到过去</span></span><br><span class="line">git reset --hard HEAD^</span><br><span class="line"><span class="comment"># HEAD~100</span></span><br><span class="line"><span class="comment"># 此时git log已经没有了最新版本的提交信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回到将来</span></span><br><span class="line">git reset --hard 1094a</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">git reflog <span class="comment"># 命令历史</span></span><br><span class="line">git reset --hard 1094a</span><br></pre></td></tr></table></figure><p><img src="https://images2017.cnblogs.com/blog/63651/201709/63651-20170905212837976-775285128.png" alt=""></p><h2 id="撤销修改"><a href="#撤销修改" class="headerlink" title="撤销修改"></a>撤销修改</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时</span></span><br><span class="line">git checkout -- readme.txt</span><br><span class="line"><span class="comment"># 当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，</span></span><br><span class="line">git reset HEAD readme.txt</span><br><span class="line">git checkout -- readme.txt</span><br></pre></td></tr></table></figure><h2 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git rm test.txt</span><br><span class="line">git commit -m <span class="string">"remove"</span></span><br></pre></td></tr></table></figure><h2 id="远程仓库克隆"><a href="#远程仓库克隆" class="headerlink" title="远程仓库克隆"></a>远程仓库克隆</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:michaelliao/gitskills.git</span><br></pre></td></tr></table></figure><h2 id="添加远程仓库"><a href="#添加远程仓库" class="headerlink" title="添加远程仓库"></a>添加远程仓库</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># git://不支持push</span></span><br><span class="line">git remote add origin git://github.com/TJJTJJTJJ/ticgit.git</span><br><span class="line"><span class="comment"># git@只是push</span></span><br><span class="line">git remote add origin git@github.com:TJJTJJTJJ/ticgit.git</span><br><span class="line"><span class="comment"># remove</span></span><br><span class="line">git remote remove origin</span><br></pre></td></tr></table></figure><h2 id="查看远程版本"><a href="#查看远程版本" class="headerlink" title="查看远程版本"></a>查看远程版本</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure><h2 id="提交到远程"><a href="#提交到远程" class="headerlink" title="提交到远程"></a>提交到远程</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><h2 id="获取远程仓库内容"><a href="#获取远程仓库内容" class="headerlink" title="获取远程仓库内容"></a>获取远程仓库内容</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git fetch origin</span><br><span class="line">git merge origin/master</span><br></pre></td></tr></table></figure><h2 id="提交远程仓库"><a href="#提交远程仓库" class="headerlink" title="提交远程仓库"></a>提交远程仓库</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin master</span><br></pre></td></tr></table></figure><p>暂时到这里，剩下的分支，自己暂时还不会用到，等用到了再去学就可以了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;learning-git&quot;&gt;&lt;a href=&quot;#learning-git&quot; class=&quot;headerlink&quot; title=&quot;learning git&quot;&gt;&lt;/a&gt;learning git&lt;/h1&gt;
    
    </summary>
    
      <category term="git" scheme="http://yoursite.com/categories/git/"/>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-chapter10-ImageCaption</title>
    <link href="http://yoursite.com/2018/09/30/pytorch-chapter10-ImageCaption/"/>
    <id>http://yoursite.com/2018/09/30/pytorch-chapter10-ImageCaption/</id>
    <published>2018-09-30T03:00:50.000Z</published>
    <updated>2018-10-04T07:58:45.171Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。<br><a id="more"></a><br>这是我的<a href="https://github.com/TJJTJJTJJ/pytorch__learn" target="_blank" rel="noopener">代码</a><br>大神链接：<a href="https://github.com/anishathalye/neural-style" target="_blank" rel="noopener">https://github.com/anishathalye/neural-style</a><br>这是论文作者写的</p><h1 id="问题以及思考"><a href="#问题以及思考" class="headerlink" title="问题以及思考"></a>问题以及思考</h1><p>这一次感觉写起来很顺利，数据的处理+基本模型的走读基本只用了两天，剩下的两天主要是耗在了beam_searching上，原理的解析和代码的思考。<br>现在记录一下这次走读的过程中学习到的东西，如果是和之前的记录有联系，那么则尽量记在一起。</p><hr><h2 id="局部反向传播管理"><a href="#局部反向传播管理" class="headerlink" title="局部反向传播管理"></a>局部反向传播管理</h2><p>部分参考第八章，基本来自官网文档<br>一共是四种</p><ul><li>@torch.no_grad()</li><li>with torch.no_grad():</li><li>torch.set_grad_enabled(bool)</li><li>with torch.set_grad_enabled(False):</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种：固定上下文管理器，torch.no_grad()和torch.enable_grad()</span></span><br><span class="line">又分为@torch.no_grad()和<span class="keyword">with</span> torch.no_grad()</span><br><span class="line">x = torch.tensor([<span class="number">1</span>], requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print(x.requires_grad)</span><br><span class="line">    y = x*<span class="number">2</span></span><br><span class="line">    print(y.requires_grad)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">y.requires_grad</span><br><span class="line"></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="comment"># 以上说明了上下文管理器内和外是一致的</span></span><br><span class="line"><span class="comment"># 下面说明上下文管理器的作用域只在局部有效</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print(x.requires_grad)</span><br><span class="line">    z = x*<span class="number">2</span></span><br><span class="line">    print(z.requires_grad)</span><br><span class="line">    <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">        print(x.requires_grad)</span><br><span class="line">        y = x*<span class="number">2</span></span><br><span class="line">        print(y.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">    print(x.requires_grad)</span><br><span class="line">    z = x*<span class="number">2</span></span><br><span class="line">    print(z.requires_grad)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        print(x.requires_grad)</span><br><span class="line">        y = x*<span class="number">2</span></span><br><span class="line">        print(y.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>@torch.no_grad()</span><br><span class="line"><span class="meta">... </span><span class="function"><span class="keyword">def</span> <span class="title">dddd</span><span class="params">()</span>:</span></span><br><span class="line"><span class="meta">... </span>    x = torch.tensor([<span class="number">2.2</span>],requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="meta">... </span>    y = <span class="number">2</span>*x</span><br><span class="line"><span class="meta">... </span>    print(y.requires_grad)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dddd()</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种：条件的上下文管理器 torch.set_grad_enabled(bool)</span></span><br><span class="line">又分为<span class="keyword">with</span>  torch.set_grad_enabled(bool)和 torch.set_grad_enabled(bool)</span><br><span class="line"><span class="keyword">with</span> torch.set_grad_enabled(<span class="keyword">False</span>):</span><br><span class="line">    print(x.requires_grad)</span><br><span class="line">    y = x*<span class="number">2</span></span><br><span class="line">    print(y.requires_grad)</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_grad_enabled(<span class="keyword">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试@torch.enable_grad()的时候没有成功，问题应该是版本问题，0.4.0的版本就不行，但是0.4.1的版本就可以了</span></span><br></pre></td></tr></table></figure><hr><h2 id="预训练模型的修改"><a href="#预训练模型的修改" class="headerlink" title="预训练模型的修改"></a>预训练模型的修改</h2><p>备注：感觉这一块应该是很条理才对，但是没有找到类似的说明<br>只能等以后见得多了，再做补充，网上有一些对特定模型的修改，但是都不全面，也没有具体说明各个方法的优劣。<br>应该是这样的，层必须和forward对应，参数的加载可以放在模型定义时，也可以放在模型定义之后。</p><h3 id="不修改原模型的forward流程"><a href="#不修改原模型的forward流程" class="headerlink" title="不修改原模型的forward流程"></a>不修改原模型的forward流程</h3><p>常用于对特定层的修改<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">model = models.resnet50(pretrained=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 只修改最后一层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">fc_features = model.fc.in_features</span><br><span class="line">model.fc = nn.Linear(fc, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">resnet50 = tv.models.resnet50(pretrained=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">del</span> resnet50.fc</span><br><span class="line">resnet50.fc = <span class="keyword">lambda</span> x: x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果直接修改out_features是没有用的</span></span><br><span class="line">model.fc.out_features = <span class="number">9</span></span><br><span class="line">resnet50.fc.weight.shape</span><br><span class="line">torch.Size([<span class="number">1000</span>, <span class="number">2048</span>])</span><br><span class="line">即如果修改某一层，要重新定义这一层</span><br></pre></td></tr></table></figure></p><h3 id="在模型内修改forward流程"><a href="#在模型内修改forward流程" class="headerlink" title="在模型内修改forward流程"></a>在模型内修改forward流程</h3><p>常用于中间层的增加<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要自己先定义类似的网络，注意定义的名字必须一致和方式需要一致，利用state_dict来更新参数</span></span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">resnet50 = models.resnet50(pretrained=<span class="keyword">True</span>)</span><br><span class="line">cnn = CNN(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line">pretrained_dict = resnet50.state_dict()</span><br><span class="line">model_dict = cnn.state_dict()</span><br><span class="line"><span class="comment"># 选取相同名字参数</span></span><br><span class="line">pretrained_dict =  &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items() <span class="keyword">if</span> k <span class="keyword">in</span> model_dict&#125;</span><br><span class="line">model_dict.update(pretrained_dict)</span><br><span class="line">cnn.load_state_dict(model_dict)</span><br><span class="line">print(cnn)</span><br></pre></td></tr></table></figure></p><h3 id="在模型外增加forward流程"><a href="#在模型外增加forward流程" class="headerlink" title="在模型外增加forward流程"></a>在模型外增加forward流程</h3><p>常用与开头或者末尾层的增加<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.add_module(<span class="string">'layer_name'</span>,layer)</span><br><span class="line">可以理解成</span><br><span class="line">self.layer_name = layer</span><br><span class="line">x = model.layer_name(x)</span><br></pre></td></tr></table></figure></p><h3 id="取特定模块，利用children-和nn-Sequential-也可以实现特定层的修改"><a href="#取特定模块，利用children-和nn-Sequential-也可以实现特定层的修改" class="headerlink" title="取特定模块，利用children()和nn.Sequential()也可以实现特定层的修改"></a>取特定模块，利用children()和nn.Sequential()也可以实现特定层的修改</h3><p>这个方法比较啰嗦，不是很推荐，或者不如第一种方法，或者不如最后一种方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model = models.vgg16(pretrained=<span class="keyword">True</span>)</span><br><span class="line">removed = list(model.classifier.children())[:<span class="number">-1</span>]</span><br><span class="line">model.classifier = torch.nn.Sequential(*removed)</span><br><span class="line">model.add_module(<span class="string">'fc'</span>, torch.nn.Linear(<span class="number">4096</span>, out_num)) <span class="comment"># out_num是你希望输出的数量 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接list(model)是不行的，但是list(model.children())就可以</span></span><br><span class="line">list(ResNet34.children())</span><br><span class="line">In [<span class="number">23</span>]: <span class="keyword">for</span> i <span class="keyword">in</span> ResNet34.children():</span><br><span class="line">    ...:     print(type(i))</span><br><span class="line">    ...:     </span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">container</span>.<span class="title">Sequential</span>'&gt;</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">modules</span>.<span class="title">linear</span>.<span class="title">Linear</span>'&gt;</span></span><br></pre></td></tr></table></figure></p><h3 id="取特定模块"><a href="#取特定模块" class="headerlink" title="取特定模块"></a>取特定模块</h3><p>利用list和modulelist，可用于对于特定模块的特定操作，可修改forward流程<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第八章的方法 定义新模型， 在模型定义时，加载原模型参数， 修改forward 对于单向的还好，对于有分支的还没有尝试 用了list和modulelist 直接在定义模型的地方取</span></span><br><span class="line">features = list(vgg16(pretrained=<span class="keyword">True</span>).features)[:<span class="number">23</span>]</span><br><span class="line">self.features = nn.ModuleList(features).eval()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ii, model <span class="keyword">in</span> enumerate(self.features):</span><br><span class="line">    x = model(x)</span><br><span class="line">    <span class="keyword">if</span> ii <span class="keyword">in</span> &#123;<span class="number">3</span>,<span class="number">8</span>,<span class="number">15</span>,<span class="number">22</span>&#125;:</span><br><span class="line">        results.append(x)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> resnet34.named_children():</span><br><span class="line">    print(k,v)</span><br><span class="line"></span><br><span class="line">conv1 Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">3</span>, <span class="number">3</span>), bias=<span class="keyword">False</span>)</span><br><span class="line">bn1 BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>)</span><br><span class="line">relu ReLU(inplace)</span><br><span class="line">maxpool MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">layer1 Sequential(</span><br><span class="line">  (<span class="number">0</span>): BasicBlock(</span><br><span class="line">    (conv1): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="keyword">False</span>)</span><br><span class="line">    (bn1): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>)</span><br><span class="line">    (relu): ReLU(inplace)</span><br><span class="line">    (conv2): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="keyword">False</span>)</span><br><span class="line">    (bn2): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>)</span><br><span class="line">  )</span><br><span class="line">...</span><br></pre></td></tr></table></figure><hr><h2 id="tensor-new和fill-和copy"><a href="#tensor-new和fill-和copy" class="headerlink" title="tensor.new和fill_和copy_"></a>tensor.new和fill_和copy_</h2><h3 id="在第九章，创建同类型的tensor用new-保证类型和cuda一致，不保证requires-grad，保证了和源类型一致，不共享内存"><a href="#在第九章，创建同类型的tensor用new-保证类型和cuda一致，不保证requires-grad，保证了和源类型一致，不共享内存" class="headerlink" title="在第九章，创建同类型的tensor用new,保证类型和cuda一致，不保证requires_grad，保证了和源类型一致，不共享内存"></a>在第九章，创建同类型的tensor用new,保证类型和cuda一致，不保证requires_grad，保证了和源类型一致，不共享内存</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.Tensor([<span class="number">2.2</span>],requires_grad=<span class="keyword">True</span>).cuda()</span><br><span class="line">x</span><br><span class="line">tensor([ <span class="number">3.2000</span>], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">y = x.new([<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">y</span><br><span class="line">y.requires_grad</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line">tensor([ <span class="number">4.</span>,  <span class="number">5.</span>], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">z = x.data.new([<span class="number">6</span>,<span class="number">7</span>])</span><br><span class="line">z.requires_grad</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line">z</span><br><span class="line">tensor([ <span class="number">6.</span>,  <span class="number">7.</span>], device=<span class="string">'cuda:0'</span>)</span><br></pre></td></tr></table></figure><h3 id="在第十章，创建同类型同样大小同cuda的tensor，用fill-，fill-也保证了类型和cuda一致，保证了和目标类型一致"><a href="#在第十章，创建同类型同样大小同cuda的tensor，用fill-，fill-也保证了类型和cuda一致，保证了和目标类型一致" class="headerlink" title="在第十章，创建同类型同样大小同cuda的tensor，用fill_，fill_也保证了类型和cuda一致，保证了和目标类型一致"></a>在第十章，创建同类型同样大小同cuda的tensor，用fill_，fill_也保证了类型和cuda一致，保证了和目标类型一致</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">36</span>]: x = t.Tensor(<span class="number">3</span>,<span class="number">4</span>).cuda()</span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: x</span><br><span class="line">Out[<span class="number">37</span>]: </span><br><span class="line">tensor([[ <span class="number">1.1395e-19</span>,  <span class="number">4.5886e-41</span>,  <span class="number">3.4482e+25</span>,  <span class="number">3.0966e-41</span>],</span><br><span class="line">        [ <span class="number">5.7353e-31</span>,  <span class="number">4.5886e-41</span>, <span class="number">-1.2545e+37</span>,  <span class="number">1.3914e+25</span>],</span><br><span class="line">        [ <span class="number">2.9680e-31</span>,  <span class="number">4.5886e-41</span>,  <span class="number">5.7344e-31</span>,  <span class="number">4.5886e-41</span>]],</span><br><span class="line">       device=<span class="string">'cuda:0'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">38</span>]: x.fill_(<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">38</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], device=<span class="string">'cuda:0'</span>)</span><br><span class="line"><span class="comment"># 测试requires_grad提示，不能</span></span><br><span class="line">In [<span class="number">43</span>]: x.requires_grad= <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">44</span>]: x</span><br><span class="line">Out[<span class="number">44</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], device=<span class="string">'cuda:0'</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">45</span>]: x.fill_(<span class="number">1</span>)</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">RuntimeError                              Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-45</span><span class="number">-0</span>c255de765ba&gt; <span class="keyword">in</span> &lt;module&gt;()</span><br><span class="line">----&gt; 1 x.fill_(1)</span><br><span class="line"></span><br><span class="line">RuntimeError: a leaf Variable that requires grad has been used <span class="keyword">in</span> an <span class="keyword">in</span>-place operation.</span><br><span class="line"><span class="comment"># 强行修改值，则grad_fn也发生了变化。</span></span><br><span class="line">In [<span class="number">46</span>]: x[<span class="number">0</span>]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: x</span><br><span class="line">Out[<span class="number">47</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], device=<span class="string">'cuda:0'</span>, grad_fn=&lt;CopySlices&gt;)</span><br></pre></td></tr></table></figure><h3 id="第十章的copy-，类型不变，"><a href="#第十章的copy-，类型不变，" class="headerlink" title="第十章的copy_，类型不变，"></a>第十章的copy_，类型不变，</h3><p>可以作为计算图进行保留<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不共享内存</span></span><br><span class="line">In [<span class="number">49</span>]: x = t.Tensor(<span class="number">2</span>,<span class="number">2</span>).fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">50</span>]: x</span><br><span class="line">Out[<span class="number">50</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">51</span>]: y = t.Tensor(<span class="number">1</span>,<span class="number">2</span>).fill_(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">52</span>]: y</span><br><span class="line">Out[<span class="number">52</span>]: tensor([[<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">53</span>]: x[<span class="number">0</span>].copy_(y[<span class="number">0</span>])</span><br><span class="line">Out[<span class="number">53</span>]: tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">54</span>]: x</span><br><span class="line">Out[<span class="number">54</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">55</span>]: y</span><br><span class="line">Out[<span class="number">55</span>]: tensor([[<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">56</span>]: y[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">57</span>]: y</span><br><span class="line">Out[<span class="number">57</span>]: tensor([[<span class="number">2.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">58</span>]: x</span><br><span class="line">Out[<span class="number">58</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类型不变</span></span><br><span class="line">In [<span class="number">60</span>]: y = t.IntTensor(<span class="number">1</span>,<span class="number">2</span>).fill_(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">61</span>]: y</span><br><span class="line">Out[<span class="number">61</span>]: tensor([[<span class="number">1</span>, <span class="number">1</span>]], dtype=torch.int32)</span><br><span class="line"></span><br><span class="line">In [<span class="number">62</span>]: x = t.Tensor(<span class="number">2</span>,<span class="number">2</span>).fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">63</span>]: x</span><br><span class="line">Out[<span class="number">63</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">64</span>]: x[<span class="number">0</span>].copy_(y[<span class="number">0</span>])</span><br><span class="line">Out[<span class="number">64</span>]: tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">65</span>]: x</span><br><span class="line">Out[<span class="number">65</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># requires_grad，会作为一个计算图保留</span></span><br><span class="line">In [<span class="number">66</span>]: x</span><br><span class="line">Out[<span class="number">66</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">67</span>]: x.requires_grad=<span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">68</span>]: x</span><br><span class="line">Out[<span class="number">68</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]], requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">69</span>]: x[<span class="number">1</span>].copy_(y[<span class="number">0</span>])</span><br><span class="line">Out[<span class="number">69</span>]: tensor([<span class="number">1.</span>, <span class="number">1.</span>], grad_fn=&lt;AsStridedBackward&gt;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cuda，可以保留</span></span><br><span class="line">In [<span class="number">76</span>]: y[<span class="number">0</span>]=<span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">77</span>]: y</span><br><span class="line">Out[<span class="number">77</span>]: tensor([[<span class="number">2</span>, <span class="number">2</span>]], dtype=torch.int32)</span><br><span class="line"></span><br><span class="line">In [<span class="number">78</span>]: x[<span class="number">1</span>].copy_(y[<span class="number">0</span>])</span><br><span class="line">Out[<span class="number">78</span>]: tensor([<span class="number">2.</span>, <span class="number">2.</span>], device=<span class="string">'cuda:0'</span>, grad_fn=&lt;AsStridedBackward&gt;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">79</span>]: x</span><br><span class="line">Out[<span class="number">79</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>]], device=<span class="string">'cuda:0'</span>, grad_fn=&lt;CopySlices&gt;)</span><br></pre></td></tr></table></figure></p><hr><h2 id="tensor赋值操作-只复制值，不共享内存"><a href="#tensor赋值操作-只复制值，不共享内存" class="headerlink" title="tensor赋值操作   只复制值，不共享内存"></a>tensor赋值操作   只复制值，不共享内存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种 利用tensor 只复制值</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: x = t.tensor([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: x</span><br><span class="line">Out[<span class="number">7</span>]: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: y = t.tensor(x)</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: y</span><br><span class="line">Out[<span class="number">9</span>]: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: x[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: y</span><br><span class="line">Out[<span class="number">11</span>]: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种 利用切片， 只复制值</span></span><br><span class="line">In [<span class="number">12</span>]: y = t.Tensor(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: y[<span class="number">0</span>:<span class="number">2</span>]=x</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: y</span><br><span class="line">Out[<span class="number">14</span>]: tensor([<span class="number">1.0000e+00</span>, <span class="number">4.0000e+00</span>, <span class="number">1.1395e-19</span>, <span class="number">4.5886e-41</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: x[<span class="number">0</span>]=<span class="number">6</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: y</span><br><span class="line">Out[<span class="number">16</span>]: tensor([<span class="number">1.0000e+00</span>, <span class="number">4.0000e+00</span>, <span class="number">1.1395e-19</span>, <span class="number">4.5886e-41</span>])</span><br></pre></td></tr></table></figure><hr><h2 id="t-save"><a href="#t-save" class="headerlink" title="t.save"></a>t.save</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单个变量 不保留名字</span></span><br><span class="line">t.save(x, <span class="string">'a.pth'</span>)</span><br><span class="line">y = t.load(<span class="string">'a.pth'</span>) <span class="comment"># 这个时候已经和x没有任何关系了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多个变量 或者保留名字</span></span><br><span class="line">dic = dict(aa=x, bb=y)</span><br><span class="line">t.save(dic, <span class="string">'a.pth'</span>)</span><br><span class="line">y = t.load(<span class="string">'a.pth'</span>) <span class="comment"># 这个时候已经和dic没有任何关系了，但是aa,bb还保留着</span></span><br><span class="line">y</span><br><span class="line">&#123;<span class="string">'aa'</span>: tensor([[ <span class="number">100.0000</span>,  <span class="number">100.0000</span>,  <span class="number">100.0000</span>,  <span class="number">100.0000</span>],</span><br><span class="line">         [  <span class="number">-0.0000</span>,    <span class="number">0.0000</span>,    <span class="number">0.0000</span>,    <span class="number">0.0000</span>],</span><br><span class="line">         [  <span class="number">-0.0000</span>,    <span class="number">0.0000</span>,   <span class="number">-0.0000</span>,    <span class="number">0.0000</span>]]),</span><br><span class="line"> <span class="string">'bb'</span>: tensor(<span class="number">1.00000e-11</span> *</span><br><span class="line">        [[<span class="number">-0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">-3.9650</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>]])&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="第十章的诡异装饰器"><a href="#第十章的诡异装饰器" class="headerlink" title="第十章的诡异装饰器"></a>第十章的诡异装饰器</h2><p>作者在这里实现了batcha_size的拼接的方式。<br>具体的函数闭包可以参考python<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># def create_collate_fn():</span></span><br><span class="line"><span class="comment">#     def collate_fn():</span></span><br><span class="line"><span class="comment">#         pass</span></span><br><span class="line"><span class="comment">#     return collate_fn</span></span><br></pre></td></tr></table></figure></p><p>来，猜一下这里为什么这么写，函数闭包，根据昨天看的，函数闭包和类函数有的一拼，或者说可以用于创建多个类似的函数，暂时先这么理解，因为还没有太多的用到，在这里的函数闭包是为了实现对作为函数的参数进行传递变量，也就是把函数作为变量传递，这种思想要注意一下。<br>设想几种情况。<br>假设函数h的定义是这样的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="function"><span class="keyword">def</span> <span class="title">h</span><span class="params">(x, f)</span>:</span></span><br><span class="line">   ...:     <span class="string">"""</span></span><br><span class="line"><span class="string">   ...:     Args:</span></span><br><span class="line"><span class="string">   ...:       x: int</span></span><br><span class="line"><span class="string">   ...:       f: function</span></span><br><span class="line"><span class="string">   ...:     """</span></span><br><span class="line">   ...:     out = f(x)</span><br><span class="line">   ...:     <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">   ...:     <span class="keyword">return</span> <span class="number">2</span>*x</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: h(<span class="number">2</span>,f)</span><br><span class="line">Out[<span class="number">3</span>]: <span class="number">4</span></span><br></pre></td></tr></table></figure></p><p>第一种情况，函数f的所有输入都是h可以给的，那么这时候如上所示，直接定义一个函数，然后把函数名或者其他等于函数的变量传进去就可以。<br>第二种情况，函数f的有一部分变量，需要是外界给的，即f的定义中，引用到了不属于h的输入的变量。就像这样。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: <span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(i)</span>:</span></span><br><span class="line">   ...:     <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">   ...:         <span class="keyword">return</span> i*x</span><br><span class="line">   ...:     <span class="keyword">return</span> f</span><br><span class="line">   ...:</span><br><span class="line">   ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: ff = g(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: h(<span class="number">4</span>,ff)</span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">12</span></span><br></pre></td></tr></table></figure></p><p>那么这个时候函数闭包就可以很好地实现这种想法。<br>这是暂时对于函数闭包的理解，但我知道这种想法肯定是有问题的。</p><hr><h2 id="rnn的pack和pad"><a href="#rnn的pack和pad" class="headerlink" title="rnn的pack和pad"></a>rnn的pack和pad</h2><p>from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence, pad_packed_sequence</span><br><span class="line">li_ = [[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">0</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>]]</span><br><span class="line">ten = t.Tensor(li_).long()</span><br><span class="line">pad_variable = ten</span><br><span class="line">embedding = nn.Embedding(<span class="number">5</span>,<span class="number">2</span>)</span><br><span class="line">pad_embeddings = embedding(pad_variable)</span><br><span class="line">lengths = [<span class="number">5</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>]</span><br><span class="line">pad_embeddings</span><br><span class="line"></span><br><span class="line">pad_embeddings</span><br><span class="line"></span><br><span class="line">tensor([[[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">         [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">         [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">         [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">         [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">         [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">         [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">         [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">         [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">         [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">         [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">         [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">         [ <span class="number">0.5581</span>,  <span class="number">0.7382</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">         [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">         [ <span class="number">0.5581</span>,  <span class="number">0.7382</span>],</span><br><span class="line">         [ <span class="number">0.5581</span>,  <span class="number">0.7382</span>]]])</span><br><span class="line"></span><br><span class="line">packed_variable = pack_padded_sequence(pad_embeddings, lengths)</span><br><span class="line">PackedSequence(data=tensor([[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">        [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">        [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">        [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>],</span><br><span class="line">        [ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">        [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">        [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">        [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>],</span><br><span class="line">        [ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">        [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">        [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">        [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>],</span><br><span class="line">        [ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">        [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">        [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">        [ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">        [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>]]), batch_sizes=tensor([ <span class="number">4</span>,  <span class="number">4</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">packed_variable.data.shape</span><br><span class="line">torch.Size([<span class="number">17</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">rnn = nn.LSTM(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">output, hn = rnn(packed_variable)</span><br><span class="line"></span><br><span class="line">output</span><br><span class="line">PackedSequence(data=tensor([[<span class="number">-0.1698</span>, <span class="number">-0.1311</span>,  <span class="number">0.2030</span>],</span><br><span class="line">        [<span class="number">-0.0984</span>, <span class="number">-0.0693</span>,  <span class="number">0.1601</span>],</span><br><span class="line">        [<span class="number">-0.0791</span>, <span class="number">-0.1195</span>,  <span class="number">0.2111</span>],</span><br><span class="line">        [<span class="number">-0.0175</span>,  <span class="number">0.0069</span>,  <span class="number">0.0978</span>],</span><br><span class="line">        [<span class="number">-0.2580</span>, <span class="number">-0.1868</span>,  <span class="number">0.3193</span>],</span><br><span class="line">        [<span class="number">-0.1392</span>, <span class="number">-0.0959</span>,  <span class="number">0.2441</span>],</span><br><span class="line">        [<span class="number">-0.1221</span>, <span class="number">-0.1489</span>,  <span class="number">0.3270</span>],</span><br><span class="line">        [<span class="number">-0.0223</span>,  <span class="number">0.0109</span>,  <span class="number">0.1334</span>],</span><br><span class="line">        [<span class="number">-0.3011</span>, <span class="number">-0.2100</span>,  <span class="number">0.3821</span>],</span><br><span class="line">        [<span class="number">-0.1544</span>, <span class="number">-0.1061</span>,  <span class="number">0.2877</span>],</span><br><span class="line">        [<span class="number">-0.1452</span>, <span class="number">-0.1551</span>,  <span class="number">0.3886</span>],</span><br><span class="line">        [<span class="number">-0.0232</span>,  <span class="number">0.0129</span>,  <span class="number">0.1460</span>],</span><br><span class="line">        [<span class="number">-0.3222</span>, <span class="number">-0.2195</span>,  <span class="number">0.4168</span>],</span><br><span class="line">        [<span class="number">-0.1593</span>, <span class="number">-0.1098</span>,  <span class="number">0.3109</span>],</span><br><span class="line">        [<span class="number">-0.1575</span>, <span class="number">-0.1556</span>,  <span class="number">0.4222</span>],</span><br><span class="line">        [<span class="number">-0.3325</span>, <span class="number">-0.2233</span>,  <span class="number">0.4370</span>],</span><br><span class="line">        [<span class="number">-0.1603</span>, <span class="number">-0.1111</span>,  <span class="number">0.3235</span>]]), batch_sizes=tensor([ <span class="number">4</span>,  <span class="number">4</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">hn[<span class="number">1</span>].shape</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">pad_packed_sequence(packed_variable) </span><br><span class="line">(tensor([[[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">          [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">          [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">          [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"> </span><br><span class="line">         [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">          [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">          [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">          [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"> </span><br><span class="line">         [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">          [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">          [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">          [<span class="number">-0.7936</span>,  <span class="number">0.9621</span>]],</span><br><span class="line"> </span><br><span class="line">         [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">          [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">          [<span class="number">-1.4719</span>, <span class="number">-0.4871</span>],</span><br><span class="line">          [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>]],</span><br><span class="line"> </span><br><span class="line">         [[ <span class="number">0.0256</span>, <span class="number">-1.6445</span>],</span><br><span class="line">          [<span class="number">-0.0939</span>, <span class="number">-0.4070</span>],</span><br><span class="line">          [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">          [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>]]]), tensor([ <span class="number">5</span>,  <span class="number">5</span>,  <span class="number">4</span>,  <span class="number">3</span>]))</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># embedding_dim=3, seq_len=4,3 batch_size=2 即把两句话a,b作为一个batch,空余补0</span></span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line">a = t.ones(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">b = t.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">pad_sequence([a,b])</span><br><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a,b两句话，分别有3,2个词，batch_size=2, 共有3个batch_size，大小分别是2,2,1</span></span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line">a = t.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = t.tensor([<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">pack_sequence([a,b])</span><br><span class="line">PackedSequence(data=tensor([ <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">2</span>,  <span class="number">5</span>,  <span class="number">3</span>]), batch_sizes=tensor([ <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">1</span>]))</span><br></pre></td></tr></table></figure><hr><h2 id="beam-searching"><a href="#beam-searching" class="headerlink" title="beam_searching"></a>beam_searching</h2><p>[参考链接]<a href="https://blog.csdn.net/xljiulong/article/details/51554780" target="_blank" rel="noopener">https://blog.csdn.net/xljiulong/article/details/51554780</a><br>[参考链接]<a href="http://jhave.org/algorithms/graphs/beamsearch/beamsearch.shtml" target="_blank" rel="noopener">http://jhave.org/algorithms/graphs/beamsearch/beamsearch.shtml</a><br>网上讲的大部分都有各自的问题，不是很清晰，只有那篇英文才是标准的，这哥们应该是翻译的，还不错<br>作者使用的是beam_searching的变种，原理类似，但是条件不一致，具体的在代码注释中，不再陈述。</p><hr><h2 id="第十章和第九章关于生成语句的流程的区别"><a href="#第十章和第九章关于生成语句的流程的区别" class="headerlink" title="第十章和第九章关于生成语句的流程的区别"></a>第十章和第九章关于生成语句的流程的区别</h2><p>第十章和第九章在模型生成的地方有两个点不一样，<br>第九章的模型本身可以进行正常的输入与输出，所以第九章也写成这个样子<br>输入(LongTensor) 1<em>1 输出 tensor 1</em>vocabsize<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">results = list(start_words)</span><br><span class="line">start_word_len = len(start_words)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(opt.max_gen_len):</span><br><span class="line">    output, hidden = model(input, hidden)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i &lt; start_word_len:</span><br><span class="line">        w = results[i]</span><br><span class="line">        input = input.data.new([word2ix[w]]).view(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># output size 1×vocab_size [[1,2,3,...]]</span></span><br><span class="line">        <span class="comment"># 这里应该看一下，输出output是个什么东西</span></span><br><span class="line">        top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()</span><br><span class="line">        w = ix2word[top_index]</span><br><span class="line">        results.append(w)</span><br><span class="line">        input = input.data.new([word2ix[w]]).view(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> w == <span class="string">'&lt;EOP&gt;'</span>:</span><br><span class="line">        <span class="keyword">del</span> results[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简写为</span></span><br><span class="line"><span class="comment"># model: embedder rnn classifier</span></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(opt.max_gen_len):</span><br><span class="line">    output, hidden = model(input, hidden) <span class="comment"># input(tensor) 1*1  output(tensor) 1*vocabsize hidden(tensor) 1*1*hidden_dim</span></span><br><span class="line">    top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()</span><br><span class="line">    w = ix2word(top_index)</span><br><span class="line">    results.append(w)</span><br><span class="line">    </span><br><span class="line">    input = input.data.new(word2ix(w)).view(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> results</span><br></pre></td></tr></table></figure></p><p>第十章因为使用了pack_padded_sequence来加速训练，那么训练的模型就不能直接拿来像第九章进行生成，另外第一个字是图片特征的转化而成的，不需要embedding层，而是需要fc层，其实也可以直接拿来用，把captions设置为空就好了,在这里作者没有直接用，直接用好像比较麻烦。而是采用beam_search中把rnn和classifier层传进去，写了一个标准的beam_search函数，即输入是第一个字，输出是beam_size句话，因为设计到其他选词保留的问题，所以直接传入的的是各个分函数，进行自行拼接。也可能是为了复用logprobs = nn.functional.log_softmax(output, dim=1) ## 暂时不清楚这里为什么用log_softmax，是负数啊，大哥，不过大小好像不变<br>在rnn中有一个问题，就是能不能用t.no_grad,会不会影响其向前传播。</p><hr><h2 id="对数据的预处理"><a href="#对数据的预处理" class="headerlink" title="对数据的预处理"></a>对数据的预处理</h2><p>第九章是把对数据的预处理写在了data里面，但事实上，这个数据预处理应该与主模型分开，是属于前一个过程。有什么需要交互的，也是通过文件进行，包括配置。</p><hr><h2 id="新建立的数据结构的对比大小"><a href="#新建立的数据结构的对比大小" class="headerlink" title="新建立的数据结构的对比大小"></a>新建立的数据结构的对比大小</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Caption</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    现在不太确定这个集合是hash_table还是set，感觉是hash_tale,是因为set不需要专门的存储结构。再看看吧</span></span><br><span class="line"><span class="string">    这里应该不是那三个集合，而是集合中的每一个元素，比如G(i),这种，作者应该是重新创建了一种数据结构来用，来进行存储</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      sentence: list(int)</span></span><br><span class="line"><span class="string">      state: tuple(hn, cn) hn:1*1*hidden_dim</span></span><br><span class="line"><span class="string">      logprob: probability</span></span><br><span class="line"><span class="string">      score: 等于logprb或者logprb/len(sentence)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sentence, state, logprob, score, metadata=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          sentence(list): </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.sentence = sentence</span><br><span class="line">        self.state = state</span><br><span class="line">        self.logprob = logprob</span><br><span class="line">        self.score = score</span><br><span class="line">        self.metadata = metadata</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这里我猜是为了实现堆排序的比较。尽管知道是，但是还是不知道为什么</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__cmp__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="string">"""Compares Captions by score."""</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(other, Caption)</span><br><span class="line">        <span class="keyword">if</span> self.score == other.score:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">elif</span> self.score &lt; other.score:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># For Python 3 compatibility (__cmp__ is deprecated).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(other, Caption)</span><br><span class="line">        <span class="keyword">return</span> self.score &lt; other.score</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Also for Python 3 compatibility.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(other, Caption)</span><br><span class="line">        <span class="keyword">return</span> self.score == other.score</span><br></pre></td></tr></table></figure><hr><h2 id="作者在固定长度的列表中，使用了小顶堆的方式，来保证每次取出去的都是最小的。"><a href="#作者在固定长度的列表中，使用了小顶堆的方式，来保证每次取出去的都是最小的。" class="headerlink" title="作者在固定长度的列表中，使用了小顶堆的方式，来保证每次取出去的都是最小的。"></a>作者在固定长度的列表中，使用了小顶堆的方式，来保证每次取出去的都是最小的。</h2><hr><h2 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h2><p>现在还有一个问题就是当一个.py文件里的函数或者类超过2、3个时，应该以什么的方式注释才能更好地让别人知道这个文件里的函数和怎么干的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>第十章的代码在难度上其实已经感觉下降了好多，当然自己又忘了写requires.txt。但是在调试改bug自己就用了三天。其中的bug有的时候自己已经忘记当初是怎么写的了，尴尬。<br>自己训练出来的模型也没有作者声称的那么好，暂时不知道</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>html</title>
    <link href="http://yoursite.com/2018/09/24/html/"/>
    <id>http://yoursite.com/2018/09/24/html/</id>
    <published>2018-09-24T04:20:24.000Z</published>
    <updated>2018-11-04T14:51:31.429Z</updated>
    
    <content type="html"><![CDATA[<p>HTML 教程<br><a id="more"></a></p><h1 id="html-基础"><a href="#html-基础" class="headerlink" title="html 基础"></a>html 基础</h1><p>因为在visdom中的text支持html标签，所以来简单学学html。<br><a href="http://www.runoob.com/html/html-intro.html" target="_blank" rel="noopener">参考链接:菜鸟教程</a><br>[菜鸟工具在线编辑工具]<a href="https://c.runoob.com/front-end/61" target="_blank" rel="noopener">https://c.runoob.com/front-end/61</a><br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>菜鸟教程(runoob.com)<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>我的第一个标题<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>我的第一个段落。<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>实例解析</p><ul><li>\&lt;!DOCTYPE html&gt; 声明为 HTML5 文档</li><li>&lt;\html&gt; 元素是 HTML 页面的根元素</li><li>&lt;\head&gt; 元素包含了文档的元（meta）数据，如 <meta charset="utf-8"> 定义网页编码格式为 utf-8。</li><li>&lt;\title&gt; 元素描述了文档的标题</li><li><body> 元素包含了可见的页面内容</body></li><li>&lt;\h1&gt; 元素定义一个大标题</li><li>&lt;\p&gt; 元素定义一个段落</li></ul><p>网页结构<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span> 页面标题<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span>这是一个标题<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>这是一个段落。<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>这是另一个段落<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>只有<body>与</body>之间的元素才会显示</p><p>标题 </p><h1>-<h6>  <h1>这是一个标题</h1><br>段落 <p>  </p><p>这是一个段落。</p> 段前段后有空行<br>链接 <a>  <a href="http://www.runoob.com" target="_blank" rel="noopener">这是一个链接</a><br>图片 <img>  <img src="/images/logo.png" width="258" height="39"><br><img src="/images/logo.png" width="258" height="39"><p></p><h1 id="html元素"><a href="#html元素" class="headerlink" title="html元素"></a>html元素</h1><p><br>空元素标签<br>html属性  html设置属性，常以键值对的形式出现  <a href="http://www.runoob.com" target="_blank" rel="noopener">这是一个链接</a> 常用属性： class id  style title<br>html<br>水平线 <hr><br>换行 <br><br>注释 <!-- 这是一个注释 --><br>格式化标签<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">b</span>&gt;</span>加粗文本<span class="tag">&lt;/<span class="name">b</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">i</span>&gt;</span>斜体文本<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">code</span>&gt;</span>电脑自动输出<span class="tag">&lt;/<span class="name">code</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">这是 <span class="tag">&lt;<span class="name">sub</span>&gt;</span> 下标<span class="tag">&lt;/<span class="name">sub</span>&gt;</span> 和 <span class="tag">&lt;<span class="name">sup</span>&gt;</span> 上标<span class="tag">&lt;/<span class="name">sup</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">small</span>&gt;</span>这个文本是缩小的<span class="tag">&lt;/<span class="name">small</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">big</span>&gt;</span>这个文本字体放大<span class="tag">&lt;/<span class="name">big</span>&gt;</span></span><br></pre></td></tr></table></figure></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">&lt;pre&gt;</span></span><br><span class="line">此例演示如何使用 <span class="keyword">pre</span> 标签</span><br><span class="line">对空行和    空格</span><br><span class="line">进行控制</span><br><span class="line">&lt;/<span class="keyword">pre</span>&gt;</span><br><span class="line"></span><br><span class="line">此例演示如何使用 <span class="keyword">pre</span> 标签</span><br><span class="line">对空行和    空格</span><br><span class="line">进行控制</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">code</span>&gt;</span>计算机输出<span class="tag">&lt;/<span class="name">code</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">kbd</span>&gt;</span>键盘输入<span class="tag">&lt;/<span class="name">kbd</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">tt</span>&gt;</span>打字机文本<span class="tag">&lt;/<span class="name">tt</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">samp</span>&gt;</span>计算机代码样本<span class="tag">&lt;/<span class="name">samp</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">var</span>&gt;</span>计算机变量<span class="tag">&lt;/<span class="name">var</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br></pre></td></tr></table></figure><p>地址<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">address</span>&gt;</span></span><br><span class="line">Written by <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"mailto:webmaster@example.com"</span>&gt;</span>Jon Doe<span class="tag">&lt;/<span class="name">a</span>&gt;</span>.<span class="tag">&lt;<span class="name">br</span>&gt;</span> </span><br><span class="line">Visit us at:<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">Example.com<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">Box 564, Disneyland<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">USA</span><br><span class="line"><span class="tag">&lt;/<span class="name">address</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>创建电子邮件标签</p><p>首字母缩写和缩写<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">abbr</span> <span class="attr">title</span>=<span class="string">"etcetera"</span>&gt;</span>etc.<span class="tag">&lt;/<span class="name">abbr</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">acronym</span> <span class="attr">title</span>=<span class="string">"World Wide Web"</span>&gt;</span>WWW<span class="tag">&lt;/<span class="name">acronym</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>文字显示方向<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;p&gt;<span class="xml"><span class="tag">&lt;<span class="name">bdo</span> <span class="attr">dir</span>=<span class="string">"rtl"</span>&gt;</span>该段落文字从右到左显示。<span class="tag">&lt;/<span class="name">bdo</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br></pre></td></tr></table></figure></p><p>块引用<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;p&gt;WWF's goal <span class="keyword">is</span> <span class="keyword">to</span>: </span><br><span class="line">&lt;q&gt;Build a future <span class="keyword">where</span> people live <span class="keyword">in</span> harmony <span class="keyword">with</span> nature.&lt;/q&gt;</span><br><span class="line">We hope they succeed.&lt;/p&gt;</span><br></pre></td></tr></table></figure></p><p>删除字和插入字的效果<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>My favorite color is <span class="tag">&lt;<span class="name">del</span>&gt;</span>blue<span class="tag">&lt;/<span class="name">del</span>&gt;</span> <span class="tag">&lt;<span class="name">ins</span>&gt;</span>red<span class="tag">&lt;/<span class="name">ins</span>&gt;</span>!<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>html 链接属性 target:定义文档在哪个窗口打开 id属性<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;a <span class="attribute">href</span>=<span class="string">"http://www.runoob.com/"</span> <span class="attribute">target</span>=<span class="string">"_blank"</span>&gt;访问菜鸟教程!&lt;/a&gt;</span><br></pre></td></tr></table></figure></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"#C4"</span>&gt;</span>查看章节 4<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h2</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">id</span>=<span class="string">"C4"</span>&gt;</span>章节 4<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>这边显示该章节的内容……<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="html"><a href="#html" class="headerlink" title="html "></a>html <head></head></h1><p><title>, <style>, <meta>, <link>, <script>, <noscript>, and <base>.</p><p><title> </p><ul><li>定义了浏览器工具栏的标题</li><li>当网页添加到收藏夹时，显示在收藏夹中的标题</li><li>显示在搜索引擎结果页面的标题<br><base> 标签描述了基本的链接地址/链接目标，该标签作为HTML文档中所有的链接标签的默认链接 <figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;base <span class="attribute">href</span>=<span class="string">"http://www.runoob.com/images/"</span> <span class="attribute">target</span>=<span class="string">"_blank"</span>&gt;</span><br></pre></td></tr></table></figure></li></ul><p><link> 标签定义了文档与外部资源之间的关系。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;link <span class="attribute">rel</span>=<span class="string">"stylesheet"</span> <span class="attribute">type</span>=<span class="string">"text/css"</span> <span class="attribute">href</span>=<span class="string">"mystyle.css"</span>&gt;</span><br></pre></td></tr></table></figure></p><p><style> 标签定义了HTML文档的样式文件引用地址.<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="css"><span class="selector-tag">body</span> &#123;<span class="attribute">background-color</span>:yellow&#125;</span></span><br><span class="line"><span class="css"><span class="selector-tag">p</span> &#123;<span class="attribute">color</span>:blue&#125;</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p><meta> 元素 META 元素通常用于指定网页的描述，关键词，文件的最后修改时间，作者，和其他元数据。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;meta <span class="attribute">name</span>=<span class="string">"keywords"</span> <span class="attribute">content</span>=<span class="string">"HTML, CSS, XML, XHTML, JavaScript"</span>&gt;</span><br><span class="line">&lt;meta <span class="attribute">name</span>=<span class="string">"description"</span> <span class="attribute">content</span>=<span class="string">"免费 Web &amp; 编程 教程"</span>&gt;</span><br><span class="line">&lt;meta <span class="attribute">http-equiv</span>=<span class="string">"refresh"</span> <span class="attribute">content</span>=<span class="string">"30"</span>&gt;</span><br></pre></td></tr></table></figure></p><h1 id="HTML-样式-CSS"><a href="#HTML-样式-CSS" class="headerlink" title="HTML 样式- CSS"></a>HTML 样式- CSS</h1><p>CSS (Cascading Style Sheets) 用于渲染HTML元素标签的样式.<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 使用添加到 <span class="tag">&lt;<span class="name">head</span>&gt;</span> 部分的样式信息对 HTML 进行格式化 内部样式表 应用于单个文件</span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>菜鸟教程(runoob.com)<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="css"><span class="selector-tag">h1</span> &#123;<span class="attribute">color</span>:red;&#125;</span></span><br><span class="line"><span class="css"><span class="selector-tag">p</span> &#123;<span class="attribute">color</span>:blue;&#125;</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"># 使用 style 属性制作一个没有下划线的链接 内联样式 应用于个别元素</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.runoob.com/"</span> <span class="attr">style</span>=<span class="string">"text-decoration:none;"</span>&gt;</span>访问 runoob.com!<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">#  标签链接到一个外部样式表 外部引用</span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>菜鸟教程(runoob.com)<span class="tag">&lt;/<span class="name">title</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"styles.css"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">font-family（字体），color（颜色），和font-size（字体大小）， text-align（文字对齐）</span><br><span class="line"></span><br><span class="line"># 图像</span><br></pre></td></tr></table></figure></p><p><img src="smiley.gif" alt="Smiley face" style="float:left" width="32" height="32"><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 表格</span></span><br></pre></td></tr></table></figure></p><p><table border="1"><br>    <tr><br>        <th>Header 1</th><br>        <th>Header 2</th><br>    </tr><br>    <tr><br>        <td>row 1, cell 1</td><br>        <td>row 1, cell 2</td><br>    </tr><br>    <tr><br>        <td>row 2, cell 1</td><br>        <td>row 2, cell 2</td><br>    </tr><br></table><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列表</span></span><br></pre></td></tr></table></figure></p><ul><br>  <li>Coffee</li><br>  <li>Tea</li><br>  <li>Milk</li><br></ul><ol><br>  <li>Coffee</li><br>  <li>Tea</li><br>  <li>Milk</li><br></ol><ol start="50"><br>  <li>Coffee</li><br>  <li>Tea</li><br>  <li>Milk</li><br></ol><p><code>`</code></p><h1 id="区块-块级和-内联级"><a href="#区块-块级和-内联级" class="headerlink" title="区块  块级和 内联级"></a>区块 <div> 块级和<span> 内联级</h1><h1 id="表单"><a href="#表单" class="headerlink" title="表单"></a>表单</h1><hr><p>2018-10-05</p><h1 id="前言：主要是为了爬虫做一些补充，此次主要是为了爬虫做补充，不涉及html的理论"><a href="#前言：主要是为了爬虫做一些补充，此次主要是为了爬虫做补充，不涉及html的理论" class="headerlink" title="前言：主要是为了爬虫做一些补充，此次主要是为了爬虫做补充，不涉及html的理论"></a>前言：主要是为了爬虫做一些补充，此次主要是为了爬虫做补充，不涉及html的理论</h1><h2 id=""><a href="#" class="headerlink" title=""></a><script></h2><p>用于加载脚本文件，指一段javascript代码，暂时不影响后续操作。</p><h2 id="amp"><a href="#amp" class="headerlink" title="&amp;"></a><div>&amp;<span></h2><p>区块 <div> 块级和<span> 内联级，本身没有太强的含义，前者以新行显示，后者不以新行显示。主要是作为容器存在，用于布局。</p><h2 id="id-amp-class"><a href="#id-amp-class" class="headerlink" title="id&amp;class"></a>id&amp;class</h2><p>id具有唯一性，在一个网页内同一个命名只能使用一次，定义以#开头<br>class命名的类可以在一个网页使用无数次，定义以.开头<br>但两者都是定义的样式而已。</p></style></title></p></a></h6></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HTML 教程&lt;br&gt;
    
    </summary>
    
      <category term="html" scheme="http://yoursite.com/categories/html/"/>
    
    
      <category term="html" scheme="http://yoursite.com/tags/html/"/>
    
  </entry>
  
  <entry>
    <title>python-picture</title>
    <link href="http://yoursite.com/2018/09/24/python-picture/"/>
    <id>http://yoursite.com/2018/09/24/python-picture/</id>
    <published>2018-09-24T04:08:02.000Z</published>
    <updated>2018-11-19T05:40:14.908Z</updated>
    
    <content type="html"><![CDATA[<h1 id="针对图像的操作-cv2-matplotlib-pylab-PIL-Image"><a href="#针对图像的操作-cv2-matplotlib-pylab-PIL-Image" class="headerlink" title="针对图像的操作 cv2, matplotlib.pylab, PIL.Image"></a>针对图像的操作 cv2, matplotlib.pylab, PIL.Image</h1><p>一般用PIL.Image或者cv2来打开或者保存，用matplotlib.pylab来显示<br>在pytorch中也可以用tv.utils.save_image()专门来保存图片。<br><a id="more"></a></p><h2 id="cv2"><a href="#cv2" class="headerlink" title="cv2"></a>cv2</h2><p><a href="https://www.jianshu.com/p/3977d674da85" target="_blank" rel="noopener">参考链接</a><br>注意：pylab.imread和PIL.Image.open读入的都是RBG顺序，而cv2.imread读入的是BGR顺序，混合使用的时候要特别注意<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'examples.png'</span>) <span class="comment"># # 默认是读入为彩色图，即使原图是灰度图也会复制成三个相同的通道变成彩色图</span></span><br><span class="line">img_gray = cv2.imread(<span class="string">'examples.png'</span>,<span class="number">0</span>) <span class="comment"># 第二个参数为0的时候读入为灰度图，即使原图是彩色图也会转成灰度图</span></span><br><span class="line">print(type(img), img.dtype, np.min(img), np.max(img))</span><br><span class="line">print(img.shape)</span><br><span class="line">print(img_gray.shape)</span><br><span class="line"></span><br><span class="line">(&lt;type <span class="string">'numpy.ndarray'</span>&gt;, dtype(<span class="string">'uint8'</span>), <span class="number">0</span>, <span class="number">255</span>)    <span class="comment"># opencv读进来的是numpy数组，类型是uint8，0-255</span></span><br><span class="line">(<span class="number">824</span>, <span class="number">987</span>, <span class="number">3</span>)    <span class="comment"># 彩色图3通道</span></span><br><span class="line">(<span class="number">824</span>, <span class="number">987</span>)    <span class="comment"># 灰度图单通道</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示</span></span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">img = cv2.imread(<span class="string">'examples.png'</span>) </span><br><span class="line">plt.imshow(img[..., <span class="number">-1</span>::<span class="number">-1</span>]) <span class="comment"># 因为opencv读取进来的是bgr顺序呢的，而imshow需要的是rgb顺序，因此需要先反过来 plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 灰度与RGB转化</span></span><br><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> plt </span><br><span class="line">img = cv2.imread(<span class="string">'examples.png'</span>) </span><br><span class="line">img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) <span class="comment"># BGR转灰度 </span></span><br><span class="line">img_bgr = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR) <span class="comment"># 灰度转BRG </span></span><br><span class="line">img_rgb = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB) <span class="comment"># 也可以灰度转RGB</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 保存图片</span></span><br><span class="line"><span class="keyword">import</span> cv2 img = cv2.imread(<span class="string">'examples.png'</span>) <span class="comment"># 这是BGR图片 </span></span><br><span class="line">cv2.imwrite(<span class="string">'examples2.png'</span>, img) <span class="comment"># 这里也应该用BGR图片保存，这里要非常注意，因为用pylab或PIL读入的图片都是RGB的，如果要用opencv存图片就必须做一个转换 </span></span><br><span class="line">img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) </span><br><span class="line">cv2.imwrite(<span class="string">'examples_gray.png'</span>, img_gray)</span><br></pre></td></tr></table></figure></p><h2 id="matplotlib-pylab"><a href="#matplotlib-pylab" class="headerlink" title="matplotlib.pylab"></a>matplotlib.pylab</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = plt.imread(<span class="string">'examples.png'</span>)</span><br><span class="line">print(type(img), img.dtype, np.min(img), np.max(img))</span><br><span class="line">[out]</span><br><span class="line">(&lt;type <span class="string">'numpy.ndarray'</span>&gt;, dtype(<span class="string">'float32'</span>), <span class="number">0.0</span>, <span class="number">1.0</span>)    <span class="comment"># matplotlib读取进来的图片是float，0-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示</span></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存</span></span><br><span class="line"><span class="comment"># 有两种</span></span><br><span class="line"><span class="comment"># 其实产生这个现象的原因很简单：在 plt.show() 后调用了 plt.savefig() ，在 plt.show() 后实际上已经创建了一个新的空白的图片（坐标轴），这时候你再 plt.savefig() 就会保存这个新生成的空白图片</span></span><br><span class="line"><span class="comment"># ref:(https://blog.csdn.net/u010099080/article/details/52912439)</span></span><br><span class="line"><span class="comment"># 第一种:在plt.show之前保存</span></span><br><span class="line">plt.savefig(<span class="string">'test.png'</span>)</span><br><span class="line"><span class="comment"># 第二种:画图的时候保存句柄</span></span><br><span class="line">fig = plt.gcf()</span><br><span class="line">plt.show()</span><br><span class="line">fig.savefig(<span class="string">'test.png'</span>)</span><br></pre></td></tr></table></figure><h2 id="PIL-Image"><a href="#PIL-Image" class="headerlink" title="PIL.Image"></a>PIL.Image</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = Image.open(<span class="string">'examples.jpg'</span>)</span><br><span class="line">print(type(img), img.dtype, np.min(img), np.max(img))</span><br><span class="line">img = np.array(img)     <span class="comment"># 将PIL格式图片转为numpy格式</span></span><br><span class="line">image_pil = Image.fromarray(image_numpy)</span><br><span class="line">print(type(img), img.dtype, np.min(img), np.max(img))</span><br><span class="line"></span><br><span class="line">(&lt;class 'PIL.PngImagePlugin.PngImageFile'&gt;, 0, 255) # 注意，PIL是有自己的数据结构的，但是可以转换成numpy数组 </span><br><span class="line">(&lt;type <span class="string">'numpy.ndarray'</span>&gt;, dtype(<span class="string">'uint8'</span>), <span class="number">0</span>, <span class="number">255</span>) <span class="comment"># 和用matplotlib读取不同，PIL和matlab相同，读进来图片和其存储在硬盘的样子是一样的，uint8，0-255</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 灰度和RGB转化</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image </span><br><span class="line">img = Image.open(<span class="string">'examples.png'</span>) </span><br><span class="line">img_gray = img.convert(<span class="string">'L'</span>) <span class="comment"># RGB转换成灰度图像 i</span></span><br><span class="line">mg_rgb = img_gray.convert(<span class="string">'RGB'</span>) <span class="comment"># 灰度转RGB </span></span><br><span class="line">print(img) </span><br><span class="line">print(img_gray) </span><br><span class="line">print(img_rgb) </span><br><span class="line">[out] </span><br><span class="line">&lt;PIL.PngImagePlugin.PngImageFile image mode=RGB size=<span class="number">987</span>x824 at <span class="number">0x7FC2CCAE04D0</span>&gt; </span><br><span class="line">&lt;PIL.Image.Image image mode=L size=<span class="number">987</span>x824 at <span class="number">0x7FC2CCAE0990</span>&gt; </span><br><span class="line">&lt;PIL.Image.Image image mode=RGB size=<span class="number">987</span>x824 at <span class="number">0x7FC2CCAE0250</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = Image.open(<span class="string">'examples.png'</span>) </span><br><span class="line">img_gray = img.convert(<span class="string">'L'</span>) <span class="comment">#转换成灰度图像 </span></span><br><span class="line">img = np.array(img) </span><br><span class="line">img_gray = np.array(img_gray) </span><br><span class="line">plt.imshow(img) <span class="comment"># or plt.imshow(img / 255.0)，matplotlib和matlab一样，如果是float类型的图像，范围是0-1才能正常imshow，如果是uint8图像，范围则需要是0-255 </span></span><br><span class="line">plt.show() </span><br><span class="line">plt.imshow(img_gray, cmap=plt.gray()) <span class="comment"># 显示灰度图要设置cmap参数 </span></span><br><span class="line">plt.show() </span><br><span class="line">plt.imshow(Image.open(<span class="string">'examples.png'</span>)) <span class="comment"># 实际上plt.imshow可以直接显示PIL格式图像 plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存</span></span><br><span class="line">img = Image.open(<span class="string">'examples.png'</span>) </span><br><span class="line">img.save(<span class="string">'examples2.png'</span>) </span><br><span class="line">img_gray = img.convert(<span class="string">'L'</span>) </span><br><span class="line">img_gray.save(<span class="string">'examples_gray.png'</span>) <span class="comment"># 不管是灰度还是彩色，直接用save函数保存就可以，但注意，只有PIL格式的图片能够用save函数</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;针对图像的操作-cv2-matplotlib-pylab-PIL-Image&quot;&gt;&lt;a href=&quot;#针对图像的操作-cv2-matplotlib-pylab-PIL-Image&quot; class=&quot;headerlink&quot; title=&quot;针对图像的操作 cv2, matplotlib.pylab, PIL.Image&quot;&gt;&lt;/a&gt;针对图像的操作 cv2, matplotlib.pylab, PIL.Image&lt;/h1&gt;&lt;p&gt;一般用PIL.Image或者cv2来打开或者保存，用matplotlib.pylab来显示&lt;br&gt;在pytorch中也可以用tv.utils.save_image()专门来保存图片。&lt;br&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python,picture" scheme="http://yoursite.com/tags/python-picture/"/>
    
  </entry>
  
  <entry>
    <title>CMC</title>
    <link href="http://yoursite.com/2018/09/24/CMC/"/>
    <id>http://yoursite.com/2018/09/24/CMC/</id>
    <published>2018-09-24T04:00:17.000Z</published>
    <updated>2018-11-05T14:43:50.570Z</updated>
    
    <content type="html"><![CDATA[<h1 id="rank-1-rank-5-mAP"><a href="#rank-1-rank-5-mAP" class="headerlink" title="rank-1,rank-5,mAP"></a>rank-1,rank-5,mAP</h1><a id="more"></a><h2 id="第一种，网上的标准计算公式"><a href="#第一种，网上的标准计算公式" class="headerlink" title="第一种，网上的标准计算公式"></a>第一种，网上的标准计算公式</h2><p><a href="https://blog.csdn.net/u013698770/article/details/60776102" target="_blank" rel="noopener">https://blog.csdn.net/u013698770/article/details/60776102</a><br><a href="https://blog.csdn.net/zkp_987/article/details/79969512" target="_blank" rel="noopener">https://blog.csdn.net/zkp_987/article/details/79969512</a><br><a href="https://blog.csdn.net/kaixinjiuxing666/article/details/81272796" target="_blank" rel="noopener">https://blog.csdn.net/kaixinjiuxing666/article/details/81272796</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">indexxx = np.array([[<span class="number">8</span>,<span class="number">9</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">9</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">0</span>],[<span class="number">7</span>,<span class="number">9</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">0</span>]])</span><br><span class="line">good_index = np.array([<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>])</span><br><span class="line">CMC = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">mAP = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> indexxx:</span><br><span class="line">    cmc = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    index = i</span><br><span class="line">    ngood = len(good_index)</span><br><span class="line">    mask = np.in1d(index, good_index)</span><br><span class="line">    rows_good = np.argwhere(mask==<span class="keyword">True</span>)</span><br><span class="line">    rows_good = rows_good.flatten()</span><br><span class="line">    cmc[rows_good[<span class="number">0</span>]:] = <span class="number">1</span></span><br><span class="line">    print(<span class="string">'cmc:'</span>,cmc)</span><br><span class="line">    CMC += cmc</span><br><span class="line"></span><br><span class="line">    ap = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(ngood):</span><br><span class="line">        d_recall = <span class="number">1.0</span>/ngood</span><br><span class="line">        precision = (i+<span class="number">1</span>)*<span class="number">1.0</span>/(rows_good[i]+<span class="number">1</span>)</span><br><span class="line">        ap = ap + d_recall*precision</span><br><span class="line">    print(<span class="string">'ap:&#123;:.2f&#125;%:'</span>.format(<span class="number">100</span>*ap))</span><br><span class="line">    mAP += ap</span><br><span class="line"></span><br><span class="line">CMC = CMC/<span class="number">3</span></span><br><span class="line">mAP = mAP/<span class="number">3</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'top1:&#123;:.2f&#125;%  top5:&#123;:.2f&#125;%  mAP:&#123;:.2f&#125;%'</span>.format(<span class="number">100</span>*CMC[<span class="number">0</span>],<span class="number">100</span>*CMC[<span class="number">4</span>],<span class="number">100</span>*mAP))</span><br><span class="line"></span><br><span class="line">cmc: [<span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line">ap:<span class="number">54.54</span>%</span><br><span class="line">cmc: [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line">ap:<span class="number">69.26</span>%</span><br><span class="line">cmc: [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line">ap:<span class="number">100.00</span>%</span><br><span class="line">top1:<span class="number">66.67</span>%  top5:<span class="number">100.00</span>%  mAP:<span class="number">74.60</span>%</span><br></pre></td></tr></table></figure></p><h3 id="第二种：baseline的计算公式"><a href="#第二种：baseline的计算公式" class="headerlink" title="第二种：baseline的计算公式"></a>第二种：baseline的计算公式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_mAP</span><span class="params">(index, good_index, junk_index)</span>:</span></span><br><span class="line">    ap = <span class="number">0</span></span><br><span class="line">    cmc = torch.IntTensor(len(index)).zero_()</span><br><span class="line">    <span class="keyword">if</span> good_index.size==<span class="number">0</span>:   <span class="comment"># if empty</span></span><br><span class="line">        cmc[<span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">return</span> ap,cmc</span><br><span class="line"></span><br><span class="line">    <span class="comment"># remove junk_index</span></span><br><span class="line">    mask = np.in1d(index, junk_index, invert=<span class="keyword">True</span>)</span><br><span class="line">    index = index[mask]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># find good_index index</span></span><br><span class="line">    ngood = len(good_index)</span><br><span class="line">    mask = np.in1d(index, good_index)</span><br><span class="line">    rows_good = np.argwhere(mask==<span class="keyword">True</span>)</span><br><span class="line">    rows_good = rows_good.flatten()</span><br><span class="line">    </span><br><span class="line">    cmc[rows_good[<span class="number">0</span>]:] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(ngood):</span><br><span class="line">        d_recall = <span class="number">1.0</span>/ngood</span><br><span class="line">        precision = (i+<span class="number">1</span>)*<span class="number">1.0</span>/(rows_good[i]+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> rows_good[i]!=<span class="number">0</span>:</span><br><span class="line">            old_precision = i*<span class="number">1.0</span>/rows_good[i]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            old_precision=<span class="number">1.0</span></span><br><span class="line">        ap = ap + d_recall*(old_precision + precision)/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ap, cmc</span><br><span class="line"></span><br><span class="line">CMC = torch.IntTensor(len(gallery_label)).zero_()</span><br><span class="line">ap = <span class="number">0.0</span></span><br><span class="line"><span class="comment">#print(query_label)</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(query_label)):</span><br><span class="line">    ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)</span><br><span class="line">    <span class="keyword">if</span> CMC_tmp[<span class="number">0</span>]==<span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    CMC = CMC + CMC_tmp</span><br><span class="line">    ap += ap_tmp</span><br><span class="line">    print(i, CMC_tmp[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">CMC = CMC.float()</span><br><span class="line">CMC = CMC/len(query_label) <span class="comment">#average CMC</span></span><br><span class="line">print(<span class="string">'top1:%f top5:%f top10:%f mAP:%f'</span>%(CMC[<span class="number">0</span>],CMC[<span class="number">4</span>],CMC[<span class="number">9</span>],ap/len(query_label)))</span><br></pre></td></tr></table></figure><h3 id="第二种的简化版本，只计算CMC"><a href="#第二种的简化版本，只计算CMC" class="headerlink" title="第二种的简化版本，只计算CMC"></a>第二种的简化版本，只计算CMC</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(qf,ql,qc,gf,gl,gc)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    qf: list [1,2,3]</span></span><br><span class="line"><span class="string">    ql: 1</span></span><br><span class="line"><span class="string">    qc: 1</span></span><br><span class="line"><span class="string">    gf: list [[1,2,3],[1,2,3]]</span></span><br><span class="line"><span class="string">    gl: [1,2,3]</span></span><br><span class="line"><span class="string">    gc: [1,2,3]</span></span><br><span class="line"><span class="string">    len(gf)==len(gl)==len(gc)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    query = qf</span><br><span class="line">    score = np.dot(gf,query)</span><br><span class="line">    <span class="comment"># predict index</span></span><br><span class="line">    index = np.argsort(score)  <span class="comment">#from small to large # 表示位置，[4,3,1,0,2]</span></span><br><span class="line">    index = index[::<span class="number">-1</span>] <span class="comment"># 表示</span></span><br><span class="line">    <span class="comment">#index = index[0:2000]</span></span><br><span class="line">    <span class="comment"># good index</span></span><br><span class="line">    query_index = np.argwhere(gl==ql) <span class="comment"># list [[1],[2]] # 表示位置，即galley中的第几个样本是相同的id</span></span><br><span class="line">    camera_index = np.argwhere(gc==qc) <span class="comment"># list [[1],[2]] # 表示位置</span></span><br><span class="line"></span><br><span class="line">    good_index = np.setdiff1d(query_index, camera_index, assume_unique=<span class="keyword">True</span>) <span class="comment"># [2,3]表示同一个id不同摄像头的图片的位置</span></span><br><span class="line">    junk_index1 = np.argwhere(gl==<span class="number">-1</span>) <span class="comment">#  [[1],[2]] 表示id为-1的图片的位置</span></span><br><span class="line">    junk_index2 = np.intersect1d(query_index, camera_index) <span class="comment">#  [1,2] 表示同一个id同一个摄像头的图片的位置</span></span><br><span class="line">    junk_index = np.append(junk_index2, junk_index1) <span class="comment">#.flatten())</span></span><br><span class="line">    </span><br><span class="line">    CMC_tmp = compute_mAP(index, good_index, junk_index)</span><br><span class="line">    <span class="keyword">return</span> CMC_tmp</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cmc</span><span class="params">(index, good_index, junk_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    index: list [4,3,1,0,2]，已经排序，数字表示第几张图片</span></span><br><span class="line"><span class="string">    good_index: [3,1] list 位置 数字表示第几张图片</span></span><br><span class="line"><span class="string">    junk_index: [4,2]list 位置 数字表示第几张图片</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cmc = torch.IntTensor(len(index)).zero_()</span><br><span class="line">    <span class="keyword">if</span> good_index.size==<span class="number">0</span>:   <span class="comment"># if empty</span></span><br><span class="line">        cmc[<span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">return</span> ap,cmc</span><br><span class="line"></span><br><span class="line">    <span class="comment"># remove junk_index</span></span><br><span class="line">    mask = np.in1d(index, junk_index, invert=<span class="keyword">True</span>)</span><br><span class="line">    index = index[mask]  <span class="comment"># [3,1,0]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># find good_index index</span></span><br><span class="line">    ngood = len(good_index)</span><br><span class="line">    mask = np.in1d(index, good_index) <span class="comment"># [t,t,f]</span></span><br><span class="line">    rows_good = np.argwhere(mask==<span class="keyword">True</span>) <span class="comment"># </span></span><br><span class="line">    rows_good = rows_good.flatten() <span class="comment"># [0,1]</span></span><br><span class="line">    </span><br><span class="line">    cmc[rows_good[<span class="number">0</span>]:] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cmc</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    result = scipy.io.loadmat(<span class="string">'pytorch_result.mat'</span>)</span><br><span class="line">    query_feature = result[<span class="string">'query_f'</span>]  <span class="comment"># list [[1,2,3],[1,2,3],[1,2,3]]</span></span><br><span class="line">    query_cam = result[<span class="string">'query_cam'</span>][<span class="number">0</span>] <span class="comment"># list [1,2,3]</span></span><br><span class="line">    query_label = result[<span class="string">'query_label'</span>][<span class="number">0</span>] <span class="comment"># list [1,2,3]</span></span><br><span class="line">    gallery_feature = result[<span class="string">'gallery_f'</span>]  <span class="comment"># list [[1,2,3],[1,2,3],[1,2,3]]</span></span><br><span class="line">    gallery_cam = result[<span class="string">'gallery_cam'</span>][<span class="number">0</span>] <span class="comment"># list [1,2,3]</span></span><br><span class="line">    gallery_label = result[<span class="string">'gallery_label'</span>][<span class="number">0</span>] <span class="comment"># list [1,2,3]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    CMC = torch.IntTensor(len(gallery_label)).zero_()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(query_label)):</span><br><span class="line">        CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)</span><br><span class="line">        <span class="keyword">if</span> CMC_tmp[<span class="number">0</span>]==<span class="number">-1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        CMC = CMC + CMC_tmp</span><br><span class="line">        print(i, CMC_tmp[<span class="number">0</span>])</span><br><span class="line">    CMC = CMC.float()</span><br><span class="line">    CMC = CMC/len(query_label) <span class="comment">#average CMC</span></span><br><span class="line">    print(<span class="string">'top1:%f top5:%f top10:%f'</span>%(CMC[<span class="number">0</span>],CMC[<span class="number">4</span>],CMC[<span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_id</span><span class="params">(img_path)</span>:</span></span><br><span class="line">    camera_id = []</span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> path, v <span class="keyword">in</span> img_path:</span><br><span class="line">        filename = path.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        label = filename[<span class="number">0</span>:<span class="number">4</span>]</span><br><span class="line">        camera = filename.split(<span class="string">'c'</span>)[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> label[<span class="number">0</span>:<span class="number">2</span>]==<span class="string">'-1'</span>:</span><br><span class="line">            labels.append(<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels.append(int(label))</span><br><span class="line">        camera_id.append(int(camera[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">return</span> camera_id, labels</span><br><span class="line"></span><br><span class="line">gallery_cam,gallery_label = get_id(gallery_path)</span><br><span class="line">query_cam,query_label = get_id(query_path)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;rank-1-rank-5-mAP&quot;&gt;&lt;a href=&quot;#rank-1-rank-5-mAP&quot; class=&quot;headerlink&quot; title=&quot;rank-1,rank-5,mAP&quot;&gt;&lt;/a&gt;rank-1,rank-5,mAP&lt;/h1&gt;
    
    </summary>
    
      <category term="person-reid" scheme="http://yoursite.com/categories/person-reid/"/>
    
    
      <category term="CMC" scheme="http://yoursite.com/tags/CMC/"/>
    
  </entry>
  
  <entry>
    <title>pytorch chapter9 CharRNN</title>
    <link href="http://yoursite.com/2018/09/23/pytorch-chapter9-CharRNN/"/>
    <id>http://yoursite.com/2018/09/23/pytorch-chapter9-CharRNN/</id>
    <published>2018-09-23T15:04:12.000Z</published>
    <updated>2018-11-05T09:21:35.883Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。<br>这是我的<a href="https://github.com/TJJTJJTJJ/pytorch__learn" target="_blank" rel="noopener">代码</a><br>大神链接：<a href="https://github.com/anishathalye/neural-style" target="_blank" rel="noopener">https://github.com/anishathalye/neural-style</a><br>这是论文作者写的<br><a id="more"></a></p><hr><h1 id="问题及其思考"><a href="#问题及其思考" class="headerlink" title="问题及其思考"></a>问题及其思考</h1><h2 id="data是鸭子类型"><a href="#data是鸭子类型" class="headerlink" title="data是鸭子类型"></a>data是鸭子类型</h2><p>因为data作为tensor，已经实现了__getitem__和__len__方法，可以被DataLoader加载。<br>或者说，只要能类似鸭子就可以，这方面掌握得还不熟悉。</p><h2 id="LSTM的输入"><a href="#LSTM的输入" class="headerlink" title="LSTM的输入"></a>LSTM的输入</h2><p>作者明确提出，LSTM的输入类型是(seq_len, batch_size, embedding_dim)，除去embedding_dim，就是(seq_len, batch_size)，原因很简单，LSTM是每次输入一个字，输出一个字，那么输入就是x[0]，对于图像，x[0]就是一张图片，那么对于文字，x[0]也应该就是一个字。好吧，还是说不通，等以后看了相关资料说不定才能理解。</p><h2 id="代码编写"><a href="#代码编写" class="headerlink" title="代码编写"></a>代码编写</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># 变成列表，方便后续的操作，因为start_words的每个字用过之后就没用了，</span><br><span class="line"># 用<span class="keyword">pop</span>不行,因为对于空列表会报错,用None作为结尾标志。可以看出，如果我们想让某个序列正常退出，可以通过设置特殊的结尾来实现。</span><br><span class="line"># 这一段的逻辑有点乱，因为prefix_words可能没有，所以对于start_words，必须先进行一个模型生成。</span><br><span class="line"># 对于或有或无的perfix_words，为了消除其存在对代码和思路的影响，应该保证prefix_words前后的代码状态不变，即</span><br><span class="line"><span class="string">""</span><span class="comment">"</span></span><br><span class="line">第一种</span><br><span class="line">这种保证了output,hidden的状态不变</span><br><span class="line">output, hidden = model(<span class="built_in">input</span>, hidden)</span><br><span class="line"></span><br><span class="line"># step： 对prefix_words进行输入</span><br><span class="line">prefix_words = <span class="string">''</span> <span class="keyword">if</span> prefix_words==None <span class="keyword">else</span> prefix_words    </span><br><span class="line"><span class="keyword">for</span> word in prefix_word<span class="variable">s:</span></span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.data.<span class="keyword">new</span>(word2ix[word]).<span class="keyword">view</span>(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    output, hidden = model(<span class="built_in">input</span>, hidden)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i in <span class="built_in">range</span>(<span class="keyword">opt</span>.max_gen_len-<span class="number">1</span>):</span><br><span class="line">    top_index = output[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()    </span><br><span class="line">    ...</span><br><span class="line">    output, hidden = model(<span class="built_in">input</span>, hidden)</span><br><span class="line"></span><br><span class="line">第二种</span><br><span class="line">这种保证了<span class="built_in">input</span>的状态不变</span><br><span class="line"><span class="keyword">for</span> word in prefix_word<span class="variable">s:</span></span><br><span class="line">    output, hidden = model(<span class="built_in">input</span>, hidden)</span><br><span class="line">    <span class="built_in">input</span> = (<span class="built_in">input</span>.data.<span class="keyword">new</span>([word2ix[word]])).<span class="keyword">view</span>(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i in <span class="built_in">range</span>(<span class="keyword">opt</span>.max_gen_len):</span><br><span class="line">    output, hidden = model(<span class="built_in">input</span>, hidden)</span><br><span class="line">    top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">决定采用第二种，因为代码的主体思路是<span class="keyword">for</span> i in <span class="built_in">range</span>(<span class="keyword">opt</span>.max_gen_len)，prefix_word是插入部分，是可有可无部分。</span><br><span class="line">第一种会造成 top_index与model的切分，不利于后期分析。</span><br><span class="line">或者说，以后碰到这种类型的代码，可以直接跳过中间部分，对后面进行分析。</span><br></pre></td></tr></table></figure><hr><h2 id="os-walk-amp-amp-os-listdir"><a href="#os-walk-amp-amp-os-listdir" class="headerlink" title="os.walk()  &amp;&amp; os.listdir()"></a>os.walk()  &amp;&amp; os.listdir()</h2><h3 id="os-walk"><a href="#os-walk" class="headerlink" title="os.walk()"></a>os.walk()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">6</span>]: <span class="keyword">for</span> i,j,k <span class="keyword">in</span> os.walk(<span class="string">'./'</span>):</span><br><span class="line">   ...:     print(i)</span><br><span class="line">./</span><br><span class="line">./b</span><br><span class="line">./a</span><br><span class="line">./a/aa2</span><br><span class="line">./a/aa1</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: <span class="keyword">for</span> i,j,k <span class="keyword">in</span> os.walk(<span class="string">'./'</span>):</span><br><span class="line">   ...:     print(j)   </span><br><span class="line">[<span class="string">'b'</span>, <span class="string">'a'</span>]</span><br><span class="line">[]</span><br><span class="line">[<span class="string">'aa2'</span>, <span class="string">'aa1'</span>]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="keyword">for</span> i,j,k <span class="keyword">in</span> os.walk(<span class="string">'./'</span>):</span><br><span class="line">   ...:     print(k)</span><br><span class="line">[<span class="string">'1'</span>]</span><br><span class="line">[<span class="string">'cc'</span>, <span class="string">'bb'</span>]</span><br><span class="line">[<span class="string">'aa'</span>]</span><br><span class="line">[<span class="string">'bbb'</span>]</span><br><span class="line">[]</span><br><span class="line"></span><br><span class="line">i+k即可</span><br></pre></td></tr></table></figure><p>os.walk()返回的是当前文件夹下所有的可遍历的文件夹，生成的生成器，i表示文件夹，j表示i文件夹下的文件夹，k表示i文件夹下的文件。以上是os.walk的for用法，下面是直接的用法。对于文件访问，直接i+k，对于文件夹访问，i+j即可。</p><h3 id="os-listdir"><a href="#os-listdir" class="headerlink" title="os.listdir()"></a>os.listdir()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">17</span>]: aa = os.walk(<span class="string">'./'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: bb = list(aa)</span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: bb</span><br><span class="line">Out[<span class="number">19</span>]: </span><br><span class="line">[(<span class="string">'./'</span>, [<span class="string">'b'</span>, <span class="string">'a'</span>], [<span class="string">'1'</span>]),</span><br><span class="line"> (<span class="string">'./b'</span>, [], [<span class="string">'cc'</span>, <span class="string">'bb'</span>]),</span><br><span class="line"> (<span class="string">'./a'</span>, [<span class="string">'aa2'</span>, <span class="string">'aa1'</span>], [<span class="string">'aa'</span>]),</span><br><span class="line"> (<span class="string">'./a/aa2'</span>, [], [<span class="string">'bbb'</span>]),</span><br><span class="line"> (<span class="string">'./a/aa1'</span>, [], [])]</span><br></pre></td></tr></table></figure><p>不知道为什么这里不能直接用 aa,bb,cc = os.walk(‘./‘)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">22</span>]: <span class="keyword">for</span> ii <span class="keyword">in</span> os.listdir(<span class="string">'./'</span>):</span><br><span class="line">    ...:     print(ii)</span><br><span class="line">    ...:     </span><br><span class="line"><span class="number">1</span></span><br><span class="line">b</span><br><span class="line">a</span><br><span class="line">In [<span class="number">23</span>]: aa = os.listdir(<span class="string">'./'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: aa</span><br><span class="line">Out[<span class="number">24</span>]: [<span class="string">'1'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>]</span><br></pre></td></tr></table></figure></p><p>os.listdir()返回的是当前文件夹下的文件夹或者文件。<br>现在碰到的情况是文件夹排列有序，直接访问文件，所以os.list()就可以了。<br>对应的就是<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(src):</span><br><span class="line">      path = os.path.join(src,filename)</span><br></pre></td></tr></table></figure></p><hr><h2 id="小发现"><a href="#小发现" class="headerlink" title="小发现"></a>小发现</h2><ul><li>刚刚发现github上的chinese中的某个文件夹是一个新的github文件。这是个啥情况</li><li>python 可以在函数内部定义函数，是局部域，不能被外界访问，很好，这样就相当于说明了哪些函数是为哪些函数服务的。</li></ul><hr><h2 id="json数据格式的读取"><a href="#json数据格式的读取" class="headerlink" title="json数据格式的读取"></a>json数据格式的读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">25</span>]: <span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: s = &#123;<span class="string">"name"</span>: <span class="string">"ACME"</span>, <span class="string">"shares"</span>: <span class="number">50</span>, <span class="string">"price"</span>: <span class="number">490.1</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: json_str = json.dumps(s)</span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: json_str.__class__</span><br><span class="line">Out[<span class="number">28</span>]: str</span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: json_str</span><br><span class="line">Out[<span class="number">29</span>]: <span class="string">'&#123;"name": "ACME", "shares": 50, "price": 490.1&#125;'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">33</span>]: ss = json.loads(json_str)</span><br><span class="line"></span><br><span class="line">In [<span class="number">34</span>]: ss</span><br><span class="line">Out[<span class="number">34</span>]: &#123;<span class="string">'name'</span>: <span class="string">'ACME'</span>, <span class="string">'shares'</span>: <span class="number">50</span>, <span class="string">'price'</span>: <span class="number">490.1</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="json文件的读取"><a href="#json文件的读取" class="headerlink" title="json文件的读取"></a>json文件的读取</h2><p>第一种:此时data里是该文件内的全部数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(file,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = json.load(f)</span><br></pre></td></tr></table></figure></p><p>第二种：此时data也是该文件内的全部数据，open(file).read()表示读取数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = json.loads(open(file).read())</span><br></pre></td></tr></table></figure></p><p>显然第一种安全，第二种还需要显示地关闭文件<br>可以使用pprint来打印data，好看<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pprint <span class="keyword">import</span> pprint </span><br><span class="line">pprint data</span><br></pre></td></tr></table></figure></p><hr><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><p>普通字符和11个元字符：<br>.  匹配任意除换行符”\n”外的字符(在DOTALL模式中也能匹配换行符 a.c<br>\  转义字符，使后一个字符改变原来的意思<br>*  匹配前一个字符0或多次<br>+  匹配前一个字符1次或无限次<br>?  匹配一个字符0次或1次<br>^  匹配字符串开头。在多行模式中匹配每一行的开头<br>$  匹配字符串末尾，在多行模式中匹配每一行的末尾<br>|  或。匹配|左右表达式任意一个，从左到右匹配，如果|没有包括在()中，则它的范围是整个正则表达式<br>{} {m}匹配前一个字符m次，{m,n}匹配前一个字符m至n次，若省略n，则匹配m至无限次<br>[] 字符集。对应的位置可以是字符集中任意字符。字符集中的字符可以逐个列出，也可以给出范围，如[abc]或[a-c]。[^abc]表示取反，即非abc。<br>所有特殊字符在字符集中都失去其原有的特殊含义。用\反斜杠转义恢复特殊字符的特殊含义。<br>()表达式作为一个整体，可以后接数量词。表达式中的|仅在该组中有效。<br>print re.split(r”;|,|\?”, line1)<br>print re.split(r”[;,?]”, line1)<br>print re.split(r”\W+”, line)<br>不知道为什么<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">para = <span class="string">'-181-欲出未出光辣达，[千山]万山[如火]发（哈哈哈大笑）。须臾[走向][天上]'</span></span><br><span class="line">re.subn(<span class="string">'[\d-]'</span>,<span class="string">''</span>,para)</span><br><span class="line">(<span class="string">'欲出未出光辣达，[千山]万山[如火]发（哈哈哈大笑）。须臾[走向][天上]'</span>, <span class="number">5</span>)</span><br><span class="line">re.subn(<span class="string">'[\d-]*'</span>,<span class="string">''</span>,para)</span><br><span class="line">(<span class="string">'欲出未出光辣达，[千山]万山[如火]发（哈哈哈大笑）。须臾[走向][天上]'</span>, <span class="number">38</span>)</span><br></pre></td></tr></table></figure></p><hr><h2 id="list越界与切片"><a href="#list越界与切片" class="headerlink" title="list越界与切片"></a>list越界与切片</h2><p>list不能越界索引访问，但是对于切片，切片是会自动匹配长度的，所以使用slice不需要担心越界问题。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">s[<span class="number">8</span>] <span class="comment"># 报错</span></span><br><span class="line">s[<span class="number">1</span>] == <span class="number">1</span></span><br><span class="line">s[:<span class="number">1</span>] == [<span class="number">1</span>]</span><br><span class="line">s[<span class="number">1</span>:<span class="number">10</span>] <span class="comment"># return s[1:3]</span></span><br></pre></td></tr></table></figure></p><p>索引位置返回的是元素的副本<br>切片返回的是list的副本</p><hr><h2 id="嵌套列表压平"><a href="#嵌套列表压平" class="headerlink" title="嵌套列表压平"></a>嵌套列表压平</h2><p>func = lambda x: [y for l in x for y in l] if type(x) is list else [x]</p><hr><h2 id="tuple的连接"><a href="#tuple的连接" class="headerlink" title="tuple的连接"></a>tuple的连接</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>,<span class="number">2</span>)+(<span class="number">3</span>,<span class="number">4</span>) == (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 定义只有一个数字的tuple，避免函数歧义</span></span><br><span class="line">t = (<span class="number">1</span>,)</span><br></pre></td></tr></table></figure><h2 id="求list的size"><a href="#求list的size" class="headerlink" title="求list的size"></a>求list的size</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">li = [[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">tu = np.asarray(li).shape</span><br><span class="line"><span class="comment"># shape返回的是tuple型，可以直接拼接</span></span><br></pre></td></tr></table></figure><hr><h2 id="异常触发"><a href="#异常触发" class="headerlink" title="异常触发"></a>异常触发</h2><p><a href="https://www.cnblogs.com/ospider/p/5267766.html" target="_blank" rel="noopener">参考链接</a><a href="http://www.runoob.com/python/python-exceptions.html" target="_blank" rel="noopener">http://www.runoob.com/python/python-exceptions.html</a><br>等总结的时候尝试一下<br>分为捕捉异常和触发异常</p><h3 id="捕捉异常"><a href="#捕捉异常" class="headerlink" title="捕捉异常"></a>捕捉异常</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 捕捉异常第一种 try/except语句</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">&lt;语句&gt;        <span class="comment">#运行别的代码</span></span><br><span class="line"><span class="keyword">except</span> &lt;名字&gt;：</span><br><span class="line">&lt;语句&gt;        <span class="comment">#如果在try部份引发了'name'异常</span></span><br><span class="line"><span class="keyword">except</span> &lt;名字&gt;，&lt;数据&gt;:</span><br><span class="line">&lt;语句&gt;        <span class="comment">#如果引发了'name'异常，获得附加的数据</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">&lt;语句&gt;        <span class="comment">#如果没有异常发生</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 捕捉异常第二种 try/finally</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">&lt;语句&gt;</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">&lt;语句&gt;    <span class="comment">#退出try时总会执行</span></span><br><span class="line"><span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例1</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"> </span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    f = open(<span class="string">'myfile.txt'</span>)</span><br><span class="line">    s = f.readline()</span><br><span class="line">    i = int(s.strip())</span><br><span class="line"><span class="keyword">except</span> OSError <span class="keyword">as</span> err:</span><br><span class="line">    print(<span class="string">"OS error: &#123;0&#125;"</span>.format(err))</span><br><span class="line"><span class="keyword">except</span> ValueError:</span><br><span class="line">    print(<span class="string">"Could not convert data to an integer."</span>)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">"Unexpected error:"</span>, sys.exc_info()[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例2</span></span><br><span class="line">如果你只想知道这是否抛出了一个异常，并不想去处理它，那么一个简单的 <span class="keyword">raise</span> 语句就可以再次把它抛出。</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">raise</span> NameError(<span class="string">'HiThere'</span>)</span><br><span class="line"><span class="keyword">except</span> NameError:</span><br><span class="line">    print(<span class="string">'An exception flew by!'</span>)</span><br><span class="line">    <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例3</span></span><br><span class="line">处理有参数的异常</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">temp_convert</span><span class="params">(var)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> int(var)</span><br><span class="line">    <span class="keyword">except</span> (ValueError) <span class="keyword">as</span> Argument:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"参数没有包含数字\n"</span>, Argument)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用函数</span></span><br><span class="line">temp_convert(<span class="string">"xyz"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 捕捉异常共同使用</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    fh = open(<span class="string">"testfile"</span>, <span class="string">"w"</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        fh.write(<span class="string">"这是一个测试文件，用于测试异常!!"</span>)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"关闭文件"</span></span><br><span class="line">        fh.close()</span><br><span class="line"><span class="keyword">except</span> IOError:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Error: 没有找到文件或读取文件失败"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注</span></span><br><span class="line"><span class="keyword">except</span> (RuntimeError, TypeError, NameError):</span><br></pre></td></tr></table></figure><h3 id="触发异常"><a href="#触发异常" class="headerlink" title="触发异常"></a>触发异常</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 函数触发异常</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">functionName</span><span class="params">( level )</span>:</span></span><br><span class="line">    <span class="keyword">if</span> level &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"Invalid level!"</span>, level)</span><br><span class="line">        <span class="comment"># 触发异常后，后面的代码就不会再执行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 捕捉异常和触发异常的配合</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mye</span><span class="params">( level )</span>:</span></span><br><span class="line">    <span class="keyword">if</span> level &lt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"Invalid level!"</span>)</span><br><span class="line">        <span class="comment"># 触发异常后，后面的代码就不会再执行</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    mye(<span class="number">0</span>)            <span class="comment"># 触发异常</span></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    <span class="keyword">print</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><h3 id="自定义异常"><a href="#自定义异常" class="headerlink" title="自定义异常"></a>自定义异常</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="class"><span class="keyword">class</span> <span class="title">MyError</span><span class="params">(Exception)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, value)</span>:</span></span><br><span class="line">            self.value = value</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> repr(self.value)</span><br><span class="line">   </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">raise</span> MyError(<span class="number">2</span>*<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">except</span> MyError <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">'My exception occurred, value:'</span>, e.value)</span><br></pre></td></tr></table></figure><hr><h2 id="list的更新"><a href="#list的更新" class="headerlink" title="list的更新"></a>list的更新</h2><p>更新分为逐元素更新和逐列表更新</p><h3 id="逐元素更新"><a href="#逐元素更新" class="headerlink" title="逐元素更新"></a>逐元素更新</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># list的逐元素更新</span></span><br><span class="line">li = [] </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    li.append(i)</span><br><span class="line"><span class="comment"># 对于空列表，等价于</span></span><br><span class="line">li = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line"><span class="comment"># 或者等价于generator</span></span><br><span class="line">li = (i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>))</span><br><span class="line"><span class="comment"># 或者转化为逐列表更新,常用与列表头和列表尾同时更新</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    li=[i]+li+[i+<span class="number">11</span>]</span><br><span class="line"><span class="comment"># 或者对于已经得到的元素</span></span><br><span class="line">[a,b,c]</span><br></pre></td></tr></table></figure><p>即对于一个空列表的append，我们总是可以将其转化成列表推导式，<br>并且对于dict和set，只需要将中括号换成大括号即可</p><h3 id="逐列表更新"><a href="#逐列表更新" class="headerlink" title="逐列表更新"></a>逐列表更新</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># list的逐列表更新</span></span><br><span class="line">li = []</span><br><span class="line">ll = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> ll:</span><br><span class="line">    li.extend(i)</span><br><span class="line">li</span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="comment"># 或者对于有限个列表</span></span><br><span class="line">c = a+b</span><br></pre></td></tr></table></figure><p>注：append()和extend()和+=都是在原有列表增加，+是生成一个新的列表<br>感觉逐列表更新应该可以更优美.</p><h2 id="os-path-join"><a href="#os-path-join" class="headerlink" title="os.path.join()"></a>os.path.join()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">42</span>]: <span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">In [<span class="number">43</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'bbbb'</span>,<span class="string">'ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: aaaa/bbbb/ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">44</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'/aaaa'</span>,<span class="string">'bbbb'</span>,<span class="string">'ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: /aaaa/bbbb/ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">45</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'/bbbb'</span>,<span class="string">'ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: /bbbb/ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">46</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'bbbb'</span>,<span class="string">'/ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: /ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'/bbbb'</span>,<span class="string">'/ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: /ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'./bbbb'</span>,<span class="string">'ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: aaaa/./bbbb/ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">49</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'../bbbb'</span>,<span class="string">'ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: aaaa/../bbbb/ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">50</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'bbbb'</span>,<span class="string">'./ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: aaaa/bbbb/./ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">51</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'bbbb'</span>,<span class="string">'../ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: aaaa/bbbb/../ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">54</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'/bbbb'</span>,<span class="string">'....../ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: /bbbb/....../ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">55</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'bbbb/'</span>,<span class="string">'ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: aaaa/bbbb/ccccc.txt</span><br><span class="line"></span><br><span class="line">In [<span class="number">57</span>]: print(<span class="string">"1:"</span>,os.path.join(<span class="string">'aaaa'</span>,<span class="string">'bbbb\\'</span>,<span class="string">'ccccc.txt'</span>))</span><br><span class="line"><span class="number">1</span>: aaaa/bbbb\/ccccc.txt</span><br></pre></td></tr></table></figure><p>对os.path.join()总结如下：从后往前，遇到绝对路径，则绝对路径前面的元素丢弃，遇到类似’…/‘，则将其看成一个普通的路径名字，而对于/在末尾的，会自动根据情况补充。</p><h2 id="list-和-的区别，字符串分割成单个字符"><a href="#list-和-的区别，字符串分割成单个字符" class="headerlink" title="list()和[]的区别，字符串分割成单个字符"></a>list()和[]的区别，字符串分割成单个字符</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">58</span>]: list(<span class="string">'abcd'</span>)</span><br><span class="line">Out[<span class="number">58</span>]: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">59</span>]: [<span class="string">'abcd'</span>]</span><br><span class="line">Out[<span class="number">59</span>]: [<span class="string">'abcd'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">61</span>]: aa = (<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">62</span>]: list(aa)</span><br><span class="line">Out[<span class="number">62</span>]: [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">63</span>]: [aa]</span><br><span class="line">Out[<span class="number">63</span>]: [(<span class="number">1</span>, <span class="number">2</span>)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">65</span>]: bb = [<span class="string">'abc'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">66</span>]: list(*bb)</span><br><span class="line">Out[<span class="number">66</span>]: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">'abcd'</span>:</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><p>即，list()会拆解输入值，拼接成list，可以用在’abcd’这样的字符串直接拆成’a’,’b’,’c’,’d’这样的形式，因为re.split不支持这种拆分法。当然，如果只是单纯地逐元素访问并逐元素地进行操作，我们可以使用for i in ‘abcd’:这样的访问。<br>也可以认为是’’.join()的逆操作<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">65</span>]: bb = [<span class="string">'abc'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">66</span>]: list(*bb)</span><br><span class="line">Out[<span class="number">66</span>]: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">71</span>]: <span class="string">''</span>.join(list(bb))</span><br><span class="line">Out[<span class="number">71</span>]: <span class="string">'abc'</span></span><br></pre></td></tr></table></figure></p><hr><h2 id="list、dict和numpy的互相转换"><a href="#list、dict和numpy的互相转换" class="headerlink" title="list、dict和numpy的互相转换"></a>list、dict和numpy的互相转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">74</span>]: c = np.array(&#123;<span class="string">'a'</span>:<span class="number">1</span>,<span class="string">'b'</span>:<span class="number">2</span>&#125;)</span><br><span class="line">In [<span class="number">75</span>]: c[<span class="number">0</span>]</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">IndexError                                Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-75</span><span class="number">-71463270</span>cd6c&gt; <span class="keyword">in</span> &lt;module&gt;()</span><br><span class="line">----&gt; 1 c[0]</span><br><span class="line"></span><br><span class="line">IndexError: too many indices <span class="keyword">for</span> array</span><br><span class="line"></span><br><span class="line">In [<span class="number">77</span>]: c</span><br><span class="line">Out[<span class="number">77</span>]: array(&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;, dtype=object)</span><br><span class="line">In [<span class="number">79</span>]: c.tolist()</span><br><span class="line">Out[<span class="number">79</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">80</span>]: d = c.tolist()</span><br><span class="line"></span><br><span class="line">In [<span class="number">81</span>]: d.__class__</span><br><span class="line">Out[<span class="number">81</span>]: dict</span><br></pre></td></tr></table></figure><h2 id="shell的基础教程"><a href="#shell的基础教程" class="headerlink" title="shell的基础教程"></a>shell的基础教程</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">for fff in `ls *.json`</span><br><span class="line">do</span><br><span class="line">cconv -f utf8-tw  -t UTF8-CN $fff  -o simplified/$fff</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">for skill in Ada Coffe Action Java; do</span><br><span class="line">    echo "I am good at $&#123;skill&#125;Script"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="shell-传递参数"><a href="#shell-传递参数" class="headerlink" title="shell 传递参数"></a>shell 传递参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> author:菜鸟教程</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> url:www.runoob.com</span></span><br><span class="line"></span><br><span class="line">echo "Shell 传递参数实例！";</span><br><span class="line">echo "执行的文件名：$0";</span><br><span class="line">echo "第一个参数为：$1";</span><br><span class="line">echo "第二个参数为：$2";</span><br><span class="line">echo "第三个参数为：$3";</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh 1 2 3</span></span><br><span class="line">Shell 传递参数实例！</span><br><span class="line">执行的文件名：./test.sh</span><br><span class="line">第一个参数为：1</span><br><span class="line">第二个参数为：2</span><br><span class="line">第三个参数为：3</span><br></pre></td></tr></table></figure><h3 id="shell-流程控制"><a href="#shell-流程控制" class="headerlink" title="shell 流程控制"></a>shell 流程控制</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">if condition1</span><br><span class="line">then</span><br><span class="line">    command1</span><br><span class="line">elif condition2 </span><br><span class="line">then </span><br><span class="line">    command2</span><br><span class="line">else</span><br><span class="line">    commandN</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="Shell-输入-输出重定向"><a href="#Shell-输入-输出重定向" class="headerlink" title="Shell 输入/输出重定向"></a>Shell 输入/输出重定向</h3><p>命令                           说明<br>command &gt; file    将输出重定向到 file。<br>command &lt; file    将输入重定向到 file。<br>command &gt;&gt; file    将输出以追加的方式重定向到 file。<br>n &gt; file    将文件描述符为 n 的文件重定向到 file。<br>n &gt;&gt; file    将文件描述符为 n 的文件以追加的方式重定向到 file。<br>n &gt;&amp; m    将输出文件 m 和 n 合并。<br>n &lt;&amp; m    将输入文件 m 和 n 合并。<br>&lt;&lt; tag    将开始标记 tag 和结束标记 tag 之间的内容作为输入。<br>需要注意的是文件描述符 0 通常是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 如果希望 stderr 重定向到 file，可以这样写：</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">command</span> 2 &gt; file</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果希望将 stdout 和 stderr 合并后重定向到 file</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">command</span> &gt; file 2&gt;&amp;1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">command</span> &gt;&gt; file 2&gt;&amp;1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果希望对 stdin 和 stdout 都重定向</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">command</span> &lt; file1 &gt;file2</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">Here Document 是 Shell 中的一种特殊的重定向方式，用来将输入重定向到一个交互式 Shell 脚本或程序。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> wc -l &lt;&lt; EOF</span></span><br><span class="line">    欢迎来到</span><br><span class="line">    菜鸟教程</span><br><span class="line">    www.runoob.com</span><br><span class="line">EOF</span><br><span class="line">3          # 输出结果为 3 行</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null：</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">command</span> &gt; /dev/null</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果希望屏蔽 stdout 和 stderr，可以这样写：</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">command</span> &gt; /dev/null 2&gt;&amp;1</span></span><br></pre></td></tr></table></figure></p><hr><h2 id="str-split-‘’"><a href="#str-split-‘’" class="headerlink" title="str.split(‘’)"></a>str.split(‘’)</h2><p>以列表形式返回<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">label_dim = <span class="string">'16803+100'</span></span><br><span class="line">cc = label_dim.split(<span class="string">'+'</span>)</span><br><span class="line">cc</span><br><span class="line">[<span class="string">'16803'</span>,<span class="string">'100'</span>]</span><br></pre></td></tr></table></figure></p><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scales_tr = <span class="string">'20,20--20,20'</span></span><br><span class="line">scale = [map(int, x.split(<span class="string">','</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> scales_tr.split(<span class="string">'--'</span>)]</span><br><span class="line">scale</span><br><span class="line">[&lt;map at <span class="number">0x7fd798714748</span>&gt;, &lt;map at <span class="number">0x7fd798714588</span>&gt;]</span><br><span class="line">list(scale[<span class="number">0</span>])</span><br><span class="line">[<span class="number">20</span>,<span class="number">20</span>]</span><br></pre></td></tr></table></figure><hr><h2 id="tensor的拼接-t-cat-t-stack"><a href="#tensor的拼接-t-cat-t-stack" class="headerlink" title="tensor的拼接 t.cat t.stack"></a>tensor的拼接 t.cat t.stack</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> index:</span><br><span class="line"><span class="comment"># tensor的截取与合并  cat, stack,cat+view=stack,stack 新增维度进行合并</span></span><br><span class="line">    result.append(fake_img.data[ii])</span><br><span class="line">tv.utils.save_image(t.stack(result), opt.gen_img, normalize=<span class="keyword">True</span>, range=(<span class="number">-1</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="tensor-view"><a href="#tensor-view" class="headerlink" title="tensor.view()"></a>tensor.view()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = x.view(x.size(<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">x = x.view(x.size()[<span class="number">0</span>],<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><hr><h2 id="Inception-V3"><a href="#Inception-V3" class="headerlink" title="Inception-V3"></a>Inception-V3</h2><p><a href="https://www.cnblogs.com/ospider/p/5267766.html" target="_blank" rel="noopener">参考链接</a><a href="https://www.jianshu.com/p/3bbf0675cfce" target="_blank" rel="noopener">https://www.jianshu.com/p/3bbf0675cfce</a><br><a href="https://blog.csdn.net/loveliuzz/article/details/79135583" target="_blank" rel="noopener">https://blog.csdn.net/loveliuzz/article/details/79135583</a><br>其中都有一些错误，还需要看着源码纠正一下。</p><hr><h2 id="python-文件IO"><a href="#python-文件IO" class="headerlink" title="python 文件IO"></a>python 文件IO</h2><p>python中的三个读read(),readline()和readlines()<br>.read() 每次读取整个文件，它通常用于将文件内容放到一个字符串变量中，然而 .read() 生成文件内容最直接的字符串表示，但对于连续的面向行的处理，它却是不必要的<br>.readlines()之间的差异是后者一次读取整个文件，象 .read()一样。.readlines()自动将文件内容分析成一个行的列表，该列表可以由 Python 的 for… in … 结构进行处理<br>.readline()每次只读取一行</p><h2 id="python打开多个文件"><a href="#python打开多个文件" class="headerlink" title="python打开多个文件"></a>python打开多个文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'a.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> a, open(<span class="string">'b.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> b:</span><br><span class="line">    print(a.read())</span><br><span class="line">    print(b.read())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">open_many</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, files=None, mode=<span class="string">'r'</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> files <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            self._files = []</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._files = files</span><br><span class="line">        self.mode = mode</span><br><span class="line">        self.fds = []  <span class="comment"># fd is short for file descriptor</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'--&gt;enter'</span>)</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> self._files:</span><br><span class="line">            print(<span class="string">'--&gt;opening file'</span>)</span><br><span class="line">            self.fds.append(open(f, self.mode))</span><br><span class="line">        <span class="keyword">return</span> self.fds</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, exc_type, exc_val, traceback)</span>:</span></span><br><span class="line">        print(<span class="string">'--&gt;exit'</span>)</span><br><span class="line">        <span class="keyword">for</span> fd <span class="keyword">in</span> self.fds:</span><br><span class="line">            print(<span class="string">'--&gt;closing file'</span>)</span><br><span class="line">            fd.close()</span><br><span class="line">        <span class="keyword">if</span> exc_type == ValueError:</span><br><span class="line">            print(<span class="string">'--&gt;exception: '</span> + str(exc_val))</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">with</span> open_many([<span class="string">'a.txt'</span>, <span class="string">'b.txt'</span>], <span class="string">'r'</span>) <span class="keyword">as</span> files:</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">print</span> f.read()</span><br><span class="line">    print(<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">with</span> open_many() <span class="keyword">as</span> files:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'captured'</span>)</span><br><span class="line">    print(<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">with</span> open_many() <span class="keyword">as</span> files:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">'uncaptureable'</span>)</span><br></pre></td></tr></table></figure><h2 id="python-csv"><a href="#python-csv" class="headerlink" title="python csv"></a>python csv</h2><p>在使用常规的读取文件的方法的时候，出现了问题，每一个数字包括小数点都被当成了一个字符，这样明显是不对的，对于数字的csv，要考虑下这个方法，我感觉应该和写的方式有关，待续。<br><a href="https://www.cnblogs.com/ospider/p/5267766.html" target="_blank" rel="noopener">参考链接</a><a href="https://www.cnblogs.com/dmir/p/5009075.html" target="_blank" rel="noopener">https://www.cnblogs.com/dmir/p/5009075.html</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(path) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    headers = next(f_csv)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> f_csv:</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">csv_data = pd.read_csv(path)</span><br></pre></td></tr></table></figure><hr><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>优化器与模型参数完全共享内存，一个改变，另一个会立即跟着改变。<br>不能重复加载同一个参数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">optimizer = t.optim.Adam(model.parameters(), lr = <span class="number">0.1</span>)</span><br><span class="line">optimizer</span><br><span class="line"></span><br><span class="line">Adam (</span><br><span class="line">Parameter Group <span class="number">0</span></span><br><span class="line">    amsgrad: <span class="keyword">False</span></span><br><span class="line">    betas: (<span class="number">0.9</span>, <span class="number">0.999</span>)</span><br><span class="line">    eps: <span class="number">1e-08</span></span><br><span class="line">    lr: <span class="number">0.1</span></span><br><span class="line">    weight_decay: <span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    print(i)</span><br><span class="line">    print(<span class="string">'______'</span>)</span><br><span class="line">    </span><br><span class="line">&#123;<span class="string">'params'</span>: [Parameter containing:</span><br><span class="line">tensor([[[[ <span class="number">0.0493</span>,  <span class="number">0.1696</span>,  <span class="number">0.0647</span>],</span><br><span class="line">          [ <span class="number">0.1935</span>,  <span class="number">0.3102</span>, <span class="number">-0.0871</span>],</span><br><span class="line">          [<span class="number">-0.2787</span>,  <span class="number">0.0894</span>, <span class="number">-0.0438</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2671</span>,  <span class="number">0.2079</span>,  <span class="number">0.2474</span>],</span><br><span class="line">          [ <span class="number">0.2068</span>, <span class="number">-0.1825</span>,  <span class="number">0.1427</span>],</span><br><span class="line">          [<span class="number">-0.0853</span>, <span class="number">-0.1799</span>, <span class="number">-0.2465</span>]]]]), Parameter containing:</span><br><span class="line">tensor([<span class="number">-0.3158</span>,  <span class="number">0.1429</span>]), Parameter containing:</span><br><span class="line">tensor([[[[ <span class="number">0.2063</span>,  <span class="number">0.0771</span>,  <span class="number">0.1579</span>],</span><br><span class="line">          [ <span class="number">0.1543</span>,  <span class="number">0.1374</span>, <span class="number">-0.1951</span>],</span><br><span class="line">          [<span class="number">-0.1221</span>,  <span class="number">0.0099</span>, <span class="number">-0.1331</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">-0.1899</span>,  <span class="number">0.1978</span>,  <span class="number">0.1065</span>],</span><br><span class="line">          [ <span class="number">0.1400</span>, <span class="number">-0.0740</span>,  <span class="number">0.0397</span>],</span><br><span class="line">          [<span class="number">-0.2165</span>, <span class="number">-0.0180</span>,  <span class="number">0.1072</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.0692</span>, <span class="number">-0.1296</span>,  <span class="number">0.0524</span>],</span><br><span class="line">          [ <span class="number">0.0577</span>, <span class="number">-0.1184</span>,  <span class="number">0.0697</span>],</span><br><span class="line">          [ <span class="number">0.0859</span>, <span class="number">-0.2086</span>,  <span class="number">0.0419</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">-0.0270</span>,  <span class="number">0.1836</span>, <span class="number">-0.0649</span>],</span><br><span class="line">          [ <span class="number">0.1680</span>, <span class="number">-0.1061</span>, <span class="number">-0.2357</span>],</span><br><span class="line">          [<span class="number">-0.0408</span>,  <span class="number">0.0799</span>,  <span class="number">0.0065</span>]]]]), Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.1269</span>, <span class="number">-0.1582</span>]), Parameter containing:</span><br><span class="line">tensor([[[[ <span class="number">0.0185</span>, <span class="number">-0.2579</span>, <span class="number">-0.1185</span>],</span><br><span class="line">          [ <span class="number">0.1269</span>,  <span class="number">0.0274</span>,  <span class="number">0.1019</span>],</span><br><span class="line">          [ <span class="number">0.0329</span>, <span class="number">-0.1229</span>, <span class="number">-0.1922</span>]]]]), Parameter containing:</span><br><span class="line">tensor([<span class="number">-0.2731</span>])], <span class="string">'lr'</span>: <span class="number">0.1</span>, <span class="string">'betas'</span>: (<span class="number">0.9</span>, <span class="number">0.999</span>), <span class="string">'eps'</span>: <span class="number">1e-08</span>, <span class="string">'weight_decay'</span>: <span class="number">0</span>, <span class="string">'amsgrad'</span>: <span class="keyword">False</span>&#125;</span><br><span class="line">______    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">optimizer = t.optim.Adam([&#123;<span class="string">'params'</span>:model.net1.parameters(),<span class="string">'lr'</span>:<span class="number">0.4</span>&#125;,</span><br><span class="line">       &#123;<span class="string">'params'</span>:model.net2.parameters(),<span class="string">'lr'</span>:<span class="number">0.1</span>&#125;],lr=<span class="number">0.04</span>)</span><br><span class="line"></span><br><span class="line">optimizer</span><br><span class="line"></span><br><span class="line">Adam (</span><br><span class="line">Parameter Group <span class="number">0</span></span><br><span class="line">    amsgrad: <span class="keyword">False</span></span><br><span class="line">    betas: (<span class="number">0.9</span>, <span class="number">0.999</span>)</span><br><span class="line">    eps: <span class="number">1e-08</span></span><br><span class="line">    lr: <span class="number">0.4</span></span><br><span class="line">    weight_decay: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">Parameter Group <span class="number">1</span></span><br><span class="line">    amsgrad: <span class="keyword">False</span></span><br><span class="line">    betas: (<span class="number">0.9</span>, <span class="number">0.999</span>)</span><br><span class="line">    eps: <span class="number">1e-08</span></span><br><span class="line">    lr: <span class="number">0.1</span></span><br><span class="line">    weight_decay: <span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    print(i)</span><br><span class="line">    print(<span class="string">'______'</span>)</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">'params'</span>: [Parameter containing:</span><br><span class="line">tensor([[[[ <span class="number">0.0493</span>,  <span class="number">0.1696</span>,  <span class="number">0.0647</span>],</span><br><span class="line">          [ <span class="number">0.1935</span>,  <span class="number">0.3102</span>, <span class="number">-0.0871</span>],</span><br><span class="line">          [<span class="number">-0.2787</span>,  <span class="number">0.0894</span>, <span class="number">-0.0438</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">-0.2671</span>,  <span class="number">0.2079</span>,  <span class="number">0.2474</span>],</span><br><span class="line">          [ <span class="number">0.2068</span>, <span class="number">-0.1825</span>,  <span class="number">0.1427</span>],</span><br><span class="line">          [<span class="number">-0.0853</span>, <span class="number">-0.1799</span>, <span class="number">-0.2465</span>]]]]), Parameter containing:</span><br><span class="line">tensor([<span class="number">-0.3158</span>,  <span class="number">0.1429</span>])], <span class="string">'lr'</span>: <span class="number">0.4</span>, <span class="string">'betas'</span>: (<span class="number">0.9</span>, <span class="number">0.999</span>), <span class="string">'eps'</span>: <span class="number">1e-08</span>, <span class="string">'weight_decay'</span>: <span class="number">0</span>, <span class="string">'amsgrad'</span>: <span class="keyword">False</span>&#125;</span><br><span class="line">______</span><br><span class="line">&#123;<span class="string">'params'</span>: [Parameter containing:</span><br><span class="line">tensor([[[[ <span class="number">0.2063</span>,  <span class="number">0.0771</span>,  <span class="number">0.1579</span>],</span><br><span class="line">          [ <span class="number">0.1543</span>,  <span class="number">0.1374</span>, <span class="number">-0.1951</span>],</span><br><span class="line">          [<span class="number">-0.1221</span>,  <span class="number">0.0099</span>, <span class="number">-0.1331</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">-0.1899</span>,  <span class="number">0.1978</span>,  <span class="number">0.1065</span>],</span><br><span class="line">          [ <span class="number">0.1400</span>, <span class="number">-0.0740</span>,  <span class="number">0.0397</span>],</span><br><span class="line">          [<span class="number">-0.2165</span>, <span class="number">-0.0180</span>,  <span class="number">0.1072</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.0692</span>, <span class="number">-0.1296</span>,  <span class="number">0.0524</span>],</span><br><span class="line">          [ <span class="number">0.0577</span>, <span class="number">-0.1184</span>,  <span class="number">0.0697</span>],</span><br><span class="line">          [ <span class="number">0.0859</span>, <span class="number">-0.2086</span>,  <span class="number">0.0419</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">-0.0270</span>,  <span class="number">0.1836</span>, <span class="number">-0.0649</span>],</span><br><span class="line">          [ <span class="number">0.1680</span>, <span class="number">-0.1061</span>, <span class="number">-0.2357</span>],</span><br><span class="line">          [<span class="number">-0.0408</span>,  <span class="number">0.0799</span>,  <span class="number">0.0065</span>]]]]), Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.1269</span>, <span class="number">-0.1582</span>]), Parameter containing:</span><br><span class="line">tensor([[[[ <span class="number">0.0185</span>, <span class="number">-0.2579</span>, <span class="number">-0.1185</span>],</span><br><span class="line">          [ <span class="number">0.1269</span>,  <span class="number">0.0274</span>,  <span class="number">0.1019</span>],</span><br><span class="line">          [ <span class="number">0.0329</span>, <span class="number">-0.1229</span>, <span class="number">-0.1922</span>]]]]), Parameter containing:</span><br><span class="line">tensor([<span class="number">-0.2731</span>])], <span class="string">'lr'</span>: <span class="number">0.1</span>, <span class="string">'betas'</span>: (<span class="number">0.9</span>, <span class="number">0.999</span>), <span class="string">'eps'</span>: <span class="number">1e-08</span>, <span class="string">'weight_decay'</span>: <span class="number">0</span>, <span class="string">'amsgrad'</span>: <span class="keyword">False</span>&#125;</span><br><span class="line">______</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer.state_dict()</span><br><span class="line">&#123;<span class="string">'state'</span>: &#123;&#125;,</span><br><span class="line"> <span class="string">'param_groups'</span>: [&#123;<span class="string">'lr'</span>: <span class="number">0.4</span>,</span><br><span class="line">   <span class="string">'betas'</span>: (<span class="number">0.9</span>, <span class="number">0.999</span>),</span><br><span class="line">   <span class="string">'eps'</span>: <span class="number">1e-08</span>,</span><br><span class="line">   <span class="string">'weight_decay'</span>: <span class="number">0</span>,</span><br><span class="line">   <span class="string">'amsgrad'</span>: <span class="keyword">False</span>,</span><br><span class="line">   <span class="string">'params'</span>: [<span class="number">140617843939944</span>, <span class="number">140617843755120</span>]&#125;,</span><br><span class="line">  &#123;<span class="string">'lr'</span>: <span class="number">0.1</span>,</span><br><span class="line">   <span class="string">'betas'</span>: (<span class="number">0.9</span>, <span class="number">0.999</span>),</span><br><span class="line">   <span class="string">'eps'</span>: <span class="number">1e-08</span>,</span><br><span class="line">   <span class="string">'weight_decay'</span>: <span class="number">0</span>,</span><br><span class="line">   <span class="string">'amsgrad'</span>: <span class="keyword">False</span>,</span><br><span class="line">   <span class="string">'params'</span>: [<span class="number">140617843755192</span>,</span><br><span class="line">    <span class="number">140617843755264</span>,</span><br><span class="line">    <span class="number">140617843755336</span>,</span><br><span class="line">    <span class="number">140617843755408</span>]&#125;]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三种</span></span><br><span class="line">optimizer = t.optim.Adam([&#123;<span class="string">'params'</span>:model.net1.parameters(),<span class="string">'lr'</span>:<span class="number">0.4</span>&#125;])</span><br><span class="line">optimizer.add_param_group(&#123;<span class="string">'params'</span>:model.net2.parameters(),<span class="string">'lr'</span>:<span class="number">0.3</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四种</span></span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">     param_group[<span class="string">'lr'</span>]=lr_new</span><br></pre></td></tr></table></figure></p><h2 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只保留模型参数并且加载</span></span><br><span class="line">t.save(model.state_dict(),<span class="string">'model_state_dict'</span>)</span><br><span class="line">model.state_dict()</span><br><span class="line"></span><br><span class="line">OrderedDict([(<span class="string">'net1.weight'</span>, tensor([[[[<span class="number">-0.3164</span>, <span class="number">-0.2508</span>, <span class="number">-0.3294</span>],</span><br><span class="line">                        [ <span class="number">0.2388</span>, <span class="number">-0.1582</span>,  <span class="number">0.0678</span>],</span><br><span class="line">                        [ <span class="number">0.0194</span>,  <span class="number">0.1120</span>,  <span class="number">0.2794</span>]]],</span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">                      [[[<span class="number">-0.2425</span>,  <span class="number">0.0833</span>, <span class="number">-0.0842</span>],</span><br><span class="line">                        [ <span class="number">0.0687</span>, <span class="number">-0.0637</span>, <span class="number">-0.3034</span>],</span><br><span class="line">                        [<span class="number">-0.3268</span>, <span class="number">-0.1049</span>, <span class="number">-0.0286</span>]]]])),</span><br><span class="line">             (<span class="string">'net1.bias'</span>, tensor([ <span class="number">0.2742</span>,  <span class="number">0.2194</span>])),</span><br><span class="line">             (<span class="string">'net2.weight'</span>, tensor([[[[ <span class="number">0.2241</span>,  <span class="number">0.2280</span>, <span class="number">-0.0597</span>],</span><br><span class="line">                        [<span class="number">-0.1045</span>, <span class="number">-0.1610</span>,  <span class="number">0.0445</span>],</span><br><span class="line">                        [<span class="number">-0.1772</span>, <span class="number">-0.0639</span>, <span class="number">-0.0172</span>]],</span><br><span class="line">              </span><br><span class="line">                       [[ <span class="number">0.0975</span>, <span class="number">-0.0081</span>,  <span class="number">0.0690</span>],</span><br><span class="line">                        [<span class="number">-0.1273</span>,  <span class="number">0.0693</span>,  <span class="number">0.1792</span>],</span><br><span class="line">                        [ <span class="number">0.0773</span>,  <span class="number">0.1652</span>, <span class="number">-0.1688</span>]]],</span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">                      [[[<span class="number">-0.2314</span>,  <span class="number">0.0494</span>, <span class="number">-0.0648</span>],</span><br><span class="line">                        [<span class="number">-0.1919</span>,  <span class="number">0.2145</span>,  <span class="number">0.0369</span>],</span><br><span class="line">                        [<span class="number">-0.1336</span>, <span class="number">-0.1077</span>, <span class="number">-0.0743</span>]],</span><br><span class="line">              </span><br><span class="line">                       [[ <span class="number">0.1510</span>, <span class="number">-0.0868</span>, <span class="number">-0.1766</span>],</span><br><span class="line">                        [<span class="number">-0.1764</span>,  <span class="number">0.0398</span>,  <span class="number">0.2146</span>],</span><br><span class="line">                        [<span class="number">-0.0269</span>,  <span class="number">0.1241</span>, <span class="number">-0.2304</span>]]]])),</span><br><span class="line">             (<span class="string">'net2.bias'</span>, tensor(<span class="number">1.00000e-02</span> *</span><br><span class="line">                     [<span class="number">-7.3981</span>, <span class="number">-0.8345</span>]))])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">temp = t.load(<span class="string">'model_state_dict.pth'</span>)</span><br><span class="line">OrderedDict([(<span class="string">'net1.weight'</span>, tensor([[[[<span class="number">-0.3164</span>, <span class="number">-0.2508</span>, <span class="number">-0.3294</span>],</span><br><span class="line">                        [ <span class="number">0.2388</span>, <span class="number">-0.1582</span>,  <span class="number">0.0678</span>],</span><br><span class="line">                        [ <span class="number">0.0194</span>,  <span class="number">0.1120</span>,  <span class="number">0.2794</span>]]],</span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">                      [[[<span class="number">-0.2425</span>,  <span class="number">0.0833</span>, <span class="number">-0.0842</span>],</span><br><span class="line">                        [ <span class="number">0.0687</span>, <span class="number">-0.0637</span>, <span class="number">-0.3034</span>],</span><br><span class="line">                        [<span class="number">-0.3268</span>, <span class="number">-0.1049</span>, <span class="number">-0.0286</span>]]]])),</span><br><span class="line">             (<span class="string">'net1.bias'</span>, tensor([ <span class="number">0.2742</span>,  <span class="number">0.2194</span>])),</span><br><span class="line">             (<span class="string">'net2.weight'</span>, tensor([[[[ <span class="number">0.2241</span>,  <span class="number">0.2280</span>, <span class="number">-0.0597</span>],</span><br><span class="line">                        [<span class="number">-0.1045</span>, <span class="number">-0.1610</span>,  <span class="number">0.0445</span>],</span><br><span class="line">                        [<span class="number">-0.1772</span>, <span class="number">-0.0639</span>, <span class="number">-0.0172</span>]],</span><br><span class="line">              </span><br><span class="line">                       [[ <span class="number">0.0975</span>, <span class="number">-0.0081</span>,  <span class="number">0.0690</span>],</span><br><span class="line">                        [<span class="number">-0.1273</span>,  <span class="number">0.0693</span>,  <span class="number">0.1792</span>],</span><br><span class="line">                        [ <span class="number">0.0773</span>,  <span class="number">0.1652</span>, <span class="number">-0.1688</span>]]],</span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">                      [[[<span class="number">-0.2314</span>,  <span class="number">0.0494</span>, <span class="number">-0.0648</span>],</span><br><span class="line">                        [<span class="number">-0.1919</span>,  <span class="number">0.2145</span>,  <span class="number">0.0369</span>],</span><br><span class="line">                        [<span class="number">-0.1336</span>, <span class="number">-0.1077</span>, <span class="number">-0.0743</span>]],</span><br><span class="line">              </span><br><span class="line">                       [[ <span class="number">0.1510</span>, <span class="number">-0.0868</span>, <span class="number">-0.1766</span>],</span><br><span class="line">                        [<span class="number">-0.1764</span>,  <span class="number">0.0398</span>,  <span class="number">0.2146</span>],</span><br><span class="line">                        [<span class="number">-0.0269</span>,  <span class="number">0.1241</span>, <span class="number">-0.2304</span>]]]])),</span><br><span class="line">             (<span class="string">'net2.bias'</span>, tensor(<span class="number">1.00000e-02</span> *</span><br><span class="line">                     [<span class="number">-7.3981</span>, <span class="number">-0.8345</span>]))])</span><br><span class="line"></span><br><span class="line">model2.state_dict()</span><br><span class="line"></span><br><span class="line">OrderedDict([(<span class="string">'net3.weight'</span>, tensor([[[[ <span class="number">0.2793</span>, <span class="number">-0.2330</span>,  <span class="number">0.3270</span>],</span><br><span class="line">                        [<span class="number">-0.1419</span>,  <span class="number">0.1562</span>,  <span class="number">0.1875</span>],</span><br><span class="line">                        [<span class="number">-0.0249</span>,  <span class="number">0.1297</span>,  <span class="number">0.1642</span>]]],</span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">                      [[[ <span class="number">0.2770</span>,  <span class="number">0.1016</span>, <span class="number">-0.1096</span>],</span><br><span class="line">                        [ <span class="number">0.1929</span>,  <span class="number">0.0210</span>,  <span class="number">0.1722</span>],</span><br><span class="line">                        [ <span class="number">0.1304</span>,  <span class="number">0.0820</span>,  <span class="number">0.1205</span>]]]])),</span><br><span class="line">             (<span class="string">'net3.bias'</span>, tensor([<span class="number">-0.3235</span>, <span class="number">-0.1770</span>])),</span><br><span class="line">             (<span class="string">'net4.weight'</span>, tensor([[[[<span class="number">-0.2043</span>, <span class="number">-0.1492</span>],</span><br><span class="line">                        [ <span class="number">0.1728</span>, <span class="number">-0.1069</span>]],</span><br><span class="line">              </span><br><span class="line">                       [[<span class="number">-0.2903</span>,  <span class="number">0.3385</span>],</span><br><span class="line">                        [ <span class="number">0.2778</span>,  <span class="number">0.1589</span>]]],</span><br><span class="line">              </span><br><span class="line">              </span><br><span class="line">                      [[[<span class="number">-0.1423</span>, <span class="number">-0.0439</span>],</span><br><span class="line">                        [ <span class="number">0.2849</span>, <span class="number">-0.0840</span>]],</span><br><span class="line">              </span><br><span class="line">                       [[ <span class="number">0.0354</span>,  <span class="number">0.1711</span>],</span><br><span class="line">                        [<span class="number">-0.0274</span>, <span class="number">-0.2220</span>]]]])),</span><br><span class="line">             (<span class="string">'net4.bias'</span>, tensor([<span class="number">-0.0264</span>, <span class="number">-0.1094</span>]))])</span><br><span class="line"></span><br><span class="line">model2.load_state_dict(temp)</span><br><span class="line"></span><br><span class="line">Missing key(s) <span class="keyword">in</span> state_dict: <span class="string">"net3.weight"</span>, <span class="string">"net3.bias"</span>, <span class="string">"net4.weight"</span>, <span class="string">"net4.bias"</span>. </span><br><span class="line">Unexpected key(s) <span class="keyword">in</span> state_dict: <span class="string">"net1.weight"</span>, <span class="string">"net1.bias"</span>, <span class="string">"net2.weight"</span>, <span class="string">"net2.bias"</span>.</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 印证了在保存模型参数的时候是根据名字进行保存，</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保留模型</span></span><br><span class="line">t.save(model,<span class="string">'model.pth'</span>)</span><br><span class="line">temp2 = t.load(<span class="string">'model.pth'</span>)</span><br><span class="line">Nettest(</span><br><span class="line">  (net1): Conv2d(<span class="number">1</span>, <span class="number">2</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (net2): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">2</span>, <span class="number">2</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">加载进来就是一个模型，包括forward什么的都还在。</span><br></pre></td></tr></table></figure><hr><h2 id="numpy和list和tensor对于size的访问区别"><a href="#numpy和list和tensor对于size的访问区别" class="headerlink" title="numpy和list和tensor对于size的访问区别"></a>numpy和list和tensor对于size的访问区别</h2><ol><li>list:   只有len()方法，返回的是最外层的个数，reshape方法</li><li>numpy:  b.size是全部个数，b.shape是(2,3)  b = np.arange(6).reshape(2,3),  b.resize(2,3)        np.arange(1,6,2)</li><li>tensor: c.shape==c.size()  len(c)==c.size(0) 返回torch.size([3,2]), c.resize_(4,4)(可以改变自身尺寸)  c.resize(1,4)（来源于torchvision，可以忽略）==c.reshape(1,4)(对于连续地址共享内存，不连续地址则复制)==c.view(1,4)(共享内存)  t.arange(1,6,2) t_.unsqueeze(1)<br>tensor的普通索引基本共享内存，而高级索引基本不共享内存。 </li><li>numpy–&gt;tensor t_ = t.from_numpy(numpy_)(共享内存）或者 t_ = t.tensor(numpy_)(返回新对象)</li><li>tensor–&gt;numpy np_ = t_.numpy()(共享内存) 或者 np_ = np.array(t_)</li><li>numpy–&gt;list   list_ = np_.tolist()(不共享内存）</li><li>list–&gt;numpy   np_ = np.array(list_)(不共享内存）</li><li>tensor–&gt;list  list_ = t_.tolist() (不共享内存)  或者 item_ = t_.item() (不共享内存）</li><li>list–&gt;tensor  t_ = t.tensor(list_)(不共享内存）<br>也就是说numpy和tensor可以做到互相共享内存，而list只是一个对外的和Python相连接的一个形式.<br>补充：基于numpy和tensor，推荐使用shape属性， 修改形状则分别使用reshape()和view(),</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># a,b,c共享内存</span></span><br><span class="line">a = np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = t.from_numpy(a)</span><br><span class="line">c = b.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># a,b,c不共享内存</span></span><br><span class="line">a = np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = t.tensor(a)</span><br><span class="line">c = np.array(b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 不共享内存</span></span><br><span class="line">a = np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = a.tolist()</span><br><span class="line">c = np.array(b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensor <span class="keyword">as</span> t</span><br><span class="line">a = t.tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = a.item()</span><br><span class="line">b = a.tolist()</span><br><span class="line">c = t.tensor(b)</span><br></pre></td></tr></table></figure><h2 id="torch-Tensor-和-torch-tensor的区别"><a href="#torch-Tensor-和-torch-tensor的区别" class="headerlink" title="torch.Tensor 和 torch.tensor的区别"></a>torch.Tensor 和 torch.tensor的区别</h2><p>暂时还没有组织好的语言，先以代码的形式记录下来<br>主要是类型和对0维元素的区别。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">12</span>]: <span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: t.Tensor(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">13</span>]: tensor([<span class="number">-4.8232e+13</span>,  <span class="number">4.5581e-41</span>, <span class="number">-1.8931e-03</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: t.Tensor([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">Out[<span class="number">14</span>]: tensor([ <span class="number">3.</span>,  <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: t.tensor(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">15</span>]: tensor(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: t.tensor([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">Out[<span class="number">16</span>]: tensor([ <span class="number">3</span>,  <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: a = t.tensor([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: type(a)</span><br><span class="line">Out[<span class="number">18</span>]: torch.Tensor</span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: a.dtype</span><br><span class="line">Out[<span class="number">19</span>]: torch.int64</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tensor只是tensor(dtype=float)的别名。</span></span><br><span class="line">x = torch.tensor([<span class="number">3.0</span>],requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># torch.Tensor不接收requires_grad参数</span></span><br><span class="line"><span class="comment"># torch.tensor只有是float型参数的情况下才接受requires_grad参数</span></span><br></pre></td></tr></table></figure></p><h2 id="tensor-new-和tensor-data-new"><a href="#tensor-new-和tensor-data-new" class="headerlink" title="tensor.new()和tensor.data.new()"></a>tensor.new()和tensor.data.new()</h2><p>暂时不知道这两者的区别，但是书上的代码多数都是tensor.data.new()</p><hr><h2 id="topk的用法"><a href="#topk的用法" class="headerlink" title="topk的用法"></a>topk的用法</h2><p>output.data[0].topk(1)[1][0].item()<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">20</span>]: x = t.arange(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: y = t.topk(x,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: type(y)</span><br><span class="line">Out[<span class="number">22</span>]: tuple</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: y</span><br><span class="line">Out[<span class="number">23</span>]: (tensor([ <span class="number">5.</span>,  <span class="number">4.</span>]), tensor([ <span class="number">4</span>,  <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: y[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">24</span>]: tensor([ <span class="number">4</span>,  <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">25</span>]: y[<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">25</span>]: tensor(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: y[<span class="number">1</span>][<span class="number">1</span>].item()</span><br><span class="line">Out[<span class="number">26</span>]: <span class="number">3</span></span><br></pre></td></tr></table></figure></p><h2 id="ipython和jupyter和pycharm"><a href="#ipython和jupyter和pycharm" class="headerlink" title="ipython和jupyter和pycharm"></a>ipython和jupyter和pycharm</h2><p>在写代码的前期，用jupyter好使，因为对于不确定的比较多的代码是可以直接看到结果，对某一段进行调试，检查某一段的基本语法错误，或者对于某个想法的实现，是简单直接的甚至对于中型代码，用代码框可以实现视觉上的分离，逻辑清晰，并且支持markdown的记录与注释，对于不了解的代码有很好的支持性。<br>但是在写代码的后期，jupyter的弊端逐渐显现，不能使用模板，<strong>init</strong>.py的生成不好使，文件与文件夹的关系不清晰。甚至一个简单的文件或者文件夹挪动位置都很麻烦，需要桌面的辅助。<br>而pycharm对于文件管理，<strong>init</strong>.py等有很好的支持性。更适合写已经成熟的代码。<br>这一下难住我了，作为新手，肯定每次都要实验好些代码，看输入输出的效果，如果是pycharm则比较麻烦，对于调试很啰嗦。<br>命令行窗口做为补充，也不好使，因为每次能看到的东西有限，重复性差，只能用于单句代码的验证。<br>所以不妨这样，前期开发还是用jupyter，等开发的差不多了，甚至等单个文件已经开发完毕，这样的话开发就可以先不管文件夹的事，等各个文件开发完毕，再转成pycharm，来实现文件夹、文件的组织管理和后期的调试，这是因为现在多数不使用jupyter直接运行，而是使用py进行运行。我觉得应该有很多人用.ipynb进行运行，但是我不知道怎么才能更好的运行。</p><h3 id="anaconda"><a href="#anaconda" class="headerlink" title="anaconda"></a>anaconda</h3><p>虚拟环境不错</p><h3 id="python"><a href="#python" class="headerlink" title="python"></a>python</h3><p>如果在 Python 脚本文件首行输入#!/usr/bin/env python，那么可以在命令行窗口中执行/path/to/script-file.py以执行该脚本文件。<br>使用三引号(‘’’或”””)可以指定一个多行字符串。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。&lt;br&gt;这是我的&lt;a href=&quot;https://github.com/TJJTJJTJJ/pytorch__learn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;代码&lt;/a&gt;&lt;br&gt;大神链接：&lt;a href=&quot;https://github.com/anishathalye/neural-style&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/anishathalye/neural-style&lt;/a&gt;&lt;br&gt;这是论文作者写的&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="numpy" scheme="http://yoursite.com/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>numpy</title>
    <link href="http://yoursite.com/2018/09/13/numpy/"/>
    <id>http://yoursite.com/2018/09/13/numpy/</id>
    <published>2018-09-13T14:27:14.000Z</published>
    <updated>2018-10-01T06:02:51.164Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>参考链接<br><a href="https://www.yiibai.com/numpy/numpy_ndarray_object.html#article-start" target="_blank" rel="noopener">NumPy Ndarray对象</a><br>这只是简单的入门，以后接触得越多，对于其中的理解也才会更加全面，并做补充。<br><a id="more"></a><br><!--more--><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.简单的array对象</span></span><br><span class="line">a = np.array([<span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>], dtype = complex)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.结构数组形式的array对象</span></span><br><span class="line">student = np.dtype([(<span class="string">'name'</span>,<span class="string">'S20'</span>),  (<span class="string">'age'</span>,  <span class="string">'i1'</span>),  (<span class="string">'marks'</span>,  <span class="string">'f4'</span>)]) </span><br><span class="line">a = np.array([(<span class="string">'abc'</span>,  <span class="number">21</span>,  <span class="number">50</span>),(<span class="string">'xyz'</span>,  <span class="number">18</span>,  <span class="number">75</span>)], dtype = student)  </span><br><span class="line">print(a)</span><br><span class="line">[(<span class="string">'abc'</span>, <span class="number">21</span>, <span class="number">50.0</span>), (<span class="string">'xyz'</span>, <span class="number">18</span>, <span class="number">75.0</span>)]</span><br><span class="line">a[<span class="number">0</span>][<span class="string">'name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.其他形式</span></span><br><span class="line">a.shape</span><br><span class="line">a.reshape(<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>)  </span><br><span class="line">x.itemsize 每个元素的字节单位长度</span><br><span class="line">numpy.frombuffer() 怎么理解？？</span><br><span class="line">np.fromiter(iter(list))</span><br><span class="line">numpy.arange(start, stop, step, dtype)</span><br><span class="line">numpy.linspace(start, stop, num, endpoint, retstep, dtype)</span><br><span class="line">numpy.logscale(start, stop, num, endpoint, base, dtype)</span><br></pre></td></tr></table></figure></p><hr><h2 id="numpy的保存和读取，这里还是有点东西的，待续"><a href="#numpy的保存和读取，这里还是有点东西的，待续" class="headerlink" title="numpy的保存和读取，这里还是有点东西的，待续"></a>numpy的保存和读取，这里还是有点东西的，待续</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy在save和load的时候没有显式保存变量名</span></span><br><span class="line">np.save(<span class="string">"A.npy"</span>,A)</span><br><span class="line">B=np.load(<span class="string">"A.npy"</span>)</span><br><span class="line">B</span><br><span class="line"></span><br><span class="line">np.savez(<span class="string">"files.npz"</span>,A,B,C_array=C)</span><br><span class="line">np.savez_compressed(<span class="string">"files.npz"</span>,A,B,C_array=C)</span><br><span class="line">D=np.load(<span class="string">"files.npz"</span>)</span><br><span class="line">D[<span class="string">'arr_1'</span>]</span><br><span class="line">D[<span class="string">'C_array'</span>]</span><br></pre></td></tr></table></figure><h2 id="numpy-savetxt-loadtxt"><a href="#numpy-savetxt-loadtxt" class="headerlink" title="numpy savetxt() loadtxt"></a>numpy savetxt() loadtxt</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.savetxt(<span class="string">'a.txt'</span>,a,fmt=<span class="string">'%d'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">b= np.loadtxt(<span class="string">'a.txt'</span>,delimiter=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.savetxt(<span class="string">'a.csv'</span>,a,fmt=<span class="string">'%d'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">b= np.loadtxt(<span class="string">'a.csv'</span>,delimiter=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><hr><h1 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h1><p>ndarray对象中的元素遵循基于零的索引。 有三种可用的索引方法类型： 字段访问，基本切片和高级索引，可以使用省略号当全部<br>切片只是返回一个视图，高级索引返回数据的副本，并且切片是全取，而高级索引是取对应的位置的元素<br>y = x[1:3,1:2]:两行元素,y = x[[1,2],[1,3]]:两个元素<br>布尔索引是数据的复制<br>广播法则<br>数组上的迭代 for i in np.nditer(a,order=’C’ or order=’F’), np.nditer(a).<strong>next</strong>()<br>广播迭代</p><p>reshape：不改变数据的条件下修改形状<br>flat：数组上的一维迭代器， for i in a.flat  ,   a.flat[2:4],  暂时看不出来flat和nditer的区别<br>flatten: 返回折叠为一维的数组副本<br>ravel： 返回连续的展开数组    flattten和ravel的区别暂时不知道在哪<br>numpy.rollaxis：这里的转轴有问题，比较好的理解是[[[000,001],[010,011]],[[100,101],[110,111]]]，[参考：]<a href="https://blog.csdn.net/liaoyuecai/article/details/80193996" target="_blank" rel="noopener">https://blog.csdn.net/liaoyuecai/article/details/80193996</a>，还有一种理解是从页，行，列的方式，每次都在原数组上以固定页，固定行的方式进行读取，保证所有的数字以一列的方式，即总是表示成000,001,010,011,100,101,110,111，再想一个更简单的方法。跨括号法，不扯<br>numpy.swapaxes(arr, axis1, axis2)</p><h1 id="修改维度"><a href="#修改维度" class="headerlink" title="修改维度"></a>修改维度</h1><p>序号     维度和描述</p><ol><li>broadcast 产生模仿广播的对象 b = np.broadcast(x,y) c = np.empty(b.shape)  c.flat = [u + v for (u,v) in b],并且np.nditer()也可以达到相同的效果 </li><li>broadcast_to 将数组广播到新形状 numpy.broadcast_to(array, shape, subok)</li><li>expand_dims 扩展数组的形状numpy.expand_dims(arr, axis)</li><li>squeeze 从数组的形状中删除单维条目 y = np.squeeze(x,axis=(0,1))</li></ol><p>数组的连接<br>序号     数组及描述</p><ol><li>concatenate 沿着现存的轴连接数据序列，不产生新的轴</li><li>stack 沿着新轴连接数组序列，产生新的轴</li><li>hstack 水平堆叠序列中的数组(列方向)</li><li>vstack 竖直堆叠序列中的数组(行方向)<br>数组分割<br>序号     数组及操作</li><li>split 将一个数组分割为多个子数组</li><li>hsplit 将一个数组水平分割为多个子数组(按列)</li><li>vsplit 将一个数组竖直分割为多个子数组(按行)</li></ol><p>添加/删除元素<br>序号     元素及描述</p><ol><li>resize 返回指定形状的新数组</li><li>append 将值添加到数组末尾</li><li>insert 沿指定轴将值插入到指定下标之前</li><li>delete 返回删掉某个轴的子数组的新数组</li><li>unique 寻找数组内的唯一元素</li></ol><p>切片的新表达式： np.s_[::2]<br>位运算：跳过</p><p>字符串函数：对dtype为numpy.string_或numpy.unicode_的数组执行向量化字符串操作</p><ol><li>add() 返回两个str或Unicode数组的逐个字符串连接</li><li>multiply() 返回按元素多重连接后的字符串</li><li>center() 返回给定字符串的副本，其中元素位于特定字符串的中央</li><li>capitalize() 返回给定字符串的副本，其中只有第一个字符串大写</li><li>title() 返回字符串或 Unicode 的按元素标题转换版本</li><li>lower() 返回一个数组，其元素转换为小写</li><li>upper() 返回一个数组，其元素转换为大写</li><li>split() 返回字符串中的单词列表，并使用分隔符来分割</li><li>splitlines() 返回元素中的行列表，以换行符分割</li><li>strip() 返回数组副本，其中元素移除了开头或者结尾处的特定字符</li><li>join() 返回一个字符串，它是序列中字符串的连接</li><li>replace() 返回字符串的副本，其中所有子字符串的出现位置都被新字符串取代</li><li>decode() 按元素调用str.decode</li><li>encode() 按元素调用str.encode</li></ol><p>dtype???<br>add()，subtract()，multiply()和divide()<br>排序quicksort, mergesort, heaqsort<br>dt = np.dtype([(‘name’,  ‘S10’),(‘age’,  int)])<br>a = np.array([(“raju”,21),(“anil”,25),(“ravi”,  17),  (“amar”,27)], dtype = dt)<br>print(np.sort(a, order =  ‘name’))<br>numpy.argsort()<br>numpy.lexsort()<br>np.argmax() np.argmin()<br>np.nonzero()<br>np.where()<br>np.extract()</p><p>改变形状： b.shape = 3,2<br>无复制： b = a 值和形状都是共享，id相同<br>浅复制: b = a.view()　值共享，形状不共享，id不同　切片也是浅复制<br>深复制：ｂ＝ a.copy()</p><p>numpy.matlib　<br>矩阵库，返回的是矩阵matrix对象，而不是ndarray对象 .empty(), .zeros(), .ones(), eye(), identity(), rand() 只能是二维的<br>np.matrix(‘1,2;3,4’) np.matirx([[1,2],[3,4]])</p><p>array和asarray都可以将结构数据转化为ndarray，但是主要区别就是当数据源是ndarray时，array仍然会copy出一个副本，占用新的内存，但asarray不会。<br>matrix和array互换： np.matrix(np.array) np.array(np.matrix),此时两者值和形状没有关系，使用asmatrix和asarray时，值共享，形状不共享<br>暂时没有想到matrix的意义</p><p>线性代数<br>numpy.linalg<br>貌似可以直接作用于列表</p><ol><li>dot 两个数组的点积，也就是矩阵式乘法 np.dot(a,b)，对于多维数组的乘法，可以同样以XXX0.和XXX.0的方法<br>对于两个1维数组，是点积，对于两个矩阵，是矩阵乘法，对于1维数组和矩阵，则对1维数组适当地转置，然后进行矩阵乘法，需要满足倒数第一维和倒数第二维相等。</li><li>vdot 两个向量的点积，也就是矩阵式对应元素的乘积和</li><li>inner 两个数组的内积 </li><li>matmul 两个数组的矩阵积</li><li>determinant 数组的行列式  numpy.linalg.det()</li><li>solve 求解线性矩阵方程</li><li>inv 寻找矩阵的乘法逆矩阵</li></ol><p>Matplotlib<br>from matplotlib import pyplot as plt</p><hr><h2 id="numpy转化数据类型"><a href="#numpy转化数据类型" class="headerlink" title="numpy转化数据类型"></a>numpy转化数据类型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">11</span>]: arr = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: arr</span><br><span class="line">Out[<span class="number">12</span>]: array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该命令查看数据类型</span></span><br><span class="line">In [<span class="number">13</span>]: arr.dtype</span><br><span class="line">Out[<span class="number">13</span>]: dtype(<span class="string">'int64'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: float_arr = arr.astype(np.float64)</span><br><span class="line"><span class="comment"># 等价于，即str和data-type是一样的</span></span><br><span class="line">float_arr = arr.asdtype(<span class="string">'float64'</span>)</span><br><span class="line"><span class="comment">#该命令查看数据类型</span></span><br><span class="line">In [<span class="number">15</span>]: float_arr.dtype</span><br><span class="line">Out[<span class="number">15</span>]: dtype(<span class="string">'float64'</span>)</span><br></pre></td></tr></table></figure><hr><p>字符串数组转化为数值型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: numeric_strings = np.array([<span class="string">'1.2'</span>,<span class="string">'2.3'</span>,<span class="string">'3.2141'</span>], dtype=np.string_)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: numeric_strings</span><br><span class="line">Out[<span class="number">5</span>]: array([<span class="string">'1.2'</span>, <span class="string">'2.3'</span>, <span class="string">'3.2141'</span>], dtype=<span class="string">'|S6'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处写的是float 而不是np.float64, Numpy很聪明，会将python类型映射到等价的dtype上</span></span><br><span class="line"><span class="comment"># # 这里的float是Python的数据类型，NumPy会自动的将其映射到等价的dtype上，即np.float64</span></span><br><span class="line">In [<span class="number">6</span>]: numeric_strings.astype(float)</span><br><span class="line">Out[<span class="number">6</span>]: array([ <span class="number">1.2</span>, <span class="number">2.3</span>, <span class="number">3.2141</span>])</span><br></pre></td></tr></table></figure></p><p>所以astype一共可以接受三种参数</p><ul><li>第一种是dtype，即np.int32这种，</li><li>第二种是字符串，即’int32’这样，与第一种相呼应，</li><li>第三种是Python的数据类型，会自动转化。</li></ul><p>numpy中的数据类型转换，不能直接改原数据的dtype!  只能用函数astype()。<br>[参考链接]<a href="https://www.cnblogs.com/hhh5460/p/5129032.html" target="_blank" rel="noopener">https://www.cnblogs.com/hhh5460/p/5129032.html</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果直接修改dtype，会导致长度发生改变</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.dtype = <span class="string">'float16'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([ <span class="number">-9.58442688e-05</span>,   <span class="number">7.19000000e+02</span>,   <span class="number">2.38159180e-01</span>,</span><br><span class="line">         <span class="number">1.92968750e+00</span>,              nan,  <span class="number">-1.66034698e-03</span>,</span><br><span class="line">        <span class="number">-2.63427734e-01</span>,   <span class="number">1.96875000e+00</span>,  <span class="number">-1.07519531e+00</span>,</span><br><span class="line">        <span class="number">-1.19625000e+02</span>,              nan,   <span class="number">1.97167969e+00</span>,</span><br><span class="line">        <span class="number">-1.60156250e-01</span>,  <span class="number">-7.76290894e-03</span>,   <span class="number">4.07226562e-01</span>,</span><br><span class="line">         <span class="number">1.94824219e+00</span>], dtype=float16)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">(<span class="number">16</span>,)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.dtype = <span class="string">'float16'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([ <span class="number">-9.58442688e-05</span>,   <span class="number">7.19000000e+02</span>,   <span class="number">2.38159180e-01</span>,</span><br><span class="line">         <span class="number">1.92968750e+00</span>,              nan,  <span class="number">-1.66034698e-03</span>,</span><br><span class="line">        <span class="number">-2.63427734e-01</span>,   <span class="number">1.96875000e+00</span>,  <span class="number">-1.07519531e+00</span>,</span><br><span class="line">        <span class="number">-1.19625000e+02</span>,              nan,   <span class="number">1.97167969e+00</span>,</span><br><span class="line">        <span class="number">-1.60156250e-01</span>,  <span class="number">-7.76290894e-03</span>,   <span class="number">4.07226562e-01</span>,</span><br><span class="line">         <span class="number">1.94824219e+00</span>], dtype=float16)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">(<span class="number">16</span>,)</span><br></pre></td></tr></table></figure></p><p>对于字符串数组还没有找到合理的说明，sad</p><hr><h2 id="np-in1d-x-y"><a href="#np-in1d-x-y" class="headerlink" title="np.in1d(x,y)"></a>np.in1d(x,y)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>test = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>states = [<span class="number">0</span>, <span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask = np.in1d(test, states)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask</span><br><span class="line">array([ <span class="keyword">True</span>, <span class="keyword">False</span>,  <span class="keyword">True</span>, <span class="keyword">False</span>,  <span class="keyword">True</span>], dtype=bool)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test[mask]</span><br><span class="line">array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask = np.in1d(test, states, invert=<span class="keyword">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask</span><br><span class="line">array([<span class="keyword">False</span>,  <span class="keyword">True</span>, <span class="keyword">False</span>,  <span class="keyword">True</span>, <span class="keyword">False</span>], dtype=bool)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test[mask]</span><br><span class="line">array([<span class="number">1</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure><h2 id="np-argsort"><a href="#np-argsort" class="headerlink" title="np.argsort()"></a>np.argsort()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = np.argsort(x)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: xx</span><br><span class="line">Out[<span class="number">48</span>]: array([<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">49</span>]: yy</span><br><span class="line">Out[<span class="number">49</span>]: array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">50</span>]: xx[yy]</span><br><span class="line">Out[<span class="number">50</span>]: array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><h2 id="np-setdiff1d"><a href="#np-setdiff1d" class="headerlink" title="np.setdiff1d()"></a>np.setdiff1d()</h2><p>这种是以集合的方式，会把列表先压平，<br>@return: sorted 1D array<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = np.array([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.setdiff1d(a, b)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure></p><h2 id="np-argwhere"><a href="#np-argwhere" class="headerlink" title="np.argwhere()"></a>np.argwhere()</h2><p>@return: index_array<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.arange(<span class="number">6</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.argwhere(x&gt;<span class="number">1</span>)</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: x[<span class="number">0</span>,<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">32</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: x[[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">0</span>]]</span><br><span class="line">Out[<span class="number">26</span>]: array([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: z = zip([<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">30</span>]: <span class="keyword">for</span> i <span class="keyword">in</span> z:</span><br><span class="line">    ...:     print(i)</span><br><span class="line">    ...:     </span><br><span class="line">(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">53</span>]: x[(<span class="number">0</span>,<span class="number">1</span>),(<span class="number">2</span>,<span class="number">0</span>)]</span><br><span class="line">Out[<span class="number">53</span>]: array([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">73</span>]: z = list(zip(*y))</span><br><span class="line"></span><br><span class="line">In [<span class="number">74</span>]: x[z]</span><br><span class="line">Out[<span class="number">74</span>]: array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure></p><h2 id="np-setdiff1d-x-y-intersect1d-x-y"><a href="#np-setdiff1d-x-y-intersect1d-x-y" class="headerlink" title="np.setdiff1d(x,y),intersect1d(x,y)"></a>np.setdiff1d(x,y),intersect1d(x,y)</h2><p>集合的减法运算,交集运算</p><hr><h2 id="np-random-choice-5-3"><a href="#np-random-choice-5-3" class="headerlink" title="np.random.choice(5,3)"></a>np.random.choice(5,3)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.choice(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">a</span><br><span class="line">array([<span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;https://www.yiibai.com/numpy/numpy_ndarray_object.html#article-start&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NumPy Ndarray对象&lt;/a&gt;&lt;br&gt;这只是简单的入门，以后接触得越多，对于其中的理解也才会更加全面，并做补充。&lt;br&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="numpy" scheme="http://yoursite.com/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>pytorch chapter8 neural style</title>
    <link href="http://yoursite.com/2018/09/11/pytorch-chapter8-neural-style/"/>
    <id>http://yoursite.com/2018/09/11/pytorch-chapter8-neural-style/</id>
    <published>2018-09-11T03:19:25.000Z</published>
    <updated>2018-10-01T07:49:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>本文主要是针对<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">陈云的PyTorch入门与实践</a>的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。<br>这是我的<a href="https://github.com/TJJTJJTJJ/pytorch__learn" target="_blank" rel="noopener">代码</a><br>大神链接：(<a href="https://github.com/anishathalye/neural-style" target="_blank" rel="noopener">https://github.com/anishathalye/neural-style</a>)</p><hr><a id="more"></a><h1 id="2-问题及其解决"><a href="#2-问题及其解决" class="headerlink" title="2 问题及其解决"></a>2 问题及其解决</h1><p>我在第六章和第七章的时候还是基于pytorch 0.4.0，而第八章的时候我开始基于pytorch 0.4.1，所以以下的内容介绍都是基于0.4.1</p><h2 id="2-1-文件组织形式"><a href="#2-1-文件组织形式" class="headerlink" title="2.1 文件组织形式"></a>2.1 文件组织形式</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">├─checkpoints/</span><br><span class="line">├─content_img/</span><br><span class="line">│  ├─<span class="selector-tag">input</span>.jpg</span><br><span class="line">│  ├─output.jpg</span><br><span class="line">│  └─style.jpg</span><br><span class="line">├─data/</span><br><span class="line">│  ├─coco/<span class="selector-tag">a</span>.jpg</span><br><span class="line"></span><br><span class="line">├─dataset/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─dataset.py</span><br><span class="line">├─models/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─PackedVGG.py</span><br><span class="line">│  └─transformer_net.py</span><br><span class="line">└─utils/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─utils.py</span><br><span class="line">│  └─visualize.py</span><br><span class="line">├─config.py</span><br><span class="line">└─main.py</span><br></pre></td></tr></table></figure><p>其中，上半部分是对数据和模型的保存组织形式，我们只需要能对应起来即可，其中，checkpoints是为了保存模型，content_img中的style.jpg是训练时候的风格图片，input.jpg是测试的输入，output.jpg是测试的输出，data中的数据是训练数据，主要是因为这个训练数据太整齐，是用ImageFolder读取的，为了避免麻烦，也为了在测试的时候方便观察图片，所以style.jpg我们暂时放在了content中。<br>下半部分是重点，我们需要写的代码，每次都是先从dataset.py和models开始写起，然后导入visualize.py，这个文件基本不会发生改变，然后同时写main.py和config.py，边写边扩展utils中的其他文件，例如main中用到的函数等等。</p><h2 id="2-2-models"><a href="#2-2-models" class="headerlink" title="2.2 models"></a>2.2 models</h2><h3 id="PackedVGG-py"><a href="#PackedVGG-py" class="headerlink" title="PackedVGG.py"></a>PackedVGG.py</h3><p>这里我们主要是取已有的网络，得到中间层的输出<br><strong>models.named_parameters()</strong>:返回的是一个生成器，每次返回一个参数的关键字和值<br><strong>models.state_dict()</strong>:返回的是一个字典，记录了参数的关键字和值<br><strong>models.parameters()</strong>:返回的是变量，没有名字，可以在requires_grad中用到<br>models.features返回的是相对应的模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">7</span>]: <span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: models = vgg16(pretrained=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: model = models.features[:<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: model</span><br><span class="line">Out[<span class="number">10</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: models.parameters()</span><br><span class="line">Out[<span class="number">11</span>]: &lt;generator object Module.parameters at <span class="number">0x7f8fad26b3b8</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: models.named_parameters()</span><br><span class="line">Out[<span class="number">12</span>]: &lt;generator object Module.named_parameters at <span class="number">0x7f8f29e99d58</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: model.named_parameters()</span><br><span class="line">Out[<span class="number">13</span>]: &lt;generator object Module.named_parameters at <span class="number">0x7f8fad26b2b0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: model.parameters()</span><br><span class="line">Out[<span class="number">14</span>]: &lt;generator object Module.parameters at <span class="number">0x7f8fad26b4c0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: model.state_dict()</span><br><span class="line">Out[<span class="number">15</span>]: </span><br><span class="line">OrderedDict([(<span class="string">'0.weight'</span>, tensor([[[[<span class="number">-0.5537</span>,  <span class="number">0.1427</span>,  <span class="number">0.5290</span>],</span><br><span class="line">                        [<span class="number">-0.5831</span>,  <span class="number">0.3566</span>,  <span class="number">0.7657</span>],</span><br><span class="line">                        [<span class="number">-0.6902</span>, <span class="number">-0.0480</span>,  <span class="number">0.4841</span>]],</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16</span><br><span class="line">models = vgg16(pretarined = <span class="keyword">True</span>)</span><br><span class="line">In [<span class="number">19</span>]: models</span><br><span class="line">Out[<span class="number">19</span>]: </span><br><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">3</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">6</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">8</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">11</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">13</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">15</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">18</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">20</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">22</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">25</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">27</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">29</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">    (<span class="number">3</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">    (<span class="number">4</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: models.features</span><br><span class="line">Out[<span class="number">20</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">3</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">6</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">8</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">11</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">13</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">15</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">18</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">20</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">22</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">25</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">27</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">29</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: models.features[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">21</span>]: ReLU(inplace)</span><br><span class="line"><span class="comment"># list</span></span><br><span class="line">In [<span class="number">27</span>]: models4 = models2[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: models4</span><br><span class="line">Out[<span class="number">28</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: models4list</span><br><span class="line">Out[<span class="number">32</span>]: </span><br><span class="line">[Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line"> ReLU(inplace)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">36</span>]: models4list[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">36</span>]: ReLU(inplace)</span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: models4list[<span class="number">1</span>].named_parameters</span><br><span class="line">Out[<span class="number">37</span>]: &lt;bound method Module.named_parameters of ReLU(inplace)&gt;</span><br></pre></td></tr></table></figure><p>sequencial是支持索引操作的<br>list(module)会变成一个list，可以通过索引来获取层，注意，nn.ModuleList, nn.Sequential, nn.Conv等都是Module,都可以通过named_parameters来获取参数。<br>为了能够提取出中间层的输出，作者换了一个方法，用的nn.ModuleList,nn.ModuleList和nn.Sequential的区别在此才真正显现，nn.Sequential更有利于直接把输入传给Module，计算是一个整体，写起来更方便，而nn.Modulist则不能直接把输入传给Module，需要用循环传输入，更有利于在层中做一些保留，提取中间层的输出。后面我们会讲到hook。或者说提取中间层的输出我们可以选择在定义网络的forward中进行，另外，就是需要注意的是，这里的输入是一个batch_size大小的矩阵，所以即便像作者这样，用一个列表保存输出，但实际输出的列表中的元素都是(b,n,h,w)大小的。后面我会验证。</p><p>提取中间层的输出有两种方法：<br>第二种方法参考链接：<a href="https://www.jianshu.com/p/0a23db1df55a" target="_blank" rel="noopener">https://www.jianshu.com/p/0a23db1df55a</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种方法，这种方法是在前向网络中提取输出，好像也是在反向传播网络中，但这种提取中间层是永久性的，也适合用这些层的做其他运算，这些运算是计算在整体网络框架中的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> ii, model <span class="keyword">in</span> enumerate(self.features):</span><br><span class="line">        x = model(x)</span><br><span class="line">        <span class="keyword">if</span> ii <span class="keyword">in</span> &#123;<span class="number">3</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">22</span>&#125;:</span><br><span class="line">            results.append(x)</span><br><span class="line"></span><br><span class="line">    vgg_outputs = namedtuple(<span class="string">"VggOutputs"</span>, [<span class="string">'relu1_2'</span>, <span class="string">'relu2_2'</span>, <span class="string">'relu3_3'</span>, <span class="string">'relu4_3'</span>])</span><br><span class="line">    <span class="keyword">return</span> vgg_outputs(*results)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二种方法，适合在在不影响整体网络的情况下拿出一个分支进行单独计算，现在还不清楚这样子会不会影响backward，个人感觉会，因为也是相当于一个变量对其进行计算，导数为1。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    x= self.model(x)</span><br><span class="line">    self.fea = x</span><br><span class="line">   x = self.main(x)</span><br></pre></td></tr></table></figure><hr><h3 id="transformer-py"><a href="#transformer-py" class="headerlink" title="transformer.py"></a>transformer.py</h3><p>可参考<a href="https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py" target="_blank" rel="noopener">链接</a></p><ol><li>padding的操作是边界反射补充</li><li>放大方法是双线性插值，而不是ConvTransposed2d，即unsample或者说是interpolate， 但是其中的一个参数align_corners一直<strong>没有理解</strong>，既然是双线性插值，那结果就是固定的，怎么还会因为其他参数发生变化。</li><li><p>其中，写的时候必要的时候可以写写子网络<br>这里我对residualblock提出了疑问，事实上left+right后面可以没有relu层，这一点我们可以从以下链接找到说明。<br><a href="https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py" target="_blank" rel="noopener">https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py</a><br><a href="http://torch.ch/blog/2016/02/04/resnets.html" target="_blank" rel="noopener">http://torch.ch/blog/2016/02/04/resnets.html</a></p><blockquote><p>The above result seems to suggest that it’s important to avoid changing data that passes through identity connections only. We can take this philosophy one step further: should we remove the ReLU layers at the end of each residual block? ReLU layers also perturb data that flows through identity connections, but unlike batch normalization, ReLU’s idempotence means that it doesn’t matter if data passes through one ReLU or thirty ReLUs. When we remove ReLU layers at the end of each building block, we observe a small improvement in test performance compared to the paper’s suggested ReLU placement after the addition. However, the effect is fairly minor. More exploration is needed.</p></blockquote></li><li><p>对于其他的出现的网络架构，其实都是有理可循的，但暂时不是本篇的重点，所以只做一个记录。<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">上卷积</a>简单地看了看这篇论文，unsample要比ConvTransposed2D要好，但是没有看懂。留作后续。  </p></li></ol><hr><h3 id="dataset-py-amp-visualize-py"><a href="#dataset-py-amp-visualize-py" class="headerlink" title="dataset.py &amp; visualize.py"></a>dataset.py &amp; visualize.py</h3><p>因为加载数据是用的tv.datasets.ImageFolder，所以dataset.py不需要写，<br>visualize.py是第六章的时候写好的，这里只写几个改进的</p><ol><li>self.vis = Visdom(env=env,use_incoming_socket=False, **kwargs)，这里的use_incoming_socket是不需要从浏览器接受数据到软件中，如果没有的话会提示 ‘&gt;’ not supported between instances of ‘float’ and ‘NoneType’</li><li>在一个函数前提示输入的大小和类型是一件很重要的事情，必要的时候需要输入分布，</li><li>这里的plot用了一个很巧的方法，用字典记录不同的点<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.index = &#123;&#125;</span><br><span class="line">x = self.index.get(win,<span class="number">0</span>)</span><br><span class="line">self.index[win] = x+<span class="number">1</span></span><br></pre></td></tr></table></figure></li></ol><p>其他的细节可以看代码中的记录，应该比较清晰了。</p><hr><h3 id="main-py-amp-utils-py-amp-config-py"><a href="#main-py-amp-utils-py-amp-config-py" class="headerlink" title="main.py &amp; utils.py &amp;  config.py"></a>main.py &amp; utils.py &amp;  config.py</h3><p>其中utils主要为main提供一些用到的函数，config提供参数，<br>main作为主函数，里面主要就是train(),val(),test(),help(),下面记录一些写main函数的一些疑问。</p><h4 id="cuda"><a href="#cuda" class="headerlink" title="cuda"></a>cuda</h4><p>这里写几种怎么从cpu到gpu的方法以及应用场景。<br><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line">models.<span class="keyword">to</span>(device)</span><br><span class="line">tensor = tensor.<span class="keyword">to</span>(device)</span><br><span class="line">此时使用默认的cuda，一般是cuda:<span class="number">0</span>，适用于全局</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">torch.cuda.current_device() <span class="comment"># 查询当前GPU</span></span><br><span class="line">torch.cuda.set_device(<span class="number">1</span>)</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">models.<span class="keyword">to</span>(device)</span><br><span class="line">此时用的是cuda:<span class="number">1</span>，使用于全局</span><br><span class="line"></span><br><span class="line"><span class="comment">#第三种</span></span><br><span class="line"><span class="comment">#上下文管理器</span></span><br><span class="line"><span class="keyword">with</span> torch.cuda.device(<span class="number">1</span>):</span><br><span class="line">    models.<span class="keyword">to</span>(device)</span><br><span class="line"><span class="comment">#第四种</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>]=<span class="string">"2"</span></span><br><span class="line">没用过</span><br></pre></td></tr></table></figure></p><h3 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a><a href="">tqdm</a></h3><p><a href="https://blog.csdn.net/langb2014/article/details/54798823?locationnum=8&amp;fps=1" target="_blank" rel="noopener">https://blog.csdn.net/langb2014/article/details/54798823?locationnum=8&amp;fps=1</a><br>进度条，但是只在jupyter和终端中用的时候效果很明显，在代码中用的效果没有那么好，tqdm试了试，用在enumerate()中时，需要写成这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">elements = (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>)</span><br><span class="line"><span class="keyword">for</span> count, ele <span class="keyword">in</span> tqdm(enumerate(elements)):</span><br><span class="line">    print(count, i)</span><br><span class="line"><span class="comment"># two arguments</span></span><br><span class="line"><span class="keyword">for</span> count, ele <span class="keyword">in</span> tqdm(enumerate(elements), total=len(train_ids), leave=<span class="keyword">False</span>):</span><br><span class="line">    print(count, i)</span><br></pre></td></tr></table></figure></p><p>包括zip也是一样，因为他们返回的是一个生成器，并不知道长度。</p><h3 id="反向传播和梯度下降"><a href="#反向传播和梯度下降" class="headerlink" title="反向传播和梯度下降"></a>反向传播和梯度下降</h3><p>参考链接<a href="https://blog.csdn.net/qq_16234613/article/details/80025832" target="_blank" rel="noopener">https://blog.csdn.net/qq_16234613/article/details/80025832</a><br>这里主要是针对第七章和第八章出现的反向传播和梯度下降出现的问题进行记录。<br>在第七章，是这么实现分别训练的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">fake_img =  netg(noises).detach() </span><br><span class="line">fake_output = netd(fake_img)</span><br><span class="line">error_d_fake = criterion(fake_output, fake_labels)</span><br><span class="line">error_d_fake.backward()</span><br><span class="line">optimizer_d.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer_g.zero_grad()</span><br><span class="line">noises.data.copy_(t.randn(opt.batch_size, opt.nz, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">fake_img = netg(noises)</span><br><span class="line">output = netd(fake_img)</span><br><span class="line">error_g = criterion(output, true_labels)</span><br><span class="line">error_g.backward()</span><br><span class="line">optimizer_g.step()</span><br></pre></td></tr></table></figure></p><p>y = x.detach()：表示将生成一个新的叶子节点，值与当前节点的值相同，但是y.requires_grad = False, y.grad_fn=None，此时x和y共享内存，对y数据的操作也会影响x，可以理解为冻结了通过y进行反向传播的路。如果在网络的输出detach，即y= models(x).detach()，可以理解成，models只进行前向传播，grad=None。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">17</span>]: <span class="selector-tag">a</span> = torch.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: <span class="selector-tag">a</span>.requires_grad=True</span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: <span class="selector-tag">b</span> = a*<span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: <span class="selector-tag">b</span>.requires_grad</span><br><span class="line">Out[<span class="number">20</span>]: True</span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: <span class="selector-tag">b</span>.grad_fn</span><br><span class="line">Out[<span class="number">21</span>]: &lt;MulBackward at <span class="number">0</span>x7f8fac6e40f0&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: c = <span class="selector-tag">b</span>.detach()</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: c.requires_grad</span><br><span class="line">Out[<span class="number">23</span>]: False</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: print(c.grad_fn)</span><br><span class="line">None</span><br><span class="line">In [<span class="number">25</span>]: c.is_leaf</span><br></pre></td></tr></table></figure></p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">In <span class="comment">[2]</span>: a = torch.ones(3,3)</span><br><span class="line">In <span class="comment">[14]</span>: b</span><br><span class="line">Out<span class="comment">[14]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>, grad_fn=&lt;MulBackward&gt;)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[15]</span>: c =  b.detach()</span><br><span class="line"></span><br><span class="line">In <span class="comment">[16]</span>: c</span><br><span class="line">Out<span class="comment">[16]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[17]</span>: c<span class="comment">[0,0]</span>=1</span><br><span class="line"></span><br><span class="line">In <span class="comment">[18]</span>: c</span><br><span class="line">Out<span class="comment">[18]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[1., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[19]</span>: b</span><br><span class="line">Out<span class="comment">[19]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[1., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>, grad_fn=&lt;MulBackward&gt;)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[20]</span>: c.requires_grad</span><br><span class="line">Out<span class="comment">[20]</span>: False</span><br><span class="line"></span><br><span class="line">In <span class="comment">[21]</span>: b.grad_fn</span><br><span class="line">Out<span class="comment">[21]</span>: &lt;MulBackward at 0x7f764429ffd0&gt;</span><br><span class="line"></span><br><span class="line">In <span class="comment">[22]</span>: b.grad_fn.next_functions</span><br><span class="line">Out<span class="comment">[22]</span>: ((&lt;AccumulateGrad at 0x7f7644428358&gt;, 0),)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[23]</span>: a.grad_fn</span><br></pre></td></tr></table></figure><p>在第八章，是这么表示的<br><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">param</span> <span class="built_in">in</span> vgg16.parameters():</span><br><span class="line">    <span class="built_in">param</span>.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure></p><p>这种表示可以使得某一个网络不参与梯度下降这个过程，但是对于网络的输入和输出还是支持梯度下降的。<br>requires_grad只是表示当前的变量不再需要梯度下降，<br>综上所述，对于中间变量，需要使用x.detach()，使其变成默认的叶子节点，对于叶子节点，使用x.requires_grad。并且对于中间变量使用requires_grad会报错。</p><p>在第八章，还有一种表示方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    features = vgg16(style_img)</span><br><span class="line">    gram_style = [gram_matrix(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> features]</span><br><span class="line"></span><br><span class="line"><span class="meta">@t.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stylize</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p><p>这种方法会使得任何计算得到的结果都是requires_grad = False,暂时不清楚和detach()的区别。也是一种表示只前向传播的方法，不参与反向传播和梯度下降。</p><h3 id="train"><a href="#train" class="headerlink" title="train()"></a>train()</h3><p>图片分为两种：风格图片，只需要一张，内容图片，很多，用于训练，这一点没有暂时没有理解为什么这么设置。其中，对输入的图片进行了乘以255，我觉得是因为为了使模型的输出直接就是255，不需要再进行处理，没有验证。<br>ensor.item()<br> tensor.tolist()<br>content_image = tv.datasets.folder.default_loader(opt.content_path)<br>在训练过程中，会发现对于整个训练过程，不仅有神经网络，而且还有自己定义的函数，nn.functional，还有两个损失函数，这是之前没有预料到的。</p><h3 id="保存图片"><a href="#保存图片" class="headerlink" title="保存图片"></a>保存图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存图片的几种方法，第七章的是 </span></span><br><span class="line"><span class="comment"># 0-1</span></span><br><span class="line">tv.utils.save_image(fix_fake_imgs,<span class="string">'%s/%s.png'</span> % (opt.img_save_path, epoch),normalize=<span class="keyword">True</span>, range=(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># vis.save竟然没找到  我的神   </span></span><br><span class="line"><span class="comment"># 0-1</span></span><br><span class="line">vis.img(<span class="string">'input'</span>)</span><br><span class="line">vis.save([opt.env])</span><br></pre></td></tr></table></figure><h3 id="utils-py"><a href="#utils-py" class="headerlink" title="utils.py"></a>utils.py</h3><p>这里的疑问是得到gram矩阵的时候，为什么要除以c*h*w,而不是h*w，虽然源码都是这么写的。</p><p>写到这里也还是还要很多疑问，暂时保留。<br>昨天发现训练的过程不对，今天在对比代码的过程中，发现了自己写代码的一些漏洞，主要有</p><ol><li>命名不规范：表示同一个东西出现了两个命名，导致了自己在写代码的过程中传参出现了问题，或者是一类东西没有一个规则进行命名，导致自己在写代码的过程中用到之前的变量的时候必须返回去去查找这个变量，效率低且容易出错。</li><li>对源码的修改不是很恰当，导致在写上卷积层的输出和源码完全不一致，这个是自己之前没有遇到的。</li><li>visdom的运用，我用不同的environment导致结果也不一样，default是之前一直用的，这次换成了test1之后显示的结果就对了。这个暂时还不清楚原因，如果是会保留信息的话，但是plot是重新开始画的，等会测试测试vis的问题。是网络的问题。但是vis.save()的介绍是序列化信息，暂时还没有理解。</li></ol><hr><p>## </p><h1 id="对单张图片进行加载验证"><a href="#对单张图片进行加载验证" class="headerlink" title="对单张图片进行加载验证"></a>对单张图片进行加载验证</h1><p>content_image = tv.datasets.folder.default_loader(opt.content_path)<br>可以理解成Image.open，看源码就可以知道的</p><p>贴两个成果图看看效果。<br><img src="/pictures/neural-style/pic1.png" alt="训练过程中的图片"></p><h2 id="遗留的问题"><a href="#遗留的问题" class="headerlink" title="遗留的问题"></a>遗留的问题</h2><p>Gram矩阵为什么可以代表图片风格，这里有个解释(<a href="https://arxiv.org/pdf/1701.01036.pdf)，还没来得及看。" target="_blank" rel="noopener">https://arxiv.org/pdf/1701.01036.pdf)，还没来得及看。</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1 前言&quot;&gt;&lt;/a&gt;1 前言&lt;/h1&gt;&lt;p&gt;本文主要是针对&lt;a href=&quot;https://github.com/chenyuntc/pytorch-book&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;陈云的PyTorch入门与实践&lt;/a&gt;的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。&lt;br&gt;这是我的&lt;a href=&quot;https://github.com/TJJTJJTJJ/pytorch__learn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;代码&lt;/a&gt;&lt;br&gt;大神链接：(&lt;a href=&quot;https://github.com/anishathalye/neural-style&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/anishathalye/neural-style&lt;/a&gt;)&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="deep learning" scheme="http://yoursite.com/categories/deep-learning/"/>
    
    
      <category term="neural style transfer" scheme="http://yoursite.com/tags/neural-style-transfer/"/>
    
  </entry>
  
</feed>
