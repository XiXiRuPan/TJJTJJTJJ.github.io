<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="做一个有趣的灵魂">
<meta name="keywords" content="代码">
<meta property="og:type" content="website">
<meta property="og:title" content="田佳杰">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="田佳杰">
<meta property="og:description" content="做一个有趣的灵魂">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="田佳杰">
<meta name="twitter:description" content="做一个有趣的灵魂">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>田佳杰</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>
    <a href="https://github.com/TJJTJJTJJ/TJJTJJTJJ.github.io.git"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">田佳杰</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">记录一些学习到的东西和论文记录</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      
        
        <li class="menu-item menu-item-something">
          <a href="/something" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            something
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/11/neural-style/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiajie Tian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="田佳杰">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/11/neural-style/" itemprop="url">neural style</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-11T11:19:25+08:00">
                2018-09-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>本文主要是针对<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">陈云的PyTorch入门与实践</a>的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。<br>这是我的<a href="https://github.com/TJJTJJTJJ/pytorch__learn" target="_blank" rel="noopener">代码</a><br>大神链接：(<a href="https://github.com/anishathalye/neural-style" target="_blank" rel="noopener">https://github.com/anishathalye/neural-style</a>)<br>这是论文作者写的</p>
<hr>
<h1 id="2-问题及其解决"><a href="#2-问题及其解决" class="headerlink" title="2 问题及其解决"></a>2 问题及其解决</h1><p>我在第六章和第七章的时候还是基于pytorch 0.4.0，而第八章的时候我开始基于pytorch 0.4.1，所以以下的内容介绍都是基于0.4.1</p>
<h2 id="2-1-文件组织形式"><a href="#2-1-文件组织形式" class="headerlink" title="2.1 文件组织形式"></a>2.1 文件组织形式</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">├─checkpoints/</span><br><span class="line">├─content_img/</span><br><span class="line">│  ├─<span class="selector-tag">input</span>.jpg</span><br><span class="line">│  ├─output.jpg</span><br><span class="line">│  └─style.jpg</span><br><span class="line">├─data/</span><br><span class="line">│  ├─coco/<span class="selector-tag">a</span>.jpg</span><br><span class="line"></span><br><span class="line">├─dataset/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─dataset.py</span><br><span class="line">├─models/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─PackedVGG.py</span><br><span class="line">│  └─transformer_net.py</span><br><span class="line">└─utils/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─utils.py</span><br><span class="line">│  └─visualize.py</span><br><span class="line">├─config.py</span><br><span class="line">└─main.py</span><br></pre></td></tr></table></figure>
<p>其中，上半部分是对数据和模型的保存组织形式，我们只需要能对应起来即可，其中，checkpoints是为了保存模型，content_img中的style.jpg是训练时候的风格图片，input.jpg是测试的输入，output.jpg是测试的输出，data中的数据是训练数据，主要是因为这个训练数据太整齐，是用ImageFolder读取的，为了避免麻烦，也为了在测试的时候方便观察图片，所以style.jpg我们暂时放在了content中。<br>下半部分是重点，我们需要写的代码，每次都是先从dataset.py和models开始写起，然后导入visualize.py，这个文件基本不会发生改变，然后同时写main.py和config.py，边写边扩展utils中的其他文件，例如main中用到的函数等等。</p>
<h2 id="2-2-models"><a href="#2-2-models" class="headerlink" title="2.2 models"></a>2.2 models</h2><h3 id="PackedVGG-py"><a href="#PackedVGG-py" class="headerlink" title="PackedVGG.py"></a>PackedVGG.py</h3><p>这里我们主要是取已有的网络，得到中间层的输出<br><strong>models.named_parameters()</strong>:返回的是一个生成器，每次返回一个参数的关键字和值<br><strong>models.state_dict()</strong>:返回的是一个字典，记录了参数的关键字和值<br><strong>models.parameters()</strong>:返回的是变量，没有名字，可以在requires_grad中用到<br>models.features返回的是相对应的模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">7</span>]: <span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: models = vgg16(pretrained=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: model = models.features[:<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: model</span><br><span class="line">Out[<span class="number">10</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: models.parameters()</span><br><span class="line">Out[<span class="number">11</span>]: &lt;generator object Module.parameters at <span class="number">0x7f8fad26b3b8</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: models.named_parameters()</span><br><span class="line">Out[<span class="number">12</span>]: &lt;generator object Module.named_parameters at <span class="number">0x7f8f29e99d58</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: model.named_parameters()</span><br><span class="line">Out[<span class="number">13</span>]: &lt;generator object Module.named_parameters at <span class="number">0x7f8fad26b2b0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: model.parameters()</span><br><span class="line">Out[<span class="number">14</span>]: &lt;generator object Module.parameters at <span class="number">0x7f8fad26b4c0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: model.state_dict()</span><br><span class="line">Out[<span class="number">15</span>]: </span><br><span class="line">OrderedDict([(<span class="string">'0.weight'</span>, tensor([[[[<span class="number">-0.5537</span>,  <span class="number">0.1427</span>,  <span class="number">0.5290</span>],</span><br><span class="line">                        [<span class="number">-0.5831</span>,  <span class="number">0.3566</span>,  <span class="number">0.7657</span>],</span><br><span class="line">                        [<span class="number">-0.6902</span>, <span class="number">-0.0480</span>,  <span class="number">0.4841</span>]],</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16</span><br><span class="line">models = vgg16(pretarined = <span class="keyword">True</span>)</span><br><span class="line">In [<span class="number">19</span>]: models</span><br><span class="line">Out[<span class="number">19</span>]: </span><br><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">3</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">6</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">8</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">11</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">13</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">15</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">18</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">20</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">22</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">    (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">25</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">27</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">29</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">    (<span class="number">3</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">    (<span class="number">4</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: models.features</span><br><span class="line">Out[<span class="number">20</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">3</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">6</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">8</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">11</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">13</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">15</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">18</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">20</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">22</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">  (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">25</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">27</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">29</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: models.features[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">21</span>]: ReLU(inplace)</span><br><span class="line"><span class="comment"># list</span></span><br><span class="line">In [<span class="number">27</span>]: models4 = models2[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: models4</span><br><span class="line">Out[<span class="number">28</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: models4list</span><br><span class="line">Out[<span class="number">32</span>]: </span><br><span class="line">[Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line"> ReLU(inplace)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">36</span>]: models4list[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">36</span>]: ReLU(inplace)</span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: models4list[<span class="number">1</span>].named_parameters</span><br><span class="line">Out[<span class="number">37</span>]: &lt;bound method Module.named_parameters of ReLU(inplace)&gt;</span><br></pre></td></tr></table></figure>
<p>sequencial是支持索引操作的<br>list(module)会变成一个list，可以通过索引来获取层，注意，nn.ModuleList, nn.Sequential, nn.Conv等都是Module,都可以通过named_parameters来获取参数。<br>为了能够提取出中间层的输出，作者换了一个方法，用的nn.ModuleList,nn.ModuleList和nn.Sequential的区别在此才真正显现，nn.Sequential更有利于直接把输入传给Module，计算是一个整体，写起来更方便，而nn.Modulist则不能直接把输入传给Module，需要用循环传输入，更有利于在层中做一些保留，提取中间层的输出。后面我们会讲到hook。或者说提取中间层的输出我们可以选择在定义网络的forward中进行，另外，就是需要注意的是，这里的输入是一个batch_size大小的矩阵，所以即便像作者这样，用一个列表保存输出，但实际输出的列表中的元素都是(b,n,h,w)大小的。后面我会验证。</p>
<p>提取中间层的输出有两种方法：<br>第二种方法参考链接：<a href="https://www.jianshu.com/p/0a23db1df55a" target="_blank" rel="noopener">https://www.jianshu.com/p/0a23db1df55a</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种方法，这种方法是在前向网络中提取输出，好像也是在反向传播网络中，但这种提取中间层是永久性的，也适合用这些层的做其他运算，这些运算是计算在整体网络框架中的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> ii, model <span class="keyword">in</span> enumerate(self.features):</span><br><span class="line">        x = model(x)</span><br><span class="line">        <span class="keyword">if</span> ii <span class="keyword">in</span> &#123;<span class="number">3</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">22</span>&#125;:</span><br><span class="line">            results.append(x)</span><br><span class="line"></span><br><span class="line">    vgg_outputs = namedtuple(<span class="string">"VggOutputs"</span>, [<span class="string">'relu1_2'</span>, <span class="string">'relu2_2'</span>, <span class="string">'relu3_3'</span>, <span class="string">'relu4_3'</span>])</span><br><span class="line">    <span class="keyword">return</span> vgg_outputs(*results)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二种方法，适合在在不影响整体网络的情况下拿出一个分支进行单独计算，现在还不清楚这样子会不会影响backward，个人感觉会，因为也是相当于一个变量对其进行计算，导数为1。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    x= self.model(x)</span><br><span class="line">    self.fea = x</span><br><span class="line">   x = self.main(x)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="transformer-py"><a href="#transformer-py" class="headerlink" title="transformer.py"></a>transformer.py</h3><p>可参考<a href="https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py" target="_blank" rel="noopener">链接</a></p>
<ol>
<li>padding的操作是边界反射补充</li>
<li>放大方法是双线性插值，而不是ConvTransposed2d，即unsample或者说是interpolate， 但是其中的一个参数align_corners一直<strong>没有理解</strong>，既然是双线性插值，那结果就是固定的，怎么还会因为其他参数发生变化。</li>
<li><p>其中，写的时候必要的时候可以写写子网络<br>这里我对residualblock提出了疑问，事实上left+right后面可以没有relu层，这一点我们可以从以下链接找到说明。<br><a href="https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py" target="_blank" rel="noopener">https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py</a><br><a href="http://torch.ch/blog/2016/02/04/resnets.html" target="_blank" rel="noopener">http://torch.ch/blog/2016/02/04/resnets.html</a></p>
<blockquote>
<p>The above result seems to suggest that it’s important to avoid changing data that passes through identity connections only. We can take this philosophy one step further: should we remove the ReLU layers at the end of each residual block? ReLU layers also perturb data that flows through identity connections, but unlike batch normalization, ReLU’s idempotence means that it doesn’t matter if data passes through one ReLU or thirty ReLUs. When we remove ReLU layers at the end of each building block, we observe a small improvement in test performance compared to the paper’s suggested ReLU placement after the addition. However, the effect is fairly minor. More exploration is needed.</p>
</blockquote>
</li>
<li><p>对于其他的出现的网络架构，其实都是有理可循的，但暂时不是本篇的重点，所以只做一个记录。<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">上卷积</a>简单地看了看这篇论文，unsample要比ConvTransposed2D要好，但是没有看懂。留作后续。  </p>
</li>
</ol>
<hr>
<h3 id="dataset-py-amp-visualize-py"><a href="#dataset-py-amp-visualize-py" class="headerlink" title="dataset.py &amp; visualize.py"></a>dataset.py &amp; visualize.py</h3><p>因为加载数据是用的tv.datasets.ImageFolder，所以dataset.py不需要写，<br>visualize.py是第六章的时候写好的，这里只写几个改进的</p>
<ol>
<li>self.vis = Visdom(env=env,use_incoming_socket=False, **kwargs)，这里的use_incoming_socket是不需要从浏览器接受数据到软件中，如果没有的话会提示 ‘&gt;’ not supported between instances of ‘float’ and ‘NoneType’</li>
<li>在一个函数前提示输入的大小和类型是一件很重要的事情，必要的时候需要输入分布，</li>
<li>这里的plot用了一个很巧的方法，用字典记录不同的点<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.index = &#123;&#125;</span><br><span class="line">x = self.index.get(win,<span class="number">0</span>)</span><br><span class="line">self.index[win] = x+<span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>其他的细节可以看代码中的记录，应该比较清晰了。</p>
<hr>
<h3 id="main-py-amp-utils-py-amp-config-py"><a href="#main-py-amp-utils-py-amp-config-py" class="headerlink" title="main.py &amp; utils.py &amp;  config.py"></a>main.py &amp; utils.py &amp;  config.py</h3><p>其中utils主要为main提供一些用到的函数，config提供参数，<br>main作为主函数，里面主要就是train(),val(),test(),help(),下面记录一些写main函数的一些疑问。</p>
<h4 id="cuda"><a href="#cuda" class="headerlink" title="cuda"></a>cuda</h4><p>这里写几种怎么从cpu到gpu的方法以及应用场景。<br><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line">models.<span class="keyword">to</span>(device)</span><br><span class="line">tensor = tensor.<span class="keyword">to</span>(device)</span><br><span class="line">此时使用默认的cuda，一般是cuda:<span class="number">0</span>，适用于全局</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">torch.cuda.current_device() <span class="comment"># 查询当前GPU</span></span><br><span class="line">torch.cuda.set_device(<span class="number">1</span>)</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">models.<span class="keyword">to</span>(device)</span><br><span class="line">此时用的是cuda:<span class="number">1</span>，使用于全局</span><br><span class="line"></span><br><span class="line"><span class="comment">#第三种</span></span><br><span class="line"><span class="comment">#上下文管理器</span></span><br><span class="line"><span class="keyword">with</span> torch.cuda.device(<span class="number">1</span>):</span><br><span class="line">    models.<span class="keyword">to</span>(device)</span><br><span class="line"><span class="comment">#第四种</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>]=<span class="string">"2"</span></span><br><span class="line">没用过</span><br></pre></td></tr></table></figure></p>
<h3 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a><a href="">tqdm</a></h3><p>进度条，但是只在jupyter和终端中用的时候效果很明显，在代码中用的效果没有那么好，tqdm试了试，用在enumerate()中时，需要写成这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">elements = (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>)</span><br><span class="line"><span class="keyword">for</span> count, ele <span class="keyword">in</span> tqdm(enumerate(elements)):</span><br><span class="line">    print(count, i)</span><br><span class="line"><span class="comment"># two arguments</span></span><br><span class="line"><span class="keyword">for</span> count, ele <span class="keyword">in</span> tqdm(enumerate(elements), total=len(train_ids)):</span><br><span class="line">    print(count, i)</span><br></pre></td></tr></table></figure></p>
<p>包括zip也是一样，因为他们返回的是一个生成器，并不知道长度。</p>
<h3 id="反向传播和梯度下降"><a href="#反向传播和梯度下降" class="headerlink" title="反向传播和梯度下降"></a>反向传播和梯度下降</h3><p>参考链接<a href="https://blog.csdn.net/qq_16234613/article/details/80025832" target="_blank" rel="noopener">https://blog.csdn.net/qq_16234613/article/details/80025832</a><br>这里主要是针对第七章和第八章出现的反向传播和梯度下降出现的问题进行记录。<br>在第七章，是这么实现分别训练的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">fake_img =  netg(noises).detach() </span><br><span class="line">fake_output = netd(fake_img)</span><br><span class="line">error_d_fake = criterion(fake_output, fake_labels)</span><br><span class="line">error_d_fake.backward()</span><br><span class="line">optimizer_d.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer_g.zero_grad()</span><br><span class="line">noises.data.copy_(t.randn(opt.batch_size, opt.nz, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">fake_img = netg(noises)</span><br><span class="line">output = netd(fake_img)</span><br><span class="line">error_g = criterion(output, true_labels)</span><br><span class="line">error_g.backward()</span><br><span class="line">optimizer_g.step()</span><br></pre></td></tr></table></figure></p>
<p>y = x.detach()：表示将生成一个新的叶子节点，值与当前节点的值相同，但是y.requires_grad = False, y.grad_fn=None，此时x和y共享内存，对y数据的操作也会影响x，可以理解为冻结了通过y进行反向传播的路。如果在网络的输出detach，即y= models(x).detach()，可以理解成，models只进行前向传播，grad=None。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">17</span>]: <span class="selector-tag">a</span> = torch.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: <span class="selector-tag">a</span>.requires_grad=True</span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: <span class="selector-tag">b</span> = a*<span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: <span class="selector-tag">b</span>.requires_grad</span><br><span class="line">Out[<span class="number">20</span>]: True</span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: <span class="selector-tag">b</span>.grad_fn</span><br><span class="line">Out[<span class="number">21</span>]: &lt;MulBackward at <span class="number">0</span>x7f8fac6e40f0&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: c = <span class="selector-tag">b</span>.detach()</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: c.requires_grad</span><br><span class="line">Out[<span class="number">23</span>]: False</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: print(c.grad_fn)</span><br><span class="line">None</span><br><span class="line">In [<span class="number">25</span>]: c.is_leaf</span><br></pre></td></tr></table></figure></p>
<figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">In <span class="comment">[2]</span>: a = torch.ones(3,3)</span><br><span class="line">In <span class="comment">[14]</span>: b</span><br><span class="line">Out<span class="comment">[14]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>, grad_fn=&lt;MulBackward&gt;)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[15]</span>: c =  b.detach()</span><br><span class="line"></span><br><span class="line">In <span class="comment">[16]</span>: c</span><br><span class="line">Out<span class="comment">[16]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[17]</span>: c<span class="comment">[0,0]</span>=1</span><br><span class="line"></span><br><span class="line">In <span class="comment">[18]</span>: c</span><br><span class="line">Out<span class="comment">[18]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[1., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[19]</span>: b</span><br><span class="line">Out<span class="comment">[19]</span>: </span><br><span class="line">tensor(<span class="comment">[<span class="comment">[1., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[2., 2., 2.]</span>]</span>, grad_fn=&lt;MulBackward&gt;)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[20]</span>: c.requires_grad</span><br><span class="line">Out<span class="comment">[20]</span>: False</span><br><span class="line"></span><br><span class="line">In <span class="comment">[21]</span>: b.grad_fn</span><br><span class="line">Out<span class="comment">[21]</span>: &lt;MulBackward at 0x7f764429ffd0&gt;</span><br><span class="line"></span><br><span class="line">In <span class="comment">[22]</span>: b.grad_fn.next_functions</span><br><span class="line">Out<span class="comment">[22]</span>: ((&lt;AccumulateGrad at 0x7f7644428358&gt;, 0),)</span><br><span class="line"></span><br><span class="line">In <span class="comment">[23]</span>: a.grad_fn</span><br></pre></td></tr></table></figure>
<p>在第八章，是这么表示的<br><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">param</span> <span class="built_in">in</span> vgg16.parameters():</span><br><span class="line">    <span class="built_in">param</span>.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure></p>
<p>这种表示可以使得某一个网络不参与梯度下降这个过程，但是对于网络的输入和输出还是支持梯度下降的。<br>requires_grad只是表示当前的变量不再需要梯度下降，<br>综上所述，对于中间变量，需要使用x.detach()，使其变成默认的叶子节点，对于叶子节点，使用x.requires_grad。并且对于中间变量使用requires_grad会报错。</p>
<p>在第八章，还有一种表示方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    features = vgg16(style_img)</span><br><span class="line">    gram_style = [gram_matrix(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> features]</span><br><span class="line"></span><br><span class="line"><span class="meta">@t.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stylize</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<p>这种方法会使得任何计算得到的结果都是requires_grad = False,暂时不清楚和detach()的区别。也是一种表示只前向传播的方法，不参与反向传播和梯度下降。</p>
<h3 id="train"><a href="#train" class="headerlink" title="train()"></a>train()</h3><p>图片分为两种：风格图片，只需要一张，内容图片，很多，用于训练，这一点没有暂时没有理解为什么这么设置。其中，对输入的图片进行了乘以255，我觉得是因为为了使模型的输出直接就是255，不需要再进行处理，没有验证。<br>ensor.item()<br> tensor.tolist()<br>content_image = tv.datasets.folder.default_loader(opt.content_path)</p>
<h3 id="保存图片"><a href="#保存图片" class="headerlink" title="保存图片"></a>保存图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存图片的几种方法，第七章的是 </span></span><br><span class="line"><span class="comment"># 0-1</span></span><br><span class="line">tv.utils.save_image(fix_fake_imgs,<span class="string">'%s/%s.png'</span> % (opt.img_save_path, epoch),normalize=<span class="keyword">True</span>, range=(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># vis.save竟然没找到  我的神   </span></span><br><span class="line"><span class="comment"># 0-1</span></span><br><span class="line">vis.img(<span class="string">'input'</span>)</span><br><span class="line">vis.save([opt.env])</span><br></pre></td></tr></table></figure>
<h3 id="utils-py"><a href="#utils-py" class="headerlink" title="utils.py"></a>utils.py</h3><p>这里的疑问是得到gram矩阵的时候，为什么要除以c*h*w,而不是h*w，虽然源码都是这么写的。</p>
<p>写到这里也还是还要很多疑问，暂时保留。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/06/data/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiajie Tian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="田佳杰">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/06/data/" itemprop="url">data</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-06T23:06:10+08:00">
                2018-09-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="Market-1501"><a href="#Market-1501" class="headerlink" title="Market-1501"></a>Market-1501</h1><ul>
<li>6个摄像头</li>
<li>1501个人，其中751个人、12936张图片用于训练，750个人、19732张图片用于测试，</li>
<li>3368张查询图片    </li>
</ul>
<hr>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/09/06/data/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/04/dataset/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiajie Tian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="田佳杰">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/04/dataset/" itemprop="url">dataset</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-04T19:28:20+08:00">
                2018-09-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://blog.fangchengjin.cn/reid-market-1501.html" target="_blank" rel="noopener">Person Re-identification数据集描述——Market-1501</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/04/Inception/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiajie Tian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="田佳杰">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/04/Inception/" itemprop="url">Inception</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-04T19:23:43+08:00">
                2018-09-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>关于Inception的好的讲解<br><a href="https://blog.csdn.net/loveliuzz/article/details/79135583" target="_blank" rel="noopener">深度学习卷积神经网络——经典网络GoogLeNet(Inception V3)网络的搭建与实现</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/04/deconv/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiajie Tian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="田佳杰">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/04/deconv/" itemprop="url">deconv\dilated conv</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-04T16:07:46+08:00">
                2018-09-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近看的一片论文里看到了反卷积，找了一些材料，记录一下，留作纪念。<br>首先定义符号：    </p>
<ul>
<li>假设本文提到的图形和卷积核都是一维的线或者二维的正方形，$x$和$y$轴方向的padding和stride相同    </li>
<li>$i,o,k,p,s,i’,o’,k’,p’,s’$分别表示卷积/反卷积输入图片的大小input size, 输出图片的大小 output size，卷积/反卷积的核大小kernel size，padding，stride.<br><a href="https://github.com/vdumoulin/conv_arithmetic#convolution-arithmetic" target="_blank" rel="noopener">动图演示</a><br><a href="https://buptldy.github.io/2016/10/29/2016-10-29-deconv/" target="_blank" rel="noopener">详细解析</a><br>反卷积对应的是直接在原图上添加0<br>对应的公式如下：<br>$$ i’=o$$<br>$$ o’=i $$<br>$$ k’=k $$<br>$$ s’=1$$<br>$$ p’=k-1-p $$<br>$$ d’=1 $$<br>$$ o’ = (s-1)(i’-1)+i’-k’+2p’ $$<br>$$ o’ =   s(i’-1)+k-2p $$<br>$$ d’ = 2$$<br>$$ k’ = k + (d-1)<em>(k-1) $$<br>$$ p’ = k-1-p+(d-1)</em>(k-1) $$<br>$$ o’ = (s-1)(i’-1)+i’-k’+2p’ $$<br>$$ o’ = s(i’-1)+k+(d-1)<em>(k-1)-2p’ $$<br>所以要保持图片图片大小不变，必须令s=1，$$ d = \frac{2}{k-1}</em>p $$<br>例如：k=3，d=p,</li>
</ul>
<p>以上只是先说明了对应尺寸的大小，具体的数学原理留作补充。</p>
<p>扩张卷积的计算公式与上面不一样<br>扩张卷积扩张的是卷积核的大小，在卷积核上添加0<br><a href="https://blog.csdn.net/Quincuntial/article/details/78743033" target="_blank" rel="noopener">扩张卷积</a><br>$$ o’=\frac{i’-k+2p-(k-1)*(d-1)}{s}+1$$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/26/pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiajie Tian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="田佳杰">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/26/pytorch/" itemprop="url">pytorch</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-26T21:38:25+08:00">
                2018-08-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/pytorch/" itemprop="url" rel="index">
                    <span itemprop="name">pytorch</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><p>魔术方法: P23  </p>
<p>调试: P27<br><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ipdb</span><br><span class="line"><span class="title">ipdb</span>.set_trace()</span><br></pre></td></tr></table></figure></p>
<p>带下划线_的函数会修改Tensor本身，比如x.add_(y)和x.add(y)的区别  </p>
<h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>Numpy与Tensor共享内存<br>b = a.numpy() # Tensor -&gt; Numpy<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/08/26/pytorch/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/26/python-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiajie Tian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="田佳杰">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/26/python-learning/" itemprop="url">python learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-26T17:34:06+08:00">
                2018-08-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="python的学习过程："><a href="#python的学习过程：" class="headerlink" title="python的学习过程："></a>python的学习过程：</h1><ul>
<li>第一个是看着廖雪峰的网站，里面的内容基础，是关于数据结构等的十分基本的内容，适合小白入门</li>
<li>第二个是流畅的python<ul>
<li>这本书比python-codebook还深入，更适合当你实现了一个功能之后，还是想知道其具体怎么实现的时候查询。</li>
<li>第三个是python-codebook. </li>
<li>它的组织形式是任务式、问题式的，而且问题也相对而言比较高级，不是算法导论那种以解决某个实际问题，而是在编程上我想实现什么更好的功能那种问题，通过每一个问题，或者说通过每一个你想怎么更优的实现一个方法的思路，来引导如何更好地写代码，实现高级功能。这本书的前提是你已经入门，并且写了一段时间的python代码，在实际写的过程中已经遇到了类似的问题，也勉强实现了，只是苦于没有更好更顺心的方法实现。我现在是个小白，看这本书用了将近两周吧，主要看了第一二三四七八章，里面的代码翔实。其他的也是略微看了看，因为没有实际操作背景，有的时候不懂为什么那样做会更好，可以在以后的编程过程中，遇到这样的情况：这个我能勉强实现，但是感觉不太好，我想实现的更优美。那就应该来看看这本书，说不定这本书的实现能给自己一些思路。不适合为了读而读，因为不是入门。</li>
</ul>
</li>
<li>接下来可以考虑看看那种直接算法任务型的。刚刚看了看python算法教程，估计要跳着看了，因为里面的关于算法的内容已经熟悉了，可以扫描着看。</li>
<li></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/22/MdEditor/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiajie Tian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="田佳杰">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/22/MdEditor/" itemprop="url">MdEditor</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-22T16:22:31+08:00">
                2018-08-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="欢迎使用-Markdown在线编辑器-MdEditor"><a href="#欢迎使用-Markdown在线编辑器-MdEditor" class="headerlink" title="欢迎使用 Markdown在线编辑器 MdEditor"></a>欢迎使用 Markdown在线编辑器 MdEditor</h1><p><strong>Markdown是一种轻量级的「标记语言」</strong></p>
<p><img src="http://www.mdeditor.com/images/logos/markdown.png" alt="markdown" title="markdown"><br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/08/22/MdEditor/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/22/test1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiajie Tian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="田佳杰">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/22/test1/" itemprop="url">test1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-22T15:30:17+08:00">
                2018-08-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>$$E=mc^2$$</p>
<p>\begin{aligned}<br>\dot{x} &amp; = \sigma(y-x) \\<br>\dot{y} &amp; = \rho x - y - xz \\<br>\dot{z} &amp; = -\beta z + xy<br>\end{aligned}</p>
<p>基准测试<a href="http:www.baidu.com" target="_blank" rel="noopener">百度</a></p>
<p>是一个链接测试<a href="http://www.baidu.com" title="baid" target="_blank" rel="noopener">百度</a></p>
<p>第二个链接测试<a href="http://www.baidu.com" target="_blank" rel="noopener">bai du</a></p>
<p>第三个链接测试<a href="http://www.baidu.com" target="_blank" rel="noopener">badu</a></p>
<p>第四个测试<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>
<p>第五个测试这是一个链接到谷歌的<a href="http://www.google.com" target="_blank" rel="noopener">^脚注</a>。</p>
<p>第六个测试 Here is a footnote reference,<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> and another.[^longnote]</p>
<p>[^longnote]: Here’s one with multiple blocks.</p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">Here is the footnote.</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;"><a href="http://www.baidu.com" target="_blank" rel="noopener">http://www.baidu.com</a></span><a href="#fnref:2" rev="footnote"> ↩</a></li></ol></div></div>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/21/欢迎使用 Cmd Markdown 编辑阅读器/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiajie Tian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="田佳杰">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/21/欢迎使用 Cmd Markdown 编辑阅读器/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-21T22:18:26+08:00">
                2018-08-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="欢迎使用-Cmd-Markdown-编辑阅读器"><a href="#欢迎使用-Cmd-Markdown-编辑阅读器" class="headerlink" title="欢迎使用 Cmd Markdown 编辑阅读器"></a>欢迎使用 Cmd Markdown 编辑阅读器</h1><hr>
<p>我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，<strong>Cmd Markdown</strong> 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown：</p>
<blockquote>
<ul>
<li>整理知识，学习笔记</li>
<li>发布日记，杂文，所见所想</li>
<li>撰写发布技术文稿（代码支持）</li>
<li>撰写发布学术论文（LaTeX 公式支持）
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/08/21/欢迎使用 Cmd Markdown 编辑阅读器/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.gif"
                alt="Jiajie Tian" />
            
              <p class="site-author-name" itemprop="name">Jiajie Tian</p>
              <p class="site-description motion-element" itemprop="description">做一个有趣的灵魂</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiajie Tian</span>

  
</div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script> 

<span id="busuanzi_container_site_pv">
    总访问量: <span id="busuanzi_value_site_pv"></span>
</span>
<span id="busuanzi_container_site_uv">
    总访客: <span id="busuanzi_value_site_uv"></span>
</span>
<span id="busuanzi_container_page_pv">
  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</span>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
<script type="text/javascript" src="/js/src/love.js"></script>
