<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[starGAN]]></title>
    <url>%2F2018%2F12%2F19%2FstarGAN%2F</url>
    <content type="text"><![CDATA[0. 前言因为在person-reid论文HHL中涉及到了starGAN，所以做一个StarGAN的阅读记录，并比较与CycleGAN的区别。 StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo code-pytorch-official: https://github.com/yunjey/stargancode-tensorflow: &lt;https://github.com/taki0112/StarGAN-Tensorflow &gt; 1. Introduction解决多域之间图像转换一对多的问题，本文主要针对人脸进行改变。 关键词：multi-domain image-image translation 效果：转换效果如图所示 网络模型：CycleGAN和StarGAN模型对比 starGAN有一个生成器G，两个判别器。 备注： multi-domain：单数据集的不同属性作为了一个domain multi-datasets：不同数据集的不同属性 starGAN 分为multi-domain和multi-dataset两种。 2. Star Generative Adversarial Networks2.1 Multi-Domain Image-to-Image TranslationstarGAN: starGAN的训练模型 To achieve this, we train G to translate an input image x into an output image y conditioned on the target domain label c, G(x; c) -&gt; y. We randomly generate the target domain label c so that G learns to flexibly translate the input image. We also introduce an auxiliary classifier [22] that allows a single discriminator to control multiple domains. That is, our discriminator produces probability distributions over both sources and domain labels, D:x-&gt;{$D_{src}$(x); $D_{cls}$(x)} 符号说明：符号表|符号|含义||:-:|:-:|| x | input image || c | target domain label || c’| source domain label || y | generate image | Loss: training loss Adversarial Loss:(CycleGAN也有)对抗损失$$L_{adv}=E_x[log D_{src}(x)]+E_{x,c}[log (1-D_{src}(G(x,c)))] \tag{1}$$ Domain Classification Loss:(特有)分类损失 That is, we decompose the objective into two terms: a domain classification loss of real images used to optimize D, and a domain classification loss of fake images used to optimize G. 优化 D:$$L_{cls}^r=E_{x,c’}[-log D_{cls}(c’|x)] \tag{2}$$ By minimizing this objective, D learns to classify a real image x to its corresponding original domain c’. 优化 G:$$L_{cls}^f=E_{x,c}[-log D_{cls}(c|G(x,c))] \tag{3}$$ G tries to minimize this objective to generate images that can be classified as the target domain c. Reconstruction Loss: (共有)重构损失$$L_{rec}=E_{x,c,c’}[\parallel x-G(G(x,c),c’) \parallel _1] \tag{4}$$ Full Objective: 共有$$L_D=-L_{adv}+\lambda_{cls} L_{cls}^r$$$$L_G=L_{adv}+\lambda_{cls} L_{cls}^f+\lambda_{rec}L_{rec} \tag{5}$$$$\lambda_{cls}=1, \lambda_{rec}=10$$ 2.2. Training with Multiple Datasets StarGAN也适用于多数据集间的转换，上述过程中的重构损失要求数据集之间的标签一致(？？？)。针对这个问题，作者引入Mask Vector. Mask Vector: 修改真值。$$\tilde{c} = [c_1, …, c_n, m]$$ $c_i$ represents a vector for the labels of the i-th dataset. The vector of the known label $c_i$ can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n−1 unknown labels we simply assign zero values. 这样的话，所有的c都需要变成$\tilde{c}$ 3. ImplementationImproved GAN training: 为了稳定训练过程，替代方程1.$$L_{adv}=E_x[log D_{src}(x)]-E_{x,c}[log (1-D_{src}(G(x,c)))]-\lambda_{gp}E_{\hat{x}}[(\parallel \nabla_{\hat{x}} D_{src}(\hat{x}) \parallel 2-1)^2]$$$$\lambda{gp}=10$$ Network Architecture: 类似CycleGAN。 G: Leaky ReLU: 0.01 D: PatchGAN 现在网络架构可以看到的是作者使用的不是70x70的patchGAN，通过patchGAN的论文，也没有看到这种结构。 4. Experiments4.1 Baseline Models 通过结果可以看出，在Gender这个属性，ICGAN的转换效果要更好一些，但是损失了ID信息。 4.2 Training Adam: $\beta_1=0.5, \beta_2=0.999$ Updates: one generator update after five discriminator updates lr: For CelebA, 0.0001 for the first 10 epochs, and linearly decay the lr to 0 over the next 10 epochs. For the RaFD, 0.0001 for the first 100 epochs, and linearly decay the lr to 0 over the next 100 epochs. batch: 16 input: For CelebA, crop: 178, resize: 128; For RaFD, 4.3 Results作者通过人脸的转换实验，不仅说明了StarGAN在单数据集的不同domian中效果好，而且在多数据集的不同domian中效果也好。 5. 代码在这里分析pytorch的代码，并对其中关键的代码进行解读。 Model： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384class ResidualBlock(nn.Module): """Residual Block with instance normalization.""" def __init__(self, dim_in, dim_out): super(ResidualBlock, self).__init__() self.main = nn.Sequential( nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False), nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True), nn.ReLU(inplace=True), nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False), nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True)) def forward(self, x): return x + self.main(x)class Generator(nn.Module): """Generator network.""" def __init__(self, conv_dim=64, c_dim=5, repeat_num=6): super(Generator, self).__init__() layers = [] layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False)) layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True)) layers.append(nn.ReLU(inplace=True)) # Down-sampling layers. curr_dim = conv_dim for i in range(2): layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False)) layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True)) layers.append(nn.ReLU(inplace=True)) curr_dim = curr_dim * 2 # Bottleneck layers. for i in range(repeat_num): layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim)) # Up-sampling layers. for i in range(2): layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False)) layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True)) layers.append(nn.ReLU(inplace=True)) curr_dim = curr_dim // 2 layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False)) layers.append(nn.Tanh()) self.main = nn.Sequential(*layers) def forward(self, x, c): # Replicate spatially and concatenate domain information. # c: N*c_dim # 生成器直接将目标域c在通道维度进行拼接 c = c.view(c.size(0), c.size(1), 1, 1) c = c.repeat(1, 1, x.size(2), x.size(3)) x = torch.cat([x, c], dim=1) return self.main(x)class Discriminator(nn.Module): """Discriminator network with PatchGAN.""" def __init__(self, image_size=128, conv_dim=64, c_dim=5, repeat_num=6): super(Discriminator, self).__init__() layers = [] layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1)) layers.append(nn.LeakyReLU(0.01)) curr_dim = conv_dim for i in range(1, repeat_num): layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1)) layers.append(nn.LeakyReLU(0.01)) curr_dim = curr_dim * 2 kernel_size = int(image_size / np.power(2, repeat_num)) self.main = nn.Sequential(*layers) self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False) self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False) def forward(self, x): h = self.main(x) # True or False out_src = self.conv1(h) # classes onehot out_cls = self.conv2(h) return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>starGAN</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CASN]]></title>
    <url>%2F2018%2F12%2F17%2FCASN%2F</url>
    <content type="text"><![CDATA[CASN: Re-Identification with Consistent Attentive Siamese Networks Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J. Radke 1. Introduction这篇论文； 采用了Grad-CAM作为attention机制 attention consistency: 采用Siamese-Net来使同一个人的attention位置是一样的 2. The Consistent Attentive Siamese Network整体网络架构如图所示： 整体网络架构以IDE为基准网络，分为两部分: Identification Module Siamese Module Identification 和 Siamese 的特征提取网络共享，不同的只是fc层 2.1 The Identification Module通过Grad-CAM的学习，已经知道了Grad-CAM的作用。 Identification loss:$$L_{ide}=-\sum_{n=1}^N log \frac{exp(y_{c_n})}{\sum_j exp(y_j)}$$ Identification loss 更偏向于不同行人之间的判别信息。 Identification attention loss:$$L_{ia}=\overline{y_{c_n}}$$ 其中，给定一张图片$I_n$和类别$c_n$，Grad-CAM得到attention map $M_n$，做归一化操作，令$\Sigma(M_n)=sigmoid(\alpha(M_n-\beta))$，从而得到去掉attention区域的新图片$\overline{I_n}=I_n*(1-\Sigma(M_n))$，$\overline{y_{c_n}}$是$\overline{I_n}$的预测值。 Identification attention loss 更偏向于行人的全部信息。 我的理解是$\overline{I_n}$中尽可能包含少的ID信息，所以预测出是$c_n$的概率更小，得到的attention区域尽可能地包括全部信息。 两种loss的效果对比图 2.2 The Siamese ModuleSiamese Module 结构图 Siamese loss$$L_{bce}=-\sum_p log(\frac{exp(z_{c_p})}{exp(z_0)+exp(z_1)})$$ Siamese attention loss$$\alpha_i=\begin{cases}1, \mbox{if} f_i^-&gt;0 \0, otherwise\end{cases}$$$$s_1=&lt;\alpha, f_1&gt;, s_2=&lt;\alpha, f_2&gt;(dot products)$$$$\alpha_1^k=GAP(\frac{\partial s_1}{\partial A_1}), \alpha_2^k=GAP(\frac{\partial s_2}{\partial A_2}) \tag{5}$$$$M_1=ReLU(\sum_k \alpha_1^k A_1^k),M_2=ReLU(\sum_k \alpha_2^k A_2^k)$$$$L_{sa}=L_{bce}+\alpha \parallel M_{m1}^{resize}-M_{m2}^{resize} \parallel _2$$$$\alpha=0.2$$ 其中，$M_{m1}$是$M_1$中超过阈值t的元素，$M_{m1}^{resize}$是$M_{m1}$resize成相同大小的元素，主要是为了解决对齐问题。 通过与作者的沟通，作者认为$s_1$表示了$f_1$中对 BCE prediction 有用的信息。但是我没有见过这么表示对预测有用的信息的方法，之前只见过通过类别进行反向传播的(Grad-CAM)，但是作者这么坚持，说明应该是有效的。 Sorry I didn’t get your first question. By finding neurons in fi which are larger than zero, we find features in fi which have positive influence on BCE prediction. s1 is not I1’s class. We here call s1 the importance score, which collect scores for every neuron which contributes to BCE prediction. 2.3 Overall Design of the CASNCASN的整体架构 The overall loss$$L=L_{ide}+\lambda_1 L_{ia}+\lambda_2 L_{sa}$$ 3. Experiments and ResultsImplementation Details input: 288x144 SGD: momentum=0.9 lr=0.03 epoch=40 lr decay=0.1 after 30 baseline: IDE and PCB( input: 384x128) batch=16 test: we send the query and gallery as pair inputs to obtain attention maps $\parallel M_{m1}^{resize}-M_{m2}^{resize} \parallel _2$ Results 通过 Ablation Study , 对比CASN(IDE)、PCB，可以看出IA或者SA的作用和简单地分成6块达到的效果是类似的，这是不是说明了这种attention机制没有很大的作用，或者说分成6块就已经是一种很好的attention机制。 另外，+IA、+SA、CASN的对比，感觉IA或者SA一种机制就已经足够了，两者达到的效果是一样的，只使用一种就可以了。 4. Others这篇论文不懂的地方： $L_{ia}$为什么可以直接这么写，不需要经过softmax之类的，或者不应该是每类的概率差不多么， $L_{ia}$还是经过相同的网络得到的吗？反向求导要怎么写？ 根据Grad-CAM的以类别反向求导，方程5给我的感觉更像是$f$的特征和作为输入图片的分类预测值，合理性站不住脚。 在测试时，需要每次输入一对图片，是不是太慢了。 如果实验结果可以复现的话，那么IA我觉得还是很有用的，解释性也强。 参考： 对于 Identification attention loss 的流程，需要参考Grad-CAM和GAIN, GAIN也可以在Grad-CAM中找到详解。]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>person-reid</tag>
        <tag>Grad-CAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Grad-CAM]]></title>
    <url>%2F2018%2F12%2F14%2FGrad-CAM%2F</url>
    <content type="text"><![CDATA[Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization Gradient-weighted Class Activation Mapping Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam code-torch: https://github.com/ramprs/grad-cam/ code-pytorch: https://github.com/jacobgil/pytorch-grad-cam code-keras: https://github.com/jacobgil/keras-grad-cam 参考链接： https://www.jianshu.com/p/b2f7efe10ad8 https://www.jianshu.com/p/1d7b5c4ecb93 http://spytensor.com/index.php/archives/20/ https://www.jianshu.com/p/e4fa1348e5bc 1. 声明最近在看到一篇person-reid的文章Re-Identification with Consistent Attentive Siamese Networks，其中涉及到了Grad-CAM，所以简单学习一下Grad-CAM，但不作为重点。 2018-12-18 在使用的过程中，发现自己写的这篇博客不太容易让自己一目了然，所以根据链接来进行更新一次。 2018-12-20 在重新看论文person-reid的过程中，发现其中涉及到的网络架构师GAIN，所以补充GAIN的说明及其代码。此时，这篇的重点变成了Grad-CAM和GAIN。 2. 前言对于深度模型的可解释性和可视化，现在已经研究出了一些方法，包括不限于Deconvolution, Guided-Backpropagation, CAM, Grad-CAM. 其中 Deconvolution 和 Guided-Backpropagation 得到更偏向于细粒度图， CAM 和 Grad-CAM 得到更偏向于类区分的热力图。 各种可视化方法及其效果图参见：https://github.com/utkuozbulak/pytorch-cnn-visualizations 参考链接: https://blog.csdn.net/geek_wh2016/article/details/81060315 2.1 DeconvolutionDeconvolution: Visualizing and Understanding Convolutional Networks code: https://github.com/kvfrans/feature-visualization 综述: 这篇paper是CNN可视化的开山之作(由 Lecun 得意门生 Matthew Zeiler 发表于2013年)，主要解决了两个问题: why CNN perform so well? how CNN might be improved? 实现: 对于CNN，可视化就是整个过程的逆过程，即Unpooling+ReLU+Deconv. Unpooling: 记录max-pool的位置，即Switches表格，unpooling时，最大值放回该位置，其他位置放0. ReLU: 继续使用ReLU. Deconv: 使用相同卷积核的转置作为新的卷积核，对特征进行卷积. 参考链接： http://kvfrans.com/visualizing-features-from-a-convolutional-neural-network/ https://blog.csdn.net/Julialove102123/article/details/78292807 https://blog.csdn.net/gm_margin/article/details/79335140 2.2 Guided-BackpropagationGuided-Backpropagation: Striving for Simplicity: The All Convolutional Net 反向传播、反卷积和导向反向传播都是反向传播，区别在于经过 ReLU 层时对梯度的不同处理策略。在这篇论文中有详细的解释。 计算公式如下： 文中提出使用 stride convolution 代替 pooling，研究这种结构的有效性。 效果显示如下： 可以看出 Guided-Backpropagation 主要提取对分类有效果的特征，但是与是哪类没有关系。 2.3 CAMCAM: Learning Deep Features for Discriminative Localization 综述：论文重新审视了global average pooling (GAP) 的有效性，并详细阐述了GAP如何使得CNN有优异的目标定位能力。介绍：摒弃FC，使用GAP。实现： 2.4 Grad-CAMGrad-CAM 是CAM的改进版， 与 CAM 的不同点在于前者的特征加权系数是反向传播得到的，后者的特征加权系数是分类器的权重。 Grad-CAM 可以加载到任意网络架构上，而不需要修改网络架构，而CAM必须使用GAP。 下面会详细介绍。 2.5 GAINGAIN: Tell Me Where to Look: Guided Attention Inference Network code: https://github.com/alokwhitewolf/Guided-Attention-Inference-Network GAIN 是 Grad-CAM 的改进版，Grad-CAM只能可视化解释现有的网络结构的结果，却不能指导网络架构，GAIN可以指导网络修正错误，关注更正确的位置。 问题：在船识别的过程中，网络的关注点是水面而不是船。 实现：通过最小化遮挡图像的物体来训练。 整体网络架构中，只有一个网络，两个处理流都是共享同一个网络。 公式：损失函数$$w_{l,k}^c=GAP(\frac{\partial s^c}{\partial f_{l,k}})$$$$A^c=ReLU(conv(f_l, w^c))$$$$T(A^c)=\frac{1}{1+exp(-\omega(A^c-\sigma))}$$$$I^{c}=I-(T(A^c)\odot I)$$$$L_{am}=\frac{1}{n}\sum_cs^c(I^{c})$$$$L_{self}=L_{cl}+\alpha L_{am}$$$$\alpha=1$$ 扩展：如果有额外的监督真值，比如分割，那么可以进行扩充网络 $$L_e=\frac{1}{n}\sum_c (A^c-H^c)^2$$$$L_{ext}=L_{cl}+\alpha L_{am}+\omega L_e$$$$\alpha=1, \omega=10$$ 2.5.1 GAIN-code第一步：训练分类网络 FCN: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133self.conv1_1 = L.Convolution2D(3, 64, 3, 1, 1)self.conv1_2 = L.Convolution2D(64, 64, 3, 1, 1)self.conv2_1 = L.Convolution2D(64, 128, 3, 1, 1)self.conv2_2 = L.Convolution2D(128, 128, 3, 1, 1)self.conv3_1 = L.Convolution2D(128, 256, 3, 1, 1)self.conv3_2 = L.Convolution2D(256, 256, 3, 1, 1)self.conv3_3 = L.Convolution2D(256, 256, 3, 1, 1)self.conv4_1 = L.Convolution2D(256, 512, 3, 1, 1)self.conv4_2 = L.Convolution2D(512, 512, 3, 1, 1)self.conv4_3 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_1 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_2 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_3 = L.Convolution2D(512, 512, 3, 1, 1)self.fc6 = L.Convolution2D(512, 4096, 7, 1, 0)self.fc7 = L.Convolution2D(4096, 4096, 1, 1, 0)self.score_fr = L.Convolution2D(4096, n_class, 1, 1, 0)def segment(self, x, t=None): # conv1 self.conv1_1.pad = (100, 100) h = F.relu(self.conv1_1(x)) conv1_1 = h h = F.relu(self.conv1_2(conv1_1)) conv1_2 = h h = _max_pooling_2d(conv1_2) pool1 = h # 1/2 # conv2 h = F.relu(self.conv2_1(pool1)) conv2_1 = h h = F.relu(self.conv2_2(conv2_1)) conv2_2 = h h = _max_pooling_2d(conv2_2) pool2 = h # 1/4 # conv3 h = F.relu(self.conv3_1(pool2)) conv3_1 = h h = F.relu(self.conv3_2(conv3_1)) conv3_2 = h h = F.relu(self.conv3_3(conv3_2)) conv3_3 = h h = _max_pooling_2d(conv3_3) pool3 = h # 1/8 # conv4 h = F.relu(self.conv4_1(pool3)) h = F.relu(self.conv4_2(h)) h = F.relu(self.conv4_3(h)) h = _max_pooling_2d(h) pool4 = h # 1/16 # conv5 h = F.relu(self.conv5_1(pool4)) h = F.relu(self.conv5_2(h)) h = F.relu(self.conv5_3(h)) h = _max_pooling_2d(h) pool5 = h # 1/32 # fc6 h = F.relu(self.fc6(pool5)) h = F.dropout(h, ratio=.5) fc6 = h # 1/32 # fc7 h = F.relu(self.fc7(fc6)) h = F.dropout(h, ratio=.5) fc7 = h # 1/32 # score_fr h = self.score_fr(fc7) score_fr = h # 1/32 # score_pool3 h = self.score_pool3(pool3) score_pool3 = h # 1/8 # score_pool4 h = self.score_pool4(pool4) score_pool4 = h # 1/16 # upscore2 h = self.upscore2(score_fr) upscore2 = h # 1/16 # score_pool4c h = score_pool4[:, :, 5:5 + upscore2.shape[2], 5:5 + upscore2.shape[3]] score_pool4c = h # 1/16 # fuse_pool4 h = upscore2 + score_pool4c fuse_pool4 = h # 1/16 # upscore_pool4 h = self.upscore_pool4(fuse_pool4) upscore_pool4 = h # 1/8 # score_pool4c h = score_pool3[:, :, 9:9 + upscore_pool4.shape[2], 9:9 + upscore_pool4.shape[3]] score_pool3c = h # 1/8 # fuse_pool3 h = upscore_pool4 + score_pool3c fuse_pool3 = h # 1/8 # upscore8 h = self.upscore8(fuse_pool3) upscore8 = h # 1/1 # score h = upscore8[:, :, 31:31 + x.shape[2], 31:31 + x.shape[3]] score = h # 1/1 self.score = score if t is None: assert not chainer.config.train return loss = F.softmax_cross_entropy(score, t, normalize=True) if np.isnan(float(loss.data)): raise ValueError('Loss is nan.') chainer.report(&#123;'loss': loss&#125;, self) self.conv1_1.pad = (1, 1) return loss FCN-v1.0: 普通的分类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364self.conv1_1 = L.Convolution2D(3, 64, 3, 1, 1)self.conv1_2 = L.Convolution2D(64, 64, 3, 1, 1)self.conv2_1 = L.Convolution2D(64, 128, 3, 1, 1)self.conv2_2 = L.Convolution2D(128, 128, 3, 1, 1)self.conv3_1 = L.Convolution2D(128, 256, 3, 1, 1)self.conv3_2 = L.Convolution2D(256, 256, 3, 1, 1)self.conv3_3 = L.Convolution2D(256, 256, 3, 1, 1)self.conv4_1 = L.Convolution2D(256, 512, 3, 1, 1)self.conv4_2 = L.Convolution2D(512, 512, 3, 1, 1)self.conv4_3 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_1 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_2 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_3 = L.Convolution2D(512, 512, 3, 1, 1)self.fc6_cl = L.Linear(512, 4096)self.fc7_cl = L.Linear(4096, 4096)self.score_cl = L.Linear(4096, n_class-1) # Disregard 0 class for classificationself.final_conv_layer = 'conv5_3'self.grad_target_layer = 'prob'self.freezed_layers = ['fc6_cl', 'fc7_cl', 'score_cl']def classify(self, x, is_training=True): with chainer.using_config('train',False): # conv1 h = F.relu(self.conv1_1(x)) h = F.relu(self.conv1_2(h)) h = _max_pooling_2d(h) # conv2 h = F.relu(self.conv2_1(h)) h = F.relu(self.conv2_2(h)) h = _max_pooling_2d(h) # conv3 h = F.relu(self.conv3_1(h)) h = F.relu(self.conv3_2(h)) h = F.relu(self.conv3_3(h)) h = _max_pooling_2d(h) # conv4 h = F.relu(self.conv4_1(h)) h = F.relu(self.conv4_2(h)) h = F.relu(self.conv4_3(h)) h = _max_pooling_2d(h) # conv5 h = F.relu(self.conv5_1(h)) h = F.relu(self.conv5_2(h)) h = F.relu(self.conv5_3(h)) h = _max_pooling_2d(h) h = _average_pooling_2d(h) with chainer.using_config('train',is_training): h = F.relu(F.dropout(self.fc6_cl(h), .5)) h = F.relu(F.dropout(self.fc7_cl(h), .5)) h = self.score_cl(h) # 1*20 return h loss: 1234# cl_output=classify(image)# cl_output: 1*20# target: 1*20 ~ [0,1] 1表示有这个label，0表示没有这个label，用的是多分类损失函数，且类别之间不排斥，类似对每个类别做二元分类。loss = F.sigmoid_cross_entropy(cl_output, target, normalize=True) 第二步：训练GAIN 12345678910111213141516171819202122232425262728293031323334self.GAIN_functions = collections.OrderedDict([ ('conv1_1', [self.conv1_1, F.relu]), ('conv1_2', [self.conv1_2, F.relu]), ('pool1', [_max_pooling_2d]), ('conv2_1', [self.conv2_1, F.relu]), ('conv2_2', [self.conv2_2, F.relu]), ('pool2', [_max_pooling_2d]), ('conv3_1', [self.conv3_1, F.relu]), ('conv3_2', [self.conv3_2, F.relu]), ('conv3_3', [self.conv3_3, F.relu]), ('pool3', [_max_pooling_2d]), ('conv4_1', [self.conv4_1, F.relu]), ('conv4_2', [self.conv4_2, F.relu]), ('conv4_3', [self.conv4_3, F.relu]), ('pool4', [_max_pooling_2d]), ('conv5_1', [self.conv5_1, F.relu]), ('conv5_2', [self.conv5_2, F.relu]), ('conv5_3', [self.conv5_3, F.relu]), ('pool5', [_max_pooling_2d]), ('avg_pool', [_average_pooling_2d]), ('fc6_cl', [self.fc6_cl, F.relu]), ('fc7_cl', [self.fc7_cl, F.relu]), ('prob', [self.score_cl, F.sigmoid])])self.final_conv_layer = 'conv5_3'self.grad_target_layer = 'prob'self.freezed_layers = ['fc6_cl', 'fc7_cl', 'score_cl'] 通过分类结果获取mask： 12345678910111213141516171819202122232425262728293031def stream_cl(self, inp, label=None): # h: 1*3*281*500 # label: 真值 array([0, 14]) # return: gcam: mask,size(1,3,281,500); h: size(1,20), class_id: 一个数字 h = inp for key, funcs in self.GAIN_functions.items(): for func in funcs: h = func(h) if key == self.final_conv_layer: activation = h if key == self.grad_target_layer: break gcam, class_id = self.get_gcam(h, activation, (inp.shape[-2], inp.shape[-1]), label=label) return gcam, h, class_iddef get_gcam(self, end_output, activations, shape, label): # end_output: size: 1,20 # activations: size: 1*512*18*32 # shape: (281, 500) # label: 真值 self.cleargrads() class_id = self.set_init_grad(end_output, label) end_output.backward(retain_grad=True) grad = activations.grad_var grad = F.average_pooling_2d(grad, (grad.shape[-2], grad.shape[-1]), 1) grad = F.expand_dims(F.reshape(grad, (grad.shape[0]*grad.shape[1], grad.shape[2], grad.shape[3])), 0) weights = activations weights = F.expand_dims(F.reshape(weights, (weights.shape[0]*weights.shape[1], weights.shape[2], weights.shape[3])), 0) gcam = F.resize_images(F.relu(F.convolution_2d(weights, grad, None, 1, 0)), shape) return gcam, class_id $L_{cl}$ 12gcam, cl_scores, class_id = self._optimizers['main'].target.stream_cl(image, gt_labels)cl_loss = F.sigmoid_cross_entropy(cl_scores, target, normalize=True) $L_{am}$ 123masked_output = self._optimizers['main'].target.stream_am(masked_image)masked_output = F.sigmoid(masked_output)am_loss = masked_output[0][class_id][0] 备注: $L_{cl}$和$L_{am}$完全共享网络。 3. Introduction可视化即应该满足高分辨率，也应该满足类别定位能力。 示例图像 4. ApproachCAM在CAM中，一个全连接层替换成GAP，参见上面的CAM图，则分类任务可以表示成$$y^c = \sum_k w_k^c \frac{1}{Z} \sum_i \sum_j A_{ij}^k$$其中，$y^c$表示分类结果，$w_k^c$表示第k个特征图(kxhxw)对第c个类别的贡献，即全连接层的系数，$Z$表示特征图的大小，$Z=h\cdot w$，$A_{ij}^k$表示第k个特征图。 则 CAM 的输出图表示为：$$L_{CAM}^c=\sum_k w_k^c A^k$$ Grad-CAM在Grad-CAM中，权重系数是通过反向传播得到的。$$\alpha_k^c=\frac{1}{Z}\sum_i \sum_j \frac{\partial y^c}{\partial A_{ij}^k}$$ 则Grad-CAM的输出图表示为：$$L_{Grad-CAM}^c=ReLU(\sum_k \alpha_k^c A^k)$$ 可以证明，Grad-CAM与CAM的公式是同一个公式的变形。 Guided Grad-CAMGuided Grad-CAM 是将 Grad-CAM 与 Guided Backpropagation 得到的输出图简单地点乘，从而获得类区分定位的高分辨率细节图。 同时作者还分析了CNN分类错误的样本。 5. 代码对于pytorch代码进行分析 5.1 Grad-CAM计算Grad-CAM: 反向传播：先计算出当前图片的分类结果output(size:1*5)(假设共5类)，选出最优分类结果，假设是第2类，然后令one-hot=[0,1,0,0,0]，求得sum-one-hot=&lt;one-hot,output&gt;得到一个数字，然后反向传播。 cam: (H,W), ~(0,1) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134class FeatureExtractor(object): """ Class for extracting activations and registering gradients from targetted intermediate layers """ """ 调用方式: outputs, x = FeatureExtractor(x) gradients = FeatureExtractor.gradients """ def __init__(self, model, target_layers): self.model = model self.target_layers = target_layers self.gradients = [] def save_gradient(self, grad): self.gradients.append(grad) def __call__(self, x): """ :param x: N*C*H*W, a picture :return: outputs: list, activations layer output, A in equation x : feature map, feature of model output n*c*h*w """ outputs = [] self.gradients = [] for name, module in self.model._modules.items(): x = module(x) if name in self.target_layers: x.register_hook(self.save_gradient) outputs += [x] return outputs, xclass ModelOutputs(object): """ Class for making a forward pass, and getting: 1. The network output. 2. Activations from intermeddiate targetted layers. 3. Gradients from intermeddiate targetted layers. """ """ 调用方式： target_activations, output = ModelOutputs(x) gradients = ModelOutputs.get_gradients() """ def __init__(self, model, target_layers): self.model = model self.feature_extractor = FeatureExtractor(self.model.features, target_layers) def get_gradients(self): return self.feature_extractor.gradients def __call__(self, x): """ :param x: N*C*H*W, a picture :return: target_activations: list, activations layer output, A in equation output : tensor, classification output. N*c. y in equation. """ target_activations, output = self.feature_extractor(x) output = output.view(output.size(0), -1) output = self.model.classifier(output) return target_activations, outputclass GradCam(object): """ Class for making Grad-CAM, and getting: 1. Grad-CAM """ """ 调用方式： mask=GradCam(input) """ def __init__(self, model, target_layer_names, use_cuda): self.model = model self.model.eval() self.cuda = use_cuda if self.cuda: self.model = model.cuda() self.extractor = ModelOutputs(self.model, target_layer_names) def forward(self, input): return self.model(input) def __call__(self, input, index=None): """ :param input: N*C*H*W, a picture :param index: int :return: cam: N*C*H*W ~(0,1) L_&#123;Grad-CAM&#125;^c """ if self.cuda: features, output = self.extractor(input.cuda()) else: features, output = self.extractor(input) if index == None: index = np.argmax(output.cpu().data.numpy()) one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32) one_hot[0][index] = 1 # After test, requires_grad could be False one_hot = Variable(torch.from_numpy(one_hot), requires_grad=True) if self.cuda: one_hot = torch.sum(one_hot.cuda() * output) else: one_hot = torch.sum(one_hot * output) self.model.features.zero_grad() self.model.classifier.zero_grad() # After test, requires_grad could be False one_hot.backward(retain_graph=True) grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy() target = features[-1] target = target.cpu().data.numpy()[0, :] weights = np.mean(grads_val, axis=(2, 3))[0, :] cam = np.zeros(target.shape[1:], dtype=np.float32) for i, w in enumerate(weights): cam += w * target[i, :, :] cam = np.maximum(cam, 0) cam = cv2.resize(cam, (224, 224)) cam = cam - np.min(cam) cam = cam / np.max(cam) return cam 显示Grad-CAM 1234567def show_cam_on_image(img, mask): heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET) heatmap = np.float32(heatmap) / 255 cam = heatmap + np.float32(img) cam = cam / np.max(cam) cv2.imwrite("cam.jpg", np.uint8(255 * cam)) 5.2 GuidedBackpropReLUModelgb: (C,H,W) 任意值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class GuidedBackpropReLU(Function): def forward(self, input): positive_mask = (input &gt; 0).type_as(input) output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask) self.save_for_backward(input, output) return output def backward(self, grad_output): input, output = self.saved_tensors grad_input = None positive_mask_1 = (input &gt; 0).type_as(grad_output) positive_mask_2 = (grad_output &gt; 0).type_as(grad_output) grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input), torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output, positive_mask_1), positive_mask_2) return grad_inputclass GuidedBackpropReLUModel: def __init__(self, model, use_cuda): self.model = model self.model.eval() self.cuda = use_cuda if self.cuda: self.model = model.cuda() # replace ReLU with GuidedBackpropReLU for idx, module in self.model.features._modules.items(): if module.__class__.__name__ == 'ReLU': self.model.features._modules[idx] = GuidedBackpropReLU() def forward(self, input): return self.model(input) def __call__(self, input, index=None): if self.cuda: output = self.forward(input.cuda()) else: output = self.forward(input) if index == None: index = np.argmax(output.cpu().data.numpy()) one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32) one_hot[0][index] = 1 # After test, requires_grad could be False one_hot = Variable(torch.from_numpy(one_hot), requires_grad=True) if self.cuda: one_hot = torch.sum(one_hot.cuda() * output) else: one_hot = torch.sum(one_hot * output) # self.model.features.zero_grad() # self.model.classifier.zero_grad() one_hot.backward(retain_graph=True) output = input.grad.cpu().data.numpy() output = output[0, :, :, :] return output 5.3 Guided Grad-CAM123456cam_mask = np.zeros(gb.shape)for i in range(0, gb.shape[0]): cam_mask[i, :, :] = maskcam_gb = np.multiply(cam_mask, gb)utils.save_image(torch.from_numpy(cam_gb), 'cam_gb.jpg') 6. 效果显示原图 Grad-CAM Guided-Backpropagation Guided Grad-CAM]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RotationNet-paper]]></title>
    <url>%2F2018%2F12%2F11%2FRotationNet-paper%2F</url>
    <content type="text"><![CDATA[1. RotationNet-paper paper: RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints(CVPR2018) Asako Kanezaki, Yasuyuki Matsushita2, and Yoshifumi Nishida. Asako Kanezaki 是日本东京研究所专门研究3D的一个老师。 code-pytorch: https://github.com/kanezaki/pytorch-rotationnetcode-caffe: https://github.com/kanezaki/rotationnetproject: https://kanezaki.github.io/rotationnet/ MIMO data: https://github.com/kanezaki/MIRO 作者是使用caffe版本提交的论文，我也只是看了看代码，作为理解作者论文的辅助，实际没有跑过代码。 这篇博客以代码和论文混杂，因为是借助代码理解论文的，又因为不主要做这个方向，所以并没有在意精度什么的。 1.1 出发点作者不仅想要预测出图片的类别label，还想预测出图片的view-points. 我觉得作者的创新点在于对view的状态顺序编码成view-rotation，限定了view的取值空间，使预测的结果变成了哪种view-rotaion的view准确率高。 因为在一般情况下，想到的是直接预测view，而不是view-rotation. 1.2 网络架构 1.2.1 训练过程：以MIRO数据集、case=3为例，nview=160，vcand=(16, 160),view-rotation=16，num-classes=12. 这里的view-roration，我的理解是view的排列方式，但是还是不太顺。 输入的图片个数batch-size必须是nview的倍数，以输入一个样本的160个角度的图片为例，即batch-size=160，nsamp=1，不影响后续的分析，因为每个样本没有任何关系。 输出是output=batch-size x ((num_classes+1) * nview)= 160 x (13 x 160). 可以理解成对每一个图片，输出网络架构的一行，可以理解成160张图片在160个view下属于13个类的概率。 预测view rotation: 利用下面的预测view公式，求log并相减得到output- = (160x160) x 12 x 1，可以理解成一个矩阵： (160x160) x 12，每行表示当前图片当前view下的属于各类的概率，第一个160行表示第一张图片在160个view下的概率分布，第二个160行表示第二张图片在160个view下的概率分布。scores = (16 x 12 x 1)。scores可以理解成当前view-rotation下这160张图片一起属于各类的概率。 123456789101112131415output = model(input_var)num_classes = int( output.size( 1 ) / nview ) - 1output = output.view( -1, num_classes + 1 )# compute scores and decide target labelsoutput_ = torch.nn.functional.log_softmax( output )output_ = output_[ :, :-1 ] - torch.t( output_[ :, -1 ].repeat( 1, output_.size(1)-1 ).view( output_.size(1)-1, -1 ) )output_ = output_.view( -1, nview * nview, num_classes )output_ = output_.data.cpu().numpy()output_ = output_.transpose( 1, 2, 0 )scores = np.zeros( ( vcand.shape[ 0 ], num_classes, nsamp ) )for j in range(vcand.shape[0]): for k in range(vcand.shape[1]): scores[ j ] = scores[ j ] + output_[ vcand[ j ][ k ] * nview + k ] 生成动态真值target-：已知这160张图片的真值target[ n * nview ]，假设是第3类，j-max表示第j-max个view-rotation下，预测为第3类的概率最大，继而生成动态真值target-=(target.size(0) x nview)=160 x 160=25600，可以理解成160张图片在160个view下的真值，在j-max个view-rotation对应的view设置为类别3，其余的设置为13. 123456target_ = torch.LongTensor( target.size(0) * nview )for n in range( nsamp ): j_max = np.argmax( scores[ :, target[ n * nview ], n ] ) # assign target labels for k in range(vcand.shape[1]): target_[ n * nview * nview + vcand[ j_max ][ k ] * nview + k ] = target[ n * nview ] 1.2.2 验证过程与训练类似，可以得到output- = (160x160) x 12 x 1，可以理解成一个矩阵： (160x160) x 12，每行表示当前图片当前view下的属于各类的概率，第一个160行表示第一张图片在160个view下的概率分布，第二个160行表示第二张图片在160个view下的概率分布。scores = (16 x 12 x 1)。scores可以理解成每个view-rotation下这160张图片一起属于各类的概率。 j-max表示在第j-max个view-rotation下，scores可以找到最大概率。 output[n] 表示每连续的160张图片一起属于某类的概率。 123456789101112131415161718192021222324252627output = model(input_var)num_classes = int( output.size( 1 ) / nview ) - 1output = output.view( -1, num_classes + 1 )output = torch.nn.functional.log_softmax( output )output = output[ :, :-1 ] - torch.t( output[ :, -1 ].repeat( 1, output.size(1)-1 ).view( output.size(1)-1, -1 ) )output = output.view( -1, nview * nview, num_classes )# measure accuracy and record lossprec1, prec5 = my_accuracy(output.data, target, topk=(1, 5))# def my_accuracytarget = target[0:-1:nview]batch_size = target.size(0)num_classes = output_.size(2)output_ = output_.cpu().numpy()output_ = output_.transpose( 1, 2, 0 )scores = np.zeros( ( vcand.shape[ 0 ], num_classes, batch_size ) )output = torch.zeros( ( batch_size, num_classes ) )for j in range(vcand.shape[0]): for k in range(vcand.shape[1]): scores[ j ] = scores[ j ] + output_[ vcand[ j ][ k ] * nview + k ]for n in range( batch_size ): j_max = int( np.argmax( scores[ :, :, n ] ) / scores.shape[ 1 ] ) output[ n ] = torch.FloatTensor( scores[ j_max, :, n ] )output = output.cuda() 1.2.3 测试过程因为caffe代码没有看懂，所以根据作者的论文和代码猜一下，当输入的图片没有160张，假设只有100张图片，那么又该怎么做？ 模型的输出是output=batch-size x ((num_classes+1) * nview)= 100 x (13 x 160)，那么怎么求scores？ 求scores是需要全部view的信息的。这里不会了，尽管已经给出了公式，但是公式只能算出output-，没有score，不会了。 坐等作者回复。 Our method is available only when the relative poses of test images are known. For example, if you captured three images where the second image is 22.5 degrees forward from the first image and the third image is 45 degrees forward from the second image, then the images should be indexed as (0, 1, 3). Then you would get 3x160x12 output values. An easy way to proceed is to create a 160x160x12 “output2” which has zero values, and then insert the output values as “output2[0] = output[0]; output2[1] = output[1]; output2[3] = output[2];”. (In our paper, we used LSD-SLAM to calculate relative poses of test images.) 根据作者的回复，不难理解，给定的测试图片是需要预先知道测试图片序列的相对位置的。 1.3 预测公式$$\max_{(v_i)_{i=1}^M}\prod_{i=1}^M(\log p_{v_{i},y}^{(i)}-\log p_{v_{i},N+1}^{(i)})$$]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>RotationNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-eval]]></title>
    <url>%2F2018%2F12%2F10%2Fpytorch-eval%2F</url>
    <content type="text"><![CDATA[eval pytorch 的eval()只是改变一些模块的状态，并不影响backward过程。 https://blog.csdn.net/u012436149/article/details/78281553 https://www.jianshu.com/p/6cb1fd785540]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SPGAN-tensorflow]]></title>
    <url>%2F2018%2F12%2F10%2FSPGAN-tensorflow%2F</url>
    <content type="text"><![CDATA[在阅读SPGAN代码源码的过程中，学习到的关于tensorflow的一些知识。 1. tf.ConfigProto参考链接:https://blog.csdn.net/dcrmg/article/details/79091941 tf.ConfigProto用于对sessison会话的参数配置。 log_device_placement=True: 可以获取到 operations 和 Tensor 被指派到哪个设备(几号CPU或几号GPU)上运行,会在终端打印出各项操作是在哪个设备上运行的 allow_soft_placement=True: 允许tf自动选择一个存在并且可用的设备来运行操作.在tf中，通过命令 “with tf.device(‘/cpu:0’):”,允许手动设置操作运行的设备 config.gpu_options.allow_growth = True: 动态申请显存 config.gpu_options.per_process_gpu_memory_fraction = 0.4: 占用40%显存,限制GPU使用率. 123config = tf.ConfigProto(allow_soft_placement=True)config.gpu_options.allow_growth = Truesess = tf.Session(config=config) 12345678# 限制GPU使用率config = tf.ConfigProto()config.gpu_options.per_process_gpu_memory_fraction = 0.4 #占用40%显存session = tf.Session(config=config)等同于gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.4)config=tf.ConfigProto(gpu_options=gpu_options)session = tf.Session(config=config) 123456# 设置使用哪块GPU方法一： 在python中设置os.environ['CUDA_VISIBLE_DEVICES'] = '0' #使用 GPU 0os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # 使用 GPU 0，1方法二： 在执行时设置CUDA_VISIBLE_DEVICES=0,1 python yourcode.py 2. tf读取数据集图片的方式参考链接: https://www.jb51.net/article/134550.htm https://www.jb51.net/article/134547.htm tf的流程是文件系统–&gt;文件名队列–&gt;内存队列 推荐使用方法一 方法一：使用WholeFileReader输入queue，decode输出是Tensor，eval后是ndarray123456789101112131415161718192021222324252627282930313233343536373839404142import tensorflow as tfimport osimport matplotlib.pyplot as pltdef file_name(file_dir): #来自//www.jb51.net/article/134543.htm for root, dirs, files in os.walk(file_dir): #模块os中的walk()函数遍历文件夹下所有的文件 print(root) #当前目录路径 print(dirs) #当前路径下所有子目录 print(files) #当前路径下所有非目录子文件def file_name2(file_dir): #特定类型的文件 L=[] for root, dirs, files in os.walk(file_dir): for file in files: if os.path.splitext(file)[1] == '.jpg': L.append(os.path.join(root, file)) return Lpath = file_name2('test')#以下参考//www.jb51.net/article/134547.htm (十图详解TensorFlow数据读取机制)#path2 = tf.train.match_filenames_once(path)file_queue = tf.train.string_input_producer(paths, shuffle=True, num_epochs=2) #创建输入队列image_reader = tf.WholeFileReader()key, image = image_reader.read(file_queue)image = tf.image.decode_jpeg(image, channerls=3)with tf.Session() as sess:# coord = tf.train.Coordinator() #协同启动的线程# threads = tf.train.start_queue_runners(sess=sess, coord=coord) #启动线程运行队列# coord.request_stop() #停止所有的线程# coord.join(threads) tf.local_variables_initializer().run() threads = tf.train.start_queue_runners(sess=sess) #print (type(image)) #print (type(image.eval())) #print(image.eval().shape) for _ in path+path: plt.figure plt.imshow(image.eval()) plt.show() 方法二：使用gfile读图片，decode输出是Tensor，eval后是ndarray12345678910111213141516171819202122232425import matplotlib.pyplot as pltimport tensorflow as tfimport numpy as npprint(tf.__version__)image_raw = tf.gfile.FastGFile('test/a.jpg','rb').read() #bytesimg = tf.image.decode_jpeg(image_raw) #Tensor#img2 = tf.image.convert_image_dtype(img, dtype = tf.uint8)with tf.Session() as sess: print(type(image_raw)) # bytes print(type(img)) # Tensor #print(type(img2)) print(type(img.eval())) # ndarray !!! print(img.eval().shape) print(img.eval().dtype)# print(type(img2.eval()))# print(img2.eval().shape)# print(img2.eval().dtype) plt.figure(1) plt.imshow(img.eval()) plt.show() 方法三：使用read_file，decode输出是Tensor，eval后是ndarray12345678910111213141516171819202122232425import matplotlib.pyplot as pltimport tensorflow as tfimport numpy as npprint(tf.__version__)image_raw = tf.gfile.FastGFile('test/a.jpg','rb').read() #bytesimg = tf.image.decode_jpeg(image_raw) #Tensor#img2 = tf.image.convert_image_dtype(img, dtype = tf.uint8)with tf.Session() as sess: print(type(image_raw)) # bytes print(type(img)) # Tensor #print(type(img2)) print(type(img.eval())) # ndarray !!! print(img.eval().shape) print(img.eval().dtype)# print(type(img2.eval()))# print(img2.eval().shape)# print(img2.eval().dtype) plt.figure(1) plt.imshow(img.eval()) plt.show() 3. tf.train.shuffle_batch参考链接: https://www.jianshu.com/p/9cfe9cadde06 https://blog.csdn.net/ying86615791/article/details/73864381 12345img_batch = tf.train.shuffle_batch([img],batch_size=batch_size, capacity=capacity,min_after_dequeue=min_after_dequeue,num_threads=num_threads,allow_smaller_final_batch=allow_smaller_final_batch) tf不是像pytorch一样全局打乱，而是每一次在较短的队列中打乱。其中，队列的最长长度是capacity，最短长度是min_after_dequeue。 4. tf.summary参考链接：https://blog.csdn.net/hongxue8888/article/details/78610305 1summary_writer = tf.summary.FileWriter('./summaries/' + dataset + '_spgan' , sess.graph) 5. tf.train.Saver参考链接：http://www.cnblogs.com/denny402/p/6940134.html 1saver = tf.train.Saver(max_to_keep= 30) 6. saver.restore参考链接：https://blog.csdn.net/changeforeve/article/details/80268522 12345678910111213def load_checkpoint(checkpoint_dir, sess, saver): print(" [*] Loading checkpoint...") ckpt = tf.train.get_checkpoint_state(checkpoint_dir) print(ckpt) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) ckpt_path = os.path.join(checkpoint_dir, ckpt_name) saver.restore(sess, ckpt_path) print(" [*] Loading successful!") return ckpt_path else: print(" [*] No suitable checkpoint!") return None 7. tf.train.Coordinator参考链接: https://blog.csdn.net/weixin_42052460/article/details/80714539 https://www.jianshu.com/p/d063804fb272 12coord = tf.train.Coordinator()threads = tf.train.start_queue_runners(sess=sess, coord=coord) 8. tf.identity参考链接: https://stackoverflow.com/questions/34877523/in-tensorflow-what-is-tf-identity-used-for https://blog.csdn.net/hu_guan_jie/article/details/78495297 tf.idenity的逻辑就是等于号，区别是前者在计算图上加了个节点，使得可以多个设备之间可以通信，但是等于号为什么不行呢？ 9. tf.reuse参考链接：https://blog.csdn.net/UESTC_C2_403/article/details/72329786]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>SPGAN</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github-markdown-mathjax]]></title>
    <url>%2F2018%2F12%2F03%2Fgithub-markdown-mathjax%2F</url>
    <content type="text"><![CDATA[前言：在github上传自己的论文记录后发现，github上的显示和在vscode上的显示不同，主要体现在公式在github上不能正确地显示，并且换行也不能正确地显示。 参考链接：https://blog.csdn.net/phdsky/article/details/81431279 搜索之后发现github的markdown不支持mathjax的渲染。Github issue - github’s markdown mathjax rendingStackoverflow - How to show math equations in general github’s markdown 解决方案或者是公式转图片，或者是使用github内嵌的公式编辑器，或者是适用于chrome的github with MathJax插件。 我最后采用的是github with MathJax插件。GitHub with MathJax 插件]]></content>
      <categories>
        <category>github-markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>github</tag>
        <tag>mathjax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown-math]]></title>
    <url>%2F2018%2F12%2F03%2Fmarkdown-math%2F</url>
    <content type="text"><![CDATA[前言: 因为最近经常用到markdown写数学公式，每次都查感觉有点啰嗦，所以做个简单小结，把平常用的做个记录。这个博客根据平时自己常用到的进行动态增加。 参考链接： https://wangcong.info/article/MarkdownWithMath.html https://blog.csdn.net/deepinC/article/details/81103326 https://blog.csdn.net/HaleyPKU/article/details/80341932 https://blog.csdn.net/qq_39599067/article/details/81184139?utm_source=blogxgwz6 https://www.zybuluo.com/fyywy520/note/82980 1. 公式使用参考1.1 插入公式插入公式分为行中公式，独立公式和自动编号公式 1.行中公式 $ a=b $ 1$ 数学公式 $ 2.独立公式 $$ a=b $$ 1$$ 数学公式 $$ 3.编号公式$$ a=b \tag {1} $$ 1$$ 数学公式 \tag &#123;1&#125; $$ 由公式$(1)$可以得出结论 1由公式$(1)$可以得出结论 4.自动编号公式自动编号公式在github上显示不出来，原则上是可以的，推荐使用手动编号。$$\begin{equation}x^n+y^n=z^n\label{eq:afa}\end{equation}$$ 1234\begin&#123;equation&#125;数学公式\label&#123;eq:当前公式名&#125;\end&#123;equation&#125; 5.自动编号公式的引用方法 在公式 $$\eqref{eq:wwqr}$$ 中，我们看到了这个被自动编号的公式。貌似没有成功 6.单个公式换行 单个公式很长的时候需要换行，但仅允许生成一个编号时，可以用split标签包围公式代码，在需要转行的地方使用\，每行需要使用1个&amp;来标识对齐的位置，结束后可使用\tag{…}标签编号。 $$\begin{split}a &amp;= b \c &amp;= d \e &amp;= f\end{split}\tag{1.2}$$ 1234567$$\begin&#123;split&#125;a &amp;= b \\c &amp;= d \\e &amp;= f \end&#123;split&#125;\tag&#123;1.3&#125;$$ 7.多行的独立公式 有时候需要罗列多个公式，可以用eqnarray*标签包围公式代码，在需要转行的地方使用\，每行需要使用2个&amp;来标识对齐位置，两个&amp;…&amp;号之间的是公式间对齐的位置，每行公式后可使用\tag{…}标签编号： github貌似对于多行公式显示不出来。 $$\begin{eqnarray}x^n+y^n &amp;=&amp; z^n \tag{1.4} \x+y &amp;=&amp; z \tag{1.5}\end{eqnarray}$$ 123456$$\begin&#123;eqnarray*&#125;x^n+y^n &amp;=&amp; z^n \tag&#123;1.4&#125; \\x+y &amp;=&amp; z \tag&#123;1.5&#125;\end&#123;eqnarray*&#125;$$ 1.2 符号 输入 显示 输入 显示 x^y $x^y$ x_y $x_y$ \sideset{\^1_2}{\^3_4}\bigotimes $\sideset{^1_2}{^3_4}\bigotimes$ \langle &lt; \lceil $\lceil$ \rceil $\rceil$ \lfloor $\lfloor$ \frac{a}{b} $\frac{a}{b}$ \sqrt{2} $\sqrt{2}$ \alpha,\gamma $\alpha$ $\gamma$ \frac{a}{b} $\frac{a}{b}$ \sum_{n=1}^N{3n} $\sum_{n=1}^N{3n}$ \prod_{n=1}^N{3n} $\prod_{n=1}^N{3n}$ \sqrt[2]{5} $\sqrt[2]{5}$ \int^5_1{f(x)}{\rm d}x $\int^5_1{f(x)}{\rm d}x$ \iint^5_1{f(x)}{\rm d}x $\iint^5_1{f(x)}{\rm d}x$ +\infty $+\infty$ -\infty $-\infty$ \lim_{n\rightarrow+\infty} n $\lim_{n\rightarrow+\infty} n$ \in $\in$ \geq\,\leq $\geq,\leq$ \subset,\supset $\subset,\supset$ \pm,\cdot $\pm,\cdot$ \times,\div $\times,\div$ \not=,\not&lt; $\not=,\not&lt;$ \not\supset $\not\supset$ \log_2{18} $\log_2{18}$ 1.3 希腊字母 输入 显示 输入 显示 \alpha,\beta,\gamma $\alpha,\beta,\gamma$ \delta,\epsilon, \varepsilon $\delta,\epsilon, \varepsilon$ \theta,\lambda,\mu $\theta,\lambda,\mu$ \phi,\varphi,\sigma $\phi,\varphi,\sigma$ 1.4 空心字母与Fraktur字母 输入 显示 输入 显示 \mathbb{A} $\mathbb{A}$ \mathfrak{B} $\mathfrak{B}$ 1.5 分段函数$$P_{r-j}= \begin{cases} 0 &amp;\mbox{if $r-j$ is odd}\ r!\,(-1)^{(r-j)/2} &amp;\mbox{if $r-j$ is even} \end{cases}$$ 1234567$$P_&#123;r-j&#125;= \begin&#123;cases&#125; 0 &amp;\mbox&#123;if $r-j$ is odd&#125;\\ r!\,(-1)^&#123;(r-j)/2&#125; &amp;\mbox&#123;if $r-j$ is even&#125; \end&#123;cases&#125;$$ 1.6 多行对齐公式$$\begin{align}h(x) =&amp; \frac{1}{\int_xt(x)\mathrm{d}x} \tag{1}\f(x) =&amp; \frac{1}{\int_x\eta(x)\mathrm{d}x}g(x)\tag{2}\end{align}$$ 123456$$\begin&#123;align&#125;h(x) =&amp; \frac&#123;1&#125;&#123;\int_xt(x)\mathrm&#123;d&#125;x&#125; \tag&#123;1&#125;\\f(x) =&amp; \frac&#123;1&#125;&#123;\int_x\eta(x)\mathrm&#123;d&#125;x&#125;g(x)\tag&#123;2&#125;\end&#123;align&#125;$$]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[person-reid-transfer-learning]]></title>
    <url>%2F2018%2F11%2F29%2Fperson-reid-transfer-learning%2F</url>
    <content type="text"><![CDATA[transfer learning 这个博客主要是因为最近看了几篇关于无监督迁移学习在行人重识别领域的论文，发现隔了几天，自己对论文就忘记得差不多了，所以对论文的关键内容做个简单记录。 参考链接: Transfer Learning 因为在某些情况下，图片或者公式无法正常显示，所以，我基本会同步到我的博客https://tjjtjjtjj.github.io/2018/11/29/person-reid-transfer-learning/#more 现有方法在transfer learning方向的性能对比 1. ARNAdaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification Yu-Jhe Li, Fu-En Yang, Yen-Cheng Liu, Yu-Ying Yeh, Xiaofei Du, and Yu-Chiang Frank Wang, CVPR 2018 Workshop 这篇论文主要分离了数据集的特有特征和行人特征，从而使不同数据集的行人特征投射到统一特征空间中。 作者是台湾人，没有公布代码。有其他人复现了代码，但是效果很差。 我下一步也会尝试复现一下。 1.1 网络架构 根据作者的描述， $E_I$是resnet50的前四个layer,输入是3X256X256,输出$X^s$是2048X7X7 $E_T,E_C,E_S$,是相同的网络架构，来自FCN的三层，通过查阅FCN的网络设置，初步猜想是FCN的conv6，conv7，conv8，相应的Decoder暂时按照反卷积来设置。这一部分还需要参考FCN的网络设置。 $E_T,E_C,E_S$ conv6:7X7X2048,relu6,drop6(0.5),conv7:1X1X2048,relu6,drop6(0.5),conv8:1X1X2048,至于conv6,7的bn和conv8的bn，relu要不要，还需要实验的验证 在FCN中，逆卷积的使用方式是 deconv(k=64, s=32, p=0)+crop(offset=19)，参考资料:FCN学习:Semantic Segmentation,经典网络复现系列（一）：FCN 反卷积的时候一般都是k=2n, s=n, 参考FCN和pytorch的入门与实践第六章的生成器，我们的Decoder使用deconv(k=1,s=1), deconv(k=1, s=1), deconv(k=7, s=1) encoder和decoder都使用bn和relu 分类层有dropout 学习率，$E_I=10^{-7}, E_T E_C E_S D_C = 10^{-3}, C_S = 2*10^{-3} $，并且在前几个epoch只更新$E_I$ 优化器：SGD 1.2 损失函数分类损失$$L_{class}=-\sum_{i=1}^{N_s}y_i^s.log\hat{y}_i^s \tag {1}$$ 对比损失$$L_{ctrs}=\sum_{i,j}{\lambda}(e_{c,i}^s-e_{c,j}^s)^2+ ({1-\lambda}) [max(0, m-(e_{c,i}^s-e_{c,j}^s))]^2 \tag {2}$$ 重构误差$$ L_{rec} = \sum_{i=1}^{N_s} ||X_i^s-\hat{X_i^s}||^2 + \sum_{i=1}^{N_t} ||X_i^t-\hat{X_i^t}||^2 \tag 3 $$ 差别损失$$ L_{diff} = || {H_c^s}^T H_p^s ||_F^2 + || {H_c^t}^T H_p^t ||_F^2 \tag 4 $$ 总损失$$ L_{total} = L_{class} + \alpha L_{ctrs} + \beta L_{rec} + \gamma L_{diff} \tag {5} $$ 其中$$ \alpha=0.01, \beta= 2.0, \gamma=1500 $$ 1.3 模块分析三个模块: $ L_{rec} $ $ L_{class} $和$ L_{ctrs} $ $ E_T$和$E_S$ 1.3.1 半监督$ L_{rec} $这里不是很懂这个重构误差损失函数的作用，下面的这个解释也不行。重构损失是半监督损失函数。暂时理解成重构损失保证在获取特征的过程中尽可能减少信息损失。或者说，类似PCA，保留主成分，这个主成分只能保证尽可能地把样本分开。至于这个主成分是否重要，是否有利于分类，不得而知。 参考链接：深度学习中的“重构” 作者在这里提示，当只有重构损失函数的时候，应该保持$E_I$不更新，只更新$E_C$. S: Market, T: Duke; S: Duke, T: Market method rank-1 mAP rank-1 mAP $L_{rec}$ 44.5 20.3 31.2 18.4 1.3.2 监督$ L_{rec} $, $ L_{class} $和$ L_{ctrs} $半监督和监督 监督损失使得共享空间捕获到行人语义信息。 S: Market, T: Duke; S: Duke, T: Market method rank-1 mAP rank-1 mAP w/o $ L_{class} $, $ L_{ctrs} $ 52.2 23.7 36.7 19.6 w $ L_{class} $, $ L_{ctrs} $ 70.3 39.4 60.2 33.4 $L_{rec}$ 44.5 20.3 31.2 18.4 $L_{rec}$, $ L_{class} $和$ L_{ctrs} $ 60.5 28.7 48.4 26.8 1.3.3 无监督$ L_{rec} $, $ E_T $和$ E_S $特有特征的提取是为了去除共享空间的噪声。 假设共享空间存在，且特有特征空间存在，如果没有特有特征的提取，那么得到的行人特征或多或少地都会包含特征空间的基向量。 当然，这里也隐含了一些假设，共享空间和特有空间一定是线性无关的。空间的基向量是2048维。 S: Market, T: Duke; S: Duke, T: Market method rank-1 mAP rank-1 mAP w/o $ E_T $, $ E_S $ 60.5 28.7 48.4 26.8 w $ L_{class} $, $ L_{ctrs} $ 70.3 39.4 60.2 33.4 $L_{rec}$ 44.5 20.3 31.2 18.4 $ L_{rec} $, $ E_T $和$ E_S $ 52.2 23.7 36.7 19.6 2. HHLGeneralizing A Person Retrieval Model Hetero- and Homogeneously Zhun Zhong, Liang Zheng, Shaozi Li, Yi Yang, ECCV 2018 code: https://github.com/zhunzhong07/HHL web: http://zhunzhong.site/paper/HHL.pdf 中文: http://www.cnblogs.com/Thinker-pcw/p/9787440.html preson-reid中主要面临的问题： 数据集之间的差异 数据集内部摄像头的差异 解决方法： 相机差异：利用StarGAN进行风格转化 数据集差异：将源域/目标域图片视为负匹配 数据集之间的三元组损失有把不同数据集的行人特征映射到同一特征空间的效果。 创新点在于使用straGAN和复杂的三元组损失。 2.1 网络架构 网络的简要介绍 CNN是resnet50，网络包括两个分支，一个计算源数据集的分类损失，一个计算相似度学习的triplet损失。 FC-2014的组成：linear(2048，1024)–&gt;bn(1024)–&gt;relu–&gt;dropout(0.5),相当于一个embedding。 FC-#ID是linear(1024,751), FC-128是linear(1024, 128), 两个分支的具体情况是： x1–&gt;linear(2048, 1024)–&gt;x2–&gt;bn(1024)–&gt;x3–&gt;relu–&gt;x4–&gt;dropout(0.5)–&gt;x5–&gt;linear(102, 751)–&gt;x6 x1–&gt;linear(2048, 1024)–&gt;x2–&gt;bn(1024)–&gt;x3–&gt;relu–&gt;x4–&gt;linear(1024, 128) 网络的triplet损失是Batch Hard Triplet Loss 网络的输入设置：在每一个batch中，对于分类损失，source domain随机选取batchsize=128张图片，对于triplet损失，source domain随机选取8个人的共batchsize=64张图片，其中连续的8张图片属于同一个人，target domain随机选取batchsize=16个人的共16X9=144张图片，假设这16个人都是不同的人。实验发现，当source domain的分类损失的图片比较少的时候，无法实现预期效果，其他情况下没有测试。当batchsize是这样的配比时，可以达到作者的效果。理由未知． starGAN是离线训练 学习率设置：base：$10^{-1}$，其他：$10^{-2}$，并且每过40个epoch，学习率阶梯性地乘以0.1.一共训练60个epoch就可以达到预期效果，这部分设置和PCB很类似。不知道是经验还是怎么。 关于StarGAN待自己复现之后再做进一步解释，现在只复现过StyleGAN。 triplet损失的margin=0.3 2.2 损失函数分类损失$$L_{cross}=-\sum_{i=1}^{N_s}y_i^s.log\hat{y}_i^s$$ triplet损失$$L_T=\sum_{x_a, x_p, x_n}[D_{x_a, x_p}+m-D_{x_a, x_n}]_+$$ 相机不变性的triplet损失 目标域中一张原始图片作为anchor，StarGAN图片为positive，其他图片为negative $$L_C=L_T((x_t^i)^{n_t}\bigcup(x_{t^*}^i)^{n_t^*})$$ 域不变性的triplet损失 源域中一张图片为anchor，同一id的其他图片作为positive，目标域的任一图片为negative $$L_D=L_T((x_s^i)^{n_s}\bigcup(x_t^i)^{n_t})$$ 相机不变性和域不变性的triplet损失 是将相机不变性和域不变性合为一体，源域的positive不变，negative为源域的其他图片和目标域的图片，目标域的positive不变，negative为源域的图片和目标域的其他行人图片 $$L_{CD}=L_T((x_s^i)^{n_s}\bigcup(x_t^i)^{n_t}\bigcup(x_{t}^i)^{n_t^})$$ 总损失：$$L_{HHL}=L_{cross}+\beta*L_{CD}$$ 其中：$$\beta=0.5$$ 2.3 模块分析 starGAN sample方法 2.3.1 starGAN在源数据集上训练，在目标数据集上测试不同图像增强方法下的图片距离，通过表格可以得出，预训练的模型对于目标数据集的随机翻转等等有很好的鲁棒性，但是，对于不同摄像头的同一个人，其距离还是很大。因此，利用StarGAN和相机不变性的triplet损失来减少由于摄像头带来的偏差。 Source Target Random Crop Random Flip CamStyle Transfer Duke Market 0.049 0.034 0.485 Market Duke 0.059 0.044 0.614 2.3.2 sample方法对于目标域的取样方法，对比了三种方法的性能，分别是随机取样、聚类取样、有监督取样，通过下图可以看出，这三种方法的性能是一样的，最后，作者给的代码是随机取样。 2.4 实验设置2.4.1 Camera style transfer model：StarGAN使用StarGAN进行对于摄像头风格转化。 2 conv + 6 residual + 2 transposed input 128X64 Adam $\beta_1=0.5, \beta_2=0.999$ 数据初始化:随机翻转和随机裁剪 学习率：前100个epoch为0.0001，后100个epoch线性衰减到0 2.4.2 Re-ID model training 设置可以参考Zhong, Z., Zheng, L., Zheng, Z., Li, S., Yang, Y.: Camera style adaptation for person re-identification input 256*128 数据初始化：随机裁剪和随机翻转 dropout=0.5 学习率：新增的层：0.1，base：0.01，每隔40个epoch乘以0.1 mini-batch：源域上对于IDE为128，对于tripletloss是64.目标域上对于triplet loss是16. epoch=60 测试：2048-dim计算欧式距离 2.5 超参数设置 triplet loss的权重$\beta$ 一个batch中目标域上$n_t$的个数 2.5.1 参数的设置$\beta$ $\beta$应该设置成0.4-0.8 2.5.2 参数的设置$n_t$ $n_t$在当前设置(源域上对于IDE为128，对于tripletloss是64)下，应该$n_t&gt;16$ 通过上述参数的设置，结合自己实验时的错误，不妨这么理解，在固定mini-batch=128的情况下 首先引入源域的triplet_loss，并调整batch和$\beta$，使效果达到最优，,batch的选取2倍数的等间隔，$\beta$可以取等间隔，最后batch=64，即128/2=64，$\beta$则可以先固定成某个值. 然后引入目标域的triplet_loss，并且要先考虑只有目标域的性能，再考虑结合的性能，每次都需要重新考虑$\beta$和batch的大小 这么一想，这篇论文做的实验还是很多的。 2.6 实验结果 通过结果我们看出来，其实提升的效果主要来源于$L_C$，说明预训练的模型对于目标域不同摄像头的图片鲁棒性很差。 是否说明预训练的模型只学习到了源数据集的跨摄像头的不变行人特征，而对于目标域的摄像头下的不同风格很敏感，而对目标域的同一摄像头下的行人特征很鲁棒。 $L_T$的提升效果很小是否可以说明目标数据集与源数据集的行人特征空间本身就已经很好地重合了，假如tripl_loss真得具有将不同数据集的行人特征映射到同一特征空间的效果的话。 通过这篇论文，我们能学到的东西很多，比如对比实验，参数设置实验，想法验证实验等等。 附录triplet_loss发现triplet_loss很厉害的样子，不妨看看是个什么情况。 参考链接：Triplet Loss and Online Triplet Mining in TensorFlow Re-ID with Triplet Loss In Defense of the Triplet Loss for Person Re-Identification code Triplet Loss and Online Triplet Mining in TensorFlow这个博客讲述了triplet_loss的起源、发展和具体使用的几种形式。最后的结论是应该使用在线的batch hard策略。 Re-ID with Triplet Loss这篇博客则逻辑性地介绍了各种triplet_loss的变体。最后的结论是batch hard+soft margin效果更好。 也有提及到，triplet_loss总是不如分类损失强。 下一步工作已经理解源代码 3. SPGANImage-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification Weijian Deng, Liang Zheng, Guoliang Kang, Yi Yang, Qixiang Ye, Jianbin Jiao, CVPR 2018 这篇论文主要是构建”Learning via Translation”的框架来进行迁移学习，利用SPGAN(CycleGAN+Simaese net)从源数据集迁移到目标数据集，然后在目标数据集上训练。 论文的重点是怎么改进CycleGAN。 web:http://www.sohu.com/a/208231404_642762 code:https://github.com/Simon4Yan/Learning-via-Translation CycleGAN Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks code:https://github.com/zhunzhong07/CamStyle 自己对代码的分析https://tjjtjjtjj.github.io/2018/11/19/cycleGAN/#more 3.1 前言一般的无监督迁移方法都是假设源域和目标域上有相同ID的图片，不太适用于跨数据集的行人重识别。 3.2 网络架构GAN网络 LMP网络 行人重识别整体网络 网络的简要介绍 整理网络由两部分组成，第一部分是SPGAN，第二部分是常见的行人重识别网络的修改版LMP，重点是第一部分。 整个网络是用Caffe搭建。 因为自己没有仔细看caffe的代码，后期有需要的还是要看看超参数设置的。 SPGAN基本沿用了CycleGAN的设置，epoch=5，更多的epoch没有用。 SPGAN的正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$,负样本是$G(x_S)$和$x_T$或者$F(x_T)$和$x_S$，作者给的代码中用的正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$，负样本是$x_S$和$x_T$。 SPGAN的训练分为生成器、判别器、SiaNet。 $L_{ide}$可以保持转换前后图片颜色保持一致。 LMP网络直接generated domain上训练。 在论文的tabel2的注释中，可以看到是分成了7份，与PCB的6份差不多。 3.3 损失函数3.3.1 CycleGAN$$L_{T_{adv}}(G,D_T,p_x,p_y)=E_{y\sim p_y}[(D_T(y)-1)^2]+E_{x\sim p_x}[(D_T(G(x))-1)^2]$$$$L_{S_{adv}}(F,D_S,p_x,p_y)=E_{x\sim p_x}[(D_S(x)-1)^2]+E_{y\sim p_y}[(D_S(F(y)))^2]$$$$L_{cyc}(G,F)=E_{x\sim p_x}\parallel F(G(x))-x \parallel_1+E_{y\sim p_y}\parallel G(F(y))-y\parallel_1$$$$L_{ide}(G,F,p_x,p_y)=E_{x\sim p_x}\parallel F(x)-x\parallel_1+E_{y\sim p_y}\parallel G(y)-y\parallel_1$$ 3.3.2 SPGANSiameses Net:$$L_{con}(i,x_1,x_2)=(1-i)(max(0,m-d))^2+id^2$$其中，$m\in [0,2]$，$d=1-cos(\theta)\in [0,2]$表示归一化后的欧式距离.正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$,负样本是$G(x_S)$和$x_T$或者$F(x_T)$和$x_S$。 Overall objective loss:$$L_{sp}=L_{T_{adv}}+L_{S_{adv}}+\lambda_1 L_{cyc}+\lambda_2 L_{ide}+\lambda_3 L_{con}$$其中，$\lambda_1=10，\lambda_2=5，\lambda_3=2, m=2$ 3.3.3 行人重识别网络以resnet50为基础网络，和PCB类似，分割成两块。 3.4 实验设置3.4.1 SPGANSPGAN的整体训练过程与CycleGAN基本是一致的，建议先参考CycleGAN，再学习SPGAN。 $\lambda_1=10，\lambda_2=5，\lambda_3=2, m=2$，学习率为0.0002，batch=1，total_epoch=5 SiaNet: 4个conv+4个max pool+1个FC。 x(3,256,256)-&gt;conv(3,64,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2) -&gt;conv(64,128,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2) -&gt;conv(128,256,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2) -&gt;conv(256,512,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)(1,1,512) -&gt;FC(512, 128)-&gt;leak_relu(0.2)-&gt;dropout(0.5)-&gt;FC(128,64) 输入预处理：随机左右翻转、resize(286)、crop(256)、img/127.5-1。 激活函数全部使用leak_relu(0.2)，没有使用bn 1234567891011121314151617181920212223242526272829def metric_net(img, scope, df_dim=64, reuse=False, train=True): bn = functools.partial(slim.batch_norm, scale=True, is_training=train, decay=0.9, epsilon=1e-5, updates_collections=None) with tf.variable_scope(scope + '_discriminator', reuse=reuse): h0 = lrelu(conv(img, df_dim, 4, 2, scope='h0_conv')) # h0 is (128 x 128 x df_dim) pool1 = Mpool(h0, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') h1 = lrelu(conv(pool1, df_dim * 2, 4, 2, scope='h1_conv')) # h1 is (32 x 32 x df_dim*2) pool2 = Mpool(h1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') h2 = lrelu(conv(pool2, df_dim * 4, 4, 2, scope='h2_conv')) # h2 is (8 x 8 x df_dim*4) pool3 = Mpool(h2, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') h3 = lrelu(conv(pool3, df_dim * 8, 4, 2, scope='h3_conv')) # h3 is (2 x 2 x df_dim*4) pool4 = Mpool(h3, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') shape = pool4.get_shape() flatten_shape = shape[1].value * shape[2].value * shape[3].value h3_reshape = tf.reshape(pool4, [-1, flatten_shape], name = 'h3_reshape') fc1 = lrelu(FC(h3_reshape, df_dim*2, scope='fc1')) dropout_fc1 = slim.dropout(fc1, 0.5, scope='dropout_fc1') net = FC(dropout_fc1, df_dim, scope='fc2') #print_activations(net) #print_activations(pool4) return net 3.4.2 LMPbatch_size=16, total_epoch=50, SGD, momentum=0.9, gamma=0.1, lr_ini=0.001, decay to 0.0001 after 40 epochs. 这部分的设置和IDE基本类似。 3.5 对比实验模块的对比实验 通过对比实验可以看到，以mAP为指标，CycleGAN增加了3个点，SiaNet(m=2)增加了3个点，LMP增加了4个点。说明作者尝试的3个模块都在一定程度上起到了作用。但是个人感觉还是差点什么。比如，为什么会有效？ 假设目标都是为了使源域与目标域的行人特征映射到同一特征空间。这里的CycleGAN做到了这一点。LMP可以认为是加在哪里都有效的一种方式。那SiaNet其实更像是在保证生成的图片不仅要保留源图片的内容，更要保留源图片的行人特征。这种保留是以一种隐空间的形式在保存，而不是明显的分类损失这样子。 $\lambda_3 $对比实验 pool 和 part的对比实验 也就是说，pool的方式和parts的取法是实验得到的，不是凭空想出来的。 通过上述实验超参数的设置对比实验，与HHL论文比较，都是固定其他，变化一个参数，然后选取最优的参数，是基于局部最优就是全局最优的思想。感觉到作者的实验做得很足。 不同base model的对比实验 附录IDE and $IDE^+$IDE https://github.com/zhunzhong07/IDE-baseline-Market-1501 We name the descriptor as ID-discriminative Embedding (IDE).感觉还是没有很好地理解IDE。 对于IDE+没有找到对应的原文，因为不是重点，暂且跳过。 IDE的pytorch代码 https://github.com/Simon4Yan/Person_reID_baseline_pytorch IDE和$IDE^+$的网络模型是一样的： resnet50(layer4)+avgpool+Linear(2048,512)+bn1d(512)+LeakReLU(0.1)+Dropout(0.5)+Linear(512, num_class) 区别在于训练时bn层是否更新： 123456789101112131415161718# model.model = resnet50(layer4)+avgpool+Linear(2048,512)+bn1d(512)+LeakReLU(0.1)+Dropout(0.5)# model.classifier = Linear(512, num_class)# IDEfor phase in ['train', 'val']: if phase == 'train': scheduler.step() model.train(True) # Set model to training mode else: model.train(False) # Set model to evaluate mode# IDE+for phase in ['train', 'val']: if phase == 'train': scheduler.step() model.eval() # Fix BN of ResNet50 model.model.fc.train(True) model.classifier.train(True) else: model.train(False) # Set model to evaluate mode 新增 2018-12-17参考论文: Re-Identification with Consistent Attentive Siamese Networks IDE的网络架构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 一般情况下，cut_at_pooling=False，num_features=256, has_embedding为true# 一般情况下，新增了feat、feat_bn、relu、drop、classifierclass ResNet(nn.Module): __factory = &#123; 18: torchvision.models.resnet18, 34: torchvision.models.resnet34, 50: torchvision.models.resnet50, 101: torchvision.models.resnet101, 152: torchvision.models.resnet152, &#125; def __init__(self, depth, pretrained=True, cut_at_pooling=False, num_features=0, norm=False, dropout=0, num_classes=0, triplet_features=0): super(ResNet, self).__init__() self.depth = depth self.pretrained = pretrained self.cut_at_pooling = cut_at_pooling # Construct base (pretrained) resnet if depth not in ResNet.__factory: raise KeyError("Unsupported depth:", depth) self.base = ResNet.__factory[depth](pretrained=pretrained) if not self.cut_at_pooling: self.num_features = num_features self.norm = norm self.dropout = dropout self.has_embedding = num_features &gt; 0 self.num_classes = num_classes self.triplet_features = triplet_features out_planes = self.base.fc.in_features # Append new layers if self.has_embedding: self.feat = nn.Linear(out_planes, self.num_features) self.feat_bn = nn.BatchNorm1d(self.num_features) init.kaiming_normal_(self.feat.weight, mode='fan_out') init.constant_(self.feat.bias, 0) init.constant_(self.feat_bn.weight, 1) init.constant_(self.feat_bn.bias, 0) else: # Change the num_features to CNN output channels self.num_features = out_planes if self.dropout &gt; 0: self.drop = nn.Dropout(self.dropout) if self.num_classes &gt; 0: self.classifier = nn.Linear(self.num_features, self.num_classes) init.normal_(self.classifier.weight, std=0.001) init.constant_(self.classifier.bias, 0) if self.triplet_features &gt; 0: self.triplet = nn.Linear(self.num_features, self.triplet_features) init.normal_(self.triplet.weight, std=0.001) init.constant_(self.triplet.bias, 0) if not self.pretrained: self.reset_params() def forward(self, x, output_feature=None): for name, module in self.base._modules.items(): if name == 'avgpool': break x = module(x) if self.cut_at_pooling: return x x = F.avg_pool2d(x, x.size()[2:]) x = x.view(x.size(0), -1) if output_feature == 'pool5': x = F.normalize(x) return x if self.has_embedding: x = self.feat(x) x = self.feat_bn(x) if self.norm: x = F.normalize(x) elif self.has_embedding: x = F.relu(x) # triplet feature if self.triplet_features &gt; 0: x_triplet = self.triplet(x) if self.dropout &gt; 0: x = self.drop(x) if self.num_classes &gt; 0: x_class = self.classifier(x) # two outputs if self.triplet_features &gt; 0: return x_class, x_triplet return x_class def reset_params(self): for m in self.modules(): if isinstance(m, nn.Conv2d): init.kaiming_normal(m.weight, mode='fan_out') if m.bias is not None: init.constant(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): init.constant(m.weight, 1) init.constant(m.bias, 0) elif isinstance(m, nn.Linear): init.normal(m.weight, std=0.001) if m.bias is not None: init.constant(m.bias, 0) Caffe and pytorchCaffe和pytorch中的bn层的计算方式不一样。 在caffe中，bn层在训练时是eval状态，也是只使用Imagenet的mean和variance The eval mode for BN layer during training, corresponding to Caffe’s batch_norm_param {use_global_stats: true}, means using ImageNet BN mean and variance during training. 在pytorch中，bn层在训练时如果设置成eval装填，才可以达到caffe的精度。 疑惑IDE和IDE+的效果区别为什么会这么大? 下一步工作 已经理解源代码 尝试在pytorch上复现结果，现在根据作者提供的代码，感觉并不是很难。主要是SPGAN。 4. 基于GAN的类似论文类似的采取GAN做person-reid方向的论文还有好多，上面两篇是现在最新的，下面就简单地介绍几篇类似的文章，其中涉及到的原理和前文提到的GAN的方法类似。 4.1 PTGANPerson Transfer GAN to Bridge Domain Gap for Person Re-Identification Longhui Wei1, Shiliang Zhang1, Wen Gao1, Qi Tian 这篇论文对Cycle-GAN进行了改进，保留ID信息的损失函数如下：$$L_{ID}=E_{a \sim p_{data}(a)} [||(G(a)-a) \odot M(a)||2] + E{b \sim p_{data}(b)} [||(F(b)-b) \odot M(b)||_2]$$ 其中，$M(b)$表示使用PSPNet分割后的结果。 转化效果如下图所示 这里的Cycle-Gan生成图片的效果和SPGAN生成的效果还是有一些区别的，不是很理解。 其他的不是本次的重点，不做介绍。 4.2 DCGAN+CNNUnlabeled Samples Generated by GAN Improve the Person Re-Identification Baseline in Vitro Zhedong Zheng Liang Zheng Yi Yang 这篇论文主要是利用DCGAN生成新的数据集进行数据集扩充。 网络架构如图所示： 生成效果图 生成图片的标签LSRO$$q_{LSR}=\begin{cases} \frac{\epsilon}{K},&amp;k\neq y\\ 1-\epsilon+\frac{\epsilon}{K},&amp;k=y \end{cases}$$$$l_{LSR}=-(1-\epsilon)log(p(y))-\frac{\epsilon}{K}\sum_{k=1}^{K}log(p(k))$$$$q_{LSRO}=\frac{1}{K}$$$$l_{LSRO}=-(1-Z)log(p(y))-\frac{Z}{K}\sum_{k=1}^Klog(p(k))$$其中，真实图片的Z=0，生成图片的Z=1. 5. MMFAMulti-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification Shan Lin, Haoliang Li, Chang-Tsun Li, Alex Chichung Kot, BMVC 2018 5.1 前言其想法也是将源域与目标域映射到同一特征空间。创新点是： 利用MMD缩小源域与目标域的分布差异 考虑了属性 MMD的参考代码 5.2 网络架构 网络架构的说明: 每一个batch中包括$n_s$张源域图片，$n_t$张目标域图片。batch=32 backbone是resnet50，并且修改resnet50的avg_pool为max_pool $H_S$是pool层的输出向量，$H_S^{id}$是ID-FC层的输出相邻，$H_S^{attr_m}$Attr-FC-m的输出向量。 input (256,128,3) FC=fc+bn+dropout(0.5)+leaky ReLU+fc SGD:momentum=0.9,weight decay=5x10e-4 lr=0.01,每20个epoch乘以0.1 测试使用max pool的2048维向量的欧式距离 Market有27个属性，Duke有23个属性 5.3 损失函数Identity Loss:$$L_{id}=-\frac{1}{n_s}\sum_{i=1}^{n_S}log(p_{id}(h_{S,i}^{id},y_{S,i}))$$ Attribute Loss:$$L_{attr}=-\frac{1}{M}\frac{1}{n_S}\sum_{m=1}^{M}\sum_{i=1}^{n_S}(a_{S,i}^{m}\cdot log(p_{attr}(h_{S,i}^{attr_m}, m)) -\\(1-a_{S,i}^{m})\cdot log(1-p_{attr}(h_{S,i}^{attr_m}, m)))$$ Attribute Feature Adaptation$$L_{AAL}=\frac{1}{M}\sum_{m=1}^{M}MMD(H_{S}^{attr_m}, H_{T}^{attr_m})^2\\ =\frac{1}{M}\sum_{m=1}^{M}\parallel \frac{1}{n_S}\sum_{i=1}^{n_S}\phi(h_{S,i}^{attr_m}) - \frac{1}{n_T}\sum_{i=1}^{n_T}\phi(h_{T,j}^{attr_m}) \parallel _{H}^2 \\ =\frac{1}{M}\sum_{m=1}^{M}[ \frac{1}{(n_S)^2}\sum_{i=1}^{n_S}\sum_{i’=1}^{n_S}k(h_{S,i}^{attr_m}, h_{S,i’}^{attr_m})\\ +\frac{1}{(n_T)^2}\sum_{i=1}^{n_T}\sum_{i’=1}^{n_T}k(h_{T,i}^{attr_m}, h_{T,i’}^{attr_m})\\ -\frac{2}{n_S\cdot n_T}\sum_{i=1}^{n_S}\sum_{j=1}^{n_T}k(h_{S,i}^{attr_m}, h_{T,j}^{attr_m}) ] $$ $$k(h_{S,i}^{attr_m}, h_{T,j}^{attr_m})=exp(-\frac{1}{2\alpha}\parallel h_{S,i}^{attr_m} - h_{T,j}^{attr_m}\parallel ^2)$$$$\alpha=1,5,10$$ Mid-level Deep Feature Adaptation$$L_{MDAL}=MMD(H_S,H_T)^2$$ Overall loss$$L_{all}=L_{id}+\lambda_1 L_{attr}+\lambda_2 L_{AAL}+\lambda_3 L_{MDAL}$$$$\lambda_1=0.1,\lambda_2=1,\lambda_3=1$$ 5.4 实验分析5.4.1 实验结果 5.4.2 实验模块实验模块对比实验结果 5.5 附录通过实验结果可以看出，在MMFA模型中，ID+Mid-level Deep Feature Adaptation的贡献最大。 下一步可以尝试考虑Mid-level Deep Feature Adaptation。 作者把avg pool 换成max pool。 6. TJ-AIDLTransferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li, ECCV 2018 6.1 前言这篇论文的创新点在于： 根据属性和id的关系，提出了Identity Inferred Attribute Space。 6.2 网络架构Attribute-Identity Transferable Joint Learning Unsupervised Target Domain Adaptation 网络架构的简要说明： (a) Identity Branch (b) Attribute Branch (c) Identity Inferred Attribute (IIA) space 训练过程分为两步: (I) 源域训练: Attribute-Identity Transferable Joint Learning (II) 目标域微调: Unsupervised Target Domain Adaptation 一般情况下Identity Branch和Attribute Branch是共享网络，但是本论文中特意分成两个非共享网络 重点在于对$e_{IIA}$的处理 IIA-encoder 是3个fc层，512/128/m，decoder是encoder的镜像。 基准网络是MobileNet Adam优化器，lr=0.002，mementum$\beta_1=0.5, \beta_2=0.999$ batch size=8 We started with training the identity branch by 100,000 iterations on the source identity labels and then the whole model by 20,000 iterations for both transferable joint learning on the labelled source data and unsupervised domain adaptation on the unlabelled target data 6.3 损失函数Identity Branch (a) softmax$$L_{id}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}log(p_{id}(I_i^S,y_i^S)) \tag{1}$$ Attribute Branch(b) sigmoid$$L_{att}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}\sum_{j=1}^{m}(a_{i,j}log(p_{att}(I_i,j))+(1-a_{i,j})log(1-p_{att}(I_i,j))) \tag{2}$$ Identity Inferred Attribute (IIA) space (c)$$L_{rec}=\parallel x_{id}-f_{IIA}(x_{id}) \parallel ^2 \tag{3}$$$$L_{ID-transfer}=\parallel e_{IIA}-\tilde{p}_{att} \parallel ^2 \tag{4}$$$$L_{att,IIA}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}\sum_{j=1}^{m}(a_{i,j}log(p_{IIA}(I_i,j))+(1-a_{i,j})log(1-p_{IIA}(I_i,j))) \tag{5}$$$$L_{IIA}=L_{att,IIA}+\lambda_1 L_{rec}+\lambda_2 L_{ID-transfer} \tag{6}$$$$\lambda_1=10, \lambda_2=10$$ Impact of IIA on Identity and Attribute Branches$$L_{att-total}=L_{att}+\lambda_2 L_{ID-transfer} \tag{7}$$ 6.4 训练与部署流程 6.5 模块分析6.5.1 ID和Attribute模块分析 通过ID only的mAP和HHL的baseline，可以看出MobileNet和Resnet50对mAP的影响不受很大。 另外，可以看出，依然是ID占据了很大比重。 6.5.2 Adapation的作用 从表格中可以看出，Adaptation的作用很小。说明，预训练的模型已经很好地能保持属性的一致性，即不同角度得到的属性是一样的。 6.6 补充还是难以理解作者这么做的出发点，感觉有点凭空就设计出这么多损失函数，可能是哪里还缺点什么东西。 训练更新的时候方程(7)的出现原因是什么？更新(6)的时候应该就已经对attr进行了影响吧？ 在step(II)中，是怎么更新方程(6)的。 Identity Inferred Attribute Space的合理性是怎么体现的？]]></content>
      <categories>
        <category>ind1</category>
      </categories>
      <tags>
        <tag>person-reid</tag>
        <tag>transfer learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cycleGAN]]></title>
    <url>%2F2018%2F11%2F19%2FcycleGAN%2F</url>
    <content type="text"><![CDATA[这篇博客主要记录在跟随cycleGAN作者的代码复现学到的东西。title: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(ICCV2017) paper: https://arxiv.org/abs/1703.10593code: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pixmycode: https://github.com/TJJTJJTJJ/pytorch_cycleGANcycle_gan的整体框架写得很漂亮，frame可以参考github的frame 1.动态导入模块以及文件内的类类似这种文件结构.models|– init.py|– base_model.py|– cycle_gan_model.py|– networks.py|– pix2pix_model.py`– test_model.py 在init.py这样写两个函数 1234567891011121314151617181920212223242526def find_model_using_name(model_name): """ 根据model_name导入具体模型'models/model_name_model.py' :param model_name: eg. cycle_gan :return: mdoel class eg.cycle_gan_model.CycleGANModle """ # step1 import 'models/model_name_model' model_filename = 'models.'+model_name+'_model' modellib = importlib.import_module(model_filename) # step2 get model_name model = None target_model_name = model_name.replace('_','')+'model' for name, cls in modellib.__dict__.items(): if name.lower() == target_model_name.lower() \ and issubclass(cls, BaseModel): model = cls if model is None: print_str = "In &#123;model_filename&#125;.py, there should be a subclass of BaseModel with class name " \ "that matches &#123;target_model_name&#125; in lowercase.".format(model_filename=model_filename, \ target_model_name=target_model_name) print(print_str) exit(0) return model 12modellib.__dict__ == vars(modellib)vars().keys() == dir() 12345678910111213141516171819import importlibmodellib = importlib.import_module(model_filename)for k in dir(modellib): print(k)CycleGANModel__builtins____cached____doc____file____loader____name____package____spec__print(modellib.__dict__)&#123;'__name__': 'cycle_gan_model',...'CycleGANModel': cycle_gan_model.CycleGANModel&#125; 12exit(0)无错误退出exit(1)有错误退出 2.学习率直线下降123456for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):def lambda_rule(epoch): lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1) return lr_lscheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule) 3.NotImplemented &amp;&amp; NotImplementedError参考:http://www.php.cn/python-tutorials-160083.htmlhttps://stackoverflow.com/questions/1062096/python-notimplemented-constant return NotImplementedraise NotImplementedError(‘initialization method [%s] is not implemented’ % init_type) 4.parser的修改这里既有外界传入的参数,也有自己的参数isTrain,在主函数里调用的时候调用方式是一致的,只是一个可以通过外界传参,一个不能通过外界传参 12345678910111213141516class TrainOptions(): def initialize(self): parser = argparse.ArgumentParser( formatter_class=argparse.ArgumentDefaultsHelpFormatter ) parser.add_argument('--batch_size', type=int, default=1, help='input batch size') parser.set_defaults(dataset_mode='single') opt, _ = parser.parse_known_args() self.isTrain = True return opt def parse(self): opt = self.initialize() opt.isTrain = self.isTrainopt = TrainOptions().parse() 5.eval()和test()函数的结合12345678910111213141516171819202122def eval(self): """ make models eval mode during test time :return: None """ for name in self.model_names: if isinstance(name, str): net = getattr(self, 'net'+name) net.eval() return Nonedef test(self): """ don't need backprop during test time :return: """ with torch.no_grad(): self.forward()model.eval()model.test() 6.多GPU结论12345678910111213141516171819202122232425262728293031323334353637modelb = torch.nn.DataParallel(modela, device_ids=[0,1,2])# savetorch.save(modelb.module.cpu().state_dict(),path)modelb.cuda(gpu_ids[0])# loaddef load_networks(self, epoch): for name in self.model_names: if isinstance(name, str): load_filename = '%s_net_%s.pth' % (epoch, name) load_path = os.path.join(self.save_dir, load_filename) net = getattr(self, 'net' + name) if isinstance(net, torch.nn.DataParallel): net = net.module print('loading the model from %s' % load_path) # if you are using PyTorch newer than 0.4 (e.g., built from # GitHub source), you can remove str() on self.device state_dict = torch.load(load_path, map_location=str(self.device)) if hasattr(state_dict, '_metadata'): del state_dict._metadata # patch InstanceNorm checkpoints prior to 0.4 for key in list(state_dict.keys()): # need to copy keys here because we mutate in loop self.__patch_instance_norm_state_dict(state_dict, net, key.split('.')) net.load_state_dict(state_dict)# ortorch.nn.DataParallel加载预训练模型import torchclass ModelA(torch.nn.Module): def __init__(self): super(ModelA, self).__init__() self.base1 = torch.nn.Conv2d(2,2,2) def forward(self,x): passaa = ModelA()bb = torch.nn.DataParallel(aa, device_ids=[0])bb.module.load_state_dict(torch.load('aa.pth')) 对于单gpu和Module对于普通的model.cuda,在保存模型会自动变成cpu,需要再次cuda一次对于DataParallel,在保存模型会自动变成cpu,需要再次cuda一次通过源码可以得知,DataParallel的device_ids初始化就已经确定,所以不用担心cuda到第一个GPU上而导致DataParallel忘记自己可以复制到哪些GPU上,会自动复制的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124import torchclass ModelA(torch.nn.Module): def __init__(self): super(ModelA, self).__init__() self.base = torch.nn.Conv2d(2,2,2,2) def forward(self,x): passaa = ModelA()print(aa)ModelA( (base): Conv2d(2, 2, kernel_size=(2, 2), stride=(2, 2)))print(aa.state_dict())OrderedDict([('base.weight', tensor([[[[ 0.0119, 0.2522], [-0.0682, 0.2366]], [[ 0.2013, 0.2106], [ 0.2242, 0.1711]]], [[[-0.2777, 0.2446], [ 0.3494, 0.1552]], [[ 0.0270, 0.1272], [-0.1878, -0.3501]]]])), ('base.bias', tensor([ 0.1433, 0.1061]))])aa.cuda()print(aa.state_dict())OrderedDict([('base.weight', tensor([[[[ 0.0119, 0.2522], [-0.0682, 0.2366]], [[ 0.2013, 0.2106], [ 0.2242, 0.1711]]], [[[-0.2777, 0.2446], [ 0.3494, 0.1552]], [[ 0.0270, 0.1272], [-0.1878, -0.3501]]]], device='cuda:0')), ('base.bias', tensor([ 0.1433, 0.1061], device='cuda:0'))]print(aa.cpu().state_dict())OrderedDict([('base.weight', tensor([[[[ 0.1570, -0.2992], [-0.2927, -0.2748]], [[-0.0097, 0.0346], [-0.3125, 0.2615]]], [[[-0.2506, -0.2632], [ 0.1302, -0.2223]], [[ 0.1422, 0.0427], [ 0.3453, 0.0219]]]])), ('base.bias', tensor([ 0.1974, -0.1549]))])print(aa.state_dict())OrderedDict([('base.weight', tensor([[[[ 0.1570, -0.2992], [-0.2927, -0.2748]], [[-0.0097, 0.0346], [-0.3125, 0.2615]]], [[[-0.2506, -0.2632], [ 0.1302, -0.2223]], [[ 0.1422, 0.0427], [ 0.3453, 0.0219]]]])), ('base.bias', tensor([ 0.1974, -0.1549]))])bb = torch.nn.DataParallel(aa, device_ids=[0])print(bb)DataParallel( (module): ModelA( (base): Conv2d(2, 2, kernel_size=(2, 2), stride=(2, 2)) ))print(bb.state_dict())OrderedDict([('module.base.weight', tensor([[[[ 0.0119, 0.2522], [-0.0682, 0.2366]], [[ 0.2013, 0.2106], [ 0.2242, 0.1711]]], [[[-0.2777, 0.2446], [ 0.3494, 0.1552]], [[ 0.0270, 0.1272], [-0.1878, -0.3501]]]], device='cuda:0')), ('module.base.bias', tensor([ 0.1433, 0.1061], device='cuda:0'))])print(bb.module.cpu().state_dict())OrderedDict([('base.weight', tensor([[[[ 0.1570, -0.2992], [-0.2927, -0.2748]], [[-0.0097, 0.0346], [-0.3125, 0.2615]]], [[[-0.2506, -0.2632], [ 0.1302, -0.2223]], [[ 0.1422, 0.0427], [ 0.3453, 0.0219]]]])), ('base.bias', tensor([ 0.1974, -0.1549]))])print(bb)DataParallel( (module): ModelA( (base): Conv2d(2, 2, kernel_size=(2, 2), stride=(2, 2)) ))print(bb.state_dict())OrderedDict([('base.weight', tensor([[[[ 0.0119, 0.2522], [-0.0682, 0.2366]], [[ 0.2013, 0.2106], [ 0.2242, 0.1711]]], [[[-0.2777, 0.2446], [ 0.3494, 0.1552]], [[ 0.0270, 0.1272], [-0.1878, -0.3501]]]])), ('base.bias', tensor([ 0.1433, 0.1061]))])bb.cuda(gpu_ids[0])print(bb.state_dict())OrderedDict([('module.base.weight', tensor([[[[ 0.0119, 0.2522], [-0.0682, 0.2366]], [[ 0.2013, 0.2106], [ 0.2242, 0.1711]]], [[[-0.2777, 0.2446], [ 0.3494, 0.1552]], [[ 0.0270, 0.1272], [-0.1878, -0.3501]]]], device='cuda:0')), ('module.base.bias', tensor([ 0.1433, 0.1061], device='cuda:0'))]) 7.Norm参考：https://blog.csdn.net/liuxiao214/article/details/81037416 输入图像：[N,C,H,W]BatchNorm: [1,C,1,1]InstanceNorm: [N,C,1,1] 经过实验,instanceNorm层的weight, bias, running_mean, running_var总是None代码中加载模型的时候对instanceNorm层进行了删除操作,为什么对于pytorch之前的版本instanceNorm层是有running_mean和running_var的,之后的版本修正了之后,就不再需要了 123456789101112def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0): key = keys[i] if i + 1 == len(keys): # at the end, pointing to a parameter/buffer if module.__class__.__name__.startswith('InstanceNorm') and \ (key == 'running_mean' or key == 'running_var'): if getattr(module, key) is None: state_dict.pop('.'.join(keys)) if module.__class__.__name__.startswith('InstanceNorm') and \ (key == 'num_batches_tracked'): state_dict.pop('.'.join(keys)) else: self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1) 8.functools偏函数：适合为多个调用函数提供一致的函数接口 123456789101112from functools import partialdef f(m,n,p): return m*n*pre=partial(f,3,4)print(re(5)# 60if type(norm_layer) == functools.partial: use_bias = norm_layer.func == nn.InstanceNorm2delse: use_bias = norm_layer == nn.InstanceNorm2d 9.论文与代码ndf模型的定义与论文有一个地方不一致,论文写的第一个conv之后通道数是32,但实现是64.与作者沟通得知,第一层不是32,而是64,剩下的也依次递增. 下采样的时候没有使用reflect进行补充,而是使用了0填充.与作者沟通后，提出的是都可以尝试一下 unet modelUnet model与网上的不是很一致3-&gt;1-&gt;2-&gt;4-&gt;8-&gt;8-&gt;83&lt;-2&lt;-4&lt;-8&lt;-16&lt;-16&lt;-16 参数 no_lsgan12345678910111213141516self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan).to(self.device)# GAN lossclass GANLoss(nn.Module): def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0): super(GANLoss, self).__init__() self.register_buffer('real_label', torch.tensor(target_real_label)) self.register_buffer('fake_label', torch.tensor(target_fake_label)) if use_lsgan: self.loss = nn.MSELoss() else: self.loss = nn.BCELoss()use_sigmoid = opt.no_lsganself.netD_A = define_D(opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids) 也就是opt.no_lsgan为True时, netD使用sigmoid, GANloss使用BCELoss()opt.no_lsgan为False时, netD不使用sigmoid, GANloss使用MSELoss() MSELoss:均方误差 (x-y)*2BCELoss:二分类的交叉熵:使用前需要使用sigmoid函数,input和target的输入维度是一样的.(N,) 根据作者提供的运行代码,猜测作者使用的是opt.no_lsgan为False,均方误差 L1loss: |x-y| G and D 的反向传播过程回顾一下G和D的反向传播train G 123456set_requires_grad(D, False)fake_A = G(real_A)loss = criterion(D(fake_A), True)optimizers_G.zero_grad()loss.backward()optimizers_G.step() train D 12345678set_requires_grad(D, False)#fake_A = fake_A.detach() # 取消G的gradloss1 = criterion(fake_A.detach(), False)loss2 = criterion(realA, True)loss = loss1 + loss2optimizers_D.zero_grad()loss.backward()optimizers_D.step() 10.ConTransposed的计算方法逆卷积后的图像大小和之前的能对应上，需要output_padding 1234nn.ConvTranspose2d(ngf*mult, int(ngf*mult/2), kernel_size=3,stride=2, padding=1, output_padding=1, bias=use_bias)]-k+2p+s-out_padding是s的整数k=3,s=2,p=1,则out_padding=1k=3,s=4,p=1,则out_padding=3 11.初始化参数123456789101112131415161718192021222324def init_weights(net, init_type='normal', gain=0.02): def init_func(m): # conv, contranspose ,linear, bn # type(m) == nn.Conv2d classname = m.__class__.__name__ if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1): if init_type == 'normal': init.normal_(m.weight.data, 0.0, gain) elif init_type == 'xavier': init.xavier_normal_(m.weight.data, gain=gain) elif init_type == 'kaiming': init.kaiming_normal_(m.weight.data, a=0, mode='fan_in') elif init_type == 'orthogonal': init.orthogonal_(m.weight.data, gain=gain) else: raise NotImplementedError('initialization method [%s] is not implemented' % init_type) if hasattr(m, 'bias') and m.bias is not None: init.constant_(m.bias.data, 0.0) elif classname.find('BatchNorm2d') != -1: init.normal_(m.weight.data, 1.0, gain) init.constant_(m.bias.data, 0.0) print('initialize network with %s' % init_type) net.apply(init_func) 12.disriminator PatchGAN and GANLossPatchGAN的kernel是4. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class GANLoss(nn.Module): def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0): super(GANLoss, self).__init__() self.register_buffer('real_label', torch.tensor(target_real_label)) self.register_buffer('fake_label', torch.tensor(target_fake_label)) if use_lsgan: self.loss = nn.MSELoss() else: self.loss = nn.BCELoss() def get_target_tensor(self, input, target_is_real): if target_is_real: target_tensor = self.real_label else: target_tensor = self.fake_label return target_tensor.expand_as(input) def __call__(self, input, target_is_real): target_tensor = self.get_target_tensor(input, target_is_real) return self.loss(input, target_tensor)# Defines the PatchGAN discriminator with the specified arguments.class NLayerDiscriminator(nn.Module): def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False): super(NLayerDiscriminator, self).__init__() if type(norm_layer) == functools.partial: use_bias = norm_layer.func == nn.InstanceNorm2d else: use_bias = norm_layer == nn.InstanceNorm2d kw = 4 padw = 1 sequence = [ nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True) ] nf_mult = 1 nf_mult_prev = 1 for n in range(1, n_layers): nf_mult_prev = nf_mult nf_mult = min(2**n, 8) sequence += [ nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True) ] nf_mult_prev = nf_mult nf_mult = min(2**n_layers, 8) sequence += [ nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True) ] sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)] if use_sigmoid: sequence += [nn.Sigmoid()] self.model = nn.Sequential(*sequence) def forward(self, input): return self.model(input)pred_real = netD(real)loss_D_real = self.criterionGAN(pred_real, True) GANLoss的备注使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.call(input)，在call函数中，主要调用的是 layer.forward(x)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。 13.PatchGAN的感受野论文使用的是70X70 PatchGANPatchGAN:paper:Image-to-Image Translation with Conditional Adversarial Networkshttps://arxiv.org/abs/1611.07004 123456789101112131415161718192021222324252627感受野的计算规则对于第m层,m=0,1,2,...,N. hm表示第m层应该有的视野,假设mN=1km, sm, pm,表示第m-1层到第m层的conv的kernel第一层对于第0层的感受野h1 = 1, (h0-k1)/s1+1=h1第二层对于第0层的感受野h2 = 1(h1-k2)/s2+1=h2(h0-k1)/s1+1=h1依次类推反之 https://fomoro.com/tools/receptive-fields/#def f(output_size, ksize, stride): return (output_size - 1) * stride + ksizelast_layer = f(output_size=1, ksize=4, stride=1)# Receptive field: 4fourth_layer = f(output_size=last_layer, ksize=4, stride=1)# Receptive field: 7third_layer = f(output_size=fourth_layer, ksize=4, stride=2)# Receptive field: 16second_layer = f(output_size=third_layer, ksize=4, stride=2)# Receptive field: 34first_layer = f(output_size=second_layer, ksize=4, stride=2)# Receptive field: 70print(first_layer) 14.torch.tensor.clone()clone()梯度受影响,clone之后的新的tensor的梯度也会影响到原tensor,但是新tensor本身是没有梯度的.clone之后的新tensor的改变不会影响原有的tensor应该这么理解,clone也是计算图中的一个操作,这样的话就可以解释通了. 1234567891011121314151617import torchinput = torch.ones(3,3,3,3)input.requires_grad = Trueinput2 = input.clone()print(input2.requires_grad)y = input.sum()y.backward()print(input.grad)# 1,1,1...print(input2.grad)# Noney = input2.sum()y.backward()print(input.grad)# 2,2,2...print(input2.grad)# None 1234567891011121314151617181920212223import torchinput = torch.ones(3,3)input.requires_grad = Trueinput2 = input.clone()input2[1,1] = 6print(input2)print(input)tensor([[ 2., 2., 2.], [ 2., 6., 2.], [ 2., 2., 2.]])tensor([[ 2., 2., 2.], [ 2., 2., 2.], [ 2., 2., 2.]])input2[1,1] = 6y = (input2*input2).sum()y.backward()print(input.grad)print(input2.grad)tensor([[ 8., 8., 8.], [ 8., 4., 8.], [ 8., 8., 8.]])None 12print(input2.grad_fn)&lt;CopySlices object at 0x7fe89dc841d0&gt; clone的用法tensor保留梯度的交换 123456789tmp = tensor1.clone()tensor2 = tmptensor1 = tensor3# ortensor1, tensor2 = tensor3, tensor1.clone()# ortmp = self.images[random_id].clone()self.images[random_id] = imagereturn_images.append(tmp) 15.from XX import这里还有一些不太对的地方 12345from .base_model import BaseModel # 同一个文件夹from util.image_pool import # 父级文件夹# 建议from .base_model import BaseModel # 同一个文件夹from ..util.image_pool import # 父级文件夹 16.register_bufferregister_bufferself.register_buffer可以将tensor注册成buffer，在forward中使用self.mybuffer, 而不是self.mybuffer_tmp.定义Parameter和buffer都只需要传入Tensor即可。也不需要将其转成gpu。这是因为，当网络进行.cuda()时候，会自动将里面的层的参数,buffer等转换成相应的GPU上。网络存储时也会将buffer存下，当网络load模型时，会将存储的模型的buffer也进行赋值。buffer的更新在forward中，optim.step只能更新nn.Parameter类型的参数。用法self.register_buffer(‘running_mean’, torch.zeros(num_features)) 17. itertools无限迭代器itertools，用于创建高效迭代器的函数,itertools.chain 连接多个列表或者迭代器。将多个网络写在一起,使用一个优化器 1234self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999)) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 自然数无限迭代器# itertools.count&gt;&gt;&gt; import itertools&gt;&gt;&gt; natuals = itertools.count(1)&gt;&gt;&gt; for n in natuals:... print n...123...# 序列无限重复&gt;&gt;&gt; import itertools&gt;&gt;&gt; cs = itertools.cycle('ABC') # 注意字符串也是序列的一种&gt;&gt;&gt; for c in cs:... print c...'A''B''C''A''B''C'...# 单元素无限重复&gt;&gt;&gt; ns = itertools.repeat('A', 10)&gt;&gt;&gt; for n in ns:... print n...打印10次'A'# 无限迭代器中截取有限序列&gt;&gt;&gt; natuals = itertools.count(1)&gt;&gt;&gt; ns = itertools.takewhile(lambda x: x &lt;= 10, natuals)&gt;&gt;&gt; for n in ns:... print n...打印出1到10# 迭代对象的串联for c in itertools.chain('ABC', 'XYZ'): print c# 迭代效果：'A' 'B' 'C' 'X' 'Y' 'Z'# 迭代器中相邻的重复元素挑出来放在一起&gt;&gt;&gt; for key, group in itertools.groupby('AAABBBCCAAA'):... print key, list(group) # 为什么这里要用list()函数呢？...A ['A', 'A', 'A']B ['B', 'B', 'B']C ['C', 'C']A ['A', 'A', 'A']# imap, ifilter&gt;&gt;&gt; for x in itertools.imap(lambda x, y: x * y, [10, 20, 30], itertools.count(1)):... print x...104090 18.visdom123self.vis = visdom.Visdom(server=opt.display_server, port=opt.display_port, env=opt.display_env, raise_exceptions=True, use_incoming_socket=False)# env根据_自动分层。e.g. cycle_gan--&gt;cycle,cycle_gan 19.三引号12345678910三引号的作用&gt;&gt;&gt; str1 = """List of name:... Hua Li... Chao Deng... &#123;&#125;... """.format('hhh')&gt;&gt;&gt; print(str1)List of name:Hua LiChao Deng 20.异常1234567891011121314151617181920212223242526272829# 1.触发异常def mye( level ): if level &lt; 1: raise Exception("Invalid level!") # 触发异常后，后面的代码就不会再执行try: mye(0) # 触发异常except Exception as err: print(1,err)else: print(2)1 Invalid level!# 2.自定义异常class MyException(Exception): def __init__(self,message): Exception.__init__(self) self.message=message print('This is MyException')a=7if a&lt;10: try: raise MyException("my excepition is raised ") except MyException as e: print('*****************') print(e.message) This is MyExceptionmy excepition is raised 21.自定义类的iter12345678910111213141516171819202122232425# 自定义类的iterclass cl1(): def __init__(self): self.N = 10 def __iter__(self): for i in range(10): print(i) if i &lt; 5: yield i else: breakcc = cl1()for i in cc: print('hhh',i)0hhh 01hhh 12hhh 23hhh 34hhh 45]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>cycleGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow]]></title>
    <url>%2F2018%2F11%2F05%2Ftensorflow%2F</url>
    <content type="text"><![CDATA[这是一篇关于tensorflow的博客，这里面很多东西都是很杂碎的，不在此做处理，等积累的多了，理解才能正确。 TensorFlow入门教程 1.TensorFlow深度学习应用实践评价不好 TensorFlow：实战Google深度学习框架（第2版）8.6分，可以用来实践 Tensorflow：实战Google深度学习框架8.4分 4.莫烦的tensorlfow教程https://github.com/MorvanZhou适合实践 5.某个网友的自己实现的教程https://www.jianshu.com/p/27a2fb320934https://github.com/zhaozhengcoder/Machine-Learning/tree/master/tensorflow_tutorials 6.官网APIhttps://tensorflow.google.cn/api_docs/python/tf 7.深度学习之TensorFlow入门、原理与进阶实战7.6分22章，内容更加详实，偏向理论，可以用来只看不实践 8.TensorFlow实战7.3分适合看看，内容不深，实践性不强，理论也很浅在github上也没有代码 不应该总是要求全部，所以应该这样的顺序来学习先学：TensorFlow：实战Google深度学习框架（第2版）再学：莫烦：https://github.com/MorvanZhou+网页的教程基本就可以了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758print(a.graph == tf.get_default_graph())tf.get_variable() name 必须双引号name的作用 https://blog.csdn.net/xiaohuihui1994/article/details/81022043可以理解成sess需要指定，不能自动加入.run,.eval能执行的两种方式with tf.Session(graph=g1) as sess: tf.global_variables_initializer().run()或者sess = tf.Session()tf.global_variables_initializer().run(session=sess)或者sess = tf.InteractiveSession() # 会自动注册为默认会话result.eval()或者sess = tf.Session()with sess.as_default(): result.eval()####初始化init = tf.global_variables_initializer()w1.initializer『TensorFlow』使用集合collection控制variableshttps://www.cnblogs.com/hellcat/p/9006904.htmlcollectionimport tensorflow as tfg1 = tf.Graph()with g1.as_default(): v = tf.get_variable("v", [1], initializer = tf.zeros_initializer()) # 设置初始值为0 gv= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES # gv = tf.global_variables() for var in gv: print(var)现在有了几个概念需要理清楚：计算图： 不同计算图中的变量是独立的collection： 不同类型的variable放在不同的collection中，主要是tf.GraphKeys.GLOBAL_VARIABLES和tf.GraphKeys.TRAINABLE_VARIABLES会话： 会话需要与计算图相连接，完成相应计算图的执行，一个会话对应一个计算图及其执行结果tf.add_to_collectionhttps://www.jianshu.com/p/6612f368e8f4这样就不需要传入weighs和biases，这里的reuse实现了定义和使用的一体化，不需要专门对weights定义和调用。def inference(input_tensor, reuse=False): with tf.variable_scope('layer1', reuse=reuse): weights = tf.get_variable("weights") biases = tf.get_variable("biases") with tf.variable_scope('layer2', reuse=reuse): weights = tf.get_variable("weights") biases = tf.get_variable("biases")TFRecord数据格式https://blog.csdn.net/u012759136/article/details/52232266]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-cuda]]></title>
    <url>%2F2018%2F11%2F05%2Fpytorch-cuda%2F</url>
    <content type="text"><![CDATA[关于pytorch中模型的多GPU 1.cudnn.benchmark = True12from torch.backends import cudnncudnn.benchmark = True 而且大家都说这样可以增加程序的运行效率。那到底有没有这样的效果，或者什么情况下应该这样做呢？总的来说，大部分情况下，设置这个 flag 可以让内置的 cuDNN 的 auto-tuner 自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。 一般来讲，应该遵循以下准则： 如果网络的输入数据维度或类型上变化不大，设置 torch.backends.cudnn.benchmark = true 可以增加运行效率； 如果网络的输入数据在每次 iteration 都变化的话，会导致 cnDNN 每次都会去寻找一遍最优配置，这样反而会降低运行效率。 这下就清晰明了很多了。 其实看完这段还是很蒙蔽，不知道具体什么情况下使用，暂且算加速过程好了。 2. nn.DataParallel12345from torch import nnmodel2 = nn.DataParallel(model1)model2.cuda()` 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from torch import nnimport torch as tclass Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(" In Model: input size", input.size(), "output size", output.size()) return output----model1 = Model(3,4)print(model1)for var in model1.parameters(): print(var)Model( (fc): Linear(in_features=3, out_features=4, bias=True))Parameter containing:tensor([[ 0.1964, 0.4389, -0.2216], [-0.1046, -0.2055, -0.5383], [ 0.0673, 0.0949, 0.5205], [ 0.5473, -0.3700, -0.4179]])Parameter containing:tensor([ 0.2416, 0.4188, -0.0096, 0.1569])----model2 = nn.DataParallel(model1)print(model2)for var in model2.parameters(): print(var)DataParallel( (module): Model( (fc): Linear(in_features=3, out_features=4, bias=True) ))Parameter containing:tensor([[ 0.1964, 0.4389, -0.2216], [-0.1046, -0.2055, -0.5383], [ 0.0673, 0.0949, 0.5205], [ 0.5473, -0.3700, -0.4179]], device='cuda:0')Parameter containing:tensor([ 0.2416, 0.4188, -0.0096, 0.1569], device='cuda:0')----model2.cuda()print(model2)for var in model2.parameters(): print(var)DataParallel( (module): Model( (fc): Linear(in_features=3, out_features=4, bias=True) ))Parameter containing:tensor([[ 0.1964, 0.4389, -0.2216], [-0.1046, -0.2055, -0.5383], [ 0.0673, 0.0949, 0.5205], [ 0.5473, -0.3700, -0.4179]], device='cuda:0')Parameter containing:tensor([ 0.2416, 0.4188, -0.0096, 0.1569], device='cuda:0')]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beyond-part-models]]></title>
    <url>%2F2018%2F11%2F05%2Fbeyond-part-models%2F</url>
    <content type="text"><![CDATA[PCB:https://github.com/huanghoujing/beyond-part-models1234567891011121314151617181920212223242526272829303132333435363738.├── bpm│ ├── dataset│ │ ├── Dataset.py│ │ ├── __init__.py│ │ ├── Prefetcher.py│ │ ├── PreProcessImage.py│ │ ├── TestSet.py│ │ └── TrainSet.py│ ├── __init__.py│ ├── model│ │ ├── __init__.py│ │ ├── PCBModel.py│ │ └── resnet.py│ └── utils│ ├── dataset_utils.py│ ├── distance.py│ ├── __init__.py│ ├── metric.py│ ├── re_ranking.py│ ├── utils.py│ └── visualization.py├── example_rank_lists_on_Market1501│└── script ├── dataset │ ├── combine_trainval_sets.py │ ├── mapping_im_names_duke.py │ ├── mapping_im_names_market1501.py │ ├── transform_cuhk03.py │ ├── transform_duke.py │ └── transform_market1501.py └── experiment ├── train_pcb.py └── visualize_rank_list.pybpm:正式的模型训练script:主要用于数据的预处理和训练的对外借口 以market1501为例 数据的预处理第一个表示id，第二个表示cam，第三个表示同id同cam的第几张图片，对zip_file中的*.jpg移动到save_dir+images中，并且重命名，将所有图片重命名保存到save_dir+images 分为 —-trainval name+label |—-train name+label |—-val name+mask |—-query 0 |—-gallery 1 —-test name+mask |—-query 0 |—-multi-query 2 |—-gallery 1 保存到save_dir, ‘partitions.pkl’中 trainval提取val的时候，val中的id只提取100个id，并且会自动跳过只在一个cam下的id。123456789partitions: dict &#123;'trainval_im_names': train_test_split['trainval_im_names'], 'trainval_ids2labels': trainval_ids2labels, 'train_im_names': train_im_names, 'train_ids2labels': train_ids2labels, 'val_im_names': val_im_names, 'val_marks': val_marks, 'test_im_names': test_im_names, 'test_marks': test_marks&#125; 数据集对于数据集是怎么加载、转化的，还是没有头绪，写法之前没有遇到过，这一部分有待提高 模型模型是一致的 模型训练test好像是re-ranking了val没有re-ranking lr: 0.1 0.01factor: 0.1epochs: 60staircase_decay_at_epochs: 41]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-2.0]]></title>
    <url>%2F2018%2F11%2F05%2Fpython-2.0%2F</url>
    <content type="text"><![CDATA[这也是关于python的一些记录，与其他python不矛盾，也不一定相互独立。 1.python+matlabpython和matlab关于.mat数据的交换123scipy.io.loadmat(file_name, mdict=None, appendmat=True, **kwargs)scipy.io.savemat(file_name, mdict, appendmat=True, format='5', long_field_names=False, do_compression=False, oned_as='row') 2.python与文件IO主要参考：python cookbook 2.1 文本.txt1234567891011121314151617181920212223242526# t:rt模式下，python在读取文本时会自动把\r\n转换成\n.# read# read the entire file as a single stringwith open('some.txt', 'rt') as f: data = f.read()# read lineswith open('some.txt', 'rt') as f: for line in f:#writewith open('some.txt', 'wt') as f: f.write(text1)with open('some.txt', 'wt') as f: print(line1, file=f)# 换行模式，默认情况下，python会自动识别，或者传入newline,# newline可以取的值有None, \n, \r, ‘\r\n'，用于区分换行符，但是这个参数只对文本模式有效；with open('some.txt', 'rt', newline='') as f: pass# 编码错误问题 errors: replace, ignorewith open('sample.txt', 'rt', encoding='ascii', errors='replace') as f: pass 2.2 print的分隔符与行尾符1234567# print seq endprint('ACME', 50, seq=',', end='!!\n')ACME,50!!row = ('ACME', 50)print(*row, seq=',', end='!!\n')ACME,50!! 2.3 二进制数据1234567# 二进制数据，比如图片、声音等with open('some.bin', 'rb') as f: data = f.read()with open('some.bin', 'wb') as f: f.write(b'hello') 2.4 模拟普通文件123456789101112131415# 模拟文本文件import ios = io.StringIO()# s = io.StringIO('hello world\n')s.write('hello world\n')print('this is a test', file=s)s.getvalue()'hello world\nthis is a test's.read(4)s.read()# 模拟二进制文件s = io.BytesIO()s.write(b'binary data')s.getvalue()b'binary data' 2.5 压缩文件12345678910111213141516# gzip,bz2import gzipwith gzip.open('some.gz', 'rt') as f: text = f.read()with gzip.open('some.gz', 'wt') as f: f.write(text)-------------------------------------import bz2with bz2.open('some.bz2', 'rt') as f: text = f.read()with bz2.open('some.bz2', 'wt') as f: f.write(text)------------------------------------from zipfile import ZipFilewith ZipFile(zip_file) as z: z.extractall(path=save_dir) 2.6 csv1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# csv# 其实namedtuple继承自OrderedDict有序字典# read------import csvfrom collections import namedtuplewith open('stock.csv') as f: # 第一种：row是列表，访问：row[0] f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: pass # 第二种：命名元组，访问：Row.Symbol f_csv = csv.reader(f) headers = next(f_csv) # headers = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) ] Row = namedtuple('Row', headers) for r in f_csv: row = Row(*r) pass # 第三种：字典序列，访问：row['Sysbol'] f_csv = csv.DictReader(f) for row in f_csv: pass# write-------# 第一种：类表headers = ['Symbol','Price','Date','Time','Change','Volume']rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ]with open('stock.csv', 'w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows)# 第二种：字典headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']rows = [&#123;'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800&#125;, &#123;'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500&#125;, &#123;'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000&#125;, ]with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows)# 以tab分割-----with open('stock.tsv') as f: f_tsv = csv.reader(f, delimiter='\t') for row in f_tsv: pass# 类型转换---------# 第一种：tuplecol_types = [str, float, str, str, float, int]with open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: # Apply conversions to the row items row = tuple(convert(value) for convert, value in zip(col_types, row))# 第二种：dictprint('Reading as dicts with type conversion')field_types = [ ('Price', float), ('Change', float), ('Volume', int) ]with open('stocks.csv') as f: for row in csv.DictReader(f): row.update((key, conversion(row[key])) for key, conversion in field_types) print(row)# 高级操作--------# pandas.read_csv() 2.7 json12345678910111213141516# json# write &amp; read--------------# 第一种 dict----strimport jsondata = &#123; 'name' : 'ACME','shares' : 100, 'price' : 542.23 &#125;json_str = json.dumps(data)daa = json.loads(json_str)# 第二种 dict----filewith open('data.json', 'w') as f: json.dump(data, f)with open('data.json', 'r') as f: data = json.load(f) 3. sys.pathsys.path:动态地改变Python搜索路径123456import syssys.path.append(’引用模块的地址')#或者import syssys.path.insert(0, '引用模块的地址') 4. os.path12345678910111213os.path.abspath(path) #返回绝对路径os.path.basename(path) #返回文件名os.path.exists(path) #路径存在则返回True,路径损坏返回Falseos.path.dirname(path) #返回文件路径os.path.expanduser(path) #把path中包含的"~"和"~user"转换成用户目录os.path.isabs(path) #判断是否为绝对路径os.path.isfile(path) #判断路径是否为文件os.path.isdir(path) #判断路径是否为目录os.path.join(path1[, path2[, ...]]) #把目录和文件名合成一个路径os.path.samefile(path1, path2) #判断目录或文件是否相同os.path.split(path) #把路径分割成dirname和basename，返回一个元组os.path.splitext(path) #分割路径，返回路径名和文件扩展名的元组os.path.walk(path, visit, arg) 5. glob.glob12345import globlistglob = []listglob = glob.glob(r"/home/xxx/picture/*.png")listglob.sort()print(listglob) 6. argparse参考链接：http://lib.csdn.net/article/python/49052https://blog.csdn.net/u010895119/article/details/78960740https://www.jianshu.com/p/a50aead61319https://blog.csdn.net/guoyajie1990/article/details/76739977 不是很适合交互式调试 命令行参数分为位置参数和选项参数 位置参数12345678910import argparseparser = argparse.ArgumentParser(description="say something about this application !!")parser.add_argument('name', type=int, help="i can tell you how to set a name argument")result = parser.parse_args()print(result.name)$python main.py taylortaylor 选项参数12345678910import argparseparser = argparse.ArgumentParser(description="say something about this application !!")parser.add_argument("-a","--age", help="this is an optional argument")result = parser.parse_args()print(result.age)$python main.py --age 888888$python main.py --age=888888 特殊的选项参数起着开关的作用12345678import argparseparser = argparse.ArgumentParser(description="say something about this application !!")parser.add_argument("-a", "--age", help="this is an optional argument", action="store_true")result = parser.parse_args()print(result.age)$python main.py -aTrue 指定选项123456import argparseparser = argparse.ArgumentParser(description="say something about this application !!")parser.add_argument("-a", "--age", help="this is an optional argument", type=int, choices=[0, 1, 2])result = parser.parse_args()print(result.age)1 nargsnargs=N(N是int类型)，nargs=’*’, nargs=’?’某个参数接受的值，nargs定义了值的个数，加了nargs后，接受的值会变成一个list，’?’代表一个值，’*’代表一个或多个值，举例：123456import argparseparser = argparse.ArgumentParser()parser.add_argument('capital', default='hello', nargs=1, help='将首字母大写')args = parser.parse_args()print(args)print(args.capital) 计数12parser.add_argument("-v", "--verbosity", action="count", default=0, help="increase output verbosity") 7. defaultdict遍历生成字典12345678910# 第一种counts = &#123;&#125;for k in strings: counts[k]=counts.setdault(k, 0)+1# 第二种from collections import defaultdictdd = defaultdict(int)for k in strings: counts[k]=counts[k]+1 8. shuti1234567891011shutil.copy 复制文件shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉shutil.move( src, dst) 移动文件或重命名shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间shutil.copy( src, dst) 复制一个文件到一个文件或一个目录shutil.copy2( src, dst) 在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，类似于cp –p的东西shutil.copy2( src, dst) 如果两个位置的文件系统是一样的话相当于是rename操作，只是改名；如果是不在相同的文件系统的话就是做move操作shutil.copytree( olddir, newdir, True/Flase)把olddir拷贝一份newdir，如果第3个参数是True，则复制目录时将保持文件夹下的符号连接，如果第3个参数是False，则将在复制的目录下生成物理副本来替代符号连接shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容 9. pickle12picklepython codebook 注释：序列化对象，将对象obj保存到文件file中去。参数protocol是序列化模式，默认是0（ASCII协议，表示以文本的形式进行序列化），protocol的值还可以是1和2（1和2表示以二进制的形式进行序列化。其中，1是老式的二进制协议；2是新二进制协议）。file表示保存到的类文件对象，file必须有write()接口，file可以是一个以’w’打开的文件或者是一个StringIO对象，也可以是任何可以实现write()接口的对象。 10.重定向1234567891011121314151617181920212223242526272829303132333435363738394041424344from __future__ import absolute_importimport osimport sysclass Logger(object): def __init__(self, fpath=None): self.console = sys.stdout self.file = None if fpath is not None: mkdir_if_missing(os.path.dirname(fpath)) self.file = open(fpath, 'w') def __del__(self): self.close() def __enter__(self): pass def __exit__(self, *args): self.close() def write(self, msg): self.console.write(msg) if self.file is not None: self.file.write(msg) def flush(self): self.console.flush() if self.file is not None: self.file.flush() os.fsync(self.file.fileno()) def close(self): self.console.close() if self.file is not None: self.file.close()def mkdir_if_missing(path): if not os.path.exists(path): os.makedirs(path)if __name__ == '__main__': sys.stdout = Logger(fpath='./log.txt') print('2222222222') . 其他12# id2labelstrainval_ids2labels = dict(zip(trainval_ids, range(len(trainval_ids))))]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-optim]]></title>
    <url>%2F2018%2F11%2F05%2Fpytorch-optim%2F</url>
    <content type="text"><![CDATA[pytorch-optim optim的学习率设置问题1.不同的学习率1234567891011121314151617# 第一种optimizer = t.optim.Adam(model.parameters(), lr = 0.1)# 第二种optim_group = [&#123;'params':model.net1.parameters(),'lr':0.4&#125;, &#123;'params':model.net2.parameters(),'lr':0.1&#125;]optimizer = t.optim.Adam(optim_group,lr=0.04)# 第三种ignored_params = list(map(id, model.model.fc.parameters() )) + list(map(id, model.classifier.parameters() ))base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())optimizer_ft = optim.SGD([ &#123;'params': base_params, 'lr': 0.01&#125;, &#123;'params': model.model.fc.parameters(), 'lr': 0.1&#125;, &#123;'params': model.classifier.parameters(), 'lr': 0.1&#125; ], weight_decay=5e-4, momentum=0.9, nesterov=True) 2. 学习率衰减12345678exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)def train_model(model, criterion, optimizer, scheduler, num_epochs=25): for epoch in range(num_epochs): scheduler.step() for data in dataloaders[phase]: optimizer.zero_grad()]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-init]]></title>
    <url>%2F2018%2F11%2F05%2Fpytorch-init%2F</url>
    <content type="text"><![CDATA[pytorch-init pytorch模型的初始化pytorch模型的初始化的常用方法。 1.apply+typeapply可以理解成从children开始遍历可以用于init，可以用于model定义之后，与type配合。 Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import torch as tfrom torch import nn# define modelclass Net(nn.Module): def __init__(self): super(Net,self).__init__() self.pre = nn.Sequential(nn.Linear(2,2), nn.Conv2d(3,3,3)) self.two = nn.Sequential(nn.Linear(3,3)) self.apply(init_weights) def forward(self,x): passdef init_weights(m): print(m) print(type(m)) print(nn.Linear) print(m.__class__) print(m.__class__.__name__) if type(m) == nn.Linear: m.weight.data.fill_(0.0) print(m.weight.data) print("_______________________")net2 = Net()Linear(in_features=2, out_features=2, bias=True)&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;Lineartensor([[ 0., 0.], [ 0., 0.]])_______________________Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))&lt;class 'torch.nn.modules.conv.Conv2d'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.conv.Conv2d'&gt;Conv2d_______________________Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)))&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;Sequential_______________________Linear(in_features=3, out_features=3, bias=True)&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;Lineartensor([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]])_______________________Sequential( (0): Linear(in_features=3, out_features=3, bias=True))&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;Sequential_______________________Net( (pre): Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) ) (two): Sequential( (0): Linear(in_features=3, out_features=3, bias=True) ))&lt;class '__main__.Net'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class '__main__.Net'&gt;Net_______________________ 2.apply+m.class.nameweights_init_kaiming还要一种初始化函数1234567891011121314151617def weights_init_kaiming(m): classname = m.__class__.__name__ # print(classname) if classname.find('Conv') != -1: init.kaiming_normal(m.weight.data, a=0, mode='fan_in') elif classname.find('Linear') != -1: init.kaiming_normal(m.weight.data, a=0, mode='fan_out') init.constant(m.bias.data, 0.0) elif classname.find('BatchNorm1d') != -1: init.normal(m.weight.data, 1.0, 0.02) init.constant(m.bias.data, 0.0)def weights_init_classifier(m): classname = m.__class__.__name__ if classname.find('Linear') != -1: init.normal(m.weight.data, std=0.001) init.constant(m.bias.data, 0.0)]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cuda安装教程]]></title>
    <url>%2F2018%2F11%2F05%2Fcuda%2F</url>
    <content type="text"><![CDATA[安装教程参考链接https://www.jianshu.com/p/35c7fde85968]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_reptilian]]></title>
    <url>%2F2018%2F10%2F05%2Fpython-reptilian%2F</url>
    <content type="text"><![CDATA[前言这是第一次爬虫，以理论为主，以实现为辅。因为是到处看的，所以不是很有逻辑性。参考链接：python爬虫原理Python爬虫的两套解析方法和四种爬虫实现爬虫基本原理 一、工具 两个解析库：BeautifulSoup, lxml 两个请求库：urllib, requests 法法 二、爬虫流程用户获取网络数据的方式方式1:浏览器提交请求—-&gt;下载网页代码—-&gt;解析成页面方式2:模拟浏览器发送请求(获取网页代码)—-&gt;提取有用数据—-&gt;存放在数据库或者文件中爬虫就是指方式2. 1.发起请求使用http库向目标站点发送请求，即发送一个Request。Request包含：请求头，请求体等。Request模块缺点：不能执行JS和CSS代码。 2.获取响应内容服务器正常响应，得到一个Response。Response包含：html，json，图片，视频等。 3.解析内容解析html数据：正则表达式(RE模块)，第三方解析库如BeautifulSoup,pyquery等解析json数据：json模块解析二进制数据：以wb形式写入文件 4.保存数据数据库（MySQL, Mongdb, Redis)文件 三、Request&amp;Response1.Request1.1.请求方式常见的有：GET/POST 1.2.请求的URLurl是全球容易资源定位符，用来丁意思互联网上一个唯一的资源，例如：一张图片、一个文件、一段视频。 1.3.请求头User-agen：访问的浏览器请求头没有user-agent客户端配置，会被当成非法用户hostcookies：cookie用来保存登录信息Referrer：访问源至哪里来 1.4.请求体get：请求体没有内容post：请求体是format data 2.Response2.1 响应状态码200：代表成功301：代表调转404：文件不存在403：无权限访问502：服务器错误 2.2 响应头Set-Cookie:BDSVRTM=0; path=/：可能有多个，是来告诉浏览器，把cookie保存下来Content-Location：服务端响应头中包含Location返回浏览器之后，浏览器就会重新访问另一个页面 2.3preview网页源代码，包括：Json数据、html、图片、二进制数据 接下来开始尝试写一些基本的爬虫代码，并做记录`python 发起请求，并获取请求内容from urllib import requestresp = request.urlopen(‘https://movie.douban.com/nowplaying/hangzhou/&#39;) # http.client.HTTPResponsehtml_data = resp.read().decode(‘utf-8’) # str 这里的print是最好看的 解析内容from bs4 import BeautifulSoup as bssoup = bs(html_data, ‘html.parser’) # bs4.BeautifulSoupnowplaying_movie = soup.find_all(‘div’, id=’nowplaying’) # bs4.element.ResultSet list的形式，可以暂时看成是多个组成的list，需要先[0]的进行访问。tmp = nowplaying_movie[0] # bs4.element.Tagnowplaying_movie_list = nowplaying_movie[0].find_all(‘li’, class_=’list-item’) # bs4.element.ResultSet list形式， bs4.element.Tagnowplaying_list = [] # 此时就是直接获取数据了，find_all是对相应片段的截取for item in nowplaying_movie_list: nowplaying_dict = {} nowplaying_dict[‘id’] = item[‘data-subject’] for tag_img_item in item.find_all(‘img’): nowplaying_dict[‘name’] = tag_img_item[‘alt’] nowplaying_list.append(nowplaying_dict) requrl = ‘https://movie.douban.com/subject/&#39;+nowplaying_list[0][&#39;id&#39;] + ‘/comments’ +’?’ +’start=0’ + ‘&amp;limit=20’ 三句一体resp = request.urlopen(requrl)html_data = resp.read().decode(‘utf-8’)soup = bs(html_data, ‘html.parser’) comment_div_lists[0].find_all(‘span’, class_=”short”)[0].string # .string 可以暂时理解成中间的字符串]]></content>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2F2018%2F10%2F04%2Fgit%2F</url>
    <content type="text"><![CDATA[learning git 自己之前已经学过一次git了，但是最近在用的时候，仍然感觉不顺手，所以今天趁这个机会，再学一遍，这一次，以命令为主，以原理为辅。 初始化仓库1$ git init 文件到Git仓库12$ git add readme.txt$ git commit -m "wrote a readme file" 查看状态1git status 查看修改内容1git diff readme.txt 查看提交日志1git log 版本回到过去和将来12345678910# 回到过去git reset --hard HEAD^# HEAD~100# 此时git log已经没有了最新版本的提交信息# 回到将来git reset --hard 1094a# 或者git reflog # 命令历史git reset --hard 1094a 撤销修改12345# 当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时git checkout -- readme.txt# 当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，git reset HEAD readme.txtgit checkout -- readme.txt 删除文件12git rm test.txtgit commit -m "remove" 远程仓库克隆1git clone git@github.com:michaelliao/gitskills.git 添加远程仓库123456# git://不支持pushgit remote add origin git://github.com/TJJTJJTJJ/ticgit.git# git@只是pushgit remote add origin git@github.com:TJJTJJTJJ/ticgit.git# removegit remote remove origin 查看远程版本1git remote -v 提交到远程1git push -u origin master 获取远程仓库内容12git fetch origingit merge origin/master 提交远程仓库1git push origin master 暂时到这里，剩下的分支，自己暂时还不会用到，等用到了再去学就可以了。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-chapter10-ImageCaption]]></title>
    <url>%2F2018%2F09%2F30%2Fpytorch-chapter10-ImageCaption%2F</url>
    <content type="text"><![CDATA[前言本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。这是我的代码大神链接：https://github.com/anishathalye/neural-style这是论文作者写的 问题以及思考这一次感觉写起来很顺利，数据的处理+基本模型的走读基本只用了两天，剩下的两天主要是耗在了beam_searching上，原理的解析和代码的思考。现在记录一下这次走读的过程中学习到的东西，如果是和之前的记录有联系，那么则尽量记在一起。 局部反向传播管理部分参考第八章，基本来自官网文档一共是四种 @torch.no_grad() with torch.no_grad(): torch.set_grad_enabled(bool) with torch.set_grad_enabled(False): 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# 第一种：固定上下文管理器，torch.no_grad()和torch.enable_grad()又分为@torch.no_grad()和with torch.no_grad()x = torch.tensor([1], requires_grad=True)with torch.no_grad(): print(x.requires_grad) y = x*2 print(y.requires_grad) TrueFalsey.requires_gradFalse# 以上说明了上下文管理器内和外是一致的# 下面说明上下文管理器的作用域只在局部有效with torch.no_grad(): print(x.requires_grad) z = x*2 print(z.requires_grad) with torch.enable_grad(): print(x.requires_grad) y = x*2 print(y.requires_grad)TrueFalseTrueTruewith torch.enable_grad(): print(x.requires_grad) z = x*2 print(z.requires_grad) with torch.no_grad(): print(x.requires_grad) y = x*2 print(y.requires_grad)TrueTrueTrueFalse&gt;&gt;&gt; @torch.no_grad()... def dddd():... x = torch.tensor([2.2],requires_grad=True)... y = 2*x... print(y.requires_grad)... &gt;&gt;&gt; dddd()False# 第二种：条件的上下文管理器 torch.set_grad_enabled(bool)又分为with torch.set_grad_enabled(bool)和 torch.set_grad_enabled(bool)with torch.set_grad_enabled(False): print(x.requires_grad) y = x*2 print(y.requires_grad)TrueFalse&gt;&gt;&gt; torch.set_grad_enabled(False)&gt;&gt;&gt; y = x * 2&gt;&gt;&gt; y.requires_gradFalse# 在测试@torch.enable_grad()的时候没有成功，问题应该是版本问题，0.4.0的版本就不行，但是0.4.1的版本就可以了 预训练模型的修改备注：感觉这一块应该是很条理才对，但是没有找到类似的说明只能等以后见得多了，再做补充，网上有一些对特定模型的修改，但是都不全面，也没有具体说明各个方法的优劣。应该是这样的，层必须和forward对应，参数的加载可以放在模型定义时，也可以放在模型定义之后。 不修改原模型的forward流程常用于对特定层的修改123456789101112131415161718import torchvision.models as modelsmodel = models.resnet50(pretrained=True)# 只修改最后一层# 第一种fc_features = model.fc.in_featuresmodel.fc = nn.Linear(fc, 9)# 第二种resnet50 = tv.models.resnet50(pretrained=True)del resnet50.fcresnet50.fc = lambda x: x# 如果直接修改out_features是没有用的model.fc.out_features = 9resnet50.fc.weight.shapetorch.Size([1000, 2048])即如果修改某一层，要重新定义这一层 在模型内修改forward流程常用于中间层的增加1234567891011# 需要自己先定义类似的网络，注意定义的名字必须一致和方式需要一致，利用state_dict来更新参数import torchvision.models as modelsresnet50 = models.resnet50(pretrained=True)cnn = CNN(Bottleneck, [3, 4, 6, 3])pretrained_dict = resnet50.state_dict()model_dict = cnn.state_dict()# 选取相同名字参数pretrained_dict = &#123;k: v for k, v in pretrained_dict.items() if k in model_dict&#125;model_dict.update(pretrained_dict)cnn.load_state_dict(model_dict)print(cnn) 在模型外增加forward流程常用与开头或者末尾层的增加1234model.add_module('layer_name',layer)可以理解成self.layer_name = layerx = model.layer_name(x) 取特定模块，利用children()和nn.Sequential()也可以实现特定层的修改这个方法比较啰嗦，不是很推荐，或者不如第一种方法，或者不如最后一种方法1234567891011121314151617model = models.vgg16(pretrained=True)removed = list(model.classifier.children())[:-1]model.classifier = torch.nn.Sequential(*removed)model.add_module('fc', torch.nn.Linear(4096, out_num)) # out_num是你希望输出的数量 # 直接list(model)是不行的，但是list(model.children())就可以list(ResNet34.children())In [23]: for i in ResNet34.children(): ...: print(type(i)) ...: &lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt; 取特定模块利用list和modulelist，可用于对于特定模块的特定操作，可修改forward流程12345678# 第八章的方法 定义新模型， 在模型定义时，加载原模型参数， 修改forward 对于单向的还好，对于有分支的还没有尝试 用了list和modulelist 直接在定义模型的地方取features = list(vgg16(pretrained=True).features)[:23]self.features = nn.ModuleList(features).eval()for ii, model in enumerate(self.features): x = model(x) if ii in &#123;3,8,15,22&#125;: results.append(x) 12345678910111213141516for k,v in resnet34.named_children(): print(k,v)conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)relu ReLU(inplace)maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)layer1 Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) )... tensor.new和fill_和copy_在第九章，创建同类型的tensor用new,保证类型和cuda一致，不保证requires_grad，保证了和源类型一致，不共享内存1234567891011121314import torchx = torch.Tensor([2.2],requires_grad=True).cuda()xtensor([ 3.2000], device='cuda:0')y = x.new([4,5])yy.requires_gradFalsetensor([ 4., 5.], device='cuda:0')z = x.data.new([6,7])z.requires_gradFalseztensor([ 6., 7.], device='cuda:0') 在第十章，创建同类型同样大小同cuda的tensor，用fill_，fill_也保证了类型和cuda一致，保证了和目标类型一致1234567891011121314151617181920212223242526272829303132333435363738In [36]: x = t.Tensor(3,4).cuda()In [37]: xOut[37]: tensor([[ 1.1395e-19, 4.5886e-41, 3.4482e+25, 3.0966e-41], [ 5.7353e-31, 4.5886e-41, -1.2545e+37, 1.3914e+25], [ 2.9680e-31, 4.5886e-41, 5.7344e-31, 4.5886e-41]], device='cuda:0')In [38]: x.fill_(0)Out[38]: tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], device='cuda:0')# 测试requires_grad提示，不能In [43]: x.requires_grad= TrueIn [44]: xOut[44]: tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], device='cuda:0', requires_grad=True)In [45]: x.fill_(1)---------------------------------------------------------------------------RuntimeError Traceback (most recent call last)&lt;ipython-input-45-0c255de765ba&gt; in &lt;module&gt;()----&gt; 1 x.fill_(1)RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.# 强行修改值，则grad_fn也发生了变化。In [46]: x[0]=1In [47]: xOut[47]: tensor([[1., 1., 1., 1.], [0., 0., 0., 0.], [0., 0., 0., 0.]], device='cuda:0', grad_fn=&lt;CopySlices&gt;) 第十章的copy_，类型不变，可以作为计算图进行保留123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 不共享内存In [49]: x = t.Tensor(2,2).fill_(0)In [50]: xOut[50]: tensor([[0., 0.], [0., 0.]])In [51]: y = t.Tensor(1,2).fill_(1)In [52]: yOut[52]: tensor([[1., 1.]])In [53]: x[0].copy_(y[0])Out[53]: tensor([1., 1.])In [54]: xOut[54]: tensor([[1., 1.], [0., 0.]])In [55]: yOut[55]: tensor([[1., 1.]])In [56]: y[0][0]=2In [57]: yOut[57]: tensor([[2., 1.]])In [58]: xOut[58]: tensor([[1., 1.], [0., 0.]])# 类型不变In [60]: y = t.IntTensor(1,2).fill_(1)In [61]: yOut[61]: tensor([[1, 1]], dtype=torch.int32)In [62]: x = t.Tensor(2,2).fill_(0)In [63]: xOut[63]: tensor([[0., 0.], [0., 0.]])In [64]: x[0].copy_(y[0])Out[64]: tensor([1., 1.])In [65]: xOut[65]: tensor([[1., 1.], [0., 0.]])# requires_grad，会作为一个计算图保留In [66]: xOut[66]: tensor([[1., 1.], [0., 0.]])In [67]: x.requires_grad=TrueIn [68]: xOut[68]: tensor([[1., 1.], [0., 0.]], requires_grad=True)In [69]: x[1].copy_(y[0])Out[69]: tensor([1., 1.], grad_fn=&lt;AsStridedBackward&gt;)# cuda，可以保留In [76]: y[0]=2In [77]: yOut[77]: tensor([[2, 2]], dtype=torch.int32)In [78]: x[1].copy_(y[0])Out[78]: tensor([2., 2.], device='cuda:0', grad_fn=&lt;AsStridedBackward&gt;)In [79]: xOut[79]: tensor([[1., 1.], [2., 2.]], device='cuda:0', grad_fn=&lt;CopySlices&gt;) tensor赋值操作 只复制值，不共享内存123456789101112131415161718192021222324252627282930# 第一种 利用tensor 只复制值In [6]: x = t.tensor([3,4])In [7]: xOut[7]: tensor([3, 4])In [8]: y = t.tensor(x)In [9]: yOut[9]: tensor([3, 4])In [10]: x[0] = 1In [11]: yOut[11]: tensor([3, 4])# 第二种 利用切片， 只复制值In [12]: y = t.Tensor(4)In [13]: y[0:2]=xIn [14]: yOut[14]: tensor([1.0000e+00, 4.0000e+00, 1.1395e-19, 4.5886e-41])In [15]: x[0]=6In [16]: yOut[16]: tensor([1.0000e+00, 4.0000e+00, 1.1395e-19, 4.5886e-41]) t.save12345678910111213141516# 单个变量 不保留名字t.save(x, 'a.pth')y = t.load('a.pth') # 这个时候已经和x没有任何关系了# 多个变量 或者保留名字dic = dict(aa=x, bb=y)t.save(dic, 'a.pth')y = t.load('a.pth') # 这个时候已经和dic没有任何关系了，但是aa,bb还保留着y&#123;'aa': tensor([[ 100.0000, 100.0000, 100.0000, 100.0000], [ -0.0000, 0.0000, 0.0000, 0.0000], [ -0.0000, 0.0000, -0.0000, 0.0000]]), 'bb': tensor(1.00000e-11 * [[-0.0000, 0.0000, -0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000], [-3.9650, 0.0000, -0.0000, 0.0000]])&#125; 第十章的诡异装饰器作者在这里实现了batcha_size的拼接的方式。具体的函数闭包可以参考python1234# def create_collate_fn():# def collate_fn():# pass# return collate_fn 来，猜一下这里为什么这么写，函数闭包，根据昨天看的，函数闭包和类函数有的一拼，或者说可以用于创建多个类似的函数，暂时先这么理解，因为还没有太多的用到，在这里的函数闭包是为了实现对作为函数的参数进行传递变量，也就是把函数作为变量传递，这种思想要注意一下。设想几种情况。假设函数h的定义是这样的：1234567891011121314In [1]: def h(x, f): ...: """ ...: Args: ...: x: int ...: f: function ...: """ ...: out = f(x) ...: return outIn [2]: def f(x): ...: return 2*xIn [3]: h(2,f)Out[3]: 4 第一种情况，函数f的所有输入都是h可以给的，那么这时候如上所示，直接定义一个函数，然后把函数名或者其他等于函数的变量传进去就可以。第二种情况，函数f的有一部分变量，需要是外界给的，即f的定义中，引用到了不属于h的输入的变量。就像这样。1234567891011In [4]: def g(i): ...: def f(x): ...: return i*x ...: return f ...: ...:In [5]: ff = g(3)In [6]: h(4,ff)Out[6]: 12 那么这个时候函数闭包就可以很好地实现这种想法。这是暂时对于函数闭包的理解，但我知道这种想法肯定是有问题的。 rnn的pack和padfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequenceli_ = [[3,4,2,1],[3,4,2,1],[3,4,2,1],[3,4,2,0],[3,4,0,0]]ten = t.Tensor(li_).long()pad_variable = tenembedding = nn.Embedding(5,2)pad_embeddings = embedding(pad_variable)lengths = [5,5,4,3]pad_embeddingspad_embeddingstensor([[[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [ 0.5581, 0.7382]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [ 0.5581, 0.7382], [ 0.5581, 0.7382]]])packed_variable = pack_padded_sequence(pad_embeddings, lengths)PackedSequence(data=tensor([[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621], [ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621], [ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621], [ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [ 0.0256, -1.6445], [-0.0939, -0.4070]]), batch_sizes=tensor([ 4, 4, 4, 3, 2]))packed_variable.data.shapetorch.Size([17, 2])rnn = nn.LSTM(2,3)output, hn = rnn(packed_variable)outputPackedSequence(data=tensor([[-0.1698, -0.1311, 0.2030], [-0.0984, -0.0693, 0.1601], [-0.0791, -0.1195, 0.2111], [-0.0175, 0.0069, 0.0978], [-0.2580, -0.1868, 0.3193], [-0.1392, -0.0959, 0.2441], [-0.1221, -0.1489, 0.3270], [-0.0223, 0.0109, 0.1334], [-0.3011, -0.2100, 0.3821], [-0.1544, -0.1061, 0.2877], [-0.1452, -0.1551, 0.3886], [-0.0232, 0.0129, 0.1460], [-0.3222, -0.2195, 0.4168], [-0.1593, -0.1098, 0.3109], [-0.1575, -0.1556, 0.4222], [-0.3325, -0.2233, 0.4370], [-0.1603, -0.1111, 0.3235]]), batch_sizes=tensor([ 4, 4, 4, 3, 2]))hn[1].shapetorch.Size([1, 4, 3])pad_packed_sequence(packed_variable) (tensor([[[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [ 0.0000, 0.0000]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [ 0.0000, 0.0000], [ 0.0000, 0.0000]]]), tensor([ 5, 5, 4, 3])) 12345678910111213141516# embedding_dim=3, seq_len=4,3 batch_size=2 即把两句话a,b作为一个batch,空余补0from torch.nn.utils.rnn import pad_sequencea = t.ones(4, 3)b = t.ones(3,3)pad_sequence([a,b])tensor([[[ 1., 1., 1.], [ 1., 1., 1.]], [[ 1., 1., 1.], [ 1., 1., 1.]], [[ 1., 1., 1.], [ 1., 1., 1.]], [[ 1., 1., 1.], [ 0., 0., 0.]]]) 123456# a,b两句话，分别有3,2个词，batch_size=2, 共有3个batch_size，大小分别是2,2,1from torch.nn.utils.rnn import pad_sequencea = t.tensor([1,2,3])b = t.tensor([4,5])pack_sequence([a,b])PackedSequence(data=tensor([ 1, 4, 2, 5, 3]), batch_sizes=tensor([ 2, 2, 1])) beam_searching[参考链接]https://blog.csdn.net/xljiulong/article/details/51554780[参考链接]http://jhave.org/algorithms/graphs/beamsearch/beamsearch.shtml网上讲的大部分都有各自的问题，不是很清晰，只有那篇英文才是标准的，这哥们应该是翻译的，还不错作者使用的是beam_searching的变种，原理类似，但是条件不一致，具体的在代码注释中，不再陈述。 第十章和第九章关于生成语句的流程的区别第十章和第九章在模型生成的地方有两个点不一样，第九章的模型本身可以进行正常的输入与输出，所以第九章也写成这个样子输入(LongTensor) 11 输出 tensor 1vocabsize123456789101112131415161718192021222324252627282930313233results = list(start_words)start_word_len = len(start_words)for i in range(opt.max_gen_len): output, hidden = model(input, hidden) if i &lt; start_word_len: w = results[i] input = input.data.new([word2ix[w]]).view(1,1) else: # output size 1×vocab_size [[1,2,3,...]] # 这里应该看一下，输出output是个什么东西 top_index = output.data[0].topk(1)[1][0].item() w = ix2word[top_index] results.append(w) input = input.data.new([word2ix[w]]).view(1,1) if w == '&lt;EOP&gt;': del results[-1] breakreturn results# 简写为# model: embedder rnn classifierresults = []for i in range(opt.max_gen_len): output, hidden = model(input, hidden) # input(tensor) 1*1 output(tensor) 1*vocabsize hidden(tensor) 1*1*hidden_dim top_index = output.data[0].topk(1)[1][0].item() w = ix2word(top_index) results.append(w) input = input.data.new(word2ix(w)).view(1,1)return results 第十章因为使用了pack_padded_sequence来加速训练，那么训练的模型就不能直接拿来像第九章进行生成，另外第一个字是图片特征的转化而成的，不需要embedding层，而是需要fc层，其实也可以直接拿来用，把captions设置为空就好了,在这里作者没有直接用，直接用好像比较麻烦。而是采用beam_search中把rnn和classifier层传进去，写了一个标准的beam_search函数，即输入是第一个字，输出是beam_size句话，因为设计到其他选词保留的问题，所以直接传入的的是各个分函数，进行自行拼接。也可能是为了复用logprobs = nn.functional.log_softmax(output, dim=1) ## 暂时不清楚这里为什么用log_softmax，是负数啊，大哥，不过大小好像不变在rnn中有一个问题，就是能不能用t.no_grad,会不会影响其向前传播。 对数据的预处理第九章是把对数据的预处理写在了data里面，但事实上，这个数据预处理应该与主模型分开，是属于前一个过程。有什么需要交互的，也是通过文件进行，包括配置。 新建立的数据结构的对比大小123456789101112131415161718192021222324252627282930313233343536373839404142434445class Caption(object): """ 现在不太确定这个集合是hash_table还是set，感觉是hash_tale,是因为set不需要专门的存储结构。再看看吧 这里应该不是那三个集合，而是集合中的每一个元素，比如G(i),这种，作者应该是重新创建了一种数据结构来用，来进行存储 Args: sentence: list(int) state: tuple(hn, cn) hn:1*1*hidden_dim logprob: probability score: 等于logprb或者logprb/len(sentence) """ def __init__(self, sentence, state, logprob, score, metadata=None): """ Args: sentence(list): """ self.sentence = sentence self.state = state self.logprob = logprob self.score = score self.metadata = metadata # 这里我猜是为了实现堆排序的比较。尽管知道是，但是还是不知道为什么 def __cmp__(self, other): """Compares Captions by score.""" assert isinstance(other, Caption) if self.score == other.score: return 0 elif self.score &lt; other.score: return -1 else: return 1 # For Python 3 compatibility (__cmp__ is deprecated). def __lt__(self, other): assert isinstance(other, Caption) return self.score &lt; other.score # Also for Python 3 compatibility. def __eq__(self, other): assert isinstance(other, Caption) return self.score == other.score 作者在固定长度的列表中，使用了小顶堆的方式，来保证每次取出去的都是最小的。 存在问题现在还有一个问题就是当一个.py文件里的函数或者类超过2、3个时，应该以什么的方式注释才能更好地让别人知道这个文件里的函数和怎么干的。 总结第十章的代码在难度上其实已经感觉下降了好多，当然自己又忘了写requires.txt。但是在调试改bug自己就用了三天。其中的bug有的时候自己已经忘记当初是怎么写的了，尴尬。自己训练出来的模型也没有作者声称的那么好，暂时不知道]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[html]]></title>
    <url>%2F2018%2F09%2F24%2Fhtml%2F</url>
    <content type="text"><![CDATA[HTML 教程 html 基础因为在visdom中的text支持html标签，所以来简单学学html。参考链接:菜鸟教程[菜鸟工具在线编辑工具]https://c.runoob.com/front-end/6112345678910111213141516&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;我的第一个标题&lt;/h1&gt; &lt;p&gt;我的第一个段落。&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 实例解析 \&lt;!DOCTYPE html&gt; 声明为 HTML5 文档 &lt;\html&gt; 元素是 HTML 页面的根元素 &lt;\head&gt; 元素包含了文档的元（meta）数据，如 定义网页编码格式为 utf-8。 &lt;\title&gt; 元素描述了文档的标题 元素包含了可见的页面内容 &lt;\h1&gt; 元素定义一个大标题 &lt;\p&gt; 元素定义一个段落 网页结构1234567891011&lt;html&gt; &lt;head&gt; &lt;title&gt; 页面标题&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;这是一个标题&lt;/h1&gt; &lt;p&gt;这是一个段落。&lt;/p&gt; &lt;p&gt;这是另一个段落&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 只有与之间的元素才会显示 标题 - 这是一个标题段落 这是一个段落。 段前段后有空行链接 这是一个链接图片 html元素空元素标签html属性 html设置属性，常以键值对的形式出现 这是一个链接 常用属性： class id style titlehtml水平线 换行 注释 格式化标签123456&lt;b&gt;加粗文本&lt;/b&gt;&lt;br&gt;&lt;i&gt;斜体文本&lt;/i&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;电脑自动输出&lt;/code&gt;&lt;br&gt;&lt;br&gt;这是 &lt;sub&gt; 下标&lt;/sub&gt; 和 &lt;sup&gt; 上标&lt;/sup&gt;&lt;small&gt;这个文本是缩小的&lt;/small&gt;&lt;big&gt;这个文本字体放大&lt;/big&gt; 123456789&lt;pre&gt;此例演示如何使用 pre 标签对空行和 空格进行控制&lt;/pre&gt;此例演示如何使用 pre 标签对空行和 空格进行控制 1234567891011&lt;code&gt;计算机输出&lt;/code&gt;&lt;br /&gt;&lt;kbd&gt;键盘输入&lt;/kbd&gt;&lt;br /&gt;&lt;tt&gt;打字机文本&lt;/tt&gt;&lt;br /&gt;&lt;samp&gt;计算机代码样本&lt;/samp&gt;&lt;br /&gt;&lt;var&gt;计算机变量&lt;/var&gt;&lt;br /&gt; 地址1234567&lt;address&gt;Written by &lt;a href="mailto:webmaster@example.com"&gt;Jon Doe&lt;/a&gt;.&lt;br&gt; Visit us at:&lt;br&gt;Example.com&lt;br&gt;Box 564, Disneyland&lt;br&gt;USA&lt;/address&gt; 创建电子邮件标签 首字母缩写和缩写123&lt;abbr title="etcetera"&gt;etc.&lt;/abbr&gt;&lt;br /&gt;&lt;acronym title="World Wide Web"&gt;WWW&lt;/acronym&gt; 文字显示方向1&lt;p&gt;&lt;bdo dir="rtl"&gt;该段落文字从右到左显示。&lt;/bdo&gt;&lt;/p&gt; 块引用123&lt;p&gt;WWF's goal is to: &lt;q&gt;Build a future where people live in harmony with nature.&lt;/q&gt;We hope they succeed.&lt;/p&gt; 删除字和插入字的效果1&lt;p&gt;My favorite color is &lt;del&gt;blue&lt;/del&gt; &lt;ins&gt;red&lt;/ins&gt;!&lt;/p&gt; html 链接属性 target:定义文档在哪个窗口打开 id属性1&lt;a href="http://www.runoob.com/" target="_blank"&gt;访问菜鸟教程!&lt;/a&gt; 12345&lt;p&gt;&lt;a href="#C4"&gt;查看章节 4&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a id="C4"&gt;章节 4&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这边显示该章节的内容……&lt;/p&gt; html , , , , , , and . 定义了浏览器工具栏的标题 当网页添加到收藏夹时，显示在收藏夹中的标题 显示在搜索引擎结果页面的标题 标签描述了基本的链接地址/链接目标，该标签作为HTML文档中所有的链接标签的默认链接 1&lt;base href="http://www.runoob.com/images/" target="_blank"&gt; 标签定义了文档与外部资源之间的关系。1&lt;link rel="stylesheet" type="text/css" href="mystyle.css"&gt; 标签定义了HTML文档的样式文件引用地址.1234&lt;style type="text/css"&gt;body &#123;background-color:yellow&#125;p &#123;color:blue&#125;&lt;/style&gt; 元素 META 元素通常用于指定网页的描述，关键词，文件的最后修改时间，作者，和其他元数据。123&lt;meta name="keywords" content="HTML, CSS, XML, XHTML, JavaScript"&gt;&lt;meta name="description" content="免费 Web &amp; 编程 教程"&gt;&lt;meta http-equiv="refresh" content="30"&gt; HTML 样式- CSSCSS (Cascading Style Sheets) 用于渲染HTML元素标签的样式.123456789101112131415161718192021# 使用添加到 &lt;head&gt; 部分的样式信息对 HTML 进行格式化 内部样式表 应用于单个文件&lt;head&gt;&lt;meta charset="utf-8"&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;style type="text/css"&gt;h1 &#123;color:red;&#125;p &#123;color:blue;&#125;&lt;/style&gt;&lt;/head&gt;# 使用 style 属性制作一个没有下划线的链接 内联样式 应用于个别元素&lt;a href="http://www.runoob.com/" style="text-decoration:none;"&gt;访问 runoob.com!&lt;/a&gt;# 标签链接到一个外部样式表 外部引用&lt;head&gt;&lt;meta charset="utf-8"&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;link rel="stylesheet" type="text/css" href="styles.css"&gt;&lt;/head&gt;font-family（字体），color（颜色），和font-size（字体大小）， text-align（文字对齐）# 图像 1# 表格 Header 1 Header 2 row 1, cell 1 row 1, cell 2 row 2, cell 1 row 2, cell 2 1# 列表 Coffee Tea Milk Coffee Tea Milk Coffee Tea Milk ` 区块 块级和 内联级表单 2018-10-05 前言：主要是为了爬虫做一些补充，此次主要是为了爬虫做补充，不涉及html的理论用于加载脚本文件，指一段javascript代码，暂时不影响后续操作。 &amp;区块 块级和 内联级，本身没有太强的含义，前者以新行显示，后者不以新行显示。主要是作为容器存在，用于布局。 id&amp;classid具有唯一性，在一个网页内同一个命名只能使用一次，定义以#开头class命名的类可以在一个网页使用无数次，定义以.开头但两者都是定义的样式而已。]]></content>
      <categories>
        <category>html</category>
      </categories>
      <tags>
        <tag>html</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-picture]]></title>
    <url>%2F2018%2F09%2F24%2Fpython-picture%2F</url>
    <content type="text"><![CDATA[针对图像的操作 cv2, matplotlib.pylab, PIL.Image一般用PIL.Image或者cv2来打开或者保存，用matplotlib.pylab来显示在pytorch中也可以用tv.utils.save_image()专门来保存图片。 cv2参考链接注意：pylab.imread和PIL.Image.open读入的都是RBG顺序，而cv2.imread读入的是BGR顺序，混合使用的时候要特别注意123456789101112131415161718192021222324252627282930313233# 读取图片import cv2import numpy as npimg = cv2.imread('examples.png') # # 默认是读入为彩色图，即使原图是灰度图也会复制成三个相同的通道变成彩色图img_gray = cv2.imread('examples.png',0) # 第二个参数为0的时候读入为灰度图，即使原图是彩色图也会转成灰度图print(type(img), img.dtype, np.min(img), np.max(img))print(img.shape)print(img_gray.shape)(&lt;type 'numpy.ndarray'&gt;, dtype('uint8'), 0, 255) # opencv读进来的是numpy数组，类型是uint8，0-255(824, 987, 3) # 彩色图3通道(824, 987) # 灰度图单通道## 显示import pylab as plt import cv2 import numpy as np img = cv2.imread('examples.png') plt.imshow(img[..., -1::-1]) # 因为opencv读取进来的是bgr顺序呢的，而imshow需要的是rgb顺序，因此需要先反过来 plt.show()## 灰度与RGB转化import cv2 import pylab as plt img = cv2.imread('examples.png') img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # BGR转灰度 img_bgr = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR) # 灰度转BRG img_rgb = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB) # 也可以灰度转RGB## 保存图片import cv2 img = cv2.imread('examples.png') # 这是BGR图片 cv2.imwrite('examples2.png', img) # 这里也应该用BGR图片保存，这里要非常注意，因为用pylab或PIL读入的图片都是RGB的，如果要用opencv存图片就必须做一个转换 img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.imwrite('examples_gray.png', img_gray) matplotlib.pylab12345678910111213141516171819202122# 读取图片import matplotlib.pylab as pltimport numpy as npimg = plt.imread('examples.png')print(type(img), img.dtype, np.min(img), np.max(img))[out](&lt;type 'numpy.ndarray'&gt;, dtype('float32'), 0.0, 1.0) # matplotlib读取进来的图片是float，0-1# 显示plt.imshow(img)plt.show()# 保存# 有两种# 其实产生这个现象的原因很简单：在 plt.show() 后调用了 plt.savefig() ，在 plt.show() 后实际上已经创建了一个新的空白的图片（坐标轴），这时候你再 plt.savefig() 就会保存这个新生成的空白图片# ref:(https://blog.csdn.net/u010099080/article/details/52912439)# 第一种:在plt.show之前保存plt.savefig('test.png')# 第二种:画图的时候保存句柄fig = plt.gcf()plt.show()fig.savefig('test.png') PIL.Image123456789101112131415161718192021222324252627282930313233343536373839404142434445# 读取图片from PIL import Imageimport numpy as npimg = Image.open('examples.jpg')print(type(img), img.dtype, np.min(img), np.max(img))img = np.array(img) # 将PIL格式图片转为numpy格式image_pil = Image.fromarray(image_numpy)print(type(img), img.dtype, np.min(img), np.max(img))(&lt;class 'PIL.PngImagePlugin.PngImageFile'&gt;, 0, 255) # 注意，PIL是有自己的数据结构的，但是可以转换成numpy数组 (&lt;type 'numpy.ndarray'&gt;, dtype('uint8'), 0, 255) # 和用matplotlib读取不同，PIL和matlab相同，读进来图片和其存储在硬盘的样子是一样的，uint8，0-255# 灰度和RGB转化from PIL import Image img = Image.open('examples.png') img_gray = img.convert('L') # RGB转换成灰度图像 img_rgb = img_gray.convert('RGB') # 灰度转RGB print(img) print(img_gray) print(img_rgb) [out] &lt;PIL.PngImagePlugin.PngImageFile image mode=RGB size=987x824 at 0x7FC2CCAE04D0&gt; &lt;PIL.Image.Image image mode=L size=987x824 at 0x7FC2CCAE0990&gt; &lt;PIL.Image.Image image mode=RGB size=987x824 at 0x7FC2CCAE0250&gt;# 显示import pylab as plt from PIL import Image import numpy as npimg = Image.open('examples.png') img_gray = img.convert('L') #转换成灰度图像 img = np.array(img) img_gray = np.array(img_gray) plt.imshow(img) # or plt.imshow(img / 255.0)，matplotlib和matlab一样，如果是float类型的图像，范围是0-1才能正常imshow，如果是uint8图像，范围则需要是0-255 plt.show() plt.imshow(img_gray, cmap=plt.gray()) # 显示灰度图要设置cmap参数 plt.show() plt.imshow(Image.open('examples.png')) # 实际上plt.imshow可以直接显示PIL格式图像 plt.show()# 保存img = Image.open('examples.png') img.save('examples2.png') img_gray = img.convert('L') img_gray.save('examples_gray.png') # 不管是灰度还是彩色，直接用save函数保存就可以，但注意，只有PIL格式的图片能够用save函数]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python,picture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMC]]></title>
    <url>%2F2018%2F09%2F24%2FCMC%2F</url>
    <content type="text"><![CDATA[rank-1,rank-5,mAP 第一种，网上的标准计算公式https://blog.csdn.net/u013698770/article/details/60776102https://blog.csdn.net/zkp_987/article/details/79969512https://blog.csdn.net/kaixinjiuxing666/article/details/812727961234567891011121314151617181920212223242526272829303132333435363738import numpy as npindexxx = np.array([[8,9,4,7,5,6,3,2,1,0],[9,6,7,2,1,4,3,5,8,0],[7,9,5,3,1,2,4,6,8,0]])good_index = np.array([1,3,5,7,9])CMC = np.array([0,0,0,0,0,0,0,0,0,0])mAP = 0.0for i in indexxx: cmc = np.array([0,0,0,0,0,0,0,0,0,0]) index = i ngood = len(good_index) mask = np.in1d(index, good_index) rows_good = np.argwhere(mask==True) rows_good = rows_good.flatten() cmc[rows_good[0]:] = 1 print('cmc:',cmc) CMC += cmc ap = 0.0 for i in range(ngood): d_recall = 1.0/ngood precision = (i+1)*1.0/(rows_good[i]+1) ap = ap + d_recall*precision print('ap:&#123;:.2f&#125;%:'.format(100*ap)) mAP += apCMC = CMC/3mAP = mAP/3print('top1:&#123;:.2f&#125;% top5:&#123;:.2f&#125;% mAP:&#123;:.2f&#125;%'.format(100*CMC[0],100*CMC[4],100*mAP))cmc: [0 1 1 1 1 1 1 1 1 1]ap:54.54%cmc: [1 1 1 1 1 1 1 1 1 1]ap:69.26%cmc: [1 1 1 1 1 1 1 1 1 1]ap:100.00%top1:66.67% top5:100.00% mAP:74.60% 第二种：baseline的计算公式12345678910111213141516171819202122232425262728293031323334353637383940414243def compute_mAP(index, good_index, junk_index): ap = 0 cmc = torch.IntTensor(len(index)).zero_() if good_index.size==0: # if empty cmc[0] = -1 return ap,cmc # remove junk_index mask = np.in1d(index, junk_index, invert=True) index = index[mask] # find good_index index ngood = len(good_index) mask = np.in1d(index, good_index) rows_good = np.argwhere(mask==True) rows_good = rows_good.flatten() cmc[rows_good[0]:] = 1 for i in range(ngood): d_recall = 1.0/ngood precision = (i+1)*1.0/(rows_good[i]+1) if rows_good[i]!=0: old_precision = i*1.0/rows_good[i] else: old_precision=1.0 ap = ap + d_recall*(old_precision + precision)/2 return ap, cmcCMC = torch.IntTensor(len(gallery_label)).zero_()ap = 0.0#print(query_label)for i in range(len(query_label)): ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam) if CMC_tmp[0]==-1: continue CMC = CMC + CMC_tmp ap += ap_tmp print(i, CMC_tmp[0])CMC = CMC.float()CMC = CMC/len(query_label) #average CMCprint('top1:%f top5:%f top10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label))) 第二种的简化版本，只计算CMC1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889def evaluate(qf,ql,qc,gf,gl,gc): """ qf: list [1,2,3] ql: 1 qc: 1 gf: list [[1,2,3],[1,2,3]] gl: [1,2,3] gc: [1,2,3] len(gf)==len(gl)==len(gc) """ query = qf score = np.dot(gf,query) # predict index index = np.argsort(score) #from small to large # 表示位置，[4,3,1,0,2] index = index[::-1] # 表示 #index = index[0:2000] # good index query_index = np.argwhere(gl==ql) # list [[1],[2]] # 表示位置，即galley中的第几个样本是相同的id camera_index = np.argwhere(gc==qc) # list [[1],[2]] # 表示位置 good_index = np.setdiff1d(query_index, camera_index, assume_unique=True) # [2,3]表示同一个id不同摄像头的图片的位置 junk_index1 = np.argwhere(gl==-1) # [[1],[2]] 表示id为-1的图片的位置 junk_index2 = np.intersect1d(query_index, camera_index) # [1,2] 表示同一个id同一个摄像头的图片的位置 junk_index = np.append(junk_index2, junk_index1) #.flatten()) CMC_tmp = compute_mAP(index, good_index, junk_index) return CMC_tmpdef compute_cmc(index, good_index, junk_index): """ index: list [4,3,1,0,2]，已经排序，数字表示第几张图片 good_index: [3,1] list 位置 数字表示第几张图片 junk_index: [4,2]list 位置 数字表示第几张图片 """ cmc = torch.IntTensor(len(index)).zero_() if good_index.size==0: # if empty cmc[0] = -1 return ap,cmc # remove junk_index mask = np.in1d(index, junk_index, invert=True) index = index[mask] # [3,1,0] # find good_index index ngood = len(good_index) mask = np.in1d(index, good_index) # [t,t,f] rows_good = np.argwhere(mask==True) # rows_good = rows_good.flatten() # [0,1] cmc[rows_good[0]:] = 1 return cmcdef main(): result = scipy.io.loadmat('pytorch_result.mat') query_feature = result['query_f'] # list [[1,2,3],[1,2,3],[1,2,3]] query_cam = result['query_cam'][0] # list [1,2,3] query_label = result['query_label'][0] # list [1,2,3] gallery_feature = result['gallery_f'] # list [[1,2,3],[1,2,3],[1,2,3]] gallery_cam = result['gallery_cam'][0] # list [1,2,3] gallery_label = result['gallery_label'][0] # list [1,2,3] CMC = torch.IntTensor(len(gallery_label)).zero_() for i in range(len(query_label)): CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam) if CMC_tmp[0]==-1: continue CMC = CMC + CMC_tmp print(i, CMC_tmp[0]) CMC = CMC.float() CMC = CMC/len(query_label) #average CMC print('top1:%f top5:%f top10:%f'%(CMC[0],CMC[4],CMC[9])def get_id(img_path): camera_id = [] labels = [] for path, v in img_path: filename = path.split('/')[-1] label = filename[0:4] camera = filename.split('c')[1] if label[0:2]=='-1': labels.append(-1) else: labels.append(int(label)) camera_id.append(int(camera[0])) return camera_id, labelsgallery_cam,gallery_label = get_id(gallery_path)query_cam,query_label = get_id(query_path)]]></content>
      <categories>
        <category>person-reid</category>
      </categories>
      <tags>
        <tag>CMC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch chapter9 CharRNN]]></title>
    <url>%2F2018%2F09%2F23%2Fpytorch-chapter9-CharRNN%2F</url>
    <content type="text"><![CDATA[前言本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。这是我的代码大神链接：https://github.com/anishathalye/neural-style这是论文作者写的 问题及其思考data是鸭子类型因为data作为tensor，已经实现了__getitem__和__len__方法，可以被DataLoader加载。或者说，只要能类似鸭子就可以，这方面掌握得还不熟悉。 LSTM的输入作者明确提出，LSTM的输入类型是(seq_len, batch_size, embedding_dim)，除去embedding_dim，就是(seq_len, batch_size)，原因很简单，LSTM是每次输入一个字，输出一个字，那么输入就是x[0]，对于图像，x[0]就是一张图片，那么对于文字，x[0]也应该就是一个字。好吧，还是说不通，等以后看了相关资料说不定才能理解。 代码编写12345678910111213141516171819202122232425262728293031323334# 变成列表，方便后续的操作，因为start_words的每个字用过之后就没用了，# 用pop不行,因为对于空列表会报错,用None作为结尾标志。可以看出，如果我们想让某个序列正常退出，可以通过设置特殊的结尾来实现。# 这一段的逻辑有点乱，因为prefix_words可能没有，所以对于start_words，必须先进行一个模型生成。# 对于或有或无的perfix_words，为了消除其存在对代码和思路的影响，应该保证prefix_words前后的代码状态不变，即"""第一种这种保证了output,hidden的状态不变output, hidden = model(input, hidden)# step： 对prefix_words进行输入prefix_words = '' if prefix_words==None else prefix_words for word in prefix_words: input = input.data.new(word2ix[word]).view(1,1) output, hidden = model(input, hidden) for i in range(opt.max_gen_len-1): top_index = output[0].topk(1)[1][0].item() ... output, hidden = model(input, hidden)第二种这种保证了input的状态不变for word in prefix_words: output, hidden = model(input, hidden) input = (input.data.new([word2ix[word]])).view(1, 1)for i in range(opt.max_gen_len): output, hidden = model(input, hidden) top_index = output.data[0].topk(1)[1][0].item() 决定采用第二种，因为代码的主体思路是for i in range(opt.max_gen_len)，prefix_word是插入部分，是可有可无部分。第一种会造成 top_index与model的切分，不利于后期分析。或者说，以后碰到这种类型的代码，可以直接跳过中间部分，对后面进行分析。 os.walk() &amp;&amp; os.listdir()os.walk()12345678910111213141516171819202122232425In [6]: for i,j,k in os.walk('./'): ...: print(i)././b./a./a/aa2./a/aa1In [7]: for i,j,k in os.walk('./'): ...: print(j) ['b', 'a'][]['aa2', 'aa1'][][]In [8]: for i,j,k in os.walk('./'): ...: print(k)['1']['cc', 'bb']['aa']['bbb'][]i+k即可 os.walk()返回的是当前文件夹下所有的可遍历的文件夹，生成的生成器，i表示文件夹，j表示i文件夹下的文件夹，k表示i文件夹下的文件。以上是os.walk的for用法，下面是直接的用法。对于文件访问，直接i+k，对于文件夹访问，i+j即可。 os.listdir()1234567891011In [17]: aa = os.walk('./')In [18]: bb = list(aa)In [19]: bbOut[19]: [('./', ['b', 'a'], ['1']), ('./b', [], ['cc', 'bb']), ('./a', ['aa2', 'aa1'], ['aa']), ('./a/aa2', [], ['bbb']), ('./a/aa1', [], [])] 不知道为什么这里不能直接用 aa,bb,cc = os.walk(‘./‘)12345678910In [22]: for ii in os.listdir('./'): ...: print(ii) ...: 1baIn [23]: aa = os.listdir('./')In [24]: aaOut[24]: ['1', 'b', 'a'] os.listdir()返回的是当前文件夹下的文件夹或者文件。现在碰到的情况是文件夹排列有序，直接访问文件，所以os.list()就可以了。对应的就是12for filename in os.listdir(src): path = os.path.join(src,filename) 小发现 刚刚发现github上的chinese中的某个文件夹是一个新的github文件。这是个啥情况 python 可以在函数内部定义函数，是局部域，不能被外界访问，很好，这样就相当于说明了哪些函数是为哪些函数服务的。 json数据格式的读取12345678910111213141516In [25]: import jsonIn [26]: s = &#123;"name": "ACME", "shares": 50, "price": 490.1&#125;In [27]: json_str = json.dumps(s)In [28]: json_str.__class__Out[28]: strIn [29]: json_strOut[29]: '&#123;"name": "ACME", "shares": 50, "price": 490.1&#125;'In [33]: ss = json.loads(json_str)In [34]: ssOut[34]: &#123;'name': 'ACME', 'shares': 50, 'price': 490.1&#125; json文件的读取第一种:此时data里是该文件内的全部数据12with open(file,'r') as f: data = json.load(f) 第二种：此时data也是该文件内的全部数据，open(file).read()表示读取数据1data = json.loads(open(file).read()) 显然第一种安全，第二种还需要显示地关闭文件可以使用pprint来打印data，好看12import pprint import pprint pprint data 正则表达式普通字符和11个元字符：. 匹配任意除换行符”\n”外的字符(在DOTALL模式中也能匹配换行符 a.c\ 转义字符，使后一个字符改变原来的意思* 匹配前一个字符0或多次+ 匹配前一个字符1次或无限次? 匹配一个字符0次或1次^ 匹配字符串开头。在多行模式中匹配每一行的开头$ 匹配字符串末尾，在多行模式中匹配每一行的末尾| 或。匹配|左右表达式任意一个，从左到右匹配，如果|没有包括在()中，则它的范围是整个正则表达式{} {m}匹配前一个字符m次，{m,n}匹配前一个字符m至n次，若省略n，则匹配m至无限次[] 字符集。对应的位置可以是字符集中任意字符。字符集中的字符可以逐个列出，也可以给出范围，如[abc]或[a-c]。[^abc]表示取反，即非abc。所有特殊字符在字符集中都失去其原有的特殊含义。用\反斜杠转义恢复特殊字符的特殊含义。()表达式作为一个整体，可以后接数量词。表达式中的|仅在该组中有效。print re.split(r”;|,|\?”, line1)print re.split(r”[;,?]”, line1)print re.split(r”\W+”, line)不知道为什么12345para = '-181-欲出未出光辣达，[千山]万山[如火]发（哈哈哈大笑）。须臾[走向][天上]'re.subn('[\d-]','',para)('欲出未出光辣达，[千山]万山[如火]发（哈哈哈大笑）。须臾[走向][天上]', 5)re.subn('[\d-]*','',para)('欲出未出光辣达，[千山]万山[如火]发（哈哈哈大笑）。须臾[走向][天上]', 38) list越界与切片list不能越界索引访问，但是对于切片，切片是会自动匹配长度的，所以使用slice不需要担心越界问题。12345s = [1,2,3]s[8] # 报错s[1] == 1s[:1] == [1]s[1:10] # return s[1:3] 索引位置返回的是元素的副本切片返回的是list的副本 嵌套列表压平func = lambda x: [y for l in x for y in l] if type(x) is list else [x] tuple的连接123(1,2)+(3,4) == (1,2,3,4)# 定义只有一个数字的tuple，避免函数歧义t = (1,) 求list的size123li = [[1,2],[3,4]]tu = np.asarray(li).shape# shape返回的是tuple型，可以直接拼接 异常触发参考链接http://www.runoob.com/python/python-exceptions.html等总结的时候尝试一下分为捕捉异常和触发异常 捕捉异常12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 捕捉异常第一种 try/except语句try:&lt;语句&gt; #运行别的代码except &lt;名字&gt;：&lt;语句&gt; #如果在try部份引发了'name'异常except &lt;名字&gt;，&lt;数据&gt;:&lt;语句&gt; #如果引发了'name'异常，获得附加的数据else:&lt;语句&gt; #如果没有异常发生# 捕捉异常第二种 try/finallytry:&lt;语句&gt;finally:&lt;语句&gt; #退出try时总会执行raise# 实例1import sys try: f = open('myfile.txt') s = f.readline() i = int(s.strip())except OSError as err: print("OS error: &#123;0&#125;".format(err))except ValueError: print("Could not convert data to an integer.")except: print("Unexpected error:", sys.exc_info()[0]) raise# 实例2如果你只想知道这是否抛出了一个异常，并不想去处理它，那么一个简单的 raise 语句就可以再次把它抛出。try: raise NameError('HiThere')except NameError: print('An exception flew by!') raise# 实例3处理有参数的异常def temp_convert(var): try: return int(var) except (ValueError) as Argument: print ("参数没有包含数字\n", Argument)# 调用函数temp_convert("xyz");# 捕捉异常共同使用try: fh = open("testfile", "w") try: fh.write("这是一个测试文件，用于测试异常!!") finally: print "关闭文件" fh.close()except IOError: print "Error: 没有找到文件或读取文件失败"# 注except (RuntimeError, TypeError, NameError): 触发异常1234567891011121314151617# 函数触发异常def functionName( level ): if level &lt; 1: raise Exception("Invalid level!", level) # 触发异常后，后面的代码就不会再执行# 捕捉异常和触发异常的配合def mye( level ): if level &lt; 1: raise Exception("Invalid level!") # 触发异常后，后面的代码就不会再执行try: mye(0) # 触发异常except Exception: print 1else: print 2 自定义异常12345678910&gt;&gt;&gt;class MyError(Exception): def __init__(self, value): self.value = value def __str__(self): return repr(self.value) &gt;&gt;&gt; try: raise MyError(2*2) except MyError as e: print('My exception occurred, value:', e.value) list的更新更新分为逐元素更新和逐列表更新 逐元素更新12345678910111213# list的逐元素更新li = [] for i in range(5): li.append(i)# 对于空列表，等价于li = [i for i in range(5)]# 或者等价于generatorli = (i for i in range(5))# 或者转化为逐列表更新,常用与列表头和列表尾同时更新for i in range(4): li=[i]+li+[i+11]# 或者对于已经得到的元素[a,b,c] 即对于一个空列表的append，我们总是可以将其转化成列表推导式，并且对于dict和set，只需要将中括号换成大括号即可 逐列表更新123456789# list的逐列表更新li = []ll = [[1,2,3],[4,5,6]]for i in ll: li.extend(i)li[1,2,3,4,5,6]# 或者对于有限个列表c = a+b 注：append()和extend()和+=都是在原有列表增加，+是生成一个新的列表感觉逐列表更新应该可以更优美. os.path.join()12345678910111213141516171819202122232425262728293031323334353637In [42]: import osIn [43]: print("1:",os.path.join('aaaa','bbbb','ccccc.txt'))1: aaaa/bbbb/ccccc.txtIn [44]: print("1:",os.path.join('/aaaa','bbbb','ccccc.txt'))1: /aaaa/bbbb/ccccc.txtIn [45]: print("1:",os.path.join('aaaa','/bbbb','ccccc.txt'))1: /bbbb/ccccc.txtIn [46]: print("1:",os.path.join('aaaa','bbbb','/ccccc.txt'))1: /ccccc.txtIn [47]: print("1:",os.path.join('aaaa','/bbbb','/ccccc.txt'))1: /ccccc.txtIn [48]: print("1:",os.path.join('aaaa','./bbbb','ccccc.txt'))1: aaaa/./bbbb/ccccc.txtIn [49]: print("1:",os.path.join('aaaa','../bbbb','ccccc.txt'))1: aaaa/../bbbb/ccccc.txtIn [50]: print("1:",os.path.join('aaaa','bbbb','./ccccc.txt'))1: aaaa/bbbb/./ccccc.txtIn [51]: print("1:",os.path.join('aaaa','bbbb','../ccccc.txt'))1: aaaa/bbbb/../ccccc.txtIn [54]: print("1:",os.path.join('aaaa','/bbbb','....../ccccc.txt'))1: /bbbb/....../ccccc.txtIn [55]: print("1:",os.path.join('aaaa','bbbb/','ccccc.txt'))1: aaaa/bbbb/ccccc.txtIn [57]: print("1:",os.path.join('aaaa','bbbb\\','ccccc.txt'))1: aaaa/bbbb\/ccccc.txt 对os.path.join()总结如下：从后往前，遇到绝对路径，则绝对路径前面的元素丢弃，遇到类似’…/‘，则将其看成一个普通的路径名字，而对于/在末尾的，会自动根据情况补充。 list()和[]的区别，字符串分割成单个字符123456789101112131415161718192021In [58]: list('abcd')Out[58]: ['a', 'b', 'c', 'd']In [59]: ['abcd']Out[59]: ['abcd']In [61]: aa = (1,2)In [62]: list(aa)Out[62]: [1, 2]In [63]: [aa]Out[63]: [(1, 2)]In [65]: bb = ['abc']In [66]: list(*bb)Out[66]: ['a', 'b', 'c']for i in 'abcd': print(i) 即，list()会拆解输入值，拼接成list，可以用在’abcd’这样的字符串直接拆成’a’,’b’,’c’,’d’这样的形式，因为re.split不支持这种拆分法。当然，如果只是单纯地逐元素访问并逐元素地进行操作，我们可以使用for i in ‘abcd’:这样的访问。也可以认为是’’.join()的逆操作1234567In [65]: bb = ['abc']In [66]: list(*bb)Out[66]: ['a', 'b', 'c']In [71]: ''.join(list(bb))Out[71]: 'abc' list、dict和numpy的互相转换123456789101112131415161718In [74]: c = np.array(&#123;'a':1,'b':2&#125;)In [75]: c[0]---------------------------------------------------------------------------IndexError Traceback (most recent call last)&lt;ipython-input-75-71463270cd6c&gt; in &lt;module&gt;()----&gt; 1 c[0]IndexError: too many indices for arrayIn [77]: cOut[77]: array(&#123;'a': 1, 'b': 2&#125;, dtype=object)In [79]: c.tolist()Out[79]: &#123;'a': 1, 'b': 2&#125;In [80]: d = c.tolist()In [81]: d.__class__Out[81]: dict shell的基础教程12345678for fff in `ls *.json`docconv -f utf8-tw -t UTF8-CN $fff -o simplified/$fffdonefor skill in Ada Coffe Action Java; do echo "I am good at $&#123;skill&#125;Script"done shell 传递参数1234567891011121314151617#!/bin/bash# author:菜鸟教程# url:www.runoob.comecho "Shell 传递参数实例！";echo "执行的文件名：$0";echo "第一个参数为：$1";echo "第二个参数为：$2";echo "第三个参数为：$3";$ chmod +x test.sh $ ./test.sh 1 2 3Shell 传递参数实例！执行的文件名：./test.sh第一个参数为：1第二个参数为：2第三个参数为：3 shell 流程控制123456789if condition1then command1elif condition2 then command2else commandNfi Shell 输入/输出重定向命令 说明command &gt; file 将输出重定向到 file。command &lt; file 将输入重定向到 file。command &gt;&gt; file 将输出以追加的方式重定向到 file。n &gt; file 将文件描述符为 n 的文件重定向到 file。n &gt;&gt; file 将文件描述符为 n 的文件以追加的方式重定向到 file。n &gt;&amp; m 将输出文件 m 和 n 合并。n &lt;&amp; m 将输入文件 m 和 n 合并。&lt;&lt; tag 将开始标记 tag 和结束标记 tag 之间的内容作为输入。需要注意的是文件描述符 0 通常是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。12345678910111213141516171819202122# 如果希望 stderr 重定向到 file，可以这样写：$ command 2 &gt; file# 如果希望将 stdout 和 stderr 合并后重定向到 file$ command &gt; file 2&gt;&amp;1$ command &gt;&gt; file 2&gt;&amp;1# 如果希望对 stdin 和 stdout 都重定向$ command &lt; file1 &gt;file2#Here Document 是 Shell 中的一种特殊的重定向方式，用来将输入重定向到一个交互式 Shell 脚本或程序。$ wc -l &lt;&lt; EOF 欢迎来到 菜鸟教程 www.runoob.comEOF3 # 输出结果为 3 行# 如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null：$ command &gt; /dev/null# 如果希望屏蔽 stdout 和 stderr，可以这样写：$ command &gt; /dev/null 2&gt;&amp;1 str.split(‘’)以列表形式返回1234label_dim = '16803+100'cc = label_dim.split('+')cc['16803','100'] map123456scales_tr = '20,20--20,20'scale = [map(int, x.split(',')) for x in scales_tr.split('--')]scale[&lt;map at 0x7fd798714748&gt;, &lt;map at 0x7fd798714588&gt;]list(scale[0])[20,20] tensor的拼接 t.cat t.stack12345result = []for ii in index:# tensor的截取与合并 cat, stack,cat+view=stack,stack 新增维度进行合并 result.append(fake_img.data[ii])tv.utils.save_image(t.stack(result), opt.gen_img, normalize=True, range=(-1,1)) tensor.view()123x = x.view(x.size(0),-1)# 或者x = x.view(x.size()[0],-1) Inception-V3参考链接https://www.jianshu.com/p/3bbf0675cfcehttps://blog.csdn.net/loveliuzz/article/details/79135583其中都有一些错误，还需要看着源码纠正一下。 python 文件IOpython中的三个读read(),readline()和readlines().read() 每次读取整个文件，它通常用于将文件内容放到一个字符串变量中，然而 .read() 生成文件内容最直接的字符串表示，但对于连续的面向行的处理，它却是不必要的.readlines()之间的差异是后者一次读取整个文件，象 .read()一样。.readlines()自动将文件内容分析成一个行的列表，该列表可以由 Python 的 for… in … 结构进行处理.readline()每次只读取一行 python打开多个文件123with open('a.txt', 'r') as a, open('b.txt', 'r') as b: print(a.read()) print(b.read()) 123456789101112131415161718192021222324252627282930313233343536373839404142#!/usr/bin/env python# coding: utf-8class open_many: def __init__(self, files=None, mode='r'): if files is None: self._files = [] else: self._files = files self.mode = mode self.fds = [] # fd is short for file descriptor def __enter__(self): print('--&gt;enter') for f in self._files: print('--&gt;opening file') self.fds.append(open(f, self.mode)) return self.fds def __exit__(self, exc_type, exc_val, traceback): print('--&gt;exit') for fd in self.fds: print('--&gt;closing file') fd.close() if exc_type == ValueError: print('--&gt;exception: ' + str(exc_val)) return Trueif __name__ == '__main__': print('') with open_many(['a.txt', 'b.txt'], 'r') as files: for f in files: print f.read() print('') with open_many() as files: raise ValueError('captured') print('') with open_many() as files: raise Exception('uncaptureable') python csv在使用常规的读取文件的方法的时候，出现了问题，每一个数字包括小数点都被当成了一个字符，这样明显是不对的，对于数字的csv，要考虑下这个方法，我感觉应该和写的方式有关，待续。参考链接https://www.cnblogs.com/dmir/p/5009075.html1234with open(path) as f: f_csv = csv.reader(f) headers = next(f_csv) for i in f_csv: 12import pandas as pdcsv_data = pd.read_csv(path) 优化器优化器与模型参数完全共享内存，一个改变，另一个会立即跟着改变。不能重复加载同一个参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138# 第一种optimizer = t.optim.Adam(model.parameters(), lr = 0.1)optimizerAdam (Parameter Group 0 amsgrad: False betas: (0.9, 0.999) eps: 1e-08 lr: 0.1 weight_decay: 0)for i in optimizer.param_groups: print(i) print('______') &#123;'params': [Parameter containing:tensor([[[[ 0.0493, 0.1696, 0.0647], [ 0.1935, 0.3102, -0.0871], [-0.2787, 0.0894, -0.0438]]], [[[-0.2671, 0.2079, 0.2474], [ 0.2068, -0.1825, 0.1427], [-0.0853, -0.1799, -0.2465]]]]), Parameter containing:tensor([-0.3158, 0.1429]), Parameter containing:tensor([[[[ 0.2063, 0.0771, 0.1579], [ 0.1543, 0.1374, -0.1951], [-0.1221, 0.0099, -0.1331]], [[-0.1899, 0.1978, 0.1065], [ 0.1400, -0.0740, 0.0397], [-0.2165, -0.0180, 0.1072]]], [[[ 0.0692, -0.1296, 0.0524], [ 0.0577, -0.1184, 0.0697], [ 0.0859, -0.2086, 0.0419]], [[-0.0270, 0.1836, -0.0649], [ 0.1680, -0.1061, -0.2357], [-0.0408, 0.0799, 0.0065]]]]), Parameter containing:tensor([ 0.1269, -0.1582]), Parameter containing:tensor([[[[ 0.0185, -0.2579, -0.1185], [ 0.1269, 0.0274, 0.1019], [ 0.0329, -0.1229, -0.1922]]]]), Parameter containing:tensor([-0.2731])], 'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False&#125;______ # 第二种optimizer = t.optim.Adam([&#123;'params':model.net1.parameters(),'lr':0.4&#125;, &#123;'params':model.net2.parameters(),'lr':0.1&#125;],lr=0.04)optimizerAdam (Parameter Group 0 amsgrad: False betas: (0.9, 0.999) eps: 1e-08 lr: 0.4 weight_decay: 0Parameter Group 1 amsgrad: False betas: (0.9, 0.999) eps: 1e-08 lr: 0.1 weight_decay: 0)for i in optimizer.param_groups: print(i) print('______')&#123;'params': [Parameter containing:tensor([[[[ 0.0493, 0.1696, 0.0647], [ 0.1935, 0.3102, -0.0871], [-0.2787, 0.0894, -0.0438]]], [[[-0.2671, 0.2079, 0.2474], [ 0.2068, -0.1825, 0.1427], [-0.0853, -0.1799, -0.2465]]]]), Parameter containing:tensor([-0.3158, 0.1429])], 'lr': 0.4, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False&#125;______&#123;'params': [Parameter containing:tensor([[[[ 0.2063, 0.0771, 0.1579], [ 0.1543, 0.1374, -0.1951], [-0.1221, 0.0099, -0.1331]], [[-0.1899, 0.1978, 0.1065], [ 0.1400, -0.0740, 0.0397], [-0.2165, -0.0180, 0.1072]]], [[[ 0.0692, -0.1296, 0.0524], [ 0.0577, -0.1184, 0.0697], [ 0.0859, -0.2086, 0.0419]], [[-0.0270, 0.1836, -0.0649], [ 0.1680, -0.1061, -0.2357], [-0.0408, 0.0799, 0.0065]]]]), Parameter containing:tensor([ 0.1269, -0.1582]), Parameter containing:tensor([[[[ 0.0185, -0.2579, -0.1185], [ 0.1269, 0.0274, 0.1019], [ 0.0329, -0.1229, -0.1922]]]]), Parameter containing:tensor([-0.2731])], 'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False&#125;______optimizer.state_dict()&#123;'state': &#123;&#125;, 'param_groups': [&#123;'lr': 0.4, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [140617843939944, 140617843755120]&#125;, &#123;'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [140617843755192, 140617843755264, 140617843755336, 140617843755408]&#125;]&#125;# 第三种optimizer = t.optim.Adam([&#123;'params':model.net1.parameters(),'lr':0.4&#125;])optimizer.add_param_group(&#123;'params':model.net2.parameters(),'lr':0.3&#125;)# 第四种for param_group in optimizer.param_groups: param_group['lr']=lr_new 模型保存123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 只保留模型参数并且加载t.save(model.state_dict(),'model_state_dict')model.state_dict()OrderedDict([('net1.weight', tensor([[[[-0.3164, -0.2508, -0.3294], [ 0.2388, -0.1582, 0.0678], [ 0.0194, 0.1120, 0.2794]]], [[[-0.2425, 0.0833, -0.0842], [ 0.0687, -0.0637, -0.3034], [-0.3268, -0.1049, -0.0286]]]])), ('net1.bias', tensor([ 0.2742, 0.2194])), ('net2.weight', tensor([[[[ 0.2241, 0.2280, -0.0597], [-0.1045, -0.1610, 0.0445], [-0.1772, -0.0639, -0.0172]], [[ 0.0975, -0.0081, 0.0690], [-0.1273, 0.0693, 0.1792], [ 0.0773, 0.1652, -0.1688]]], [[[-0.2314, 0.0494, -0.0648], [-0.1919, 0.2145, 0.0369], [-0.1336, -0.1077, -0.0743]], [[ 0.1510, -0.0868, -0.1766], [-0.1764, 0.0398, 0.2146], [-0.0269, 0.1241, -0.2304]]]])), ('net2.bias', tensor(1.00000e-02 * [-7.3981, -0.8345]))])temp = t.load('model_state_dict.pth')OrderedDict([('net1.weight', tensor([[[[-0.3164, -0.2508, -0.3294], [ 0.2388, -0.1582, 0.0678], [ 0.0194, 0.1120, 0.2794]]], [[[-0.2425, 0.0833, -0.0842], [ 0.0687, -0.0637, -0.3034], [-0.3268, -0.1049, -0.0286]]]])), ('net1.bias', tensor([ 0.2742, 0.2194])), ('net2.weight', tensor([[[[ 0.2241, 0.2280, -0.0597], [-0.1045, -0.1610, 0.0445], [-0.1772, -0.0639, -0.0172]], [[ 0.0975, -0.0081, 0.0690], [-0.1273, 0.0693, 0.1792], [ 0.0773, 0.1652, -0.1688]]], [[[-0.2314, 0.0494, -0.0648], [-0.1919, 0.2145, 0.0369], [-0.1336, -0.1077, -0.0743]], [[ 0.1510, -0.0868, -0.1766], [-0.1764, 0.0398, 0.2146], [-0.0269, 0.1241, -0.2304]]]])), ('net2.bias', tensor(1.00000e-02 * [-7.3981, -0.8345]))])model2.state_dict()OrderedDict([('net3.weight', tensor([[[[ 0.2793, -0.2330, 0.3270], [-0.1419, 0.1562, 0.1875], [-0.0249, 0.1297, 0.1642]]], [[[ 0.2770, 0.1016, -0.1096], [ 0.1929, 0.0210, 0.1722], [ 0.1304, 0.0820, 0.1205]]]])), ('net3.bias', tensor([-0.3235, -0.1770])), ('net4.weight', tensor([[[[-0.2043, -0.1492], [ 0.1728, -0.1069]], [[-0.2903, 0.3385], [ 0.2778, 0.1589]]], [[[-0.1423, -0.0439], [ 0.2849, -0.0840]], [[ 0.0354, 0.1711], [-0.0274, -0.2220]]]])), ('net4.bias', tensor([-0.0264, -0.1094]))])model2.load_state_dict(temp)Missing key(s) in state_dict: "net3.weight", "net3.bias", "net4.weight", "net4.bias". Unexpected key(s) in state_dict: "net1.weight", "net1.bias", "net2.weight", "net2.bias". # 印证了在保存模型参数的时候是根据名字进行保存，# 保留模型t.save(model,'model.pth')temp2 = t.load('model.pth')Nettest( (net1): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (net2): Sequential( (0): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)) ))加载进来就是一个模型，包括forward什么的都还在。 numpy和list和tensor对于size的访问区别 list: 只有len()方法，返回的是最外层的个数，reshape方法 numpy: b.size是全部个数，b.shape是(2,3) b = np.arange(6).reshape(2,3), b.resize(2,3) np.arange(1,6,2) tensor: c.shape==c.size() len(c)==c.size(0) 返回torch.size([3,2]), c.resize_(4,4)(可以改变自身尺寸) c.resize(1,4)（来源于torchvision，可以忽略）==c.reshape(1,4)(对于连续地址共享内存，不连续地址则复制)==c.view(1,4)(共享内存) t.arange(1,6,2) t_.unsqueeze(1)tensor的普通索引基本共享内存，而高级索引基本不共享内存。 numpy–&gt;tensor t_ = t.from_numpy(numpy_)(共享内存）或者 t_ = t.tensor(numpy_)(返回新对象) tensor–&gt;numpy np_ = t_.numpy()(共享内存) 或者 np_ = np.array(t_) numpy–&gt;list list_ = np_.tolist()(不共享内存） list–&gt;numpy np_ = np.array(list_)(不共享内存） tensor–&gt;list list_ = t_.tolist() (不共享内存) 或者 item_ = t_.item() (不共享内存） list–&gt;tensor t_ = t.tensor(list_)(不共享内存）也就是说numpy和tensor可以做到互相共享内存，而list只是一个对外的和Python相连接的一个形式.补充：基于numpy和tensor，推荐使用shape属性， 修改形状则分别使用reshape()和view(), 123456789101112131415161718192021222324import numpy as npimport torch as t# a,b,c共享内存a = np.ones([2,3])b = t.from_numpy(a)c = b.numpy()# a,b,c不共享内存a = np.ones([2,3])b = t.tensor(a)c = np.array(b)import numpy as np# 不共享内存a = np.ones([2,3])b = a.tolist()c = np.array(b)import tensor as ta = t.tensor([2,3])b = a.item()b = a.tolist()c = t.tensor(b) torch.Tensor 和 torch.tensor的区别暂时还没有组织好的语言，先以代码的形式记录下来主要是类型和对0维元素的区别。1234567891011121314151617181920212223242526In [12]: import torch as tIn [13]: t.Tensor(3)Out[13]: tensor([-4.8232e+13, 4.5581e-41, -1.8931e-03])In [14]: t.Tensor([3,4])Out[14]: tensor([ 3., 4.])In [15]: t.tensor(3)Out[15]: tensor(3)In [16]: t.tensor([3,4])Out[16]: tensor([ 3, 4])In [17]: a = t.tensor([3,4])In [18]: type(a)Out[18]: torch.TensorIn [19]: a.dtypeOut[19]: torch.int64# Tensor只是tensor(dtype=float)的别名。x = torch.tensor([3.0],requires_grad=True)# torch.Tensor不接收requires_grad参数# torch.tensor只有是float型参数的情况下才接受requires_grad参数 tensor.new()和tensor.data.new()暂时不知道这两者的区别，但是书上的代码多数都是tensor.data.new() topk的用法output.data[0].topk(1)[1][0].item()123456789101112131415161718In [20]: x = t.arange(1,6)In [21]: y = t.topk(x,2)In [22]: type(y)Out[22]: tupleIn [23]: yOut[23]: (tensor([ 5., 4.]), tensor([ 4, 3]))In [24]: y[1]Out[24]: tensor([ 4, 3])In [25]: y[1][1]Out[25]: tensor(3)In [26]: y[1][1].item()Out[26]: 3 ipython和jupyter和pycharm在写代码的前期，用jupyter好使，因为对于不确定的比较多的代码是可以直接看到结果，对某一段进行调试，检查某一段的基本语法错误，或者对于某个想法的实现，是简单直接的甚至对于中型代码，用代码框可以实现视觉上的分离，逻辑清晰，并且支持markdown的记录与注释，对于不了解的代码有很好的支持性。但是在写代码的后期，jupyter的弊端逐渐显现，不能使用模板，init.py的生成不好使，文件与文件夹的关系不清晰。甚至一个简单的文件或者文件夹挪动位置都很麻烦，需要桌面的辅助。而pycharm对于文件管理，init.py等有很好的支持性。更适合写已经成熟的代码。这一下难住我了，作为新手，肯定每次都要实验好些代码，看输入输出的效果，如果是pycharm则比较麻烦，对于调试很啰嗦。命令行窗口做为补充，也不好使，因为每次能看到的东西有限，重复性差，只能用于单句代码的验证。所以不妨这样，前期开发还是用jupyter，等开发的差不多了，甚至等单个文件已经开发完毕，这样的话开发就可以先不管文件夹的事，等各个文件开发完毕，再转成pycharm，来实现文件夹、文件的组织管理和后期的调试，这是因为现在多数不使用jupyter直接运行，而是使用py进行运行。我觉得应该有很多人用.ipynb进行运行，但是我不知道怎么才能更好的运行。 anaconda虚拟环境不错 python如果在 Python 脚本文件首行输入#!/usr/bin/env python，那么可以在命令行窗口中执行/path/to/script-file.py以执行该脚本文件。使用三引号(‘’’或”””)可以指定一个多行字符串。]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2018%2F09%2F13%2Fnumpy%2F</url>
    <content type="text"><![CDATA[简介参考链接NumPy Ndarray对象这只是简单的入门，以后接触得越多，对于其中的理解也才会更加全面，并做补充。12345678910111213141516171819# 1.简单的array对象a = np.array([1, 2, 3], dtype = complex)# 2.结构数组形式的array对象student = np.dtype([('name','S20'), ('age', 'i1'), ('marks', 'f4')]) a = np.array([('abc', 21, 50),('xyz', 18, 75)], dtype = student) print(a)[('abc', 21, 50.0), ('xyz', 18, 75.0)]a[0]['name']# 3.其他形式a.shapea.reshape(2,4,3) x.itemsize 每个元素的字节单位长度numpy.frombuffer() 怎么理解？？np.fromiter(iter(list))numpy.arange(start, stop, step, dtype)numpy.linspace(start, stop, num, endpoint, retstep, dtype)numpy.logscale(start, stop, num, endpoint, base, dtype) numpy的保存和读取，这里还是有点东西的，待续12345678910# numpy在save和load的时候没有显式保存变量名np.save("A.npy",A)B=np.load("A.npy")Bnp.savez("files.npz",A,B,C_array=C)np.savez_compressed("files.npz",A,B,C_array=C)D=np.load("files.npz")D['arr_1']D['C_array'] numpy savetxt() loadtxt12np.savetxt('a.txt',a,fmt='%d', delimiter=',')b= np.loadtxt('a.txt',delimiter=',') 12np.savetxt('a.csv',a,fmt='%d', delimiter=',')b= np.loadtxt('a.csv',delimiter=',') 索引ndarray对象中的元素遵循基于零的索引。 有三种可用的索引方法类型： 字段访问，基本切片和高级索引，可以使用省略号当全部切片只是返回一个视图，高级索引返回数据的副本，并且切片是全取，而高级索引是取对应的位置的元素y = x[1:3,1:2]:两行元素,y = x[[1,2],[1,3]]:两个元素布尔索引是数据的复制广播法则数组上的迭代 for i in np.nditer(a,order=’C’ or order=’F’), np.nditer(a).next()广播迭代 reshape：不改变数据的条件下修改形状flat：数组上的一维迭代器， for i in a.flat , a.flat[2:4], 暂时看不出来flat和nditer的区别flatten: 返回折叠为一维的数组副本ravel： 返回连续的展开数组 flattten和ravel的区别暂时不知道在哪numpy.rollaxis：这里的转轴有问题，比较好的理解是[[[000,001],[010,011]],[[100,101],[110,111]]]，[参考：]https://blog.csdn.net/liaoyuecai/article/details/80193996，还有一种理解是从页，行，列的方式，每次都在原数组上以固定页，固定行的方式进行读取，保证所有的数字以一列的方式，即总是表示成000,001,010,011,100,101,110,111，再想一个更简单的方法。跨括号法，不扯numpy.swapaxes(arr, axis1, axis2) 修改维度序号 维度和描述 broadcast 产生模仿广播的对象 b = np.broadcast(x,y) c = np.empty(b.shape) c.flat = [u + v for (u,v) in b],并且np.nditer()也可以达到相同的效果 broadcast_to 将数组广播到新形状 numpy.broadcast_to(array, shape, subok) expand_dims 扩展数组的形状numpy.expand_dims(arr, axis) squeeze 从数组的形状中删除单维条目 y = np.squeeze(x,axis=(0,1)) 数组的连接序号 数组及描述 concatenate 沿着现存的轴连接数据序列，不产生新的轴 stack 沿着新轴连接数组序列，产生新的轴 hstack 水平堆叠序列中的数组(列方向) vstack 竖直堆叠序列中的数组(行方向)数组分割序号 数组及操作 split 将一个数组分割为多个子数组 hsplit 将一个数组水平分割为多个子数组(按列) vsplit 将一个数组竖直分割为多个子数组(按行) 添加/删除元素序号 元素及描述 resize 返回指定形状的新数组 append 将值添加到数组末尾 insert 沿指定轴将值插入到指定下标之前 delete 返回删掉某个轴的子数组的新数组 unique 寻找数组内的唯一元素 切片的新表达式： np.s_[::2]位运算：跳过 字符串函数：对dtype为numpy.string_或numpy.unicode_的数组执行向量化字符串操作 add() 返回两个str或Unicode数组的逐个字符串连接 multiply() 返回按元素多重连接后的字符串 center() 返回给定字符串的副本，其中元素位于特定字符串的中央 capitalize() 返回给定字符串的副本，其中只有第一个字符串大写 title() 返回字符串或 Unicode 的按元素标题转换版本 lower() 返回一个数组，其元素转换为小写 upper() 返回一个数组，其元素转换为大写 split() 返回字符串中的单词列表，并使用分隔符来分割 splitlines() 返回元素中的行列表，以换行符分割 strip() 返回数组副本，其中元素移除了开头或者结尾处的特定字符 join() 返回一个字符串，它是序列中字符串的连接 replace() 返回字符串的副本，其中所有子字符串的出现位置都被新字符串取代 decode() 按元素调用str.decode encode() 按元素调用str.encode dtype???add()，subtract()，multiply()和divide()排序quicksort, mergesort, heaqsortdt = np.dtype([(‘name’, ‘S10’),(‘age’, int)])a = np.array([(“raju”,21),(“anil”,25),(“ravi”, 17), (“amar”,27)], dtype = dt)print(np.sort(a, order = ‘name’))numpy.argsort()numpy.lexsort()np.argmax() np.argmin()np.nonzero()np.where()np.extract() 改变形状： b.shape = 3,2无复制： b = a 值和形状都是共享，id相同浅复制: b = a.view() 值共享，形状不共享，id不同 切片也是浅复制深复制：ｂ＝ a.copy() numpy.matlib 矩阵库，返回的是矩阵matrix对象，而不是ndarray对象 .empty(), .zeros(), .ones(), eye(), identity(), rand() 只能是二维的np.matrix(‘1,2;3,4’) np.matirx([[1,2],[3,4]]) array和asarray都可以将结构数据转化为ndarray，但是主要区别就是当数据源是ndarray时，array仍然会copy出一个副本，占用新的内存，但asarray不会。matrix和array互换： np.matrix(np.array) np.array(np.matrix),此时两者值和形状没有关系，使用asmatrix和asarray时，值共享，形状不共享暂时没有想到matrix的意义 线性代数numpy.linalg貌似可以直接作用于列表 dot 两个数组的点积，也就是矩阵式乘法 np.dot(a,b)，对于多维数组的乘法，可以同样以XXX0.和XXX.0的方法对于两个1维数组，是点积，对于两个矩阵，是矩阵乘法，对于1维数组和矩阵，则对1维数组适当地转置，然后进行矩阵乘法，需要满足倒数第一维和倒数第二维相等。 vdot 两个向量的点积，也就是矩阵式对应元素的乘积和 inner 两个数组的内积 matmul 两个数组的矩阵积 determinant 数组的行列式 numpy.linalg.det() solve 求解线性矩阵方程 inv 寻找矩阵的乘法逆矩阵 Matplotlibfrom matplotlib import pyplot as plt numpy转化数据类型123456789101112131415In [11]: arr = np.array([1,2,3,4,5])In [12]: arrOut[12]: array([1, 2, 3, 4, 5])# 该命令查看数据类型In [13]: arr.dtypeOut[13]: dtype('int64')In [14]: float_arr = arr.astype(np.float64)# 等价于，即str和data-type是一样的float_arr = arr.asdtype('float64')#该命令查看数据类型In [15]: float_arr.dtypeOut[15]: dtype('float64') 字符串数组转化为数值型123456789In [4]: numeric_strings = np.array(['1.2','2.3','3.2141'], dtype=np.string_)In [5]: numeric_stringsOut[5]: array(['1.2', '2.3', '3.2141'], dtype='|S6')# 此处写的是float 而不是np.float64, Numpy很聪明，会将python类型映射到等价的dtype上# # 这里的float是Python的数据类型，NumPy会自动的将其映射到等价的dtype上，即np.float64In [6]: numeric_strings.astype(float)Out[6]: array([ 1.2, 2.3, 3.2141]) 所以astype一共可以接受三种参数 第一种是dtype，即np.int32这种， 第二种是字符串，即’int32’这样，与第一种相呼应， 第三种是Python的数据类型，会自动转化。 numpy中的数据类型转换，不能直接改原数据的dtype! 只能用函数astype()。[参考链接]https://www.cnblogs.com/hhh5460/p/5129032.html123456789101112131415161718192021# 如果直接修改dtype，会导致长度发生改变&gt;&gt;&gt; a.dtype = 'float16'&gt;&gt;&gt; aarray([ -9.58442688e-05, 7.19000000e+02, 2.38159180e-01, 1.92968750e+00, nan, -1.66034698e-03, -2.63427734e-01, 1.96875000e+00, -1.07519531e+00, -1.19625000e+02, nan, 1.97167969e+00, -1.60156250e-01, -7.76290894e-03, 4.07226562e-01, 1.94824219e+00], dtype=float16)&gt;&gt;&gt; a.shape(16,)&gt;&gt;&gt; a.dtype = 'float16'&gt;&gt;&gt; aarray([ -9.58442688e-05, 7.19000000e+02, 2.38159180e-01, 1.92968750e+00, nan, -1.66034698e-03, -2.63427734e-01, 1.96875000e+00, -1.07519531e+00, -1.19625000e+02, nan, 1.97167969e+00, -1.60156250e-01, -7.76290894e-03, 4.07226562e-01, 1.94824219e+00], dtype=float16)&gt;&gt;&gt; a.shape(16,) 对于字符串数组还没有找到合理的说明，sad np.in1d(x,y)123456789101112&gt;&gt;&gt; test = np.array([0, 1, 2, 5, 0])&gt;&gt;&gt; states = [0, 2]&gt;&gt;&gt; mask = np.in1d(test, states)&gt;&gt;&gt; maskarray([ True, False, True, False, True], dtype=bool)&gt;&gt;&gt; test[mask]array([0, 2, 0])&gt;&gt;&gt; mask = np.in1d(test, states, invert=True)&gt;&gt;&gt; maskarray([False, True, False, True, False], dtype=bool)&gt;&gt;&gt; test[mask]array([1, 5]) np.argsort()123456789101112x = np.array([3, 1, 2])y = np.argsort(x)array([1, 2, 0])In [48]: xxOut[48]: array([3, 1, 2])In [49]: yyOut[49]: array([1, 2, 0])In [50]: xx[yy]Out[50]: array([1, 2, 3]) np.setdiff1d()这种是以集合的方式，会把列表先压平，@return: sorted 1D array1234&gt;&gt;&gt; a = np.array([1, 2, 3, 2, 4, 1])&gt;&gt;&gt; b = np.array([3, 4, 5, 6])&gt;&gt;&gt; np.setdiff1d(a, b)array([1, 2]) np.argwhere()@return: index_array12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; x = np.arange(6).reshape(2,3)&gt;&gt;&gt; xarray([[0, 1, 2], [3, 4, 5]])&gt;&gt;&gt; np.argwhere(x&gt;1)array([[0, 2], [1, 0], [1, 1], [1, 2]])In [32]: x[0,2]Out[32]: 2In [26]: x[[0,1],[2,0]]Out[26]: array([2, 3])In [29]: z = zip([0,1],[2,0])In [30]: for i in z: ...: print(i) ...: (0, 2)(1, 0)In [53]: x[(0,1),(2,0)]Out[53]: array([2, 3])In [73]: z = list(zip(*y))In [74]: x[z]Out[74]: array([2, 3, 4, 5]) np.setdiff1d(x,y),intersect1d(x,y)集合的减法运算,交集运算 np.random.choice(5,3)123a = np.random.choice(5,3)aarray([0, 3, 2])]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch chapter8 neural style]]></title>
    <url>%2F2018%2F09%2F11%2Fpytorch-chapter8-neural-style%2F</url>
    <content type="text"><![CDATA[1 前言本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。这是我的代码大神链接：(https://github.com/anishathalye/neural-style) 2 问题及其解决我在第六章和第七章的时候还是基于pytorch 0.4.0，而第八章的时候我开始基于pytorch 0.4.1，所以以下的内容介绍都是基于0.4.1 2.1 文件组织形式123456789101112131415161718192021├─checkpoints/├─content_img/│ ├─input.jpg│ ├─output.jpg│ └─style.jpg├─data/│ ├─coco/a.jpg├─dataset/│ ├─__init__.py│ └─dataset.py├─models/│ ├─__init__.py│ └─PackedVGG.py│ └─transformer_net.py└─utils/│ ├─__init__.py│ └─utils.py│ └─visualize.py├─config.py└─main.py 其中，上半部分是对数据和模型的保存组织形式，我们只需要能对应起来即可，其中，checkpoints是为了保存模型，content_img中的style.jpg是训练时候的风格图片，input.jpg是测试的输入，output.jpg是测试的输出，data中的数据是训练数据，主要是因为这个训练数据太整齐，是用ImageFolder读取的，为了避免麻烦，也为了在测试的时候方便观察图片，所以style.jpg我们暂时放在了content中。下半部分是重点，我们需要写的代码，每次都是先从dataset.py和models开始写起，然后导入visualize.py，这个文件基本不会发生改变，然后同时写main.py和config.py，边写边扩展utils中的其他文件，例如main中用到的函数等等。 2.2 modelsPackedVGG.py这里我们主要是取已有的网络，得到中间层的输出models.named_parameters():返回的是一个生成器，每次返回一个参数的关键字和值models.state_dict():返回的是一个字典，记录了参数的关键字和值models.parameters():返回的是变量，没有名字，可以在requires_grad中用到models.features返回的是相对应的模型1234567891011121314151617181920212223242526272829In [7]: from torchvision.models import vgg16In [8]: models = vgg16(pretrained=True)In [9]: model = models.features[:1]In [10]: modelOut[10]: Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))In [11]: models.parameters()Out[11]: &lt;generator object Module.parameters at 0x7f8fad26b3b8&gt;In [12]: models.named_parameters()Out[12]: &lt;generator object Module.named_parameters at 0x7f8f29e99d58&gt;In [13]: model.named_parameters()Out[13]: &lt;generator object Module.named_parameters at 0x7f8fad26b2b0&gt;In [14]: model.parameters()Out[14]: &lt;generator object Module.parameters at 0x7f8fad26b4c0&gt;In [15]: model.state_dict()Out[15]: OrderedDict([('0.weight', tensor([[[[-0.5537, 0.1427, 0.5290], [-0.5831, 0.3566, 0.7657], [-0.6902, -0.0480, 0.4841]], 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107from torchvision.models import vgg16models = vgg16(pretarined = True)In [19]: modelsOut[19]: VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace) (2): Dropout(p=0.5) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace) (5): Dropout(p=0.5) (6): Linear(in_features=4096, out_features=1000, bias=True) ))In [20]: models.featuresOut[20]: Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))In [21]: models.features[1]Out[21]: ReLU(inplace)# listIn [27]: models4 = models2[0:2]In [28]: models4Out[28]: Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace))In [32]: models4listOut[32]: [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace)]In [36]: models4list[1]Out[36]: ReLU(inplace)In [37]: models4list[1].named_parametersOut[37]: &lt;bound method Module.named_parameters of ReLU(inplace)&gt; sequencial是支持索引操作的list(module)会变成一个list，可以通过索引来获取层，注意，nn.ModuleList, nn.Sequential, nn.Conv等都是Module,都可以通过named_parameters来获取参数。为了能够提取出中间层的输出，作者换了一个方法，用的nn.ModuleList,nn.ModuleList和nn.Sequential的区别在此才真正显现，nn.Sequential更有利于直接把输入传给Module，计算是一个整体，写起来更方便，而nn.Modulist则不能直接把输入传给Module，需要用循环传输入，更有利于在层中做一些保留，提取中间层的输出。后面我们会讲到hook。或者说提取中间层的输出我们可以选择在定义网络的forward中进行，另外，就是需要注意的是，这里的输入是一个batch_size大小的矩阵，所以即便像作者这样，用一个列表保存输出，但实际输出的列表中的元素都是(b,n,h,w)大小的。后面我会验证。 提取中间层的输出有两种方法：第二种方法参考链接：https://www.jianshu.com/p/0a23db1df55a12345678910# 第一种方法，这种方法是在前向网络中提取输出，好像也是在反向传播网络中，但这种提取中间层是永久性的，也适合用这些层的做其他运算，这些运算是计算在整体网络框架中的def forward(self, x): results = [] for ii, model in enumerate(self.features): x = model(x) if ii in &#123;3, 8, 15, 22&#125;: results.append(x) vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3']) return vgg_outputs(*results) 12345# 第二种方法，适合在在不影响整体网络的情况下拿出一个分支进行单独计算，现在还不清楚这样子会不会影响backward，个人感觉会，因为也是相当于一个变量对其进行计算，导数为1。def forward(self, x): x= self.model(x) self.fea = x x = self.main(x) transformer.py可参考链接 padding的操作是边界反射补充 放大方法是双线性插值，而不是ConvTransposed2d，即unsample或者说是interpolate， 但是其中的一个参数align_corners一直没有理解，既然是双线性插值，那结果就是固定的，怎么还会因为其他参数发生变化。 其中，写的时候必要的时候可以写写子网络这里我对residualblock提出了疑问，事实上left+right后面可以没有relu层，这一点我们可以从以下链接找到说明。https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.pyhttp://torch.ch/blog/2016/02/04/resnets.html The above result seems to suggest that it’s important to avoid changing data that passes through identity connections only. We can take this philosophy one step further: should we remove the ReLU layers at the end of each residual block? ReLU layers also perturb data that flows through identity connections, but unlike batch normalization, ReLU’s idempotence means that it doesn’t matter if data passes through one ReLU or thirty ReLUs. When we remove ReLU layers at the end of each building block, we observe a small improvement in test performance compared to the paper’s suggested ReLU placement after the addition. However, the effect is fairly minor. More exploration is needed. 对于其他的出现的网络架构，其实都是有理可循的，但暂时不是本篇的重点，所以只做一个记录。上卷积简单地看了看这篇论文，unsample要比ConvTransposed2D要好，但是没有看懂。留作后续。 dataset.py &amp; visualize.py因为加载数据是用的tv.datasets.ImageFolder，所以dataset.py不需要写，visualize.py是第六章的时候写好的，这里只写几个改进的 self.vis = Visdom(env=env,use_incoming_socket=False, **kwargs)，这里的use_incoming_socket是不需要从浏览器接受数据到软件中，如果没有的话会提示 ‘&gt;’ not supported between instances of ‘float’ and ‘NoneType’ 在一个函数前提示输入的大小和类型是一件很重要的事情，必要的时候需要输入分布， 这里的plot用了一个很巧的方法，用字典记录不同的点123self.index = &#123;&#125;x = self.index.get(win,0)self.index[win] = x+1 其他的细节可以看代码中的记录，应该比较清晰了。 main.py &amp; utils.py &amp; config.py其中utils主要为main提供一些用到的函数，config提供参数，main作为主函数，里面主要就是train(),val(),test(),help(),下面记录一些写main函数的一些疑问。 cuda这里写几种怎么从cpu到gpu的方法以及应用场景。123456789101112131415161718192021# 第一种device = t.device('cuda') if opt.use_gpu else t.device('cpu')models.to(device)tensor = tensor.to(device)此时使用默认的cuda，一般是cuda:0，适用于全局# 第二种torch.cuda.current_device() # 查询当前GPUtorch.cuda.set_device(1)device = torch.device('cuda')models.to(device)此时用的是cuda:1，使用于全局#第三种#上下文管理器with torch.cuda.device(1): models.to(device)#第四种import osos.environ["CUDA_VISIBLE_DEVICES"]="2"没用过 tqdmhttps://blog.csdn.net/langb2014/article/details/54798823?locationnum=8&amp;fps=1进度条，但是只在jupyter和终端中用的时候效果很明显，在代码中用的效果没有那么好，tqdm试了试，用在enumerate()中时，需要写成这样：123456elements = ('a', 'b', 'c')for count, ele in tqdm(enumerate(elements)): print(count, i)# two argumentsfor count, ele in tqdm(enumerate(elements), total=len(train_ids), leave=False): print(count, i) 包括zip也是一样，因为他们返回的是一个生成器，并不知道长度。 反向传播和梯度下降参考链接https://blog.csdn.net/qq_16234613/article/details/80025832这里主要是针对第七章和第八章出现的反向传播和梯度下降出现的问题进行记录。在第七章，是这么实现分别训练的1234567891011121314fake_img = netg(noises).detach() fake_output = netd(fake_img)error_d_fake = criterion(fake_output, fake_labels)error_d_fake.backward()optimizer_d.step()optimizer_g.zero_grad()noises.data.copy_(t.randn(opt.batch_size, opt.nz, 1, 1))fake_img = netg(noises)output = netd(fake_img)error_g = criterion(output, true_labels)error_g.backward()optimizer_g.step() y = x.detach()：表示将生成一个新的叶子节点，值与当前节点的值相同，但是y.requires_grad = False, y.grad_fn=None，此时x和y共享内存，对y数据的操作也会影响x，可以理解为冻结了通过y进行反向传播的路。如果在网络的输出detach，即y= models(x).detach()，可以理解成，models只进行前向传播，grad=None。1234567891011121314151617181920In [17]: a = torch.ones(3,3)In [18]: a.requires_grad=TrueIn [19]: b = a*2In [20]: b.requires_gradOut[20]: TrueIn [21]: b.grad_fnOut[21]: &lt;MulBackward at 0x7f8fac6e40f0&gt;In [22]: c = b.detach()In [23]: c.requires_gradOut[23]: FalseIn [24]: print(c.grad_fn)NoneIn [25]: c.is_leaf 123456789101112131415161718192021222324252627282930313233343536373839In [2]: a = torch.ones(3,3)In [14]: bOut[14]: tensor([[2., 2., 2.], [2., 2., 2.], [2., 2., 2.]], grad_fn=&lt;MulBackward&gt;)In [15]: c = b.detach()In [16]: cOut[16]: tensor([[2., 2., 2.], [2., 2., 2.], [2., 2., 2.]])In [17]: c[0,0]=1In [18]: cOut[18]: tensor([[1., 2., 2.], [2., 2., 2.], [2., 2., 2.]])In [19]: bOut[19]: tensor([[1., 2., 2.], [2., 2., 2.], [2., 2., 2.]], grad_fn=&lt;MulBackward&gt;)In [20]: c.requires_gradOut[20]: FalseIn [21]: b.grad_fnOut[21]: &lt;MulBackward at 0x7f764429ffd0&gt;In [22]: b.grad_fn.next_functionsOut[22]: ((&lt;AccumulateGrad at 0x7f7644428358&gt;, 0),)In [23]: a.grad_fn 在第八章，是这么表示的12for param in vgg16.parameters(): param.requires_grad = False 这种表示可以使得某一个网络不参与梯度下降这个过程，但是对于网络的输入和输出还是支持梯度下降的。requires_grad只是表示当前的变量不再需要梯度下降，综上所述，对于中间变量，需要使用x.detach()，使其变成默认的叶子节点，对于叶子节点，使用x.requires_grad。并且对于中间变量使用requires_grad会报错。 在第八章，还有一种表示方法：1234567with t.no_grad(): features = vgg16(style_img) gram_style = [gram_matrix(feature) for feature in features]@t.no_grad()def stylize(**kwargs): pass 这种方法会使得任何计算得到的结果都是requires_grad = False,暂时不清楚和detach()的区别。也是一种表示只前向传播的方法，不参与反向传播和梯度下降。 train()图片分为两种：风格图片，只需要一张，内容图片，很多，用于训练，这一点没有暂时没有理解为什么这么设置。其中，对输入的图片进行了乘以255，我觉得是因为为了使模型的输出直接就是255，不需要再进行处理，没有验证。ensor.item() tensor.tolist()content_image = tv.datasets.folder.default_loader(opt.content_path)在训练过程中，会发现对于整个训练过程，不仅有神经网络，而且还有自己定义的函数，nn.functional，还有两个损失函数，这是之前没有预料到的。 保存图片1234567# 保存图片的几种方法，第七章的是 # 0-1tv.utils.save_image(fix_fake_imgs,'%s/%s.png' % (opt.img_save_path, epoch),normalize=True, range=(-1,1))# vis.save竟然没找到 我的神 # 0-1vis.img('input')vis.save([opt.env]) utils.py这里的疑问是得到gram矩阵的时候，为什么要除以c*h*w,而不是h*w，虽然源码都是这么写的。 写到这里也还是还要很多疑问，暂时保留。昨天发现训练的过程不对，今天在对比代码的过程中，发现了自己写代码的一些漏洞，主要有 命名不规范：表示同一个东西出现了两个命名，导致了自己在写代码的过程中传参出现了问题，或者是一类东西没有一个规则进行命名，导致自己在写代码的过程中用到之前的变量的时候必须返回去去查找这个变量，效率低且容易出错。 对源码的修改不是很恰当，导致在写上卷积层的输出和源码完全不一致，这个是自己之前没有遇到的。 visdom的运用，我用不同的environment导致结果也不一样，default是之前一直用的，这次换成了test1之后显示的结果就对了。这个暂时还不清楚原因，如果是会保留信息的话，但是plot是重新开始画的，等会测试测试vis的问题。是网络的问题。但是vis.save()的介绍是序列化信息，暂时还没有理解。 ## 对单张图片进行加载验证content_image = tv.datasets.folder.default_loader(opt.content_path)可以理解成Image.open，看源码就可以知道的 贴两个成果图看看效果。 遗留的问题Gram矩阵为什么可以代表图片风格，这里有个解释(https://arxiv.org/pdf/1701.01036.pdf)，还没来得及看。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>neural style transfer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[data]]></title>
    <url>%2F2018%2F09%2F06%2Fdata%2F</url>
    <content type="text"><![CDATA[Market-1501[参考链接]http://blog.fangchengjin.cn/reid-market-1501.htmlhttps://github.com/RSL-NEU/person-reid-benchmark 6个摄像头 1501个人，其中751个人、12936张图片用于训练，750个人、19732张图片用于测试， 3368张查询图片 目录说明 bounding_box_test 19732张测试图片 0000_c1s1_000151_01.jpg 前缀为 0000 表示在提取这 750 人的过程中DPM检测错的图（可能与query是同一个人），-1 表示检测出来其他人的图（不在这 750 人中） DPM检测出的 gallery样本 bounding_box_train 12936张训练图片 0002_c1s1_000451_03.jpg train样本 query 3368张图片，与test的750人对应 但是是人工绘制的 与bounding_box_test中的图片略微有所不同 与gt_bbox中的图片是一样的 0001_c1s1_001051_00.jpg 为 750 人在每个摄像头中随机选择一张图像作为query，因此一个人的query最多有 6 个，共有 3,368 张图像 query样本 gt_bbox 25259张图片 手工绘制 包含1501个行人 0001_c1s1_001051_00.jpg 手工标注的bounding box，用于判断DPM检测的bounding box是不是一个好的box gt_query 是对3368张图片的查询图片的判定，好坏， 0001_c1s1_001051_00_good.mat matlab格式，用于判断一个query的哪些图片是好的匹配（同一个人不同摄像头的图像）和不好的匹配（同一个人同一个摄像头的图像或非同一个人的图像） 命名规则以 0001_c1s1_000151_01.jpg 为例1） 0001 表示每个人的标签编号，从0001到1501；2） c1 表示第一个摄像头(camera1)，共有6个摄像头；3） s1 表示第一个录像片段(sequece1)，每个摄像机都有数个录像段；4） 000151 表示 c1s1 的第000151帧图片，视频帧率25fps；5） 01 表示 c1s1_001051 这一帧上的第1个检测框，由于采用DPM检测器，对于每一帧上的行人可能会框出好几个bbox。00 表示手工标注框 DukeMTMC-reIDDukeMTMC是多目标多摄像机行人跟踪数据集，8个摄像头，2700多个人物，DukeMTMC-reID是DukeMTMC的行人重识别子集，并且提供了人工标注的bounding box。从视频中每 120 帧采样一张图像，得到了 36,411 张图像。一共有 1,404 个人出现在大于两个摄像头下，有 408 个人 (distractor ID) 只出现在一个摄像头下[参考链接]:http://blog.fangchengjin.cn/reid-duke.html 目录结构 bounding_box_test 0002_c1_f0044158.jpg 用于测试集的 702 人 包含 17,661 张图像（随机采样，702 ID + 408 distractor ID） bounding_box_train 0001_c2_f0046182.jpg 用于训练集的 702 人 包含 16,522 张图像（随机采样） query 0005_c2_f0046985.jpg 为测试集中的 702 人在每个摄像头中随机选择一张图像作为 query，共有 2,228 张图像 。 命名规则0001_c2_f0046182.jpg1） 0001 表示每个人的标签编号；2） c2 表示来自第二个摄像头(camera2)，共有 8 个摄像头；3） f0046182 表示来自第二个摄像头的第 46182 帧。 CUHK03CUHK03是第一个足以进行深度学习的大规模行人重识别数据集，该数据集的图像采集于香港中文大学（CUHK）校园。数据以”cuhk-03.mat”的 MAT 文件格式存储，含有 1467 个不同的人物，由 5 对摄像头采集。[参考链接]http://blog.fangchengjin.cn/reid-cuhk03.html 目录结构 detected - 5 * 1 cell 由机器标注，每个 cell 中包含一对摄像头组采集的照片，每个摄像头组由 M x 10 cells 组成，M 为行人索引，前 5 列和后 5 列分别来自同一组的不同摄像头。cell 内每个元素为一幅 H x W x 3 的行人框图像(uint8 数据类型)，个别图像可能空缺，为空集。 843*10 cell 摄像头组pair 1 440*10 cell 摄像头组pair 2 77*10 cell 摄像头组pair 3 58*10 cell 摄像头组pair 4 49*10 cell摄像头组pair 5 labeled - 5 * 1 cell 行人框由人工标注，格式和内容和”detected”相同。 843*10 cell 440*10 cell 77*10 cell 58*10 cell 49*10 cell testsets - 20*1 cell 测试协议，由 20 个 100 x 2 double 类型矩阵组成 (重复二十次) 100*2 double matrix 100 行代表 100 个测试样本，第 1 列为摄像头 pair 索引，第 2 列为行人索引]]></content>
      <categories>
        <category>re-id</category>
      </categories>
      <tags>
        <tag>data</tag>
        <tag>re-id</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inception]]></title>
    <url>%2F2018%2F09%2F04%2FInception%2F</url>
    <content type="text"><![CDATA[关于Inception的好的讲解深度学习卷积神经网络——经典网络GoogLeNet(Inception V3)网络的搭建与实现]]></content>
  </entry>
  <entry>
    <title><![CDATA[deconv\dilated conv]]></title>
    <url>%2F2018%2F09%2F04%2Fdeconv%2F</url>
    <content type="text"><![CDATA[反卷积之前一直以为反卷积和空洞卷积是一回事，后来才发现是两个事情，反卷积是为了能够将小图片生成大图片，空洞卷积是为了扩大感受野。 反卷积多用于图像生成中，例如从特征生成图片，GAN，图像分割等等中，常与conv相对应，也有其他名称，比如: Transposed Convolution, Fractional Strided Convolution。 首先定义符号： 假设本文提到的图形和卷积核都是一维的线或者二维的正方形，$x$和$y$轴方向的padding和stride相同 $i,o,k,p,s,i’,o’,k’,p’,s’$分别表示卷积/反卷积输入图片的大小input size, 输出图片的大小 output size，卷积/反卷积的核大小kernel size，padding，stride. 动图演示https://github.com/TJJTJJTJJ/conv_arithmetic 详细解析 卷积的公式如下：$$ o = \lfloor\frac{i-k+2p}{s}\rfloor+1 $$ 反卷积对应的是直接在原图上填充0$$ o’ = s’(i’-1)+k’-2p’+out_padding $$ 其中涉及到的数学公式非常简单，因为卷积和反卷积都可以对应到一个矩阵相乘上，可以简单地理解成卷积的卷积核矩阵和反卷积的卷积核矩阵互为转置，当然，我们知道，这只是在大小上可以这么理解，其中的数值是不一样的，除非是正交矩阵。 根据动图演示的动态图可以看出： 反卷积的No padding的演示效果就是卷积的full padding，在padding效果上，反卷积和卷积总是以互补的形式出现。 从stride的效果上来看，conv的stride好理解，deconv的stride实际上是在输入图片内加入0 out_padding就是为了应对odd情况，这种情况下，out_padding指得是在padding和stride填充之后，为了能得到预期大小的图片，在输入图片最外面单侧填充0. 空洞卷积扩张卷积的计算公式与上面不一样 扩张卷积扩张的是卷积，在卷积核内部和外部同时填充0.主要是为了扩大感受野，应用在目标分割等上面. 扩张卷积$$ o’=\lfloor\frac{i’-k+2p-(k-1)*(d-1)}{s}\rfloor+1$$ 感受野的计算公式(a) 普通卷积，1-dilated convolution，卷积核的感受野为$3 \times 3 = 9$。(b) 扩张卷积，2-dilated convolution，卷积核的感受野为$7 \times 7 = 49$。(c) 扩张卷积，4-dilated convolution，卷积核的感受野为$15 \times 15 = 225$。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积\反卷积 空洞卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch]]></title>
    <url>%2F2018%2F08%2F26%2Fpytorch-1.0%2F</url>
    <content type="text"><![CDATA[基础知识魔术方法: P23 调试: P2712import ipdbipdb.set_trace() 带下划线_的函数会修改Tensor本身，比如x.add_(y)和x.add(y)的区别 TensorNumpy与Tensor共享内存b = a.numpy() # Tensor -&gt; Numpya = t.from_numpy(a)# Numpy -&gt; Tensorx = x.cuda()tensor的操作:torch.function,tensor.function.普通索引共享内存高级索引不共享内存线性代数函数 P70自动广播原则: unsqueese(view),expand(expand_as)tensor=Tensor+Storage持久化和加载: t.save(a,’a.pth’) b=t.load(‘a.pth’) P77%timeit -n 10 Variable和autogradfrom torch.autograd import Variable三个属性data: 对应Tensorgrad: 梯度，和data大小一样，也是Variablegrad_fn: 指向Function对象,用于构建计算图。用户创建对应叶子节点,grad_fn=None.记录的是它什么操作的输出。Variable的构造函数的关键字参数:requires_grad(bool):是否需要求导; volatile(bool):True表示之上的计算图都不会求导123456x = Variable(t.ones(2,2),requires_grad = True)y = x.sum()y.grad_fny.backward()x.gradx.grad.data.zero_() # 反向传播清零 variable.backward(grad_variable=None, retain_graph=None, create_graph=None)假设用户输入的数据是真实的不需要求导的。数值在前向传导过程成会保存成buffer,计算梯度之后自动清空。多次反向求导可以使用关键字参数retain_graph=Trueretain_graph=True 实现多次反向传播？？？？ 反向传播过程中非叶子节点的导数在计算完之后就会清空，y=x*w,z=y.sum() 其中y.grad会清空。其对应的方法有两种，P92，t.autograd.grad(z,y)和hook扩展Autograd Function：P95 自己实现前向和反向 nn.Module123456789101112131415161718import torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def \_\_init\_\_(): super(Net,self).__init__() \# 有参数的层的定义 def forward(self,x):net = Net()print(net)list(net.parameters())for name,paramenters in net.named_parameters(): print(name,':',parameters.size())out = net(input)net.zero_grad()out.backwad(Variable(t.ones(1,10)))&gt; ??? nn.Sequential()nn.ModuleList()nn.ParameterList()在优化器中为各层分别设置学习率nn.functional对应nn.Module参数初始化:nn.Conv2d: nSamplesnChannelsHeight*Widh单样本: input.unsqueeze(0)model.train()model.eval()前向或者后向注册钩子函数，P125，可以查看中间层。获取网络的模块属性：getattr(module)P128保存模型：t.save(net.state_dict(), ‘net.pth’)加载模型：net2=Net() net2.load_state_dict(t.load(‘net.pth’))多个GPU并行操作 损失函数12345output = net(input)target = Variable(t.arrange(0,10))criterion = nn.MSELoss()loss = criterion(output, target)loss 1234net.zero_grad()print(net.con1.bias.grad)loss.backward()print(net.conv1.bias.grad) 优化器123learning_rate=0.01for f in net.parameters(): f.data.sub_(f.grad.data*leaning_data) 123456789101112import torch.optim as optim# 优化器optimizer = optim.SGD(net.parameters(), lr=0.01)# 训练过程中 梯度清零optimizer.zero_grad() # 等效于 net.zero_grad()# 损失函数output=net(input)loss = criterion(output, target)# 反向传播loss.backward()# 更新参数optimizer.step() CIFAR-10分类 数据预处理:transform,trainset,trainloader,testset,testloader 定义网络:Net(nn.Module),super(Net,self).init(),forward() 定义损失函数和优化器 训练网络 输入数据 梯度清零 前向传播+反向传播 更新参数 数据处理自定义的数据集需要继承Dataset类，并实现两个Python魔术方法__getitem__:返回一个样本。obj[index]=obj.getitem(index)__len__:返回样本数量。transform=T.Compose()trans=T.Lambda(lambda img:img.rotate(randonm()*360))ImageFolder(root,transform,target_transform,loader)P139self.class_to_idx了解label和文件夹名的映射关系DataLoader()定义shuffle等P142取样:P146工具包:torchvision P147 models:训练好模型 dataset:数据集加载 transforms:数据预处理操作，主要针对Tensor和PIL Image对象的操作 make_grid和save_img可视化工具 Tensorboard和visdomtensor_board和TensorboardXvisdom:env pane %env LS_COLORS=None!tree –charset ascii data/dogcatTensor–numpy:np.array(Tensor) torch.Tensor(np.darray)PIL.image–numpy:np.asarray(PIL.image) image.fromarray(numpy.ndarray)PIL.image–Tensor:trans = transforms.Compose([transforms.ToTensor()]) tens = trans(img) ToPILImage() GPUP158with t.cuda.device(1):t.cuda.set_device(1)export CUDA_VISIVLE_DEVICES=1b = t.load(‘a.pth’)c = t.load(‘a.pth’,map_location=lamdba storage, loc: storage)d = t.load(‘a.pth’,map_location={‘cuda:1’:’cuda:0’})Module和Optimizer: state_dict Dog.vs.Catcheckpoints/ 中间模型data/ __init__.py dataset.py def init(self,root,transform=None, train=True, test=False): def getitem(self,index): def len(self): get_data.shmodels/ __init__.py AlexNet.py BasicModule.py ResNet34.pyutils/ __init__.py visualize.pyconfig.pymain.pyrequirements.txtREADME.md main.py def train(**kwargs): def test(**kwargs): def val(model,dataloader): def help(): 123if __name__=='main': import fire fire.Fire() pytorch 中文文档https://pytorch-cn.readthedocs.io/zh/latest/package_references/functional/ PyTorch实战指南 第六章 Dog.VS.Cat这是根据深度学习框架：PyTorch入门与实践这本书的第六章写的代码，是关于猫狗识别的，在这个过程中，一边看，一边写，刚开始是运行作者已经写好的代码，后来自己在jupyter上进行复制的复现，发现import无法导入ipynb文件，在使用了Ipynb_importer.py之后可以实现同一文件内导入ipynb模块，如果是在其他文件中进行导入，会有点费事，以下会记录Ipynb_importer.py的用法。因为费事，自己开始开始使用pycharm+jupyter的方式，直接自己根据作者提供的源码进行编写，在编写的过程中接受作者的思想。用pycharm的不方便的地方是无法直接运行测试，所以采取的是对自己不熟悉的模块或者方法，用jupyter进行测试，而直接编写则是pycharm。但是感觉pycharm还是没有那么好用，可能是自己用的少。我是按照data、model、util、main+config、requirement的顺序编写的。在编写函数的过程中，因为刚开始不理解各个模块是怎么组织起来的，所以都是从简单的开始，所以函数的位置和作者的不一样，其中对于model.save和model.load、vis.plot和vis.log的封装让我感觉很有意思，刚开始是编写的时候只能直接打上问号，因为不懂这么编写的意义，但在编写主函数main的时候才感觉到了这种编写的好处，基本把模型训练和对模型、结果的处理完全分离开，避免了耦合性很强的后果。 Ipynb_importer.py我通过几次测试发现，import Ipynb_importer 只需要放在你的当前要运行的文件中即可，然后在其他文件下的init.py 中导入所有的当前文件夹中的Module，就像这样/first/second/models/—–init.py————- None—–BasicModule.ipynb—–AlexNet.ipynb———-from models.BasicModule import BasicModule /first/main.pyimport Ipynb_importerfrom models import AlexNet 之所以在AlexNet中写models.BasicModule是因为直接导入BasicModule会报错，我根据dict的输出发现有问题，这一点和官网介绍的有一点区别，我没有实现官网说明的跨文件夹导入。因为如果改文件夹导入的话，models.BasicModule要接着换成相应的名字，与我预想的不一致，我预想的是不管在哪里导入，已经导入的应该不受影响才对。 ipynb-py.sh之后发现了这个神器，可以把ipynb转化成.py，还是挺好用的，转化之后也没问题。 同时，借助这次实验，自己对python的掌握也更深了一点。 不过对于网络的构成还是有一些问题，那就是网络为什么这么写，这应该属于理论的东西。还需要进一步加强。 这次实验一共用了三天才完全搞懂，可以说其中涉及到的函数的用法基本都明白了。本意是记录自己，不过如果有任何问题，欢迎交流。 PyTorch实战指南 第七章 DCGAN这一次实现的也比较慢，用了小三天才做完，现在记录一下其中学到的几个东西。 __file__:用来获取模块所在路径 可能是一个相对路径，可能是一个绝对路径，如果当前文件包含在sys.path里面，那么，__file__返回一个相对路径！也可以认为获取模块的名字最后的落脚点一定是XX/XX.py类没有这个属性 12345678In [1]: import numpyIn [3]: numpy.__file__ Out[3]: 'F:\\Programs\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py'In [6]: numpy.random.__file__Out[6]: 'F:\\Programs\\Anaconda3\\lib\\site-packages\\numpy\\random\\__init__.py'$ python test.py ##print(__file__)test.py __name__:__name__就是标识模块的名字的一个系统变量。这里分两种情况：假如当前模块是主模块（也就是调用其他模块的模块），那么此模块名字就是__main__，通过if判断这样就可以执行“__mian__:”后面的主函数内容；假如此模块是被import的，则此模块名字为文件名字（不加后面的.py），通过if判断这样就会跳过“__main__:”后面的内容。这个模块可以是文件夹的名字，可以是类的名字，可以是__mian__”,XX的形式。可以用于获取当前文件的文件名通过上面方式，python就可以分清楚哪些是主函数，进入主函数执行；并且可以调用其他模块的各个函数等等。12345678910111213141516171819# test.py## print(__file__)## print(__name__)# test2.py## import test## print(__file__)## print(__name__)H:\GitHub\pytorch_learn\Chapter7\test.pytesttest2.py__main__## from test import ccc## print(ccc.__name__)## print(__file__)## print(__name__)ccctest2.py__main__ type():返回对象的类型如果是module,则返回module如果是类的实例，则返回类的名称，这个名称以XXX.XXX的形式返回，从import的第一个开始算起。常用于判断数据类型，在pytorch中，用于返回模型名称，这个用法很巧妙，相当于返回了子类的类型名字我觉得没有理解作者是怎么用的。在父类里的type(self) 返回的是子类的类名123456789101112In [1]: import numpyIn [2]: type(numpy)Out[2]: moduleIn [3]: a = numpy.array(1)In [4]: type(a)Out[4]: numpy.ndarrayIn [5]: type(numpy.array)Out[5]: builtin_function_or_method 123456789101112131415161718192021222324252627class A: pass class B(A): pass isinstance(A(), A) # returns Truetype(A()) == A # returns Trueisinstance(B(), A) # returns Truetype(B()) == A # returns Falseclass A(object): def __init__(self): print(type(self)) passclass B(A): def __init__(self): super(B,self).__init__() print(type(self)) passimport testtest.B()&lt;class 'test.B'&gt; &lt;class 'test.B'&gt; __class__:和type类似1234567891011121314151617class A(object): def __init__(self): print(type(self)) print(self.__class__) passclass B(A): def __init__(self): super(B,self).__init__() print(type(self)) print(self.__class__) pass&lt;class 'test.B'&gt;&lt;class 'test.B'&gt;&lt;class 'test.B'&gt;&lt;class 'test.B'&gt; 获取config源码打印参数，方便输入参数inspect.getsource123from inspect import getsourcesource = getsource(opt.__class__)print(source)]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch-learn chenyun</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cmd Markdown]]></title>
    <url>%2F2018%2F08%2F26%2FCmd%20Markdown%2F</url>
    <content type="text"><![CDATA[欢迎使用 Cmd Markdown 编辑阅读器 我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，Cmd Markdown 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown： 整理知识，学习笔记 发布日记，杂文，所见所想 撰写发布技术文稿（代码支持） 撰写发布学术论文（LaTeX 公式支持） 除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载： Windows/Mac/Linux 全平台客户端 请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 新文稿 或者使用快捷键 Ctrl+Alt+N。 什么是 MarkdownMarkdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，粗体 或者 斜体 某些文字，更棒的是，它还可以 1. 制作一份待办事宜 Todo 列表 支持以 PDF 格式导出文稿 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 新增 Todo 列表功能 修复 LaTex 公式渲染问题 新增 LaTex 公式编号功能 2. 书写一个质能守恒公式[^LaTeX]$$E=mc^2$$$$ x = {-b \pm \sqrt{b^2-4ac} \over 2a} $$ 3. 高亮一段代码[^code]1234567@requires_authorizationclass SomeClass: passif __name__ == '__main__': # A comment print 'hello world' 4. 高效绘制 流程图12345678st=&gt;start: Startop=&gt;operation: Your Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 5. 高效绘制 序列图123Alice-&gt;Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob--&gt;Alice: I am good thanks! 6. 高效绘制 甘特图12345678910111213title 项目开发流程section 项目确定 需求分析 :a1, 2016-06-22, 3d 可行性报告 :after a1, 5d 概念验证 : 5dsection 项目实施 概要设计 :2016-07-05 , 5d 详细设计 :2016-07-08, 10d 编码 :2016-07-15, 10d 测试 :2016-07-22, 5dsection 发布验收 发布: 2d 验收: 3d 7. 绘制表格 项目 价格 数量 计算机 \$1600 5 手机 \$12 12 管线 \$1 234 8. 更详细语法说明想要查看更详细的语法说明，可以参考我们准备的 Cmd Markdown 简明语法手册，进阶用户可以参考 Cmd Markdown 高阶语法手册 了解更多高级功能。 总而言之，不同于其它 所见即所得 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。 什么是 Cmd Markdown您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 编辑/发布/阅读 Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。 1. 实时同步预览我们将 Cmd Markdown 的主界面一分为二，左边为编辑区，右边为预览区，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！ 2. 编辑工具栏也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 编辑区 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。 3. 编辑模式完全心无旁骛的方式编辑文字：点击 编辑工具栏 最右侧的拉伸按钮或者按下 Ctrl + M，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！ 4. 实时的云端文稿为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 编辑工具栏 的最右侧提示 已保存 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。 5. 离线模式在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。 6. 管理工具栏为了便于管理您的文稿，在 预览区 的顶部放置了如下所示的 管理工具栏： 通过管理工具栏可以： 发布：将当前的文稿生成固定链接，在网络上发布，分享 新建：开始撰写一篇新的文稿 删除：删除当前的文稿 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地 列表：所有新增和过往的文稿都可以在这里查看、操作 模式：切换 普通/Vim/Emacs 编辑模式 7. 阅读工具栏 通过 预览区 右上角的 阅读工具栏，可以查看当前文稿的目录并增强阅读体验。 工具栏上的五个图标依次为： 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落 视图：互换左边编辑区和右边预览区的位置 主题：内置了黑白两种模式的主题，试试 黑色主题，超炫！ 阅读：心无旁骛的阅读模式提供超一流的阅读体验 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境 8. 阅读模式在 阅读工具栏 点击 或者按下 Ctrl+Alt+M 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。 9. 标签、分类和搜索在编辑区任意行首位置输入以下格式的文字可以标签当前文档： 标签： 未分类 标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示： 10. 文稿发布和分享在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 (Ctrl+Alt+P) 发布这份文档给好友吧！ 再一次感谢您花费时间阅读这份欢迎稿，点击 (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！ 作者 @ghosert2016 年 07月 07日 [^LaTeX]: 支持 LaTeX 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 MathJax 参考更多使用方法。 [^code]: 代码高亮功能支持包括 Java, Python, JavaScript 在内的，四十一种主流编程语言。]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python]]></title>
    <url>%2F2018%2F08%2F26%2Fpython%2F</url>
    <content type="text"><![CDATA[python的学习过程： 第一个是看着廖雪峰的网站，里面的内容基础，是关于数据结构等的十分基本的内容，适合小白入门 第二个是流畅的python 这本书比python-codebook还深入，更适合当你实现了一个功能之后，还是想知道其具体怎么实现的时候查询。 第三个是python-codebook. 它的组织形式是任务式、问题式的，而且问题也相对而言比较高级，不是算法导论那种以解决某个实际问题，而是在编程上我想实现什么更好的功能那种问题，通过每一个问题，或者说通过每一个你想怎么更优的实现一个方法的思路，来引导如何更好地写代码，实现高级功能。这本书的前提是你已经入门，并且写了一段时间的python代码，在实际写的过程中已经遇到了类似的问题，也勉强实现了，只是苦于没有更好更顺心的方法实现。我现在是个小白，看这本书用了将近两周吧，主要看了第一二三四七八章，里面的代码翔实。其他的也是略微看了看，因为没有实际操作背景，有的时候不懂为什么那样做会更好，可以在以后的编程过程中，遇到这样的情况：这个我能勉强实现，但是感觉不太好，我想实现的更优美。那就应该来看看这本书，说不定这本书的实现能给自己一些思路。不适合为了读而读，因为不是入门。 接下来可以考虑看看那种直接算法任务型的。刚刚看了看python算法教程，估计要跳着看了，因为里面的关于算法的内容已经熟悉了，可以扫描着看 python的点二重列表生成式123456789# 第一种[[i+j for i in range(3)] for j in range(2)][[0, 1, 2], [1, 2, 3]]# 第二种[i+j for i in range(3) for j in range(2)][0, 1, 1, 2, 2, 3]# 按列访问类似列表的结构[a[i][j] for j in range(len(a[0]) for i in range(len(a.size))] 所以对于二重列表生成式，一般可以认为是对于同层的列表，是从前到后，对于不同层的列表，是从外到内 li_ = list(str_)和str_ = ‘’.join(li_)互为相反12345678910111213141516171819202122232425# 一重列表In [1]: str = '我你'In [2]: li = ['我','你']In [3]: list(str)Out[3]: ['我', '你']In [4]: ''.join(li)Out[4]: '我你'# 二重列表In [5]: lli_=[['我','你'],['北','京']]In [6]: str_ = [''.join(li) for li in lli_]In [7]: str_ Out[7]: ['我你', '北京']In [8]: st_ ='/n'.join([''.join(li) for li in lli_])In [9]: st_Out[9]: '我你/n北京'#同理，可以推广到多重列表 list删除元素12345678910111213141516171819# 利用pop，根据位置删除# 存在返回值，与append相对应li_.pop()a = li_.pop(i)# 利用remove，根据值删除# 删除第一个匹配的值aList = [123, 'xyz', 'zara', 'abc', 'xyz'];aList.remove('xyz');aList.remove(aList[1])# 利用del,根据位置删除，没有返回值del(n[4])del n[4]# str.replace()```pythonu'afafafa'.replace('a',u'eee').replace('f',u'rr')'eeerreeerreeerreee' python函数的互换对于类似的函数，并且有相同输入和输出，只是对于实现的功能有一些不一样暂时无法评价这两种写法的优劣123456789# 第一种写法if opt.acrostic: result = gen_acrostic(model, start_words, ix2word, word2ix, prefix_words)else: result = generate(model, start_words, ix2word, word2ix, prefix_words)# 第二种写法gen_poetry = gen_acrostic if opt.acrostic else generateresult = gen_poetry(model, start_words, ix2word, word2ix, prefix_words) 奇怪12345ipdb&gt; y = 10ipdb&gt; [[ 1 for w in range(2)] for j in range(y)][[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]ipdb&gt; [[ 1 for w in range(y)] for j in range(2)]*** NameError: name 'y' is not defined assert123456789101112# 第一种assert 3 &gt;=5,'2不等于1'----&gt; 1 assert 3 &gt;=5AssertionError:# 第二种assert 3 &gt;=5,'3不小于等于5'----&gt; 1 assert 3 &gt;=5,'3不小于等于5'AssertionError: 3不小于等于5 sort和sortedsort是对list的操作sorted是对所有可迭代的序列的操作 requirements.txt12345pip freeze &gt; requirements.txt # 生成requirements.txtpip install -r requirements.txt # 从requirements.txt安装依赖pip install pipreqspipreqs /home/project/location 装饰器第一步：函数也是一个对象，也可以赋值给变量，也可以通过变量来调用函数。12345678910111213141516171819In [1]: def now(): ...: print('now 2020202')In [2]: f = nowIn [3]: f()now 2020202In [4]: now.__name__Out[4]: 'now'In [5]: f.__name__Out[5]: 'now'In [6]: type(f)Out[6]: functionIn [7]: type(now)Out[7]: function 第二步：简单的装饰器，在函数调用前后，进行一些没有参数的操作，在代码运行期间增加功能两层的装饰器,@log123456789101112131415161718192021222324252627In [15]: def log(func): ...: print('wrapper early') ...: print(func.__name__) ...: def wrapper(*args, **kwargs): ...: print('fun early&#123;0&#125;'.format(func.__name__)) ...: func(*args,**kwargs) ...: print('fun later') ...: print('wrapper later') ...: return wrapper ...: ...:In [16]: @log ...: def now(): ...: print('now test3') ...:wrapper earlynowwrapper laterIn [17]: now()fun earlynownow test3fun laterIn [18]: now.__name__Out[18]: 'wrapper' 第三步：复杂的装饰器，在函数调用前后，进行有参数的操作，三层的装饰器，@log()123456789101112131415161718192021222324252627282930313233343536373839404142In [19]: def log(text): ...: print('log----decorator') ...: def decorator(func): ...: print('decorator----wrapper') ...: def wrapper(*args,**kwargs): ...: print('wrapper 1') ...: print(text,func.__name__) ...: print(' wrapper 2') ...: return func(*args, **kwargs) ...: print('wrapper----decorator') ...: return wrapper ...: print('decorator----log') ...: return decorator ...: ...: ...:In [20]: @log('hello') ...: def now(): ...: print('1000') ...: return 1 ...: ...:log----decoratordecorator----logdecorator----wrapperwrapper----decoratorIn [21]: now()wrapper 1hello now wrapper 21000Out[21]: 1In [22]: f = nowIn [23]: f.__name__Out[23]: 'wrapper'In [24]: now.__name__Out[24]: 'wrapper' 第四步：完整的装饰器123456789101112131415161718192021import functoolsdef log(func): @functools.wraps(func) def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper# 或者import functoolsdef log(text): def decorator(func): @functools.wraps(func) def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator等价于wrapper.__name__ = func.__name__ 参数分为位置参数（可以设置为默认参数），可变参数，关键字参数位置参数很简单，跳过可变参数或者关键字参数可以没有值赋予 可变参数 第一步：对于不确定个数的参数，使用list或者tuple作为参数传入缺点：在调用的时候必须先组装成list或者tuple123456789101112131415161718192021In [7]: def calc(number): ...: sum = 0 ...: print(type(number)) ...: for i in number: ...: sum+=i ...: return sum ...: ...:In [8]: calc([1,2,3])&lt;class 'list'&gt;Out[8]: 6In [9]: calc((1,2,3) ...: )&lt;class 'tuple'&gt;Out[9]: 6In [10]: calc((1,2,3))&lt;class 'tuple'&gt;Out[10]: 6 第二步：利用可变参数*number，在第一步的基础上修正缺点，使其调用不再需要先组装缺点：输入参数是list和tuple时就会很麻烦，需要先拆解123456789101112131415161718192021222324252627282930313233In [11]: def calc(*number): ...: sum = 0 ...: print(type(number)) ...: for i in number: ...: sum+=i ...: return sum ...: ...:In [12]: calc(1,2,3)&lt;class 'tuple'&gt;Out[12]: 6In [13]: calc()&lt;class 'tuple'&gt;Out[13]: 0In [14]: num = [1,2,3]In [15]: calc(num)&lt;class 'tuple'&gt;---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-15-bb5c0a89404a&gt; in &lt;module&gt;()----&gt; 1 calc(num)&lt;ipython-input-11-34cdd7bd7eaa&gt; in calc(*number) 3 print(type(number)) 4 for i in number:----&gt; 5 sum+=i 6 return sumTypeError: unsupported operand type(s) for +=: 'int' and 'list' 注意，可变参数在函数内部是作为一个tuple存在的，如果对于tuple进行修改，先变成list 第三步：对第一步的优点和第二步的优点进行整合，使调用时既能直接接收list或者tuple作为输入参数，也能不需要组装成list或者tuple进行输入。也就是在函数定义时注明number,输入直接输入或者list.123456789101112131415161718192021In [11]: def calc(*number): ...: sum = 0 ...: print(type(number)) ...: for i in number: ...: sum+=i ...: return sum ...: ...:# 合理调用的几种方式In [14]: num = [1,2,3]In [16]: calc(*num)&lt;class 'tuple'&gt;Out[16]: 6In [20]: calc(1,2,3)&lt;class 'tuple'&gt;Out[20]: 6In [22]: calc(*(1,2,3))&lt;class 'tuple'&gt;Out[22]: 6 关键字参数类比可变参数，就很容易地理解dict型参数的输入12345678910111213In [18]: def person(name, **kwargs): ...: print(type(kwargs)) ...: print(kwargs)In [23]: person('fa',city='beijing',age='44')&lt;class 'dict'&gt;&#123;'city': 'beijing', 'age': '44'&#125;In [24]: extra = &#123;'city': 'Beijing', 'job': 'Engineer'&#125;In [25]: person('hh',**extra)&lt;class 'dict'&gt;&#123;'city': 'Beijing', 'job': 'Engineer'&#125; 对于可变参数或者关键字参树，或者按照正常地类似位置参数或者默认参数的形式传入，或者按照*list或者**dict进行传入。 命名关键字参数1234567891011# 第一种，直接*In [26]: def person(name, * ,city, job): ...: print(name, city, job) ...:# 其调用时必须显示输入参数名，city和job，当然，命名关键字参数也可以设置默认值In [27]: person('11',city='beijing', job='enginner')11 beijing enginner# 第二种 有可变参数的存在def person(name, age, *args, city, job): print(name, age, args, city, job) 参数组合参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。123456789101112131415161718192021222324def f1(a, b, c=0, *args, **kw): print('a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw)def f2(a, b, c=0, *, d, **kw): print('a =', a, 'b =', b, 'c =', c, 'd =', d, 'kw =', kw)In [28]: def f(a,b,c=0,*args,d,**kw): ...: print('a',a,'b',b,'c',c,'args',args,'d',d,'kw',kw) ...:In [29]: f(1,2,3,4,d=5,e=6)a 1 b 2 c 3 args (4,) d 5 kw &#123;'e': 6&#125;# 看一个神奇的东西，输入参数用tuple和dict直接代替In [30]: args = (1,2,3,4)In [31]: dic = &#123;'d':5, 'e':6&#125;In [33]: f(*args,**dic)a 1 b 2 c 3 args (4,) d 5 kw &#123;'e': 6&#125;# 由此引出一个重要结论# 所以，对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的。# 这个结论解释了后期定义装饰器的时候，参数的定义直接是(*args, **kw)的形式，而不用去管函数本身的参数的定义是什么样的，而在调用的时候，按照原函数的参数定义调用即可。 闭包1234567891011121314151617181920212223242526272829303132333435In [29]: def lazy_sum(*args): ...: def sum(): ...: ax = 0 ...: for i in args: ...: ax = ax+i ...: return ax ...: return sum ...: ...:In [30]: f = lazy_sum(1,2,3,4)In [31]: f.__name__Out[31]: 'sum'In [32]: f.__file__---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-32-9c8edc3d9e41&gt; in &lt;module&gt;()----&gt; 1 f.__file__AttributeError: 'function' object has no attribute '__file__'In [33]: fOut[33]: &lt;function __main__.lazy_sum.&lt;locals&gt;.sum()&gt;In [34]: f()Out[34]: 10In [35]: f1 = lazy_sum()In [36]: f2 = lazy_sum()In [37]: f1==f2Out[37]: False 闭包存在的问题123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 返回多个函数In [38]: def count(): ...: fs=[] ...: for i in range(4): ...: def f(): ...: return i*i ...: fs.append(f) ...: return fs ...: ...:In [39]: f1,f2,f3,f4 = count()In [40]: f1()Out[40]: 9In [42]: f2()Out[42]: 9In [43]: f3()Out[43]: 9In [44]: f4()Out[44]: 9In [63]: def count(): ...: def f(): ...: return i*i ...: fs=[] ...: for i in range(4): ...: fs.append(f) ...: print(i) ...: return fsIn [64]: f1, f2, f3, f4 = count()0123In [65]: f1()Out[65]: 9In [66]: f2()Out[66]: 9# 返回单个函数In [48]: def lazy_sum(*args): ...: j=0 ...: def sum(): ...: ax = 0 ...: for i in args: ...: ax = ax+i+j ...: return ax ...: j=100 ...: return sum ...: ...:In [49]: f1 = lazy_sum(1,2,3)In [50]: f1()Out[50]: 306 返回函数不要引用任何循环变量，或者后续会发生变化的变量。应该满足内层函数定义后其所引用的外部变量不发生变化。 针对外部变量发生变化的解决方案就是再用一个函数，令变化的外部变量从隐式参数变成显式参数或者说，对于变化的外部变量，令其执行，不再以变量的形式存在，而是以其值的形式存在。第一种方法1234567891011121314151617181920212223def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1, 4): fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f() return fsIn [52]: f1, f2, f3 = count()In [53]: f1()Out[53]: 1In [60]: f1Out[60]: &lt;function __main__.count.&lt;locals&gt;.f.&lt;locals&gt;.g()&gt;In [61]: f2Out[61]: &lt;function __main__.count.&lt;locals&gt;.f.&lt;locals&gt;.g()&gt;In [62]: f1.__name__Out[62]: 'g' 第二种方法1234567891011121314In [113]: def count(): ...: fs=[] ...: for i in range(2): ...: def f(m=i): ...: return m*m ...: fs.append(f) ...: return fs ...: ...:In [114]: count()Out[114]:[&lt;function __main__.count.&lt;locals&gt;.f(m=0)&gt;, &lt;function __main__.count.&lt;locals&gt;.f(m=1)&gt;] 廖雪峰说可以用lambda函数进行代码缩写，但是没有想通怎么用。 内部函数内修改外部函数局部变量12counterA = createCounter()print(counterA(), counterA(), counterA(), counterA(), counterA()) # 1 2 3 4 5 1234567891011121314151617181920212223242526272829一、在内部函数内修改外部函数局部变量的两种方法1法：把外部变量变成容器或者说可变变量def createCounter(): a = [0] def counter(): a[0] += 1 return a[0] return counter2法：在内部函数里给予外部函数局部变量nonlocal声明，让内部函数去其他领域获取这个变量def createCounter(): a = 0 def counter(): nonlocal a a += 1 return a return counter二、在内部函数内修改全局变量def createCounter(): global a a = 0 def counter(): global a a += 1 return a return counter 快速解压或者list+tuple转置123456789101112li_ = [(1,2,3),(4,5,6)]ll = list(zip(*li_))[(1, 4), (2, 5), (3, 6)]# 甚至可以达到按列取值的效果l1, l2, l3 = list(zip(*li_))l1(1, 4)l2(2, 5)l3(3, 6) 求一个序列中，与固定值之间的最大值，这个最大值不能超过序列的最大值和固定值或者说求两个序列的最小最大值或者说在这个序列中，如果没有比固定值大的数，则取这个序列的最大值作为最大值，如果有，则取固定值作为最大值或者说对这个序列进行截断12345ll = [min(c, max_len) for c in l]max_query = max(ll)# 或者max_l = max(l)max_query = min(max_l, max_len) dict1234567a = 1b = 2d = dict(aa = a, bb = b)&#123;'aa': 1, 'bb': 2&#125;dd = &#123;'aa':a, 'bb':2&#125;dd&#123;'aa': 1, 'bb': 2&#125; 注释文档12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273'''文档快速生成注释的方法介绍,首先我们要用到__all__属性在Py中使用为导出__all__中的所有类、函数、变量成员等在模块使用__all__属性可避免相互引用时命名冲突'''__all__ = ['Login', 'check', 'Shop', 'upDateIt', 'findIt', 'deleteIt']class Login: ''' 测试注释一可以写上此类的作用说明等 例如此方法用来写登录 ''' def __init__(self): ''' 初始化你要的参数说明 那么登录可能要用到 用户名username 密码password ''' pass def check(self): ''' 协商你要实现的功能说明 功能也有很多例如验证 判断语句，验证码之类的 ''' passclass Shop: ''' 商品类所包含的属性及方法 update改/更新 find查找 delete删除 create添加 ''' def __init__(self): ''' 初始化商品的价格、日期、分类等 ''' pass def upDateIt(self): ''' 用来更新商品信息 ''' pass def findIt(self): ''' 查找商品信息 ''' pass def deleteIt(self): ''' 删除过期下架商品信息 ''' pass def createIt(self): ''' 创建新商品及上架信息 ''' passif __name__=="__main__": import test print(help(test)) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283$ python test.pyHelp on module test:NAME testDESCRIPTION 文档快速生成注释的方法介绍,首先我们要用到__all__属性 在Py中使用为导出__all__中的所有类、函数、变量成员等 在模块使用__all__属性可避免相互引用时命名冲突CLASSES builtins.object Login Shop class Login(builtins.object) | 测试注释一可以写上此类的作用说明等 | 例如此方法用来写登录 | | Methods defined here: | | __init__(self) | 初始化你要的参数说明 | 那么登录可能要用到 | 用户名username | 密码password | | check(self) | 协商你要实现的功能说明 | 功能也有很多例如验证 | 判断语句，验证码之类的 | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) class Shop(builtins.object) | 商品类所包含的属性及方法 | update改/更新 | find查找 | delete删除 | create添加 | | Methods defined here: | | __init__(self) | 初始化商品的价格、日期、分类等 | | createIt(self) | 创建新商品及上架信息 | | deleteIt(self) | 删除过期下架商品信息 | | findIt(self) | 查找商品信息 | | upDateIt(self) | 用来更新商品信息 | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined)DATA __all__ = ['Login', 'check', 'Shop', 'upDateIt', 'findIt', 'deleteIt']FILE h:\桌面\test.pyNone 用help或者.doc1print(math.sin.__doc__) all只对from xx import *有作用参考链接https://stackoverflow.com/questions/44834/can-someone-explain-all-in-python12345678910111213# foo.py__all__ = ['bar', 'baz'] waz = 5bar = 10def baz(): return 'baz'# run-foo.pyfrom foo import * print(bar) # 正常print(baz) # 正常print(waz) # 显示错误 from future import absolute_import从python2.1开始以后, 当一个新的语言特性首次出现在发行版中时候, 如果该新特性与以前旧版本python不兼容, 则该特性将会被默认禁用. 如果想启用这个新特性, 则必须使用 “from futureimport *” 语句进行导入.1234from __future__ import absolute_import# https://blog.csdn.net/caiqiiqi/article/details/51050800from __future__ import division# https://blog.csdn.net/feixingfei/article/details/7081446 保持原有维度的取元素 narrow 或者切片1234567tt = t.Tensor([[1,2,3],[3,4,5]])ttt = tt.narrow(0,0,1)ttttensor([[ 1., 2., 3.]])tt[0:1]tensor([[ 1., 2., 3.]]) 堆https://www.bbsmax.com/A/gAJGaGeZdZ/ 生成器函数 迭代器 迭代对象对于生成器函数，可以理解成列表，yield的值就是列表中的元素，用next()或者for in来调用迭代器 Iterator: 可以用于next的，惰性计算迭代对象 Iteratable: 可以用于for的 list, tuple, 生成器迭代对象可以使用iter变成迭代器迭代对象范围更广 对于012345678910a = 0if a ==0: True等价于if not a:并且if a!=0等价于if a 2018-10-05关于Python爬虫涉及到的编码问题计算机内存中，使用Unicode编码硬盘或者传输时，使用UTF-8编码 bytes和strPython3默认的字符串类型是str，在内存中以Unicode表示，如果要想保存到硬盘上或者在网络上传输，就需要把str转换成以字节为单位的bytes。str–encode–&gt;bytes–decode–&gt;str看了一些博客，感觉还是没有讲清楚，不是很清楚。 re参考链接]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MdEditor]]></title>
    <url>%2F2018%2F08%2F22%2FMdEditor%2F</url>
    <content type="text"><![CDATA[欢迎使用 Markdown在线编辑器 MdEditorMarkdown是一种轻量级的「标记语言」 Markdown是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面，Markdown文件的后缀名便是“.md” MdEditor是一个在线编辑Markdown文档的编辑器MdEditor扩展了Markdown的功能（如表格、脚注、内嵌HTML等等），以使让Markdown转换成更多的格式，和更丰富的展示效果，这些功能原初的Markdown尚不具备。 Markdown增强版中比较有名的有Markdown Extra、MultiMarkdown、 Maruku等。这些衍生版本要么基于工具，如Pandoc，Pandao；要么基于网站，如GitHub和Wikipedia，在语法上基本兼容，但在一些语法和渲染效果上有改动。 MdEditor源于Pandao的JavaScript开源项目，开源地址Editor.md，并在MIT开源协议的许可范围内进行了优化，以适应广大用户群体的需求。向优秀的markdown开源编辑器原作者Pandao致敬。 MdEditor的功能列表演示标题H1标题H2标题H3标题H4标题H5标题H5字符效果和横线等 删除线 删除线（开启识别HTML标签时） 斜体字 斜体字 粗体 粗体 粗斜体 粗斜体 上标：X2，下标：O2 缩写(同HTML的abbr标签) 即更长的单词或短语的缩写形式，前提是开启识别HTML标签时，已默认开启 The HTML specification is maintained by the W3C. 引用 Blockquotes 引用文本 Blockquotes 引用的行内混合 Blockquotes 引用：如果想要插入空白换行即&lt;br /&gt;标签，在插入处先键入两个以上的空格然后回车即可，普通链接。 锚点与链接 Links普通链接普通链接带标题直接链接：http://www.mdeditor.com[锚点链接][anchor-id][anchor-id]: http://www.mdeditor.com/mailto:test.test@gmail.comGFM a-tail link @pandao邮箱地址自动链接 test.test@gmail.com www@vip.qq.com @pandao 多语言代码高亮 Codes行内代码 Inline code执行命令：npm install marked 缩进风格即缩进四个空格，也做为实现类似 &lt;pre&gt; 预格式化文本 ( Preformatted Text ) 的功能。 &lt;?php echo &quot;Hello world!&quot;; ?&gt; 预格式化文本： | First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell | JS代码123function test() &#123; console.log("Hello world!");&#125; HTML 代码 HTML codes1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;mate charest="utf-8" /&gt; &lt;meta name="keywords" content="Editor.md, Markdown, Editor" /&gt; &lt;title&gt;Hello world!&lt;/title&gt; &lt;style type="text/css"&gt; body&#123;font-size:14px;color:#444;font-family: "Microsoft Yahei", Tahoma, "Hiragino Sans GB", Arial;background:#fff;&#125; ul&#123;list-style: none;&#125; img&#123;border:none;vertical-align: middle;&#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1 class="text-xxl"&gt;Hello world!&lt;/h1&gt; &lt;p class="text-green"&gt;Plain text&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 图片 Images图片加链接 (Image + Link)： Follow your heart. 列表 Lists无序列表（减号）Unordered Lists (-) 列表一 列表二 列表三 无序列表（星号）Unordered Lists (*) 列表一 列表二 列表三 无序列表（加号和嵌套）Unordered Lists (+) 列表一 列表二 列表二-1 列表二-2 列表二-3 列表三 列表一 列表二 列表三 有序列表 Ordered Lists (-) 第一行 第二行 第三行 GFM task list GFM task list 1 GFM task list 2 GFM task list 3 GFM task list 3-1 GFM task list 3-2 GFM task list 3-3 GFM task list 4 GFM task list 4-1 GFM task list 4-2 绘制表格 Tables 项目 价格 数量 计算机 $1600 5 手机 $12 12 管线 $1 234 First Header Second Header Content Cell Content Cell Content Cell Content Cell First Header Second Header Content Cell Content Cell Content Cell Content Cell Function name Description help() Display the help window. destroy() Destroy your computer! Left-Aligned Center Aligned Right Aligned col 3 is some wordy text $1600 col 2 is centered $12 zebra stripes are neat $1 Item Value Computer $1600 Phone $12 Pipe $1 特殊符号 HTML Entities Codes&copy; &amp; &uml; &trade; &iexcl; &pound;&amp; &lt; &gt; &yen; &euro; &reg; &plusmn; &para; &sect; &brvbar; &macr; &laquo; &middot; X&sup2; Y&sup3; &frac34; &frac14; &times; &divide; &raquo; 18&ordm;C &quot; &apos; [========] Emoji表情 :smiley: Blockquotes :star: GFM task lists &amp; Emoji &amp; fontAwesome icon emoji &amp; editormd logo emoji :editormd-logo-5x: :smiley: @mentions, :smiley: #refs, links, formatting, and tags supported :editormd-logo:; list syntax required (any unordered or ordered list supported) :editormd-logo-3x:; [ ] :smiley: this is a complete item :smiley:; []this is an incomplete item test link :fa-star: @pandao; [ ]this is an incomplete item :fa-star: :fa-gear:; :smiley: this is an incomplete item test link :fa-star: :fa-gear:; :smiley: this is :fa-star: :fa-gear: an incomplete item test link; 反斜杠 Escape*literal asterisks* [========] 科学公式 TeX(KaTeX)$$E=mc^2$$ 行内的公式$$E=mc^2$$行内的公式，行内的$$E=mc^2$$公式。 $$x &gt; y$$ $$(\sqrt{3x-1}+(1+x)^2)$$ $$\sin(\alpha)^{\theta}=\sum_{i=0}^{n}(x^i + \cos(f))$$ 多行公式： 12345\displaystyle\left( \sum\_&#123;k=1&#125;^n a\_k b\_k \right)^2\leq\left( \sum\_&#123;k=1&#125;^n a\_k^2 \right)\left( \sum\_&#123;k=1&#125;^n b\_k^2 \right) 123456789\displaystyle \frac&#123;1&#125;&#123; \Bigl(\sqrt&#123;\phi \sqrt&#123;5&#125;&#125;-\phi\Bigr) e^&#123; \frac25 \pi&#125;&#125; = 1+\frac&#123;e^&#123;-2\pi&#125;&#125; &#123;1+\frac&#123;e^&#123;-4\pi&#125;&#125; &#123; 1+\frac&#123;e^&#123;-6\pi&#125;&#125; &#123;1+\frac&#123;e^&#123;-8\pi&#125;&#125; &#123;1+\cdots&#125; &#125; &#125; &#125; 123f(x) = \int_&#123;-\infty&#125;^\infty \hat f(\xi)\,e^&#123;2 \pi i \xi x&#125; \,d\xi 分页符 Page break Print Test: Ctrl + P [========] 绘制流程图 Flowchart12345678st=&gt;start: 用户登陆op=&gt;operation: 登陆操作cond=&gt;condition: 登陆成功 Yes or No?e=&gt;end: 进入后台st-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op [========] 绘制序列图 Sequence Diagram1234Andrew-&gt;China: Says HelloNote right of China: China thinks\nabout itChina--&gt;Andrew: How are you?Andrew-&gt;&gt;China: I am good thanks! End]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo]]></title>
    <url>%2F2018%2F08%2F17%2Fhexo%2F</url>
    <content type="text"><![CDATA[常用命令(cmd)hexo n “postName”hexo cleanhexo g 本地生成hexo d 同步到githubhexo d -ghexo new page aboutmehexo s 本地服务器预览hexo s -p 4100 换端口 搭建Github+hexo 博客的过程参考链接如下使用Hexo+Github一步步搭建属于自己的博客（基础）使用Hexo+Github一步步搭建属于自己的博客（进阶）[1] http://blog.haoji.me/build-blog-website-by-hexo-github.html?from=xa[2] https://www.cnblogs.com/fengxiongZz/p/7707568.html[3] https://www.jianshu.com/p/84a8384be1ae[4] ttp://tengj.top/2016/02/22/hexo1/$ npm install -g hexo-cli安装Node.js →安装Hexo → 安装主题 → 本地测试运行 → 注册给github与coding并创建pages仓库 → 部署安装Git →node.js的解释是高并发npm是模块的包管理器,与node.js一起安装的。npm install hexo -g 全局安装hexo是基于node.js的静态博客，所以我们才需要安装node.jsGit是为了让其他人也可以看到你的博客，把本地的内容提交到github上面去常用命令hexo g 生成 generatehexo s 启动服务器预览 serverhexo d 部署 deployhexo clean 清除缓存hexo server -p 4100hexo generate –deploy 完成后部署hexo deploy –generate 完成后部hexo new “postName” node_modules 依赖包public 生成的页面scaffolds 模板文件夹 post draft pagesource 用户资源的地方 hexo解释hexo中_config.ymlmarkdown写博客hexo中的配置信息域名绑定hexo渲染MathJax数学公式markdownpad渲染数学公式只能F6浏览器预览，并且不能实时预览，所以考虑换个markdown编辑器cmd markdown对本地文件支持不友好，并且不能加载本地图片Hexo Markdown 简明语法手册hexo的脚注问题只能实现数字的脚注 2018-09-12 添加功能实现评论功能此次评论功能使用disqus，理由：同学推荐参考链接：Hexo折腾记之科学使用Disqus与Next的集成科学使用 DisqusDisqus PHP API基于disqus-php-api在Hexo博客中使用DisqusDisqus-Proxy 配置说明Github 搭建 hexo （四）——更换主题，disqus，RSS 添加rss功能不知道是干嘛的，好像是为了实现订阅的。暂时不是很清楚。参考链接：最简便的方法搭建Hexo+Github博客,基于Next主题 添加site-map功能参考链接不知道是干嘛的Github 搭建 hexo （五）- 站点地图（sitemap.xml）站点地图还挺高级，以后再说。 百度自动推送参考链接Hexo+Next主题博客提交百度谷歌收录 添加公益404界面参考链接hexo添加404公益界面最简便的方法搭建Hexo+Github博客,基于Next主题 添加搜索参考链接hexo-genarator-search 高级教程以后再说利用Gitpage+hexo开发自己的博客Hexo个人免费博客(三) next主题、评论、阅读量统计和站内搜索官网插件 title: Hello Worldtoc: true Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment hexo实现显示本地图片2018-12-03，文章资源文件夹 在vscode可以显示本地图片，但是上传到blog中后无法显示本地图片。 参考链接https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referral 实际操作 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save，这是下载安装一个可以上传本地图片的插件，来自dalao：dalao的git，如果后续步骤报错，可以卸载后执行后续步骤 hexo n后会发现除了生成一个md文件，同目录下还生成了一个同名文件夹，用于放入图片。markdown中引入图片的语法： 1![你想输入的替代文字](xxxx/图片名.jpg) 嗯 最后检查一下，hexo g生成页面后，进入public\2017\02\26\index.html文件中查看相关字段，可以发现，html标签内的语句是 12&lt;img src="2017/02/26/xxxx/图片名.jpg"&gt;而不是&lt;img src="xxxx/图片名.jpg&gt; 这很重要，关乎你的网页是否可以真正加载你想插入的图片。如果发现有问题，可以直接查看源码，查看src，来进行修正。 引用方式二： 123&#123;% asset_path slug %&#125;&#123;% asset_img slug [title] %&#125;&#123;% asset_link slug [title] %&#125; 我最后采用的是 1&#123;% asset_img example.jpg This is an example image %&#125; hexo文件上传到github之后，会发现图片和md文件在同一个目录下。]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>github</tag>
        <tag>hexo</tag>
        <tag>npm</tag>
        <tag>基础</tag>
      </tags>
  </entry>
</search>
