<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[OIM]]></title>
    <url>%2F2019%2F06%2F10%2FOIM%2F</url>
    <content type="text"><![CDATA[1. Introduction paper: CVPR2017_Joint Detection and Identification Feature Learning for Person Search code: caffe, [pytorch][https://github.com/Cysu/open-reid] project: End-to-End Deep Learning for Person Search memory: 1作: Shuang Li，跟过汤晓鸥。 这一篇文章之前就已经看过了，这次的主要目的是 memory 的使用，因为看到 memory 在 ECN、DIMN 中都有使用，所以看看大家都是怎么用的。 作者提出 Online Instance Matching (OIM) loss function 来融合 pedestrian detection and person re-id. 作者提出了新的数据集，18184 张图片，8432 个行人，96143 个行人框。 作者重新定义了 test 的过程。在传统的 test 过程中，gallery 中的图片是已经裁剪好的单个行人图片，作者定义 gallery 中的图片是未经裁剪、有多个行人的图片，需要匹配是哪张图片的哪个行人。 CNN 由两部分组成，给定一张 gallery image, a pedestrian proposal net 用于生成行人的 bounding boxes, 更偏向于召回率而不是精确度, 然后 a identification net 用于提取相应的特征。 3. Method a pedestrian proposal net a identification net 3.1 Online Instance Matching Lossa pedestrian proposal net 共检测出三类：labeled identities, unlabeled identities, background clutter. 这里只考虑 labeled identities and unlabeled identities. 假设 a labeled identity $x\in R^D$，其中 $D$ 表示特征维度，作者建立了一个 lookup table (LUT) $V\in R^{D\times L}$ 来储存所有的 feature of all labeled identities，前向传播过程中，计算 cos 距离 $V^T x$，反向传播时 $v_t \gets \gamma v_t (1-\gamma) x$，其中 $\gamma\in [0, 1]$, 并进行归一化。这个过程和 ECN 基本一样。 对于 a unlabeled identites，使用 a circular queue $U\in R^{D\times Q}$ 来储存 the features of unlabeed identites that appear in recent mini-batches，其中 $Q$ 表示 queue size. 前向传播时，计算 cos 距离 $V^T x$，反向传播时，弹出 queue 顶端的 features，插入当前 batch 的特征。 通过上面两个结构，作者重新定义了 $x$ 属于某类的概率: p_i = \frac{\exp(v_i^T x/\tau)}{\sum_{j=1}^L \exp(v_j^T x/\tau)}+ \sum_{k=1}^Q \exp(u_k^T x/\tau)其中，更高的 $\tau$ 导致更平缓的分布。 q_i = \frac{\exp(u_i^T x/\tau)}{\sum_{j=1}^L \exp(v_j^T x/\tau)}+ \sum_{k=1}^Q \exp(u_k^T x/\tau)损失函数为: L = E_x [-\log p_t]我觉得作者这里写错了，但无伤大雅。 其对 $x$ 的反向推导很有意思，是： \frac{\partial L}{\partial x}=-\frac{1}{\tau}[ v_t-\sum_{j=1}^L p_j v_t-\sum_{k=1}^Q q_k u_k ]后面会补充整个推导过程 Why not Softmax loss: 作者不使用 Softmax loss 有两个方面的原因：第一个原因不是很理解，不是很赞同，第二个原因是 unkown identities 没有 label. Scalability: 随着 id 的增加，分母的计算时间会成为瓶颈，所以采用 sub-sampling 的方法计算，具体见下文 除此之外，OIM loss 看似和 Softmax 相似，但是OIM loss是非参数的，缺点是容易过拟合，L2-normalized 能减少过拟合。 Question: Memory 是否真的比 fc 好用，是否可以单独做一个 memory 和 fc 的对比实验？ 2019-06-12: ICCV 2019 三个 WR, 凉凉。 补充反向传播推导过程： 第一步：假设只有两个变量，很容易就可以推导到多个变量，令 f_1=\frac{e^{x_1}}{e^{x_1}+e^{x_2}}, f_2=\frac{e^{x_2}}{e^{x_1}+e^{x_2}}不能随便地使用$f_1+f_2=1$，则 \frac{\partial f_1}{\partial x_1} = \frac{e^{x_1}(e^{x_1}+e^{x_2})-e^{x_1} e^{x_1}}{(e^{x_1}+e^{x_2})^2}=f_1-(f_1)^2\frac{\partial f_1}{\partial x_2} = -e^{x_1} \frac{e^{x_2}}{(e^{x_1}+e^{x_2})^2}=-f_1 \cdot f_2第二步：令 $x_1=v_1 x, x_2=v_2 x$: f_1=\frac{e^{v_1 x}}{e^{v_1 x}+e^{v_2 x}}, f_2=\frac{e^{v_2 x}}{e^{v_1 x}+e^{v_2 x}}则 \frac{\partial f_1}{\partial x} = \frac{\partial f_1}{\partial x_1} \frac{\partial x_1}{\partial x} + \frac{\partial f_1}{\partial x_2} \frac{\partial x_2}{\partial x}=(f_1-(f_1)^2)v_1 + (-f_1 \cdot f_2)v_2\frac{\partial f_1}{\partial v_1} = \frac{\partial f_1}{\partial x_1} \frac{\partial x_1}{\partial v_1}=(f_1-(f_1)^2)x\frac{\partial f_1}{\partial v_2} = \frac{\partial f_1}{\partial x_2} \frac{\partial x_2}{\partial v_2}= (-f_1 \cdot f_2)x第三步：计算损失函数，假设最优值是第一个， $L=-\log(f_1)$ \begin{aligned} \frac{\partial L}{\partial x} &= \frac{\partial L}{\partial f_1} \frac{\partial f_1}{\partial x} \\ &= - \frac{1}{f_1} \cdot ((f_1-(f_1)^2)v_1 + (-f_1 \cdot f_2)v_2) \\ &= (1-f_1) v_1 - f_2 v_2 \\ &= v_1 - (f_1 v_1 + v_2 v_2) \end{aligned}\frac{\partial L}{\partial v_1} = \frac{\partial L}{\partial f_1} \frac{\partial f_1}{\partial v_1}=-(1-f_1)x\frac{\partial L}{\partial v_2} = \frac{\partial L}{\partial f_1} \frac{\partial f_1}{\partial v_2}= f_2 x第四步：推广到多个变量$v_1, v_2, v_3…$ \begin{aligned} \frac{\partial L}{\partial x} &= v1-(\sum_i v_i x) \\ \frac{\partial L}{\partial v_1} &= -(1-f_1)x \\ \frac{\partial L}{\partial v_2} &= f_2 x \\ \frac{\partial L}{\partial v_3} &= f_3 x \end{aligned}4. Experiments4.1 Effectiveness of Online Instance Matching Sub-sampling the identities: 当 sub-sampling 的 size 更小的时候，最终性能差不多，但是收敛速度更快，说明了作者提出的方法能有效地处理大规模数据集。 Low-dimensional subspace: 作者对比了 128, 256, 512, 1024， 2048-dimention，发现原始的 2048 维特征得到结果不如其他。 5. code从 open-reid 中只能看到 LUT 的代码，可以看出来，这个代码和 ECN 的代码可以说是一样，牛逼啊。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152from __future__ import absolute_importimport torchimport torch.nn.functional as Ffrom torch import nn, autogradclass OIM(autograd.Function): def __init__(self, lut, momentum=0.5): super(OIM, self).__init__() self.lut = lut self.momentum = momentum def forward(self, inputs, targets): self.save_for_backward(inputs, targets) outputs = inputs.mm(self.lut.t()) return outputs def backward(self, grad_outputs): inputs, targets = self.saved_tensors grad_inputs = None if self.needs_input_grad[0]: grad_inputs = grad_outputs.mm(self.lut) for x, y in zip(inputs, targets): self.lut[y] = self.momentum * self.lut[y] + (1. - self.momentum) * x self.lut[y] /= self.lut[y].norm() return grad_inputs, Nonedef oim(inputs, targets, lut, momentum=0.5): return OIM(lut, momentum=momentum)(inputs, targets)class OIMLoss(nn.Module): def __init__(self, num_features, num_classes, scalar=1.0, momentum=0.5, weight=None, size_average=True): super(OIMLoss, self).__init__() self.num_features = num_features self.num_classes = num_classes self.momentum = momentum self.scalar = scalar # Temperature self.weight = weight self.size_average = size_average # ECN 这里用的 nn.Parapmeter self.register_buffer('lut', torch.zeros(num_classes, num_features)) def forward(self, inputs, targets): inputs = oim(inputs, targets, self.lut, momentum=self.momentum) inputs *= self.scalar loss = F.cross_entropy(inputs, targets, weight=self.weight, size_average=self.size_average) return loss, inputs 这里我有点晕，简单理一理。 第一，定义 operation: $output = F (input, target;\theta)$。因为 LUT 的前向传播和反向传播不同于一般的 operation，所以需要重新定义 operation，手动实现前向传播和反向传播，前向传播需要 lut, inputs，反向传播对 inputs 求导和 利用 target 对 lut 的更新。 第二，定义损失函数 Loss. 这个 OIMLoss 用了一个层来包装整个 operation 和 变量，方便管理。]]></content>
      <categories>
        <category>person re-id</category>
      </categories>
      <tags>
        <tag>person re-id</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIMN]]></title>
    <url>%2F2019%2F06%2F03%2FDIMN%2F</url>
    <content type="text"><![CDATA[1. 前言 paper: Generalizable Person Re-identification by Domain-Invariant Mapping Network code: 暂无 这篇文章和 ECN 那篇有相似的地方：Memory 记录所有的特征，并把这些记录的特征直接当做分类器使用，我觉得这样的使用应该有更合理的解释，不同点在于使用 Memory 的具体方式不同，我暂时觉得这里面还有东西可以挖。我记得我之前读过一篇 也是用 Memory 用于分类的论文，那篇直接把已知的和未知的特征都放在 memory 中，更简单巧妙。 该论文首次提到 meta-learning, few-shot learning，我简单查了查。人工智能的理论进展呈现： Artificial Intelligence—&gt;Machine Learning—&gt;Deep Learning—&gt;Deep Reinforcement Learning—&gt;Deep Meta Learning 深度学习可以在特定的 task 上得到一个很好的模型，但是在其他 task 上效果就会很差，是否可以理解成过拟合呢？应该和过拟合不是一个角度，过拟合指得的是训练集和测试集不可能同时达到各自的最优解，但是 meta learning 更偏向于当出现一个新的少量的样本的时候，不需要巨量的训练，原始数据训练得到的模型作为先验知识，有助于学习新的样本，有点 FUNIt 的味道，又有点像之前看过的一篇 TIP2019 Progressive Learning for Person Re-Identification with One Example，暂时还说不上来这种感觉。郑哲东那个团队是直接 meta-learing 过来的思路吗？不得而知了。 举个例子：分类器可以很好地区分在各种环境下的猫狗之类的动物，这个时候给一张站着的老虎图片用于二次训练，使得二次训练后的分类器既能对于分猫狗的性能不下降，又对各种环境下的老虎图片也有相同的区分能力。如果直接从头开始训练，训练集为一张老虎+各种环境下的猫狗，显然对于老虎的区分能力很弱。 meta-leaning 现在的研究思路 基于记忆Memory的方法 基于预测梯度的方法 利用Attention注意力机制的方法 借鉴LSTM的方法 面向RL的Meta Learning方法 通过训练一个好的base model的方法，并且同时应用到监督学习和增强学习 利用WaveNet的方法 预测Loss的方法 参考链接: https://zhuanlan.zhihu.com/p/28639662 示例视频: https://zhuanlan.zhihu.com/p/46002992 其他资料: https://www.zhihu.com/topic/20140823/hot 三个趋势: 自动驾驶，强化学习，元学习 作者 Jifei Song，博三，敦煌玛丽女王大学的 SketchX 实验室，16年 BMVC，17年 ICCV，18年 CPVR，19年 CVPR，大佬发文章都白送的吗，刚不过刚不过。 2. Introduction作者希望可以在 a set of source domains 上一次训练，直接直接应用到 unseen dataset 上，称之为 Domain-Invariant Mapping Network (DIMN)，遵循 meta-learning pipeline amd sample a subset of source domain training tasks(identities) during each training episode。 现有的 cross domain person Re-ID 可以分为 unsupervised domain adaptation (UDA)，domain generalization(DG)(few-shot meta learning)。 However, existing DG methods [20, 34, 23, 39] assume that the source and target domain have the same label space; whilst existing metalearning models [49, 49, 10, 35, 40] assume a fixed number of classes for target domains and are trained specifically for that number using source data. 补充: 之前看过的 UDA 论文旨在寻找 domain-invariant feature space，而 DG(domain generalization) 旨在更好地模型迅速地适应新的 domain。 DIMN 的目的是学习 a mapping between a person image and its identity classifier weight vector. DIMN 不同于传统的 meta-learning ： 在 target domain 中不需要更新，这一点和 ECN 有点区别，ECN 使用了 target domain 的信息 不同的 training tasks share a memory bank, memory bank 使用 running average strategy 的方式更新，这一点和 ECN 差不多 训练得到的模型可以应用于有任意数量行人的 target domain，这一点和 ECN 不一样，ECN 基本只保证了在哪训练用在哪 感觉这个目标定的要比 ECN 更高一些。 作者在实际实验的时候使用5个 datasets 作为 source domains，另外4个 datasets 作为 target domains，实验结果不是很全，比如没有做只有一个 dataset 的 source domain 的效果。所以不能完全排除因为扩大数据集而造成的性能提高。但是根据现有的使用过 memory 的论文，我觉得还是可信的。 3. Methodology背景：已知有标签的 M 个 datasets(domains), $D_1, D_2,…, D_M$，每个 domain 有自己的 label space，希望在这些数据集上训练得到的模型直接应用到新的 domain/dataset 得到好的效果，而不需要更新模型。 The network: the encoding subnets: $g_{\phi}$ mapping subnets: gallery image embedding—&gt;classifier’s weight vector memory bank: store all classifiers in training domains Encoding Subnet(最基本的分类损失): MobilenetV2，这个也是 (MobilenetV2，另外一篇 distill 也是) 。假设在融合后的 $M$ 个数据集中一共有 $C$ 个行人，每个 mini-batch 中取 $C_b$ 个行人，对于每个行人 $l_i$，取两张图片，分别设置为 gallery $\tilde{x}_i$ 和 probe $x_i$，所以在每个 batch 中有 $2C_b$ 个行人图片，组成 $2C_b$ pairs(两张图片可以调过来)。 最基本的分类损失： L_{id}=\sum_{i=1}^{C_b} Cross\_Entropy(l_i, Softmax(f_{\theta}(g_{\phi}(x_i)))其中 $x_i$ 表示输入图片，$l_i$ 表示标签，$g_{\phi}$ 表示 encoding subnet，得到的是 D 维特征向量，$f_{\theta}$表示分类器，$\theta \in R^{D\times C}$ Mapping Subnet：相对于传统的分类权重 $\theta_{\cdot,j}$ 作为模型参数的一部分，作者直接生成分类权重 identity classifier weight vector： \hat{\theta}_{\cdot,j}=h_w(g_{\phi}(\tilde{x}_i))其中 mapping subnet $h_w$ 可以理解成 hyper-network(不懂，难道为其他网络生成参数就是 hyper-network)，这里可以使用简单的 multi-layer perception (MLP)。 对于给定的 gallery image $\tilde{x}_j$ and probe image $x_i$，得到 $\hat{\theta}_{\cdot,j}$，进而得到 logit vector $p$: p_j=h_w(g_{\phi}(\tilde{x}_ j))\cdot g_{\phi}(x_i)然后对 $p_j$ 进行softmax，这是表示对于 probe image $x_i$, 所有的 gallery images $\tilde{x}_j$ 根据相似度进行排序，亦或者可以理解成 gallery images 为基本分类向量，判断 probe image 属于哪类，真值为如果 probe image 和 gallery image 是同一类，设为1，否则设为0，根据之前写的 mini-batch 的取法，只有一个为1，剩下都为0，可以理解成基本的分类损失。 这里 logit vector $p$ 有两种写法，一种是 $p\in R^C$，表示 probe image 属于哪类，优点是具有 discriminative power，一种是 $p\in R^{C_b}$，表示 probe image 与 mini-batch 的哪张相似，优点是容易收敛训练。为了兼顾 scalability and discriminativity，引入 memory bank. Memory Bank：Memory bank is a weight matrix$W\in R^{D\times C}$。每个 mini-batch 的 gallery branch 中，有 $C_b$ 个不同 id 的样本，$[ \tilde{x}_1, \tilde{x}_2,…, \tilde{x}_{C _b}]$，经过 encoding subnet $g _{\phi}$ and the mapping subnet $h_w$，得到 $C_b$ 个 predicted weight vector $\lbrace \hat{\theta}_{\cdot, j}, j=[ 1,2,…, C_b] \rbrace$，即 $\hat{\theta}\in R^{D\times C_b}$. 然后根据 id 更新 $W$，这里的更新还蛮奇怪的，和 ECN 的更新不一样。 先得到 $\hat{W}$: 复制 $W$，$\hat{W} \gets W$，再复制 $\hat{\theta}$，$\hat{W}_ {\cdot, L(j)} \gets \hat{\theta}_{\cdot, j}, \forall j\in [ 1,2,…,C_b]$($L(j)$ 表示第 $j$ 张图片的 id)， 然后用类似 ECN 那样，用分类损失 L_{mat}=\sum_{i=1}^{C_b} Cross\_Entropy(y_i, Softmax(\hat{W}g_{\phi}(x_i)))与 ECN 不同地是，第一，ECN 没有真值，所以只能自己像自己，DIMN 有真值，第二，ECN 用的是未更新的 memory，DIMN 用的是半旧半新的 memory。 最后更新: W\gets (1-\alpha)W+\alpha \hat{W}作者发现有两个 trick 可以帮助稳定训练：对 $W$ 进行二范约束；$W$ 更新前后变化比较小。 L_{reg}=\sum_{i=1}^{C_b} \parallel W_{\cdot, L(j)} - \hat{\theta}_{\cdot, j} \parallel_2^2Question: 对 $L_{reg}$ 的出现表示存疑，感觉出现的很勉强 Training Objective: 三元组损失，对于 $x_i$，$\tilde{x}_i$ 为正样本，其他的 $\hat{W} _{\cdot, j’}| _{j’\not = i}$ 为负样本，得到 $p=h_w(g_{\phi}(\tilde{x_i}))\cdot g_{\phi}(x_i)$，$n=\hat{W} _{\cdot, j’}\cdot g _{\phi}(x_i)|_{j’\not = i}$，归一化为 $S(x_i, \tilde{x}_i)$, $S(x_i, \tilde{x}_j’| _{j’\not = i})$，从而有 L_{tri}=\sum_{i=1}^{C_b} \max (0, \triangle - \max S(x_i, \tilde{x}_j'| _{j'\not = i}) + S(x_i, \tilde{x}_i)Overall： L_{full}=L_{id}+\lambda_1 L_{mat} +\lambda_2 L_{reg} + \lambda_3 L_{tri}备注 ECN: memory 的使用：ECN 先计算损失后更新， DIMN 半更新计算损失后全更新 损失函数: ECN 使用 memory 进行了两次分类损失和一次假标签分类损失，DIMN 使用 memory 进行了一次分类损失和一次三元组损失。 memory 的输入: ECN 中 memory 和基本分类损失用的是同一张图片，DIMN 中 memory 和基本分类损失用的不是同一张图片 memory 的特征：ECN 中直接共享网络提取到的，DIMN 又经过了一次 mapping subnet. Model Testing: 上述的损失函数可以学习到 encoding subnet $g_{\phi}(\cdot)$ and mapping subnet $h_w(\cdot)$，在测试阶段，给定 query image $x_i$ and gallery image $\tilde{x}_j$，两张图片的相似度定义为 $h_w(g_{\phi}(\tilde{x}_j))\cdot g _{\phi}(x_i)$. 备注： DIMN 是一个 DG method 而不是一个 one-shot learning，因为只在 source domain 上训练 传统的 deep ReID 的相似度计算是 $g_{\phi}(\tilde{x}_j)\cdot g _{\phi}(x_i)$ 4. Experiments4.1 Datasets and SettingsA Large-Scale ReID benchmark: source datasets: CUHK02, CUHK03, Market-1501, DukeMTMC-ReID, CUHK-SYSU PersonSearch test datasets: VIPeR, PRID, GRID, i-LIDS Implementation Details: encoding subnet: MobileNetV2, output: 1792 mapping subnet: a single fully-connected layer running average parameter $\alpha=0.5$ triplet loss margin $\triangle=0.8$ weight of loss $\lambda_1=\lambda_3=1, \lambda_2=0.01$ 4.3 Ablation Study Question: 直接使用 memory 进行分类损失和使用 fc 进行分类损失有区别吗？前向计算过程是一样的，memory 的权重等同于 fc 的权重。反向求导过程不一样，memory 使用 running average 并进行二范归一化进行更新，fc 的权重使用链式求导进行更新，memory 和 fc 的 feature 反向求导也是一样的公式。也就是说两种唯一的不同在于权重的更新方式，memory 更新时其权重是固定的，fc 更新时其权重是变化的，memory 更新时会进行二范归一化，fc 更新时没有这个操作。这些区别足以产生这么大的影响吗？可能更需要其他理论上的支持 从 ablation study 上看，缺失每一个对最终结果的影响都不大。]]></content>
      <categories>
        <category>person re-id</category>
      </categories>
      <tags>
        <tag>meta-learning</tag>
        <tag>few-shot learning</tag>
        <tag>cross-domain person re-id</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArcFace]]></title>
    <url>%2F2019%2F05%2F30%2FArcFace%2F</url>
    <content type="text"><![CDATA[0. 前言 paper: CVPR2019_ArcFace: Additive Angular Margin Loss for Deep Face Recognition code: MXNet, pytorch, pytorch, tensorflow author: 邓健康 开源代码中称之为 insightface，思路简单，效果却很好，并且与其他变种做了详尽的对比。(这篇论文原名是ArcFace，但是由于与虹软重名，后改名为Insight Face) 帝国理工学院博士生，CVPR17, 18, 19 都有他的文章，牛逼。 此外，他们在ICCV 2019 举办了 Lightweight Face Recognition Challenge Workshop(LFR19) ，LFR19 是第一个强调“轻量级推理” 和“可重现系统” 的人脸识别比赛。竞赛于 4 月 25 日~7 月 10 日进行，比赛结果将于 10 月 28 日在韩国首尔举办的 ICCV 2019 会议上进行公布，欢迎前往比赛主页了解详细规则. 1. Introduction腾讯AI lab CVPR paper把玩：cosFace、ArcFace—additive margin softmax 深度学习界从开始到现在，所有的论文基本就干了两件事情，一、发现一个牛逼的网络结构，提高了网络的表达能力；二、发现了一个好玩的损失函数，提升了模型的鲁棒性和精度。人脸识别的优化方向就两个，一是不断增大不同人之间的距离，二是不断降低一个人不同人脸照片之间的距离。 center loss 和 tripleloss都是干这个事情。但他们损失函数的根基还是softmax的交叉熵. 主流: the softmax-loss-based methods and the triplet-loss-based methods softmax: angular margin penalty: Sphereface, CosFace, ArcFace 2. Proposed Approach2.1 ArcFaceL_1=-\frac{1}{N} \sum_{i=1}^N \log \frac{e^{W_{y_i}^T x_i+b_{y_i}}}{\sum_{j=1}^n e^{W_j^T x_i+b_j}}令 $b_j=0, \parallel W_j \parallel=1, \parallel x_i \parallel=s, W_j^T x_i=s \cos \theta_j$ L_2=-\frac{1}{N} \sum_{i=1}^N \log \frac{e^{s \cos \theta_{y_i}}}{e^{s \cos \theta_{y_i}}+\sum_{j=1,j\not =y_i}^n e^{s \cos \theta_j}}angular margin penalty: L_3=-\frac{1}{N} \sum_{i=1}^N \log \frac{e^{s \cos (\theta_{y_i}+m)}}{e^{s \cos (\theta_{y_i}+m)}+\sum_{j=1,j\not =y_i}^n e^{s \cos \theta_j}} 2.2 Comparison with SphereFace and CosFace SphereFace: multiplicative angular margin $m_1$ ArcFace: additive angular margin $m_2$ CosFace: additive cosine margin $m_3$ 补充：按照作者的说法，ArcFace 相当于融合了 softmax 和 triplet，形成了 image-to-class 的 margin 损失，softmax 是 image-to-class 损失， triplet 是 image-to-image 的 margin 损失。 各种实验结果也证明了这个损失很强。 补充：作者在直播中还提到了 RetinaFace Face Detector，他也参加了一些比赛，结果也说明了他们提出的整体框架效果很好。 自己标了1w多张人脸的五点图，大佬大佬。]]></content>
      <categories>
        <category>face recognition</category>
      </categories>
      <tags>
        <tag>face recognition</tag>
        <tag>softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FUNIT]]></title>
    <url>%2F2019%2F05%2F29%2FFUNIT%2F</url>
    <content type="text"><![CDATA[0. 前言 paper: FUNIT: Few-Shot Unsupervised Image-to-Image Translation code: code 代码还需要等段时间，才能下载到所有代码。 这篇文章和郑哲东的 Joint Discriminative and Generative Learning for Person Re-identification 这篇文章看着有点像，果然是高手的思路都是相同的。不对，两个都是 NVIDIA 的，哈哈哈。 这篇文章思路奇特，实现简单。 1. Introduction传统的 GAN 在训练阶段需要 many images in both source and target classes (其中 source 提供 content, target 提供 classes)(这里的 content 可以粗略的理解成形态或者表情或者属性或者姿势这种与类别无关的内容，classes 表示与类别相关的内容)，并且测试图片需要来自训练时所使用的 source and target classes.而作者提出的方法在训练阶段需要很少的 images in both source and target classes 并且测试时可以提供训练未曾见到过的 target classes. 其基本假设是当人类见到 new object (target class) 时，可以根据以往的经验推断出它的其他形态，比如曾经见过猫的站立和趴着的状态，见到老虎时，能自动脑补出老虎站立和趴着的状态。 Question: 如果见过猫的站立和趴着的状态，看到蛇会咋想，蛇没有站或者趴这种说法，这种转换会考虑到这种情况吗？还是会生成一个站着的蛇？哈哈哈？ Question: 测试时如果把 target dataset 作为 content image，不知道会是什么效果。 Question: 作者是否隐藏了一个假设：content image 提供的信息或者属性是所有 object 都共享的？ 2. Few-shot Unsupervised Image Translation 明确几个相关定义： 数据集一共分为两个：source dataset (source class images), target dataset (target class images), 训练时只使用 source dataset, 测试时使用 source dataset 和 target dataset。 对于模型而言，输入分为一张 content image 和多张 class image，content image 提供姿势等信息，class image 提供类别等信息. 模型在训练时，content image 和 class image 都来自 source dataset，在测试时，content image 来自 source dataset，class image 来自 target dataset. generator G 的输入是 一张 content image $x$ and a set of K class images $\lbrace y_1,…,y_k \rbrace$, 从而得到输出图片 $\bar{x}$: \bar{x}=G(x, \lbrace y_1,...,y_k \rbrace)其中，content image 属于 object class $c_x$，K 张 class images 属于 object class $c_y$，一般情况下，K 比较小并且 $c_x$ 不等于 $c_y$，称 G 为 few-shot image translator.得到的结果在外形看更像 $c_y$，在姿势等方面更像 $c_x$. 令 $\mathbb{S}$ and $\mathbb{T}$ 表示 the set of source classes and the set of target classes.在训练阶段，随机从 $\mathbb{S}$ 中选取两类图片 $c_x, c_y \in \mathbb{S}$ 输入 G，在测试时，class image 选自 $\mathbb{T}$，content image 选自 $\mathbb{S}$. 2.1 Few-shot Image Translator G: generator, consists of a content encoder $E_x$, a class encoder $E_y$, and a decoder $F_x$ $E_x: x \to z_x$: class-invariant latent representation, determines the local structure $E_y: \lbrace y_1,…,y_k \rbrace \to z_y$: class-specific latent representation, control the global look, K 可大可小 $F_x: (z_x, z_y)\to \bar{x}$ 2.2 Multi-task Adversarial Discriminator D: discriminator, multiple adversarial binary classification task D 输入是一张图片，得到 $|\mathbb{S}|$ 个输出 当输入图片取自 real image of source class $c_x$，希望第 $c_x$ 个输出为真 当输入图片取自 fake image of translation out $c_x$，希望第 $c_x$ 个输出为假 属于 $c_x$ 类的图片预测为其他类 $(\mathbb{S}\setminus {c_x})$ 是真是假，则不关心 更新 G 时，仅仅希望第 $c_x$ 个输出为假 作者通过经验发现 $|\mathbb{S}|$ 个二分类器比一个 $|\mathbb{S}|$ 分类器得到的效果更好 Question: 这种设置分类器的方法倒是没想过，不知道作者的思路由来，感觉这种多个二分类器要比多分类器更容易扩展，如果多一个类别，可以在不改变原有分类器的情况下加一个即可，但是多分类器的话需要全部替换 2.3 LearningGAN loss: L_{GAN}(G,D)=E_x[-\log D^{c_x}(x)]+E_{x,\lbrace y_1,...,y_k \rbrace}[\log (1-D^{c_y}(\bar{x}))]content reconstruction loss: L_{R}(G)=E_x[\parallel x-G(x, \lbrace x \rbrace ) \parallel]Question: 这个损失是为了约束 $\bar{x}$ 与 $x$ 相似吗？ feature matching loss: 记 $D_f$ 为 feature extractor, 即移除 D 的分类器。 L_F(G)=E_{x,\lbrace y_1,...,y_k \rbrace}[ \parallel D_f(\bar{x}-\sum _k \frac{D_f(y_k)}{K}) \parallel _1^1]Question: 按理来说，通过分类损失就已经可以约束 $\bar{x}$ 和 $\lbrace y_1,…,y_k \rbrace$ 相似了，再加上 feature matching 的效果更明显吗？ Question: 是怎么约束 $\bar{x}$ 与 $x$ 在姿势等方面相似的？ 4. Experiments训练时 $K=1$，测试时 $K=1, 5, 10, 15, 20$ Baseline: 作者把 target class image 在训练阶段是否出现划分为 fair (unavailable) and unfair (available) Fair: StarGAN-Fair-K Unfair: StarGAN-Unfair-K, CycleGAN-Unfair-K, UNIT-Unfair-K, MUNIT-Unfair-K 至于关于 GAN 的其他指标，则不列了，一是肯定效果好，二是自己也不是特别懂这些指标的意义和难度。 训练时 source classes 越多，效果越好。 在跨物种时，效果也会很差，只能改变 content image 的颜色。其类别没有发生变化。]]></content>
      <categories>
        <category>GAN</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>few-shot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[STGAN]]></title>
    <url>%2F2019%2F05%2F27%2FSTGAN%2F</url>
    <content type="text"><![CDATA[0. 前言 paper: CVPR2019_STGAN: A Unified Selective Transfer Network for Arbitrary Image Attribute Editing code: tensorflow, pytorch reference: CVPR2018_StarGAN：StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation code: pytorch reference: Valse2018_AttGAN: Facial Attribute Editing by Only Changing What You Want code: tensorflow, pytorch 这也是一篇做GAN的文章，根据论文的叙述，一直和 StarGAN, AttGAN 做对比。我只看过 StarGANs，没有看过 AttGAN. 1. Introduction作者首先发现了 skip connection 和 target attribute vector 的缺点: skip connection 对重建有用，但是对于 attribute manipulation 有害，target attribute vector 对重建有害。 作者提出自己的创新点时论据很充分。 作者在使用GAN时，发现 skip connection 可以提高重建图片质量(图片属性和生成图片属性一致时)(reconstruction)，但是会减弱属性变化的程度(图片属性和生成图片不一致时)(attribute manipulation)。 作者从 selective transfer 角度来解决这些问题: selective: a. 仅仅考虑改变的属性；b. 有选择地拼接 encoder feature and decoder feature transfer: 对局部和全局属性提供一个统一的框架 Question: transfer 的解释不是很理解，看后面如何应用吧。 创新点： 输入的是 different attribute vector z针对skip connection, 使用 selective transfer units，同时提高输入属性的影响力和图片质量 2. Proposed Method 2.1 Limitation of Skip Connection in AttGAN 可以看出，增加 skip connections 有益于重建图像(图片属性和生成属性一致)(reconstruction)，但是，会生成其他属性的图片时的精度会降低(图片属性和生成属性不一致)(attribute manipulation ability)。主要原因是直接 concatenates encoder and decoder features. 作者为了解决这个问题，使用 selective transfer units to adaptively transform encoder features guided by attributes to be changed. 2.2 Taking Difference Attribute Vector as Input StarGAN and AttGAN 是把 target attribute vector $att_t$ 做为输入，作者通过观察 StarGAN and AttGAN 的重建图片，发现 attribute vector 会对重建图片有害。其实也不难理解，要想不发生变换，最好是 x0，但是 attribute vector 肯定不是0，卷积层也不可能是0，自然 attribute vector 会改变生成的图片，尤其是重建图片，因为此时是希望完全不变的，要求更高一些。 作者针对 target attribute vector 的缺点提出 difference attribute vector。 att_{diff}=att_t-att_s2.3 Selective Transfer Units(STU)作者使用修改版的 GRU 来建立 STUs 作为 skip connection. 设第 $l$ 层的 encoder feature 表示为 $f_{enc}^l$，第 $l+1$ 层的 hidden state 为 $s^{l+1}$，从而有下述方程，更新 hidden state $s^l$ 和 transformed encoder feature $f_t^l$: \begin{aligned} \hat{s}^{l+1}&=W_{t*T}[s^{l+1}, att_{diff}]\\ r^l &= \sigma(W_r * [f_{enc}^l, \hat{s}^{l+1}])\\ z^l &= \sigma(W_z * [f_{enc}^l, \hat{s}^{l+1}])\\ s^l&=r^l\circ \hat{s}^{l+1}\\ \hat{f}_t^l &= tanh(W_h*[f_{enc}^l, s^l])\\ f_t^l &= (1-z^l)\circ\hat{s}^{l+1}+z^l\circ \hat{f}_t^l \end{aligned}其中 $[\cdot ,\cdot]$ 表示 concatenation operation，$*T$ 表示 transposed convolution. $*$ 表示 convolution operation，$\circ$ 表示 entry-wise product，$\sigma(\cdot)$ 表示 sigmoid function. 看着很像常规意义上的 LSTM 的方程，但是自己在这方面接触比较少。所以简单地记录下GRU的原始公式。 两者的公式基本上是一致的，但是不太懂其原理或者可以达到的效果。另外其含义也发生了一些变化，主要是 GRU 中，$f_t^l$ 是 output of hidden state，作者把 $s^l$ 作为 output of hidden state，$f_t^l$ 作为 output of transformed encoder feature。 不懂。但我觉得能提出这个，就很厉害。 2.4 Network ArchitectureSTGAN 分为两部分: generator G and discriminator D. G: $G_{enc}$ and $G_{dec}$，都分别有5个卷积操作。STU应用在前四个 encoder layers,即 (f_t^l, s^l)=G_{st}^l(f_{enc}^l, s^{l+1}, att_{diff})D: $D_{adv}$ and $D_{att}$. $D_{adv}$ and $D_{att}$ 共享前五个卷积层，分别有两个全连接层用于预测。 2.5 Loss Functions第一步: 给定一张图片 $x$，可以得到 encoder features: f=\lbrace f_{enc}^1, f_{enc}^2,..., f_{enc}^5 \rbrace=G_{enc}(x)第二步: 进而通过 STUs 得到 transform encoder features: (f_t^l, s^l)=G_{st}^l(f_{enc}^l, s^{l+1}, att_{diff})其中，不同的 STU 之间不共享参数。即： f_t=\lbrace f_t^1, f_t^2, f_t^3, f_t^4 \rbrace第三步: 得到结果, \hat{y}=G_{dec}(f_{enc}^5, f_t)即: \hat{y}=G(x, att_{diff})Reconstruction loss: L_{rec}=\parallel x-G(x,0) \parallel_1Adversarial loss: \max_{D_{adv}} L_{D_{adv}} = \mathbb{E}_x D _{adv}(x)-\mathbb{E} _{\hat{y}} D _{adv}(\hat{y})+\lambda \mathbb{E} [ (\parallel \nabla _{\hat{x}} D _{adv}(\hat{x}) \parallel_2 -1)^2 ]\max_{G} L_{G_{adv}} = \mathbb{E}_{x, att _{diff}} D _{adv}(G(x, att _{diff}))其中 $\hat{x}$ 表示真和生成图片的线性插值，在 StarGAN 中也见到过。 Attribute manipulation loss: L_{D_{att}}=- \sum _{i=1}^c[ att_s^{(i)}\log D_{att}^{(i)}(x)+(1-att_s^{(i)})\log (1-D_{att}^{(i)}(x)) ]L_{G_{att}}=- \sum _{i=1}^c[ att_t^{(i)}\log D_{att}^{(i)}(\hat{y})+(1-att_s^{(i)})\log (1-D_{att}^{(i)}(\hat{y})) ]这里应该只是一个简单的分类器。 Question: 为什么还会有第二项？ Model Objective: \min_D L_D= -L_{D_{adv}} + \lambda_1 L_{D_{att}}\min_G L_G= -L_{G_{adv}} + \lambda_2 L_{G_{att}} + \lambda_3 L_{rec}4. Experiments结果很棒，在真实性和分类准确率上都要比其它的GAN好很多。 5. Ablation Study STGAN: original STGAN STGAN-dst: target attribute vector, not difference attribute vector STGAN-conv: conv(encoder feature and difference attribute vector), not STU STGAN-conv-res: residual learning formulation to learn the convolution operator in STGAN-conv STGAN-gru: GRU not STU STGAN-res: residual learning formulation to learn the STU in STGAN 作者的对比实验做的是真多。 Difference attribute vector vs. target attribute vector: 可以看出，AttGAN, StarGAN, STGAN 三个模型上 difference attribute vector 都要好于 target attribute vector。 Selective Transfer Unit vs. its variants: STGAN-conv, STGAN-conv-res 性能很低，STGAN-gru, STGAN-res, STGAN 三种方法的性能差不太多，可能具体到某个属性会略有区别，因为作者没有一个最终的指标，所以说不上来到底差多少，但是从各个属性上看，STGAN最好，其他两个略微低一些。 Question: 具体 GRU, STU 的工作机制有机会的话还是需要多了解一下。 6. code因为最近在跑实验，所以没法具体跑论文的代码，以后再看情况把。]]></content>
      <categories>
        <category>GAN</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep CV-MIML]]></title>
    <url>%2F2019%2F05%2F22%2FDeep-CV-MIML%2F</url>
    <content type="text"><![CDATA[0. 前言 paper: CVPR2019 Weakly Supervised Person Re-Identification code: 这篇论文主要的创新点是：不再需要在 video 的每一帧都给出行人的位置和 id，而是直接赋予一段 video 一个 video-level label，即这段 video 中有哪些人。主要目的也是为了降低标注成本。有点重新定义行人重识别问题的意思。我觉得很有意思，也更贴近实际一些。 1. Introduction在传统的行人重识别设定中，每张图片表示用方框圈出来的行人。因此，作者提出了 weakly supervised setting，即在训练阶段，只有 video-level label，因此此时每段 video 中有多个行人，有个标签，即 multi-instance multi-label(MIML)。 订正(2019-05-24)：论文中的 train set 分为：probe set 和 gallery set，probe set 每个行人的一系列图片组成一个 bag ，有准确的标签，gallery set 一段 video 组成一个 bag，取出这段 video 每一帧自动检测出的行人图片，有 video level label，但是不知道具体哪个行人对应哪个标签。 提出： intra-bag alignment cross-view bag alignment 基本定义： gallery set 中的 every video clip 为一个 bag 每个 bag 包含多个行人的图片，并标记 video-level label probe set 中是经过手工标记的图片，为了统一，也视这些图片为一个 bag 解决的问题： 每个 bag 中包含相同的行人: intra-bag alignment bag 与 bag （跨摄像头）之间也包含相同的行人: cross-view bag alignment 现有的 MIML 直接应用到 person re-id 中存在的问题 现有的 MIML 忽略了 intra-bag variation，即 bag 内会有相同类别的实例 现有的 MIML 假设 instance-level label 是高度相关的，但是 person re-id 中的行人都相互独立的 现有的 MIML 没有考虑到 bag 之间的 (cross-view) 相关性 2. The Proposed Approach2.1 Problem Statement and Notation在 weakly supervised person re-id setting 中，不再是以图搜图，而是以图搜视频，或者以视频搜视频，即给定同一个行人组成的视频，在只经过自动画框的视频中找出这个行人。 符号 含义 $C$ identities $V$ camera views $\tilde{C}$ $\tilde{C}=C+1$ $\cal{X}$ the training set $N_{\cal{X}}$ $N_{\cal{X}}$ video in the training set $\cal{X}_p, \cal{X}_g$ probe set and gallery set,and $\cal{X}_p + \cal{X}_g=\cal{X}$ $\cal{X}_p=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_p}$ probe set $\cal{X}_g=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_g}$ gallery set $v_b \in \lbrace 1,2,…,V \rbrace$ 摄像头 $y_b=[y_b^0, y_b^1,…, y_b^C]\in\lbrace 0,1 \rbrace^{\tilde{C}}$ 行人标签 $X_b=\lbrace x_{b,1}, x_{b,2},…, x_{b,n_b} \rbrace $ 多实例 $x_{b,i}=f_e(I_b;\theta)\in \mathbb{R}^d$ 图片 $I_b$，特征提取方程 $f_e(\cdot;\theta)$ $f_c(\cdot;W)$ classifier $y_{b,i}=f_c(x_{b,i};W)$ 分类器 基本保证每个行人至少在两个摄像头下出现过，并视在 untrimmed videos 中的出现的未知行人为新的一类，即第0类，所以 $\tilde{C}=C+1$ Question: probe set 是图片，gallery set 是视频，那training set 是什么？也是视频吗？ gallery set 和 training set 是一个集合还是完全不同的集合？ gallery set 中的视频没有经过人工画框，只有 video-level weak label，其中的行人是自动检测出的，没有人工标定行人标签，所以具体到行人对应哪个标签也是未知的。 probe set 中，each query 由一系列同一个行人的检测图片组成。为了统一，视 probe set 中的 each query 也为一个 bag，此时 video-level 应该是单实例的。 所以，可以统一为：$\cal{X}=\lbrace \cal{X}_p, \cal{X}_g \rbrace$，其中 $\cal{X}_p=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_p}$，$\cal{X}_g=\lbrace (X_b, y_b, v_b) \rbrace _{b=1}^{N_g}$.其中 probe set 的 bag 是单标签的，gallery set 的 bag label 为0时，表示不确定这个行人是否在这个 video 中出现过。 重新定义问题需要重新定义好多概念。 2.2 Cross-View MIML for Person Re-id2.2.1 Weakly Supervised Person Re-id MIML令 classifier $f_c(\cdot;W)$, 对于 probe set $\mathcal{X}_p$，一个 bag 内的所有实例 $\lbrace x _{b,i} \rbrace _{i=1}^{n_b}$ 标签都是一样的 label $y_b$，此时可以可以直接使用交叉熵损失函数。对于 gallery set $\mathcal{X}_g$，共享 weak video-level label $y_b$，很明显这时候不能直接使用交叉熵损失函数，所以 probe set 和 gallery set 需要分开训练。 对于 probe set 中的 bag，可以直接使用 instance-level 交叉熵损失函数。 L_p=\frac{1}{N_p}\displaystyle \sum_{X_b \in \mathcal{X}_p} \sum _{i\in \lbrace 1,...,n_b \rbrace} \sum _{c\in \lbrace 0,...,C \rbrace} (-y_b^c\log \tilde{y}_{b,i}^c)其中，$y_b^c$ 表示第 b 个 bag 的第 c 类的真实 bag 标签，$\tilde{y}_{b,i}^c$ 表示第 b 个包的第 i 个图片的第 c 类的预测标签。 Question: probe set 中，一个 bag 是由行人的图片组成的像上面的一样，而 gallery set 中，一个bag的组成图片是一大张，行人在其中只占到很小的比例，明显两个不是一个尺度的，所以(根据 gallery set 的预测函数得出的猜测)猜测 gallery set 在进行训练时，是把每一帧中的行人(已经自动检测出)图片抠出来，然后送入网络，那么 gallery set 中，bag 大小不是表示这个 video 的帧数，而是这段 video 自动检测出来的行人图片个数，如果是这样的话，那怎么解决检测出的未知人(第0类)和丢失的行人。 对于 gallery set 中的 bag，因为是多实例多标签，所以用的方法略微不同，之前没见过，先举个例子解释下。 假设一个 bag 内有两个图片，label 是三类，即: $X_b=\lbrace x_{b,1}, x_{b,2} \rbrace $, $y_b=[1,0,1]$，预测标签是 $\tilde{y}_ {b,1}=[0.1, 0.2, 0.7], \tilde{y}_{b,2}=[0.3, 0.4, 0.3]$，对于每一类，先找出该类预测概率最大的图片，即 $q_1=2, q_2=2, q_3=1$，即第一类和第二类在第二张图片出现的概率最大，第三类在第一张图片出现的概率最大(纵向比较，而非常规意义上的横向比较)，从而得到bag的预测概率 $\tilde{y}=[0,3, 0.4, 0.7]$，然后再做个交叉熵。 Question:其理论基础是什么。 选择第 c 类后验概率最大的实例，记为 seed instance $x_{b, q_c}$，其中 $q_c$ 的定义为： q_c= \mathop{argmax}_{i\in \lbrace 1,2,...,n_b \rbrace} \lbrace \tilde{y}_{b,i}^c \rbrace然后定义 bag-level 分类损失: L_g=\frac{1}{N_g}\displaystyle \sum_{X_b \in \mathcal{X}_g} \sum _{c\in \lbrace 0,...,C \rbrace} (-y_b^c\log \max \lbrace \tilde{y}_ {b,1}^c, \tilde{y}_ {b,2}^c, ..., \tilde{y}_{b,n_b}^c \rbrace )从而得到 MIML classification loss: L_C=L_p+L_g2.2.2 Intra-bag Alignment 应该是指一段视频中即一个 bag，因为行人出现在连续的帧中，所以每一帧的行人分布是差不多的，因此可以尽可能地将属于同一个人的图片聚在一起。 在 gallery set 中，对于第 c 类已经找到 seed instance $x_{b, q_c}$，即对于第 c 类，这张图片比其他图片更像，从而构建一个 group 包含类似第 c 类的图片 G_{b,c}=\lbrace p|x_{b,p}\in \mathcal{N}_{q_c} \text{ and } \tilde{y}_{b,p}^c \ge \gamma \tilde{y} _{b,q_c}^c \rbrace其中，$\mathcal{N}_{q_c}$ 表示 $x_{b,q_c}$ 的 K-近邻，$\gamma\in (0, 1)$，上述的公式表示这个 group 中的图片既需要满足在特征上相近，也要满足预测概率大于预定值。 Question: soft multi-label, ECN 在选择正样本和负样本的时候的构建规则和这个就不太一样，或者说这么多构建最相似的规则，其区别大概在什么地方，特征相似，概率相似。不太懂，可能还是自己见得少吧。 因此可以得到 intra-bag alignment loss: L_{IA}=\frac{1}{N_{IA}} \displaystyle\sum_{X_b\in \mathcal{X}_g} \sum _{c\in \lbrace 0,...C \rbrace} \sum _{p \in G _{b,c}} y_b^c D_{KL}(\tilde{y}_{b,p}||\tilde{y} _{b,q_c})D_{KL}(\tilde{y}_{b,p}||\tilde{y} _{b,q_c})=\sum_{c\in\lbrace 0,...,C \rbrace}\tilde{y}_{b,p}^c(\log \tilde{y} _{b,p}^c -\log \tilde{y} _{b,q_c}^c)这个是用KL散度衡量分布一致性。 2.2.3 Cross-view Bag Alignmentintra-bag alignment and cross-view bag alignment 都是在尽量找到同一个行人的不同图片并且使这些图片得到特征相似，预测类别相似。 理论上来说，不同 view 下的同一个行人的特征应该也是相似的，因此，对每个行人引入 distribution prototype，并且同一个行人的图片特征应该近似 distribution prototype 的特征。公式定义为：第 t 个 epoch 时 第 c 类行人的 distribution prototype 为 $\hat{p}_c^t$: p_c^t=\frac{1}{|V_c|} \sum_{v\in V_c} (\frac{1}{|I_{c,v}|} \sum_{i\in I_{c,v}}\tilde{y}_i )\hat{p}_c^t=\alpha \hat{p}_c^{t-1}+(1-\alpha)p_c^t其中，$V_c$ 表示所有的 camera view，$I_{c,v}$ 表示在第 v 个 camera view 下第 c 类的所有实例，可以理解成，在 gallery set 中，每个 bag 是一段视频，视频的每一帧是一张大图片，然后从上面扣下来行人图片，即行人图片是指第几个 bag 的第几帧的第几个行人，同时 bag 还有 camera view 信息，即第几个摄像头拍的。 Question: $I_{c,v}$ 是怎么得到的，在训练阶段， probe set 是可以具体到 instance-level label，在 gallery set 只有 bag-level label，那怎么得出属于同一类的 instance，是通过 $G_{b,c}$ 吗？总感觉怪怪的。 从而得到对齐损失函数： L_{CA}=\frac{1}{N_{CA}} \sum_{X_b\in(\mathcal{X}_p \bigcup \mathcal{X}_g)} \sum _{c\in \lbrace 0,...C \rbrace} \sum _{i \in I_c} y_b^c D_{KL}(\tilde{y}_{b,i}||\hat{p}_c^t)$I_c$ 表示第 c 类的所有 instance. 需要注意的是：两次求 KL-散度 都在前面乘以了 bag-level label 用以衡量当前 video 当前类的重要性。 这两个公式都写的很长哈。而且是那种需要把全部图片遍历一遍才可以计算的那种。 2.3 Deep Cross-view MIML Model作者提出了一个新的 entropy regularization term，在 gallery set 中，总是会存在一些 outlier instance，在 intra-bag and cross-view alignment 中远离 data group，为了避免这种情况，设计了个正则项: L_E=\frac{1}{N_E} \sum_{X_b\in \mathcal{X}_g} \sum _{i\in \lbrace 1,...n_b \rbrace} \sum _{c\in \lbrace 0,...C \rbrace} (-\tilde{y} _{b,i}^c \log \tilde{y} _{b,i}^c )这是信息熵公式，应该是想尽可能使预测的概率陡峭，或为1或为0. 这个公式来得有点突然啊，是有什么实验证明吗？ Cross-view Multi-label Multi-Instance learning (CV-MIML): L_{CV-MIML}=L_C+\delta (L_{IA}+L_{CA}+L{E})2.4 Implementation Details实现还是有点意思的，希望看到代码吧。 4. Experiments]]></content>
      <categories>
        <category>person re-id</category>
      </categories>
      <tags>
        <tag>person re-id</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BNNeck]]></title>
    <url>%2F2019%2F05%2F17%2FBNNeck%2F</url>
    <content type="text"><![CDATA[0. 前言 paper: Bag of Tricks and A Strong Baseline for Deep Person Re-identification code: pytorch 这篇论文是旷视科技 Video Team 做的。 这篇论文主要介绍 re-id 代码中的各种 trick 的作用。 1. Introduction作者总结了各种 trick 并与 ECCV2018 CVPR2018 的论文做了比较，实验证明，只需要通过各种 trick 的组合，就比提出的算法高很多。 2. Standard Baseline ResNet50 为基础网络，修改最后一个 fc 层的输出维度为行人类别 $N$. 随机取 $P$ 个人， 每个人 $K$ 张图片，所以每个 batch $B=P\times K$，作者设置 $P=16, K=4$. 图片 resize 成 256x128，并添加10个0元素的 pad，然后随机 crop 成 256x128. 图片随机水平翻转概率设置为0.5. 图片的像素值转化为 [0,1]，并且 normalize: mean=0.485, 0.456, 0.406, std=0.229, 0.224, 0.225. 模型提取特征记为 $f$，ID 的预测值为 $p$. ReID features $f$ 用于计算 triplet loss，ID prediction logits $p$ 用于计算交叉熵，margin $m=0.3$ 优化器: Adam: lr=0.00035, epoch=120, 第40和第70个 epoch 乘 0.1. 3. Training Tricks 3.1 Warmup Learning RateQuestion: 前10个epoch是不是写错了，应该是-4，不是-5 lr(t)=\begin{cases} 3.5\times 10^{-5}\times \frac{t}{10}, &\text{if } 10 \ge t \ge 1 \\ 3.5\times 10^{-4}, &\text{if } 40 \ge t \ge 10 \\ 3.5\times 10^{-5}, &\text{if } 70 \ge t \ge 40 \\ 3.5\times 10^{-6}, &\text{if } 120 \ge t \ge 70 \\ \end{cases}3.2 Random Erasing Augmentation probability: $p_e=0.5$ rectangle region $I_e: S_e=W_e\times H_e$, $0.02&lt;S_e&lt;0.4$ area ratio: $r_1&lt; r_e=\frac{S_e}{S} &lt; r_2$, $r_1=0.3, r_2=3.33$ 3.3 Label Smoothingq_i=\begin{cases} 1-\frac{N-1}{N}\epsilon, &\text{if } i=y \\ \frac{\epsilon}{N}, &\text{otherwise} \end{cases}$\epsilon=0.1$ 3.4 Last Stride stride=2: 256x128-&gt;8x4 stride=1: 256x128-&gt;16x8 3.5 BNNeck ID loss: 更偏向于 cosine distance triplet loss: 更偏向于 Euclidean distance 3.6 Center Loss三元组损失只能使一个 batch 内的正负样本的值相差比较大，却不能考虑全局的正负样本值。 Triplet loss: T_{Tri}=[ d_p-d_n+\alpha ]_+Center loss: L_{C}=\frac{1}{2} \sum_{j=1}^B \parallel f_{t_j} -c_{y_j} \parallel_2^2其中 $y_j$ 是第 j 张图片的label，$c_{y_j}$ 表示第 $y_j$ 类的特征的中心，$f_{t_j}$ 表示提取的特征 $f_t$. Overall: L=L_{ID}+L_{Triplet}+\beta L_{C}$\beta=0.0005$ 4. Experimental Results作者一共做了两组实验，一组是在 source damain 上的，一组是在 cross domain 上的。 4.1 Influences of Each Trich (Same domain) 这些应该是在前面各种 trick 已经有的情况再加一个得到的结果。 4.2 Analysis of BNNeck 我觉得这个表格说明了 bn 层是用的，但是在测试的时候取 $f_t$ 还是 $f_i$ ，用 cosine 还是 Euclidean 是无所谓的。 4.3 Influences of Each Trick (Cross domain) warmup and label smoothing 更有用一些，stride=1, center loss 没啥用，REA 有负作用。 5. Supplementary Experiments5.1 Influences of the Number of Batch Size 差别也就在2个点足有，不是特别大，但是更大的 batch 是更有用的。 5.2 Influences of Image Size 也差不太多]]></content>
      <categories>
        <category>re-id</category>
      </categories>
      <tags>
        <tag>re-id</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MAR]]></title>
    <url>%2F2019%2F05%2F13%2FMAR%2F</url>
    <content type="text"><![CDATA[0. 前言 paper: CVPR2019 Unsupervised Person Re-identification by Soft Multilabel Learning code: pytorch 参考链接: https://www.cnblogs.com/Thinker-pcw/p/10807681.html 出发点 Multi-label 很强，效果的确好，就是论文看得有点头晕，有些公式自己之前从来没见过，并且有些公式的出发点没有实验证明。 这篇论文是腾讯的，今年腾讯优图实验室25篇、腾讯AILab33篇共计55篇论文被 CVPR 2019 录取。 1. Introduction这是篇跨数据集的行人重识别，source 是 MSMT，target 是 Market 和 Duke。 作者针对跨数据集行人重识别没有标签问题，提出四个工作： soft multilabel learning, 示例如下图 soft multilabel-guided hard negative mining cross-view consistent soft multi-label learning reference agent learning 用分类结果作为图片的真值，即 soft multilabel 确定正负样本：特征相似但是 soft multilabel 不相似的作为负样本，特征相似且 soft multilabel 相似的作为正样本 跨摄像头的一致性，摄像头同一数据集下，应该是不管哪个摄像头下，得到的 soft multilabel 的分布是近似的， reference agent 应该满足：有标签数据集中，和 agent 同类别的图片得到的特征应该和 agent 相似，不同类别的图片得到的特征应该不相似，在无标签数据集中，图片得到的特征和 agent 应该不相似。 这几个点感觉完全没有联系啊，作者是咋想到的并放在一起的呢？ 剩下的三个创新点后面依次阐述，其理解还是有点费劲的。 注: 本论文中的 auxiliary dataset 等价于 source dataset agent 是一个单独的 classx2048 维的数据，代码中是直接调用的 fc.weight。 2. Deep Soft Multilabel Reference Learning2.1 Problem formulation and Overview 符号 含义 $X=\lbrace x_i \rbrace_{i=1}^{N_u}$ 没有标签的数据集，$N_u$张图片 $Z=\lbrace z_i, w_i \rbrace_{i=1}^{N_a}, \text{where }w_i=1,2,…, N_p$ 有标签的数据集 auxiliary，$z_i$表示图片，$w_i$表示 label，$N_a$ 张图片，$N_p$个人，有标签数据集和无标签数据集人物没有重叠 $f(\cdot)$ discriminative deep feature embedding，应该是特征提取模型，即 $f(x)$，满足 $\parallel f(\cdot) \parallel_2=1$ $\lbrace a_i \rbrace_{i=1}^{N_p}$ reference person feature, $\parallel a_i \parallel_2=1$ $y=l(f(x),\lbrace a_i \rbrace_{i=1}^{N_p})\in R^{N_p}$ soft multilabel function $l(\cdot)$, where $y=(y^{(1)}, y^{(2)}, …, y^{(N_p)})$, $\sum_i^{N_p} y^{(i)}=1, y^{(i)}\in [0, 1]$ 一共有两个内容需要学习: $f(\cdot)$, $\lbrace a_i \rbrace_{i=1}^{N_p}$ 2.2 Soft multilabel-guided hard negative mining大哥，你的上下标能不能提前说清楚啊[捂脸]，算了算了，腾讯的，惹不起惹不起。 定义 soft multilabel function: y^{(k)}=l(f(x),\lbrace a_i \rbrace_{i=1}^{N_p})^{(k)}=\frac{\exp(a_k^Tf(x))}{\sum_i \exp(a_i^Tf(x))}也就是说，用 $f(x)$ 去依次点乘 $a_k$， 然后用一个 softmax。 按照这个公式来说的话，是在特征空间上做的操作，有点类似 ECN 中的预测概率，越相似，值越大，而不是通过分类器预测概率。 代码中有温度T。 假设：如果一对样本 $x_i, x_j$ 有很高的特征相似性, 即 $f(x_i)^Tf(x_j)$，称之为相似样本。如果这对相似样本的其他特性也相似，则大概率为一对正样本，如果其他特性不相似，则大概率为一个难负样本 hard negative pair。 注：这里并没有实验证明一对 hard negative pair 的其他特性（主要指下文提到的 soft multilabel agreement）大概率不相似。所以表示存疑其假设的正确性。又想了想，在难采样三元组损失中，hard negative pair 就是指特征相似但是 label 不同的样本，positive pair 指 label 相同的样本的，easy positive pair 指 label 相同特征相似的样本，hard positive pair 指 label 相同特征不相似的样本，这样的话可以把 soft multilabel 看成样本的 label 的话，也是可以说得通的。 引理1：其他特性的相似性：作者选用 soft multilabel 作为其他特性，soft multilabel agreement $A(\cdot, \cdot)$ 表示作为其他特性的相似性。定义为 A(y_i,y_j)=y_i \land y_j=\sum_k \min(y_i^{(k)},y_j^{(k)})=1-\frac{\parallel y_i-y_j \parallel_1}{2} \in [0,1]越相似，值越大。最后一个等号通过画图很容易求得，就不解释了。Question: 这里的相似性定义成了向量之间的一范，没有定义成熟悉的点积，暂时不知道原因。 引理2： hard negative pair：对于无标签数据集 $X$ 的所有样本对 $M=N_u\times (N_u-1)/2$，设置比例 $p$，取 $pM$ 个特征最相似的样本对，即 $\hat{M}=\lbrace (i,j)|f(x_i)^T f(x_j)\ge S\rbrace, \parallel \hat{M} \parallel=pM$, 其中 $S$ 表示 $pM$ 个特征最相似样本对的阈值，动态变化，不是很重要的，重要的是取 $pM$个样本对。然后根据 label 的相似性将这些样本对划分为 positive set $P$ and hard negative set $N$，即 P=\lbrace (i,j)|f(x_i)^T f(x_j)\ge S, A(y_i, y_j)\ge T \rbraceN=\lbrace (k,l)|f(x_k)^T f(x_l)\ge S, A(y_k, y_l)< T \rbrace其中 $T$ 表示 soft multilabel agreement 的阈值。会更新。 loss：soft Multilabel-guided Discriminative embedding Learning: L_{MDL}=-\log \frac{\bar{P}}{\bar{P}+\bar{N}}where, \bar{P}=\frac{1}{|P|}\sum_{(i,j)\in P}\exp(-\parallel f(x_i)-f(x_j) \parallel_2^2)\bar{N}=\frac{1}{|N|}\sum_{(k,l)\in N}\exp(-\parallel f(x_k)-f(x_l) \parallel_2^2)so, \begin{aligned} L_{MDL}&=-\log \frac{\bar{P}}{\bar{P}+\bar{N}} \\ &=-\log \frac{\frac{1}{|P|}\sum_{(i,j)\in P}\exp(-\parallel f(x_i)-f(x_j) \parallel_2^2)}{\frac{1}{|P|}\sum_{(i,j)\in P}\exp(-\parallel f(x_i)-f(x_j) \parallel_2^2)+\frac{1}{|N|}\sum_{(k,l)\in N}\exp(-\parallel f(x_k)-f(x_l) \parallel_2^2)} \end{aligned}Question: 这是个啥公式啊，都没有见过类似的公式，作者也没有给出解释。 此时固定 agent $\lbrace a_i \rbrace_{i=1}^{N_p}$ ，学习 $f(\cdot)$. 实际训练时，$M=M_{batch}=N_{batch}\times (N_{batch}-1)/2$ 2.3 Cross-view consistent soft multilabel learning因为行人重识别要求跨摄像头识别，所以考虑到行人的分布应该与摄像头无关。 Loss: L_{CML}=\sum_v d(P_v(y), P(y))^2其中，$P(y)$ 表示数据集 $X$ 的 soft multilabel 分布，$P_v(y)$ 表示数据集 $X$ 在摄像头 $v$ 的 soft multilabel 分布，$d(\cdot, \cdot)$ 表示分布的距离，可以是 KL divergence 或者 Wasserstein distance.因为实际观察到服从 log-normal 分布，所以采取 simplified 2-Wasserstein distance。 L_{CML}=\sum_v \parallel \mu_v-\mu \parallel_2^2 + \parallel \sigma_v-\sigma\parallel_2^2其中，$\mu/\sigma$表示总体数据集的 log-soft multilabel 的均值和方差，$\mu_v/\sigma_v$表示总体数据集在摄像头$v$的 log-soft multilabel 的均值和方差.Question: 这个公式又是咋推出来的，这是妥妥地写出来也看不懂系列。 此时固定 agent $\lbrace a_i \rbrace_{i=1}^{N_p}$ ，学习 $f(\cdot)$. 2.4 Reference agent Learning考虑到 referentce agent 需要与 soft multilabel function $l(\cdot, \cdot)$ 有关，因此得到损失函数 L_{AL}=\sum_k -\log l(f(z_k), {a_i})^{(w_k)}=\sum_k -\log \frac{\exp(a_{w_k}^T f(z_k))}{\sum_j \exp(a_j^T f(z_k))}其中，$z_k$ 表示有标签数据集 $Z$ 中标签为 $w_k$ 的第 $k$ 张图片。 这里可以理解成 $z_k$ 的预测概率和真实概率的交叉熵损失。这个损失函数不仅训练 $a_i$ 更接近第i个人的所有图片的特征，也训练 feature embedding $f$，使 $l(\cdot, \cdot)$ 得到的标签更具有表示同一个人的能力，符合 soft multilabel-guided hard negative mining 的假设：特征相似，但是 soft multilabel 不相似的为 hard negative mining。 这个公式更新的是 $f$ 和 agent $a_i$. 注：该论文中的公式其实按照从广义的定义到实际的应用的具体化过程，所以刚开始才会感觉有点乱，公式里面的字符也会一变再变，其实是从理论的公式到具体化实际代码的过程。 Joint embedding learning for reference comparability: 为了更好地提高 soft multilabel function 表示无标签数据集图片的正确性，提出 Joint embedding learning for reference comparability，为了修正 domain shift，利用无标签数据集 $f(x)$ 和 $a_i$ 肯定不是一对，提出 loss: L_{RJ}=\sum_i \sum_{j\in M_i} \sum_{k:w_k=i}[m-\parallel a_i-f(x_j) \parallel_2^2]_+ + \parallel a_i-f(z_k) \parallel_2^2其中，其目的是为了保证$a_i$所表示的有标签数据集中的同一id的图片和$a_i$特征相似，$a_i$和所有无标签数据集中的图片特征都不相似。 $M_i=\lbrace j| \parallel a_i-f(x_j) \parallel_2^2 &lt; m \rbrace$，表示对第 $i$ 个 agent $a_i$ 而言，特征最为相似 ($\parallel a_i-f(x_j) \parallel_2^2=2(1-a_i^Tf(x_j))$, 越相似，值越小) 的无标签数据集中的图片，按照作者推荐的论文 44:Normface: l2 hypersphere embedding for face verification，建议 $m=1$。 此时固定 agent $\lbrace a_i \rbrace_{i=1}^{N_p}$ ，学习 $f(\cdot)$. 对 $L_{RJ}$ 根据代码再次做出解释，$L_{RJ}$ 的目的是学习更好的特征提取器 $f$，使有标签数据集提取出的特征与同类别的 agent 的特征相似，与不同类别的agent不相似，无标签数据集提取出的特征与 agent 都不相似，有点三元组损失的意思。此时的 $a_i$ 是常量，不进行反向求导的。其二，对于任意一个 agent $a_i$，有标签数据集中，label 等于 $i$ 的图片视为正样本，其他图片视为负样本，对于无标签数据集，则直接视为 $a_i$ 的负样本。具体来说，就是对每一张有标签数据集的图片，$a_{label}$ 为正， 其余 $a_i$ 为负，对于每一张无标签数据集的图片，$a_i$ 都为负。 所以总的 reference agent learning loss为: L_{RAL}=L_{AL}+\beta L_{RJ}2.5.1 Model training and testingL_{MAR}=L_{MDL}+\lambda_1 L_{CML}+\lambda_2 L_{RAL}3. ExperimentsMSMT17 为辅助数据集， Market-1501、Duke 为无标签数据集。 备注: 论文中的 agent 其实并不是之前以为通过图片输入模型得到的特征求出来的，而是 ResNet-50 的 fc.weight(classx2048) ，也就是分类器的分类向量。和 ECN 论文中的使用方法有很大的不同吧。在 ECN 中，使用的就是图片输入模型得到的特征，可能是因为 ECN 中一张图片对应一个特征，而本论文中是多个图片对应一个特征。 4. code4.1 Logger两种logger 1234567891011121314151617181920212223# 第一种：定义简单，使用繁琐# 定义import timedef time_string(): ISOTIMEFORMAT = '%Y-%m-%d %X' string = '[&#123;&#125;]'.format(time.strftime(ISOTIMEFORMAT, time.localtime(time.time()))) return stringclass Logger(object): def __init__(self, save_path): if not os.path.isdir(save_path): os.makedirs(save_path) self.file = open(os.path.join(save_path, 'log_&#123;&#125;.txt'.format(time_string())), 'w') self.print_log("python version : &#123;&#125;".format(sys.version.replace('\n', ' '))) self.print_log("torch version : &#123;&#125;".format(torch.__version__)) def print_log(self, string): self.file.write("&#123;&#125;\n".format(string)) self.file.flush() print(string)# 使用logger = Logger(args.save_path)logger.print_log("=&gt; loading checkpoint '&#123;&#125;'".format(load_path)) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 第二种，定义繁琐，使用简单，重定向# 推荐用这种# 定义# .\logging.pyfrom __future__ import absolute_importimport osimport sysimport errnodef mkdir_if_missing(dir_path): try: os.makedirs(dir_path) except OSError as e: if e.errno != errno.EEXIST: raiseclass Logger(object): def __init__(self, fpath=None): self.console = sys.stdout self.file = None if fpath is not None: mkdir_if_missing(os.path.dirname(fpath)) self.file = open(fpath, 'w') def __del__(self): self.close() def __enter__(self): pass def __exit__(self, *args): self.close() def write(self, msg): self.console.write(msg) if self.file is not None: self.file.write(msg) def flush(self): self.console.flush() if self.file is not None: self.file.flush() os.fsync(self.file.fileno()) def close(self): self.console.close() if self.file is not None: self.file.close()# 使用import sysimport os.path as ospsys.stdout = Logger(osp.join(args.logs_dir, 'log.txt'))print(args) 4.2 model12345678910111213141516171819202122232425262728class ResNet(nn.Module): def forward(self, x): # 3x384x128 x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) feature_maps = self.layer4(x) # 2048x12x4 x = self.avgpool(feature_maps) x = x.view(x.size(0), -1) # bx2048 # renorm 使每一行的向量的2范进行了截断处理 # 使之变成[0,1e-5]，再线性变成[0,1] # 这里的renorm可以暂时理解成进行了二范处理， feature = x.renorm(2, 0, 1e-5).mul(1e5) # bx2048 w = self.fc.weight # 注: 这个self.fc.weight(classx2048)就是论文中的agent ww = w.renorm(2, 0, 1e-5).mul(1e5) sim = feature.mm(ww.t()) # sim: bxclass # feature(f): bx2048, sim(y): bxclass, feature_maps: 2048x12x4 return feature, sim, feature_maps 4.3 optim123bn_params, other_params = partition_params(self.net, 'bn')self.optimizer = torch.optim.SGD([&#123;'params': bn_params, 'weight_decay': 0&#125;, &#123;'params': other_params&#125;], lr=args.lr, momentum=0.9, weight_decay=args.wd) 4.4 trainer/init_losses1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495class ReidTrainer(Trainer): def __init__(self, args, logger): self.al_loss = nn.CrossEntropyLoss().cuda() self.rj_loss = JointLoss(args.margin).cuda() self.cml_loss = MultilabelLoss(args.batch_size).cuda() # L_CML self.mdl_loss = DiscriminativeLoss(args.mining_ratio).cuda() # L_MDL self.net = resnet50(pretrained=False, num_classes=self.args.num_classes) self.multilabel_memory = torch.zeros(N_target_samples, 4101) def init_losses(self, target_loader): self.logger.print_log('initializing centers/threshold ...') if os.path.isfile(self.args.ml_path): (multilabels, views, pairwise_agreements) = torch.load(self.args.ml_path) self.logger.print_log('loaded ml from &#123;&#125;'.format(self.args.ml_path)) else: self.logger.print_log('not found &#123;&#125;. computing ml...'.format(self.args.ml_path)) sim, _, views = extract_features(target_loader, self.net, index_feature=1, return_numpy=False) # sim: bxclass, views: bx1 multilabels = F.softmax(sim * self.args.scala_ce, dim=1) # multilabels: bxclass 这里应该对于soft multilabel funtion得到的结果y^&#123;(k)&#125; # Question: sim*self.args.scala_ce 是什么意思 ml_np = multilabels.cpu().numpy() pairwise_agreements = 1 - pdist(ml_np, 'minkowski', p=1)/2 # pairwise_agreements: soft multilabel agreement A(.,.) 公式2 log_multilabels = torch.log(multilabels) self.cml_loss.init_centers(log_multilabels, views) self.logger.print_log('initializing centers done.') self.mdl_loss.init_threshold(pairwise_agreements) self.logger.print_log('initializing threshold done.') def train_epoch(self, source_loader, target_loader, epoch): self.lr_scheduler.step() if not self.cml_loss.initialized or not self.mdl_loss.initialized: self.init_losses(target_loader) batch_time_meter = AverageMeter() stats = ('loss_source', 'loss_st', 'loss_ml', 'loss_target', 'loss_total') meters_trn = &#123;stat: AverageMeter() for stat in stats&#125; self.train() end = time.time() target_iter = iter(target_loader) for i, source_tuple in enumerate(source_loader): imgs = source_tuple[0].cuda() labels = source_tuple[1].cuda() try: target_tuple = next(target_iter) except StopIteration: target_iter = iter(target_loader) target_tuple = next(target_iter) imgs_target = target_tuple[0].cuda() labels_target = target_tuple[1].cuda() views_target = target_tuple[2].cuda() idx_target = target_tuple[3] features, similarity, _ = self.net(imgs) features_target, similarity_target, _ = self.net(imgs_target) # features: bx2048, similarity: bxclass scores = similarity * self.args.scala_ce loss_source = self.al_loss(scores, labels) # 公式7，同时训练 agent 和 f() agents = self.net.module.fc.weight.renorm(2, 0, 1e-5).mul(1e5) # features: bx2048, agents: classx2048, labels: bx1, similarity: bxclass loss_st = self.rj_loss(features, agents.detach(), labels, similarity.detach(), features_target, similarity_target.detach()) multilabels = F.softmax(features_target.mm(agents.detach().t_()*self.args.scala_ce), dim=1) loss_ml = self.cml_loss(torch.log(multilabels), views_target) if epoch &lt; 1: loss_target = torch.Tensor([0]).cuda() else: multilabels_cpu = multilabels.detach().cpu() is_init_batch = self.initialized[idx_target] initialized_idx = idx_target[is_init_batch] uninitialized_idx = idx_target[~is_init_batch] self.multilabel_memory[uninitialized_idx] = multilabels_cpu[~is_init_batch] self.initialized[uninitialized_idx] = 1 self.multilabel_memory[initialized_idx] = 0.9 * self.multilabel_memory[initialized_idx] \ + 0.1 * multilabels_cpu[is_init_batch] loss_target = self.mdl_loss(features_target, self.multilabel_memory[idx_target], labels_target) self.optimizer.zero_grad() loss_total = loss_target + self.args.lamb_1 * loss_ml + self.args.lamb_2 * \ (loss_source + self.args.beta * loss_st) loss_total.backward() self.optimizer.step() for k in stats: v = locals()[k] meters_trn[k].update(v.item(), self.args.batch_size) batch_time_meter.update(time.time() - end) freq = self.args.batch_size / batch_time_meter.avg end = time.time() if i % self.args.print_freq == 0: self.logger.print_log(' Iter: [&#123;:03d&#125;/&#123;:03d&#125;] Freq &#123;:.1f&#125; '.format( i, len(source_loader), freq) + create_stat_string(meters_trn) + time_string()) save_checkpoint(self, epoch, os.path.join(self.args.save_path, "checkpoints.pth")) return meters_trn 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# L_CML 公式6class MultilabelLoss(torch.nn.Module): def __init__(self, batch_size, use_std=True): super(MultilabelLoss, self).__init__() self.use_std = use_std self.moment = batch_size / 10000 self.initialized = False def init_centers(self, log_multilabels, views): """ :param log_multilabels: shape=(N, n_class) :param views: (N,) :return: # 用于初始化全局的均值和方差 """ univiews = torch.unique(views) mean_ml = [] std_ml = [] for v in univiews: ml_in_v = log_multilabels[views == v] mean = ml_in_v.mean(dim=0) std = ml_in_v.std(dim=0) mean_ml.append(mean) std_ml.append(std) center_mean = torch.mean(torch.stack(mean_ml), dim=0) center_std = torch.mean(torch.stack(std_ml), dim=0) self.register_buffer('center_mean', center_mean) self.register_buffer('center_std', center_std) self.initialized = True def _update_centers(self, log_multilabels, views): """ :param log_multilabels: shape=(BS, n_class) :param views: shape=(BS,) :return: """ univiews = torch.unique(views) means = [] stds = [] for v in univiews: ml_in_v = log_multilabels[views == v] if len(ml_in_v) == 1: continue mean = ml_in_v.mean(dim=0) means.append(mean) if self.use_std: std = ml_in_v.std(dim=0) stds.append(std) new_mean = torch.mean(torch.stack(means), dim=0) self.center_mean = self.center_mean * (1 - self.moment) + new_mean * self.moment if self.use_std: new_std = torch.mean(torch.stack(stds), dim=0) self.center_std = self.center_std * (1 - self.moment) + new_std * self.moment def forward(self, log_multilabels, views): """ :param log_multilabels: shape=(BS, n_class) :param views: shape=(BS,) :return: """ self._update_centers(log_multilabels.detach(), views) univiews = torch.unique(views) loss_terms = [] for v in univiews: ml_in_v = log_multilabels[views == v] if len(ml_in_v) == 1: continue mean = ml_in_v.mean(dim=0) loss_mean = (mean - self.center_mean).pow(2).sum() loss_terms.append(loss_mean) if self.use_std: std = ml_in_v.std(dim=0) loss_std = (std - self.center_std).pow(2).sum() loss_terms.append(loss_std) loss_total = torch.mean(torch.stack(loss_terms)) return loss_total 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# L_MDL 公式4class DiscriminativeLoss(torch.nn.Module): def __init__(self, mining_ratio=0.001): super(DiscriminativeLoss, self).__init__() self.mining_ratio = mining_ratio self.register_buffer('n_pos_pairs', torch.Tensor([0])) self.register_buffer('rate_TP', torch.Tensor([0])) self.moment = 0.1 self.initialized = False def init_threshold(self, pairwise_agreements): # Question：论文中还有一个限制条件，f(x_i)f(x_j)&gt;S，代码只考虑了A(y_i, y_j) pos = int(len(pairwise_agreements) * self.mining_ratio) sorted_agreements = np.sort(pairwise_agreements) t = sorted_agreements[-pos] self.register_buffer('threshold', torch.Tensor([t]).cuda()) self.initialized = True def forward(self, features, multilabels, labels): """ :param features: shape=(BS, dim) :param multilabels: (BS, n_class) :param labels: (BS,) :return: """ P, N = self._partition_sets(features.detach(), multilabels, labels) if P is None: pos_exponant = torch.Tensor([1]).cuda() num = 0 else: sdist_pos_pairs = [] for (i, j) in zip(P[0], P[1]): sdist_pos_pair = (features[i] - features[j]).pow(2).sum() sdist_pos_pairs.append(sdist_pos_pair) pos_exponant = torch.exp(- torch.stack(sdist_pos_pairs)).mean() num = -torch.log(pos_exponant) if N is None: neg_exponant = torch.Tensor([0.5]).cuda() else: sdist_neg_pairs = [] for (i, j) in zip(N[0], N[1]): sdist_neg_pair = (features[i] - features[j]).pow(2).sum() sdist_neg_pairs.append(sdist_neg_pair) neg_exponant = torch.exp(- torch.stack(sdist_neg_pairs)).mean() den = torch.log(pos_exponant + neg_exponant) loss = num + den return loss def _partition_sets(self, features, multilabels, labels): """ partition the batch into confident positive, hard negative and others :param features: shape=(BS, dim) :param multilabels: shape=(BS, n_class) :param labels: shape=(BS,) :return: P: positive pair set. tuple of 2 np.array i and j. i contains smaller indices and j larger indices in the batch. if P is None, no positive pair found in this batch. N: negative pair set. similar to P, but will never be None. """ f_np = features.cpu().numpy() ml_np = multilabels.cpu().numpy() p_dist = pdist(f_np) p_agree = 1 - pdist(ml_np, 'minkowski', p=1) / 2 sorting_idx = np.argsort(p_dist) n_similar = int(len(p_dist) * self.mining_ratio) similar_idx = sorting_idx[:n_similar] is_positive = p_agree[similar_idx] &gt; self.threshold.item() pos_idx = similar_idx[is_positive] neg_idx = similar_idx[~is_positive] P = dist_idx_to_pair_idx(len(f_np), pos_idx) N = dist_idx_to_pair_idx(len(f_np), neg_idx) self._update_threshold(p_agree) self._update_buffers(P, labels) return P, N def _update_threshold(self, pairwise_agreements): pos = int(len(pairwise_agreements) * self.mining_ratio) sorted_agreements = np.sort(pairwise_agreements) t = torch.Tensor([sorted_agreements[-pos]]).cuda() self.threshold = self.threshold * (1 - self.moment) + t * self.moment def _update_buffers(self, P, labels): if P is None: self.n_pos_pairs = 0.9 * self.n_pos_pairs return 0 n_pos_pairs = len(P[0]) count = 0 for (i, j) in zip(P[0], P[1]): count += labels[i] == labels[j] rate_TP = float(count) / n_pos_pairs self.n_pos_pairs = 0.9 * self.n_pos_pairs + 0.1 * n_pos_pairs self.rate_TP = 0.9 * self.rate_TP + 0.1 * rate_TP 12345678910111213141516171819202122232425262728293031323334353637# L_RJ 公式 8# 与公式8略有不同class JointLoss(torch.nn.Module): def __init__(self, margin=1): super(JointLoss, self).__init__() self.margin = margin self.sim_margin = 1 - margin / 2 def forward(self, features, agents, labels, similarity, features_target, similarity_target): """ :param features: shape=(BS/2, dim) :param agents: shape=(n_class, dim) :param labels: shape=(BS/2,) :param features_target: shape=(BS/2, n_class) :return: """ loss_terms = [] arange = torch.arange(len(agents)).cuda() zero = torch.Tensor([0]).cuda() for (f, l, s) in zip(features, labels, similarity): loss_pos = (f - agents[l]).pow(2).sum() # 公式8的最后一项，a_i-f(z_k) loss_terms.append(loss_pos) neg_idx = arange != l # 从agent中选出与当前图片特征相似度高于阈值，但不是同一类的的agent hard_agent_idx = neg_idx &amp; (s &gt; self.sim_margin) # 越相似，值越大 if torch.any(hard_agent_idx): hard_neg_sdist = (f - agents[hard_agent_idx]).pow(2).sum(dim=1) loss_neg = torch.max(zero, self.margin - hard_neg_sdist).mean() loss_terms.append(loss_neg) for (f, s) in zip(features_target, similarity_target): hard_agent_idx = s &gt; self.sim_margin if torch.any(hard_agent_idx): hard_neg_sdist = (f - agents[hard_agent_idx]).pow(2).sum(dim=1) loss_neg = torch.max(zero, self.margin - hard_neg_sdist).mean() loss_terms.append(loss_neg) loss_total = torch.mean(torch.stack(loss_terms)) return loss_total 根据代码，重新明确两个定义: similarity 指的是图片的 feature1 和 agent 的 feature2 的特征相似性: feature1*feature2 multilabels 指的是 similarity.mul(self.args.scala_ce) 再softmax得到的 算了，有些代码还是跑的时候看吧，因为有些更新方式有些看不懂。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>person re-id</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[residual_attention]]></title>
    <url>%2F2019%2F05%2F10%2Fresidual-attention_and_CBAM_GCNet%2F</url>
    <content type="text"><![CDATA[0. 前言 paper: 商汤 CVPR2017 Residual Attention_Network for Image Classification code: caffe, caffe网络可视化工具 Netscope, pytorch paper: ECCV2018_CBAM: Convolutional Block Attention Module code: pytorch paper: GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond code: pytorch 这篇论文也是讲 attention map 的，主要用于分类，说实话，没有太理解其中的创新点，可能是因为不懂整个 attention map 的进程，前人做到了什么地步，从效果图上看，感觉比 Dual Attention 那篇论文还是差一些。不过能上 CVPR 的肯定有牛的地方，只是自己水平不够。 1. Residual Attention Network每个 Attention Module 都分为两个分支：mask branch and trunk branch。 H_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)其中，c表示通道，i表示所有的位置，$M_{i,c}(x)\in[0,1]$ 1.1 Spatial Attention and Channel AttentionMixed attention: f_1(x_{i,c})=\frac{1}{1+\exp (-x_{i,c})}Channel Attention: f_2(x_{i,c})=\frac{x_{i,c}}{\parallel x_i \parallel}Spatial Attention: f_3(x_{i,c})=\frac{1}{1+\exp (-(x_{i,c}-mean_c)/std_c)} 2. code代码有点绕，暂时没有看懂。 3. CBAM 4. code简单明了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class ChannelAttention(nn.Module): def __init__(self, in_planes, ratio=16): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) self.relu1 = nn.ReLU() self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x)))) max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x)))) out = avg_out + max_out return self.sigmoid(out)class Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None): super(Bottleneck, self).__init__() self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False) self.bn3 = nn.BatchNorm2d(planes * 4) self.relu = nn.ReLU(inplace=True) self.ca = ChannelAttention(planes * 4) self.sa = SpatialAttention() self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) out = self.ca(out) * out out = self.sa(out) * out if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out 5. GCNet 综合考虑了 SE-block 和 NL-block。 代码有点绕，暂时不看。 5. 优化器 optimizerhttps://zhuanlan.zhihu.com/p/64882877?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=589386839161311232 code 不同的优化器，对结果也有很大的影响，先只记录一下 SGD, Adam 的常见参数配置。 原始 SGD： 学习率太小 0.20 ，有限步内无法达到最优，全程都没有震荡 学习率太大 0.61 ，会造成剧烈震荡，随着学习率的增加，后期震荡逐渐变大，0.52时后期震荡就已经无法收敛了 学习率适中 0.42 ，会成功达到最优点，前期会有震荡，后期没有震荡 对学习率很敏感 带动量的 SGD： 适中的动量 beta=0.42 可以减少大学习率 lr=0.61 的震荡，达到最优点 较大的动量 beta=0.81 会对大学习率 lr=0.61 引入更大的震荡 一般的做法是大的动量 beta=0.9 和小的学习率 lr=0.02 or 0.03，会以比较平缓的方式加速达到最优 当 lr=0.01 时，beta不管怎么取都达不到最优，或者没到，或者超过 当 beta=0.9 时，对学习率也比较敏感，0.01会到不了最优点、0.02会到最优点前面、0.03会到最优点后面，区别比较大 常用设置：beta=0.9，weight-decay=5e-4，nesterov=True，之后再调节 lr 吧。 1torch.optim.SGD(params, lr, momentum=0.9, weight_decay=5e-4, nesterov=True) Adam： 对学习率不敏感 训练稳定 在最优点附件 参数设置：beta1=0.9, beta2=0.999, eta=1e-8 虽然 Adam 对初始学习率不敏感，训练也比较稳定，但最终能达到的精度没有手动调好的SGD来得高。 123torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)# 更推荐 0.5, 0.999torch.optim.Adam(params, lr=0.001, betas=(0.5, 0.999), eps=1e-08, weight_decay=1e-4)]]></content>
      <categories>
        <category>attention</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECN]]></title>
    <url>%2F2019%2F05%2F06%2FECN%2F</url>
    <content type="text"><![CDATA[0. 前言钟准团队的CCPR2019文章，牛逼。思想有一丢丢类似聚类，但是比聚类强的是，赋予了id。与HHL文章有很大的不同。什也不说了，虚心学习。 paper: CVPR2019_Invariance Matters Exemplar Memory for Domain Adaptive Person Re-identification code: pytorch [x] 2019-06-03: 这篇文章和 One Example reID 有些相似啊。感觉大家的论文之间的联系错综复杂。 1. Introduction作者的注意力还是集中在如何区分 target domain 中的图片。考虑了三个因素，exemplar-invariance, camera-invariance and neighborhood-invariance. 可以简单地理解成每个人是一类，StarGAN生成的图片是一类，特征相似的是一类，并且为了实现上述目标，需要存储 target domain 中 train 数据集的所有特征，这一点有点疑问，需要的存储空间和计算时间消耗大吗？作者在第一节的最后有解答，消耗的空间和时间都很小。等自己实验的时候就知道了。它是用了两个分类器，一个用于训练 source 数据集， label 是 source 的label，一个用于训练 target 数据集， label 是 target 的label，两个label不一样。网络模型与HHL略有不同。 第一，通过分类损失使 target domain 的图片全部可分。分类模型学到的更可能是图片视觉上的相似性而不是语义相似性，并且数据集中同一id的图片在视觉上变化很大，所以，索性将target数据集上的每张图片作为一类，迫使不同id的相似图片变得不相似。行吧，这个理由，(⊙o⊙)…，怎么有点牵强吧，因为和后面的特征相似的作为一类有一丢丢矛盾。不对，有点类似既然不知道你们的id，那就把让你们之间先全部可分，这个思路在哪篇论文见过，想不起来了。 第二，同一 content 不同 camera style 的图片视为一类。这个和HHL的数据集处理方法一样，用 StarGAN 生成同一图片在不同 camera 下的图片，并视为一类，因为刚刚对图片使用了分类损失，所以这时候可以直接用到分类损失上，而不用再考虑三元组损失，(我觉得可能是基于分类损失的作用一直大于二元组损失，三元组损失)，这时可以保证同一 content 不同 camera style 的图片视为一类。这时 StarGAN 生成图片中的人的形态等基本保留，感觉更多是亮度上的变化(肉眼可见)，偶尔会有衣服颜色的变换，但是不能变换这个人的姿势，比如正面走变成侧面走这种，HHL 已经证明了 camera style 的影响，但是我觉得姿势的变换也是一个重要的因素才对，因为分类模型的起家就是识别同一类物体不同形态的图片。好像 CVPR 2019 这个团队有做这方面的内容，抽空膜拜一波。 第三，k近邻的图片视为一类。这应该像是之间看到的一些关于聚类的或者k近邻的论文，总体思想是因为不知道图片的真id，那就可能赋予一个比较接近真实的id，又因为同一id的图片的特征应该是临近的，所以把临近的图片作为同一id也不过分，其实有的论文用的是聚类，但本质是一样的。 我觉得钟准厉害的地方不仅仅是能想到这三个创新点，更重要的是能把这三个创新点融合到一起，因为这三个创新点在别的论文里或多或少都可以看到影子，但是因为实现的的方法、思路、具体过程都不一样，很难看见一个创新点就把融到一起，看到一个想法就放到自己的模型里，或者看到一个idea就觉得有用并且一试还真有用，并能给出属于自己的解释，而不是明显的生搬硬造，比如这里的 memory module 很有想法。 2. Related WorkUnsupervised domain adaptation. 一种方法是对齐两个域的特征，其基本假设是源域和目标域的类别一样。另一种方法是丢弃目标域的未知类别样本，学习源域到目标域的映射。 Unsupervised person re-identification. ECCV2018_HHL 可能忽略了 target domain 中其他因素对 id 的影响，比如姿势等。与 BMVC2018_DAL 不同的是，作者设计了软分类损失。 3. The Proposed Method简单定义下数学符号，直接用英文吧，能知道就行。 数学符号 含义 $\lbrace X_s, Y_s \rbrace, N_s$ source domain, $N_s$ person images, $(x_{si}, y_{s,i})$, $M$ person identities $X_t, N_t$ target domain, $N_t$ person images 3.1 Overview of Framework 其实模型没有画的这么复杂，基本还是保留ResNet-50的基本网络，后面加个FC-4049，FC-#id/ exemplar memory，其中FC-#id用于 source domain 的数据， exemplar memory 用于计算 target domain 的数据，等下直接看代码一目了然。 3.2 Supervised Learning for Source Domain这个容易理解， source domain 直接用个分类损失。 L_{src}=-\frac{1}{n_s}\sum_{i=1}^{n_s}\log p(y_{s,i}|x_{s,i})3.3 Exemplar MemoryExemplar Memory 借鉴的是 Joint Detection and Identification Feature Learning for Person Search，（已经有两篇论文没有看过了，自己菜不是没有原因的），采用的是 key-value 架构 $K,V$，其中在 key-memory 中存储每张图片的 FC-4096 特征， value-memory 中存储图片的lable，其中视每张图片为一类，index 即为 label。key-memory 中的特征初始化成0，value-memory 初始化成 $V[i]=i$，并且在整个训练过程中保持不变。key-memory 的更新策略是: K[i]\gets \alpha K[i]+(1-\alpha)f(x_{t,i})其中$\alpha\in [0,1]$，$K[i]\gets \parallel K[i] \parallel_2$，也就是每次都会进行一次归一化。这里的 $\alpha$ 是线性变化的，初始化为 0.01, 后续变化为 $\alpha = 0.01*epoch $。其中每次更新都是发生在反向传播时，顺便进行更新的。写的很巧妙。 3.4 Invariance Learning for Target Domain开始祭出三板斧了。 Exemplar-invariance: 因为同一id的图片在外观上也会有很大的变化，即每一张图片都应该只与自己相似，与其他图片差别很大，因此，可以视每一张图片为一类。具体过程是，对于给定图片$x_{t,i}$，先计算 $x_{t,i}$ 的特征与其他图片在 key-memory 存储的特征的 cosine 距离，距离越大越相似。然后用这个距离去预测 $x_{t,i}$ 属于第 i 类的概率。这种预测与我见过的分类不太一样，因为分类模型一般都是直接用特征去预测类别，但这里是用距离去预测类别，或者说用距离当预测概率，还是第一次见。 p(i|x_{t,i})=\frac{exp(K[i]^T f(x_{t,i})/\beta)}{\sum_{j=1}^{N_t}exp(K[j]^T f(x_{t,i})/\beta)}其中 $\beta\in (0,1]$ ，用于平衡分布的趋势，记为 temperature，$\beta=0.05$，根据代码来看，$K[i]$ 表示当前 model 运行之前存储的， $f(x_{t,i})$表示当前 model 的结果，也就是说两者不一样。下面求概率的过程同理。 这个公式和 distillation 很像。 q_i=\frac{exp(z_i/T)}{\sum_j exp(z_j/T)}其中，$z_i$是类别预测概率。 所以分类损失可以写成： L_{ei}=-\log p(i|x_{t,i})Camera-invariance：视经过 StarGAN 转换的图片$\hat{x}_{t,i}$是同一类，与HHL一样，但是与HHL不同的是，HHL用于三元组损失，ECN用于分类损失，类似 Exemplar-invariance 的概率求解和loss损失，得： p(i|\hat{x} _ {t,i})=\frac{exp(K[i]^T f(\hat{x} _ {t,i})/\beta)}{\sum_{j=1}^{N_t}exp(K[j]^T f(\hat{x}_{t,i})/\beta)}L_{ci}=-\log p(i|\hat{x}_{t,i})其中，$\hat{x}_ {t,i}$是在生成的C张图片中$\lbrace \hat{x}_ {t,i,1},…, \hat{x}_{t,i,C} \rbrace$随机选了一张。 这里有个问题，Exemplar Memory 中是否包含 CamStyle 生成的图片。等看代码就知道了。 Neighborhood-invariance: 对每一张图片，在数据集中一定至少存在一张与之相同id的图片，如果能得到这些图片，那就更好了，这样就可以获得同一id不同姿态的图片。具体方法是在 exemplar memory 中计算 cos 距离，用k近邻方法得到最近的k张图片的index，记为 $M(x_{t,i},k)$，显然最近的是i. $k=6$ 基于假设k近邻得到的图片$M(x_{t,i},k)$属于同一类，因此可以得到 $x_{t,i}$ 属于第 j 类的概率的权重是: w_{i,j}=\begin{cases} \frac{1}{k}, j\not = i \\ 1, j=i \end{cases}, \forall j \in M(x_{t,i},k)这种赋予权重的方法是把 $x_{t,i}$ 属于另一张图片类别的概率，而不是我之前以为的 $M(x_{t,i},k)$ 中的图片属于 $x_{t,i}$ 类别的概率。 因此损失可以写成： L_{n,i}=-\sum_{j\not = i} w_{i,j} \log p(j|x_{t,i}), \forall j \in M(x_{t,i},k)其中，为了区分 Exemplar-invariance 和 Neighborhood-invariance ，这里没有再次计算 $x_{t,i}$ 属于第 i 类的损失。 Overall loss of invariance learning: invariance learning 的总损失可以表示成： L_{tgt}=-\frac{1}{n_t} \sum_{i=1}^{n_t} \sum_{j \in M(x_{t,i},k)} w_{i,j} \log p(j|x^*_{t,i})其中，$x^*_ {t,i}$ 表示随机从$\lbrace x_{t,i}, \hat{x}_ {t,i,1},…, \hat{x}_{t,i,C}, \rbrace$ 选一张。 看看代码再说这个损失的具体实现吧。 3.5 Final Loss for NetworkL=(1-\lambda)L_{src}+\lambda L_{tgt}其中，$\lambda \in (0, 1]$， $\lambda=0.3$ 这里的损失没有用常见的加法，而是一个线性组合？ 4. Experiment4.1 Experiment Setting前5个 epoch 只训练 exemplar-invariance and camera-invariance，也就是通用的交叉熵损失，5个epoch之后再加入 neighborhood-invariance。 4.2 Parameter Analysistemperature fact $\beta$, weight of loss $\lambda$, number of candidate positive samples $k$. Temperature fact $\beta$ 当 $\beta$ 比较小的时候，分布越陡，值越大，损失越小，结果也越好。最好的结果是 $\beta$ 在0.05 左右。同时通过表格可以看出，当 $\beta$ 变化从0.05到0.5时，rank-1下降了17。但同时，在最优解的附近也接近最优解。 12345678910# 在蒸馏网络中，T越大，分布越平缓from torch.nn import functional as Fimport torchfun1 = F.softmaxaa = torch.tensor([0.2, 0.8, 0.2])fun1(aa, dim=0)tensor([0.2616, 0.4767, 0.2616])bb = torch.tensor([0.2, 0.8, 0.2])/0.1fun1(bb, dim=0)tensor([0.0025, 0.9951, 0.0025]) The weight of source and target losses $\lambda$: 可以看出即便只有 target loss， 性能也超过 baseline， 当取其他值的时候，性能几乎不变。 Number of positive samples $k$: 可以看出，性能对k挺敏感的。 4.4 Evaluation 通过表格中的数据集可以计算出，C+N&gt;C,N，C和N共同使用互相还有促进作用。 作者也在 MSMT 数据集上做了训练测试。 感觉大家都开始在 MSMT17 上做实验了，不怎么管 CUHK 了。 5. code代码和HHL的差不多 5.1 model5.1.1 baseline有以下几个改变： conv1 ~ layer2 的参数不再更新 source: layer4-avgpool-Linear(4096)-bn-relu-drop(0.5)-Linear(num_class) target: layer4-avgpool-Linear(4096)-bn-F.normalize-drop(0.5) Question：先normalize后drop的话就不满足归一化的定义了 Question: conv1~layer2 固定参数是有什么含义吗？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112class ResNet(nn.Module): __factory = &#123; 18: torchvision.models.resnet18, 34: torchvision.models.resnet34, 50: torchvision.models.resnet50, 101: torchvision.models.resnet101, 152: torchvision.models.resnet152, &#125; def __init__(self, depth, pretrained=True, cut_at_pooling=False, num_features=0, norm=False, dropout=0, num_classes=0, num_triplet_features=0): super(ResNet, self).__init__() self.depth = depth self.pretrained = pretrained self.cut_at_pooling = cut_at_pooling # Construct base (pretrained) resnet if depth not in ResNet.__factory: raise KeyError("Unsupported depth:", depth) self.base = ResNet.__factory[depth](pretrained=pretrained) # Fix layers [conv1 ~ layer2] fixed_names = [] for name, module in self.base._modules.items(): if name == "layer3": # assert fixed_names == ["conv1", "bn1", "relu", "maxpool", "layer1", "layer2"] break fixed_names.append(name) for param in module.parameters(): param.requires_grad = False if not self.cut_at_pooling: self.num_features = num_features self.norm = norm self.dropout = dropout self.has_embedding = num_features &gt; 0 self.num_classes = num_classes self.num_triplet_features = num_triplet_features self.l2norm = Normalize(2) out_planes = self.base.fc.in_features # Append new layers if self.has_embedding: self.feat = nn.Linear(out_planes, self.num_features) self.feat_bn = nn.BatchNorm1d(self.num_features) init.kaiming_normal_(self.feat.weight, mode='fan_out') init.constant_(self.feat.bias, 0) init.constant_(self.feat_bn.weight, 1) init.constant_(self.feat_bn.bias, 0) else: # Change the num_features to CNN output channels self.num_features = out_planes if self.dropout &gt;= 0: self.drop = nn.Dropout(self.dropout) if self.num_classes &gt; 0: self.classifier = nn.Linear(self.num_features, self.num_classes) init.normal_(self.classifier.weight, std=0.001) init.constant_(self.classifier.bias, 0) if not self.pretrained: self.reset_params() def forward(self, x, output_feature=None): for name, module in self.base._modules.items(): if name == 'avgpool': break else: x = module(x) if self.cut_at_pooling: return x x = F.avg_pool2d(x, x.size()[2:]) x = x.view(x.size(0), -1) if output_feature == 'pool5': x = F.normalize(x) return x if self.has_embedding: x = self.feat(x) x = self.feat_bn(x) tgt_feat = F.normalize(x) tgt_feat = self.drop(tgt_feat) if output_feature == 'tgt_feat': return tgt_feat if self.norm: x = F.normalize(x) elif self.has_embedding: x = F.relu(x) if self.dropout &gt; 0: x = self.drop(x) if self.num_classes &gt; 0: x = self.classifier(x) return x def reset_params(self): for m in self.modules(): if isinstance(m, nn.Conv2d): init.kaiming_normal(m.weight, mode='fan_out') if m.bias is not None: init.constant(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): init.constant(m.weight, 1) init.constant(m.bias, 0) elif isinstance(m, nn.Linear): init.normal(m.weight, std=0.001) if m.bias is not None: init.constant(m.bias, 0) L2 正则化定义了专门的层，但没有使用，可能是嫌慢吧。 1234567891011121314import torchfrom torch.autograd import Variablefrom torch import nnclass Normalize(nn.Module): def __init__(self, power=2): super(Normalize, self).__init__() self.power = power def forward(self, x): norm = x.pow(self.power).sum(1, keepdim=True).pow(1./self.power) out = x.div(norm) return out 5.1.2 InvNet123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# Pytorch&gt;=1.0.0import torchimport torch.nn.functional as Ffrom torch import nn, autogradfrom torch.autograd import Variable, Functionimport numpy as npimport math# 这应该是最新版的自定义 Function 的实现class ExemplarMemory(Function): def __init__(self, em, alpha=0.01): super(ExemplarMemory, self).__init__() self.em = em self.alpha = alpha def forward(self, inputs, targets): # inputs: b*2048, targets: b*1 (index) self.save_for_backward(inputs, targets) # outputs: b*12936 outputs = inputs.mm(self.em.t()) return outputs def backward(self, grad_outputs): # 这个backward 主要用于更新 em inputs, targets = self.saved_tensors grad_inputs = None if self.needs_input_grad[0]: grad_inputs = grad_outputs.mm(self.em) for x, y in zip(inputs, targets): self.em[y] = self.alpha * self.em[y] + (1. - self.alpha) * x self.em[y] /= self.em[y].norm() return grad_inputs, None# Invariance learning lossclass InvNet(nn.Module): def __init__(self, num_features, num_classes, beta=0.05, knn=6, alpha=0.01): super(InvNet, self).__init__() self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') self.num_features = num_features self.num_classes = num_classes self.alpha = alpha # Memory update rate self.beta = beta # Temperature fact self.knn = knn # Knn for neighborhood invariance # Exemplar memory, 12936x2048 self.em = nn.Parameter(torch.zeros(num_classes, num_features)) def forward(self, inputs, targets, epoch=None): # inputs: b*2048, targets: b*1 (index) alpha = self.alpha * epoch # 每次都是重建一个 ExemplarMemory，我觉得可能是因为alpha每次要改变。 inputs = ExemplarMemory(self.em, alpha=alpha)(inputs, targets) # inputs: b*12936 inputs /= self.beta if self.knn &gt; 0 and epoch &gt; 4: # With neighborhood invariance loss = self.smooth_loss(inputs, targets) else: # Without neighborhood invariance loss = F.cross_entropy(inputs, targets) return loss def smooth_loss(self, inputs, targets): # overall loss of invariance loss # inputs: b*12936, targets: b*1 (index) targets = self.smooth_hot(inputs.detach().clone(), targets.detach().clone(), self.knn) # targets: b*12936, weights outputs = F.log_softmax(inputs, dim=1) loss = - (targets * outputs) loss = loss.sum(dim=1) loss = loss.mean(dim=0) return loss def smooth_hot(self, inputs, targets, k=6): # Sort # inputs: b*12936, targets: b*1 (index) # targets_onehot: b*12936 # targets_onehot: 记录的是当前样本属于其他类的概率的权重 1/k or 1 _, index_sorted = torch.sort(inputs, dim=1, descending=True) ones_mat = torch.ones(targets.size(0), k).to(self.device) targets = torch.unsqueeze(targets, 1) targets_onehot = torch.zeros(inputs.size()).to(self.device) # 这里的 weights 应该是 1/k， 因为每个值都一样，所以 softmax 之后就是 1/k # 猜测作者这里刚开始不是直接想要 1/k 的权重，而是根据距离远近赋予权重，比如选 # 择6个最近的，然后根据相似性赋予权重（等同于概率的求解） # Question:weights = F.softmax(ones_mat, dim=1) # targets_onehot.scatter_(1, index_sorted[:, 0:k], ones_mat * weights) weights = F.softmax(ones_mat, dim=1) # 根据位置填充权重 targets_onehot.scatter_(1, index_sorted[:, 0:k], ones_mat * weights) # Question: 怎么保证前k个就一定没有index？ targets_onehot.scatter_(1, targets, float(1)) return targets_onehot 补充 gather 和 scatter 的用法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# gather# torch.gather(input, dim, index, out=None)# output[i][j][k]=input[index[[i][j][k]]][j][k] # if dim=0# output[i][j][k]=input[i][index[[i][j][k]]][k] # if dim=1# output[i][j][k]=input[i][j][index[[i][j][k]]] # if dim=2# 光看公式很绕，# 其中 out 的size和 index 的size相同# input: a0*a1*a2*...ai-1*ai*ai+1,...,an-1# index: a0*a1*a2*...ai-1*y*ai+1,...,an-1# output:a0*a1*a2*...ai-1*y*ai+1,...,an-1# 除了第dim维，其他维度上 index 的size和input的size相同# 或者我们换个角度，不要管公式，把index分成y份，index_0，index_1, ..., index_y-1# index_0: a0*a1*a2*...ai-1*1*ai+1,...,an-1# index_0 就是在第i个维度上选取对应位置第y[0]个通道的数字，# 以此类推，下面用二维矩阵做个示范# 可以理解成降维import torchinput = torch.Tensor([[1,2],[3,4]])tensor([[1., 2.], [3., 4.]])# [0,0] 表示选取第0维第0个通道的数字，第0维第0个通道的数字torch.gather(input, 0, torch.LongTensor([[0,0]]) )tensor([[1., 2.]])# [0,0] 表示选取第0维第0个通道的数字，第0维第1个通道的数字torch.gather(input, 0, torch.LongTensor([[0,1]]) )tensor([[1., 4.]])torch.gather(input, 0, torch.LongTensor([[0,1], [0,1], [0,1]]) )tensor([[1., 4.], [1., 4.], [1., 4.]])# 三维矩阵# 三维矩阵更好理解，可以通过投射的方式理解a = torch.arange(0,27).view(3,3,3)# 第0维第0个通道tensor([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8]],# 第0维第1个通道 [[ 9, 10, 11], [12, 13, 14], [15, 16, 17]],# 第0维第2个通道 [[18, 19, 20], [21, 22, 23], [24, 25, 26]]])# 在对应位置，分别选取第0维的第0个，第1个，第2个通道的数字c = [[[0,1,2],[0,1,2],[0,1,2]]]index = torch.LongTensor(c)a.gather(0, index)tensor([[[ 0, 10, 20], [ 3, 13, 23], [ 6, 16, 26]]])# output的第0维第0个通道：分别选取0,1,2# output的第0维第1个通道：分别选取0,1,0c = [[[0,1,2],[0,1,2],[0,1,2]], [[0,1,0],[0,1,0],[0,1,0]]]index = torch.LongTensor(c)a.gather(0, index)tensor([[[ 0, 10, 20], [ 3, 13, 23], [ 6, 16, 26]], [[ 0, 10, 2], [ 3, 13, 5], [ 6, 16, 8]]]) 12345678910111213141516# scatter# torch.scatter_(dim, index, src)# 可以理解成gather的相反操作# src 和 index 的size相同# out[index[i][j][k]][j][k]=src[i][j][k] if dim=0# out[i][index[i][j][k]][k]=src[i][j][k] if dim=1# out[i][j][index[i][j][k]]=src[i][j][k] if dim=2x = torch.rand(2, 5) 0.4319 0.6500 0.4080 0.8760 0.2355 0.2609 0.4711 0.8486 0.8573 0.1029[torch.FloatTensor of size 2x5]torch.zeros(3, 5).scatter_(0, torch.LongTensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) 0.4319 0.4711 0.8486 0.8760 0.2355 0.0000 0.6500 0.0000 0.8573 0.0000 0.2609 0.0000 0.4080 0.0000 0.1029[torch.FloatTensor of size 3x5] 5.2 data对于 target domain 中的图片，每次都是从原始图片和生成图片组成的集合中随机取一张图片，当选取的 camid 是原始图片的 camid 时，取原始图片，当取到的 camid 不是原始图片的 camid 时，取生成图片，实际只使用了 C-1 张生成图片和 1 张原始图片。 1234567891011121314151617# reid/utils/data/preprocessor.py def _get_single_item(self, index): fname, pid, camid = self.dataset[index] sel_cam = torch.randperm(self.num_cam)[0] if sel_cam == camid: fpath = osp.join(self.root, fname) img = Image.open(fpath).convert('RGB') else: if 'msmt' in self.root: fname = fname[:-4] + '_fake_' + str(sel_cam.numpy() + 1) + '.jpg' else: fname = fname[:-4] + '_fake_' + str(camid + 1) + 'to' + str(sel_cam.numpy() + 1) + '.jpg' fpath = osp.join(self.camstyle_root, fname) img = Image.open(fpath).convert('RGB') if self.transform is not None: img = self.transform(img) return img, fname, pid, index 5.3 optimizer123456789101112131415# 这个写的可以，清晰明了，只是在HHL中，只有classifier层设置为1.0，其他新层则是0.0，这里则把新层全部设置成了1.0，应该也是实验所得吧。base_param_ids = set(map(id, model.module.base.parameters()))base_params_need_for_grad = filter(lambda p: p.requires_grad, model.module.base.parameters())new_params = [p for p in model.parameters() if id(p) not in base_param_ids]param_groups = [ &#123;'params': base_params_need_for_grad, 'lr_mult': 0.1&#125;, &#123;'params': new_params, 'lr_mult': 1.0&#125;]optimizer = torch.optim.SGD(param_groups, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True) 5.4 train12345678910111213# main.py and reid/trainers.py# self.model: ResNet-50# self.model_inv : InvNet# source domain 的交叉熵损失inputs, pids = self._parse_data(inputs)outputs = self.model(inputs)source_pid_loss = self.pid_criterion(outputs, pids)# target domain 的损失outputs = self.model(inputs_target, 'tgt_feat')loss_un = self.model_inv(outputs, index_target, epoch=epoch)# overall lossloss = (1 - self.lmd) * source_pid_loss + self.lmd * loss_un 知识点补充：因为 ResNet-50 的 conv1~layer2 的参数已经设置不参与更新，自然这些层的 bn 层也应该是 eval 状态。 Question: 上面的知识点补充是否正确？ 123456789101112131415161718192021222324252627282930313233343536def set_model_train(self): self.model.train() # Fix first BN fixed_bns = [] for idx, (name, module) in enumerate(self.model.module.named_modules()): if name.find("layer3") != -1: # assert len(fixed_bns) == 22 break if name.find("bn") != -1: fixed_bns.append(name) module.eval()len(fixed_bns)22['bn1', 'layer1.0.bn1', 'layer1.0.bn2', 'layer1.0.bn3', 'layer1.1.bn1', 'layer1.1.bn2', 'layer1.1.bn3', 'layer1.2.bn1', 'layer1.2.bn2', 'layer1.2.bn3', 'layer2.0.bn1', 'layer2.0.bn2', 'layer2.0.bn3', 'layer2.1.bn1', 'layer2.1.bn2', 'layer2.1.bn3', 'layer2.2.bn1', 'layer2.2.bn2', 'layer2.2.bn3', 'layer2.3.bn1', 'layer2.3.bn2', 'layer2.3.bn3'] 到此应该代码全部看完了。]]></content>
      <categories>
        <category>preson re-id</category>
      </categories>
      <tags>
        <tag>person re-id</tag>
        <tag>cross domain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dual Attention Network for Scene Segmentation]]></title>
    <url>%2F2019%2F05%2F05%2FDual-Attention-Network-for-Scene-Segmentation%2F</url>
    <content type="text"><![CDATA[0. 前言这篇文章的重点在于 dual attention 的作用，并且attention的使用和之前看到的 SE block 还不太一样。dual attention 主要解决了全局依赖性，即其他位置的物体对当前位置的的物体的特征的影响。重点不是场景分割，自己也不是很懂分割的代码和实现，暂时对分割不做过多研究。 paper: CPVR2019: Dual Attention Network for Scene Segmentation code: pytorch team: 中科院自动化所图像与视频分析团队（IVA），隶属于模式识别国家重点实验室，在 ICCV 2017 COCO-Places 场景解析竞赛、京东 AI 时尚挑战赛和阿里巴巴大规模图像搜索大赛踢馆赛等多次拔得头筹。嗯，一句话，很牛逼。 解读 1. Introduction作者提出了 Dual Attention Network(DANet) 来融合局部特征。具体的过程是在 dilated FCN 上添加了两种 attention modules: the position attention module and the channel attention module，这两个attention主要解决的是全局依赖性。 场景分割需要解决的两个问题：区分相似的东西(田地和草)，识别不同大小外观的同一个东西(车)。因此，场景分割模型需要提高像素级别识别的特征表示。 一种方法是多尺度融合来识别不同大小的物体，但是不能以全局地角度来很好地处理物体与物体之间的关系。应该是指最后的特征的感受野有大有小，可以理解成不同大小的物体都能识别到。 还有一种方法是利用了LSTM来实现 long-range dependencies，可以理解成物体的识别不仅依靠自己的特征，还依赖于其他物体的的特征，即全局依赖或者空间依赖性。 其中 the position attention module 主要用于解决全局的空间位置依赖问题，the channel attention module 解决的是全局的通道依赖性。 所以作者主要解决的是全局依赖性，并没有考虑不同大小的物体的分割问题。 按照作者的说法，DANet 有两种作用：第一，可以避免显眼的大的物体的特征影响不起眼的小的物体的标签；第二，可以在一定程度上融合不同尺寸的物体的相似特征；第三，利用空间和通道的依懒性解决全局依赖问题。 2. Dual Attention Network3.1 Overview基准网络是 dilated FCN. 3.2 Position Attention Module 通过特征提取网络得到特征图 $A\in R^{C\times H\times W}$，分别通过一个卷积层得两个特征图 $\lbrace B,C \rbrace \in R^{C\times H\times W}$，并且 reshape 成 $R^{C\times N}$，其中$N=H\times W$，然后得到$S\in R^{N\times N}$，此时把$R^{C}$看成这个位置的特征。下面阐述下具体的过程： \begin{aligned} \hat{S}&=C^T\times B, (where, \hat{S}\in R^{N\times N}) \\ &=[C_1, C_2,..., C_N]^T\times [B_1, B_2,..., B_N] \\ &=\begin{bmatrix} C_1^T\times B_1 & C_1^T\times B_2 & ... & C_1^T\times B_N \\ C_2^T\times B_1 & C_2^T\times B_2 & ... & C_2^T\times B_N \\ ... & ... & ... & ... \\ C_N^T\times B_1 & C_N^T\times B_2 & ... & C_N^T\times B_N \end{bmatrix}(where,C_j,B_i\in R^C) \\ \hat{s}_{ji}&=B_i^T \times C_j \end{aligned}从而得到$S$ s_{ji}=\frac{\exp(B_i \cdot C_j)}{\sum_{i=1}^N \exp(B_i \cdot C_j)}可以理解成对$\hat{S}$的每一行都做一次softmax，即 S 的每一行和为1，可以解释成C中的点与B中所有点的相似性，越相似值越大。其中B和C是对称的。 同时，将A送进第三个滤波器得到 $D\in R^{C\times H\times W}$ 并且 reshape 成 $R^{C\times N}$ ，从而得到最后的输出$E$，下面阐述具体计算过程: \begin{aligned} \hat{E}&=D\times S^T (where,\hat{E}\in R^{C\times N}, D\in R^{C\times N}, C\in R^{N\times N})\\ &=[D_1, D_2,..., D_N] \times \begin{bmatrix} s_{11} & s_{12} & ... & s_{1N} \\ s_{21} & s_{22} & ... & s_{2N} \\ ... & ... & ... & ... \\ s_{N1} & s_{N2} & ... & s_{NN} \end{bmatrix}^T(where,D_i\in R^C) \\ &=[D_1\cdot s_{11}+D_2\cdot s_{12}+...+D_N\cdot s_{1N}, ..., D_1\cdot s_{N1}+D_2\cdot s_{N2}+...+D_N\cdot s_{NN}] \\ &=[\sum_i s_{1i}D_i, \sum_i s_{2i}D_i,... ,\sum_i s_{Ni}D_i] \\ \hat{e}_{j}&=\sum_i^N(s_{ji}D_i) \end{aligned}从而得到$E\in R^{C\times N}$, E_j=\alpha \sum_i^N(s_{ji}D_i)+A_j相当于 $\hat{E}$ 与 $A$ 进行了线性组合，并对其reshape变成$E\in R^{C\times H\times W}$，其中$\alpha$是一个可学习参数，网络自动学习，初始化为0。 如果不考虑其中的 softmax, 可以写成: E=\alpha D\times (C^T \times B)^T+A3.3 Channel Attention ModulePosition Attention Module 是把每个位置的通道作为其特征 $R^C$ ，Channel Attention Module 是把每个通道的特征图作为其特征 $R^N$。 与 Position Attention Module 不同的地方还有没有经过三个滤波器得到 $B,C,D$ ，而是直接使用A。 仍然是先把 A reshape 成 $A\in R^{C\times N}$，然后进行和上述类似的操作，可以令$B,C,D=A^T$下面阐述具体过程: \begin{aligned} \hat{X}&=A\cdot A^T (where, A\in R^{C\times N}, X\in R^{C\times C}) \\ \hat{x}_{ji}&=A_i^T \cdot A_j (where, A_i \in R^N) \end{aligned}结合后面的代码分析，从而得到$X\in R^{C\times C}$: x_{ji}=\frac{\exp(-A_i \cdot A_j)}{\sum_{i=1}^N \exp(-A_i \cdot A_j)}同样可以理解成对 $\hat{X}$ 的每一行做一次softmax，可以理解成A的自相关性。结合后面的 channel attention 的可视化，不同通道代表的类别不同，所以这里应该是越不相似值越大。 然后类似地我们得到$E\in R^{C\times H \times W}$: E_j=\beta \sum_i^N(x_{ji}A_i)+A_j如果不考虑其中的 softmax, 可以写成: E=\beta (A \times A^T)\times A+A这里给我的感觉更多地是在加法，而不是 SE block 用的乘法。 其实看到这里我是表示很怀疑的，这种 attention 能有效果吗？后面的可视化证明了作者的思路是正确的。 其中$\beta$也是一个可学习参数，网络自动学习，初始化为0。 4. Experiments4.1 Implementation Details学习率: 多项式衰减 (1-\frac{iter}{total iter})^{0.9}下面专门做一个学习率衰减的情况。 4.2 Results on Datasets4.2.1 Ablation Study for Attention Modules 从实验结果可以看出， the position module 和 the channel module 互为补充，两个合起来后的提升效果没有单个的提升效果明显。 4.2.3 Visualization of Attention Module这一小节很有意思的。 对于 position attention，得到的 $E\in R^{(H\times W)\times (H\times W)}$ ，可以理解点与点之间的相似性，对每个图片，选两个点，记为 ( #1 and #2 )，并且展示这两个点的 position attention map. 第一张图 #1 标记的是建筑物， #2 标记的是车，第二张图分别标记的是交通标记和行人，第三行标记的是植物和行人。可以看出来，同一类事物哪怕离得远也可以标记出来，不同事物哪怕离得近也标记不出来。或者说， position attention 具有在全局的角度来标记同一类事物，哪怕离得远，哪怕事物很小，同时区分近距离的不同事物。 对于 channel attention, 从图片中可以看出来，主要是同一通道得到的是同一类别。 现在还不知道是怎么可视化的。 4.2.4 Comparing with State-of-the-art 嗯，比其他方法都强。 作者一共在四个数据集上做了实验，说明是真的强。 5. Learning rate以前虽然一直在用一些学习率衰减方式，但是都不系统。 5.1 fixedlr=base\_lr5.2 step离散的学习率变化策略 lr=base\_lr\cdot \gamma^{epoch//step\_size}其中，向下取整，并且 $\gamma$ 和 step_size 都需要设置gamma一般取0.1， step_wise一般取40 5.3 explr=base\_lr\cdot \gamma^{epoch}其中 $\gamma$ 需要设置gamma一般取0.99 5.4 invlr=base\_lr\cdot (1+y\cdot epoch)^{-power}其中$\gamma$和power都需要设置gamma控制下降速率，power控制曲线在饱和状态下学习率达到的最低值。可以理解成当epoch达到最大值的时候，学习率在不同的power下最低值不一样。 5.5 multistep多次step，只是学习率改变的迭代次数不均匀 12345lr_policy: "multistep"gamma: 0.5stepvalue: 10000stepvalue: 30000stepvalue: 60000 5.6 polylr=base\_lr\cdot (1-epoch/max\_epoch)^{power}其中，power需要设置，并且epoch为0时，lr是base_lr，当达到最大次数时，学习率变成0. 5.7 sigmoidlr=base\_lr\cdot \frac{1}{1+exp^{-\gamma \cdot (epoch-step\_size)}} 其中step_size控制sigmoid为0.5的位置，gamma学习率的变化速率。 5.8 warm up在前10个epoch使用较小的lr，之后正常使用 5.9 all 其中，step和multi_step最好，其次是exp,ploy，最差的是 inv,sigmoid. 5.10 code1234567891011121314151617181920212223242526272829303132import matplotlib.pyplot as pltx = list(range(1000))base_lr=0.01def step_lr(epoch): step_wise=50 gamma=0.1 return base_lr*gamma**(epoch//step_wise)y = [step_lr(i) for i in x]plt.plot(x,y)def exp_lr(epoch): gamma=0.999 return base_lr*gamma**epochy = [exp_lr(i) for i in x]plt.plot(x,y)def inv_lr(epoch): gamma=0.1 power=0.75 return base_lr*(1+gamma*epoch)**(-power)y = [inv_lr(i) for i in x]plt.plot(x,y)def multi_lr(epoch): step_wise1=200 step_wise2=300 step_wise3=400 gamma=0.5 power = [0]*step_wise1 + [1]*step_wise2+[2]*step_wise3 if epoch&lt;len(power): return base_lr*gamma**power[epoch] else: return base_lr*gamma**3y = [multi_lr(i) for i in x]plt.plot(x,y) 6. code这次的代码很有含金量，用到了多GPU。 一是涉及到的代码有点多，二是自己没有跑过分割的代码，不清楚具体的代码组织形式。所以下面从小到大一个个讲关键的地方。有些代码和作者的论文描述不是非常一致，但不影响总体。 6.1 PAM and CAMposition attention and channel attention 1234567891011121314151617181920212223242526272829303132333435363738394041class PAM_Module(Module): """ Position attention module""" #Ref from SAGAN def __init__(self, in_dim): super(PAM_Module, self).__init__() self.chanel_in = in_dim self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1) self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1) self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1) self.gamma = Parameter(torch.zeros(1)) self.softmax = Softmax(dim=-1) def forward(self, x): """ inputs : x : input feature maps( B X C X H X W) returns : out : attention value + input feature attention: B X (HxW) X (HxW) """ # C: 512, C//8: 64 m_batchsize, C, height, width = x.size() # x: B,C,H,W proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1) # C': B,HxW,C//8 proj_key = self.key_conv(x).view(m_batchsize, -1, width*height) # B: B,C//8,HxW energy = torch.bmm(proj_query, proj_key) # \hat&#123;S&#125; = C'xB : B,HxW,HxW attention = self.softmax(energy) # S: B,HxW,HxW proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) # D: B,C,HxW out = torch.bmm(proj_value, attention.permute(0, 2, 1)) # \hat&#123;E&#125; = DxS': B,C,HxW out = out.view(m_batchsize, C, height, width) # \hat&#123;E&#125; : B,C,H,W out = self.gamma*out + x # E: B,C,H,W return out 123456789101112131415161718192021222324252627282930313233343536373839class CAM_Module(Module): """ Channel attention module""" def __init__(self, in_dim): super(CAM_Module, self).__init__() self.chanel_in = in_dim self.gamma = Parameter(torch.zeros(1)) self.softmax = Softmax(dim=-1) def forward(self,x): """ inputs : x : input feature maps( B X C X H X W) returns : out : attention value + input feature attention: B X C X C """ m_batchsize, C, height, width = x.size() # x: B,C,H,W proj_query = x.view(m_batchsize, C, -1) # A: B,C,HxW proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1) # A': B,HxW,C energy = torch.bmm(proj_query, proj_key) # \hat&#123;X&#125; = AxA': B,C,C energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy # note that, 作者在这里用了一次 max-v_i，而不是常见的v_i-max，按照github上的解释， # 作者选用前者而不是后者的原因是后者的效果不好，不知道该怎么反驳， # channel attention 主要衡量的是通道与通道之间的相似性， # 按照这个公式，结合channel的可视化，只能强行解释成，希望通道之间不相似，越不相似给的值越高， attention = self.softmax(energy_new) # X: B,C,C proj_value = x.view(m_batchsize, C, -1) # D: B,C,HxW out = torch.bmm(attention, proj_value) # \hat&#123;E&#125; = XxD : B,C,HxW，这里也满足\hat&#123;E&#125;中的每个元素的系数之后为1 out = out.view(m_batchsize, C, height, width) # \hat&#123;E&#125;: B,C,H,W out = self.gamma*out + x # E: B,C,H,W return out 6.2 DANetHead从代码上看，过程大概是： 是在进入 attention module 会进行一次通道缩小，2048-&gt;512， position attention module: sa_conv, channel attention module: sc_conv 得到三种预测的类别：sa_conv+sc_conv-&gt;sasc_output, sa_conv-&gt;sa_output, sc_conv-&gt;sc_output 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class DANetHead(nn.Module): def __init__(self, in_channels, out_channels, norm_layer): # in_channels: 2048 # out_channels: dataset.num_classes super(DANetHead, self).__init__() inter_channels = in_channels // 4 self.conv5a = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False), norm_layer(inter_channels), nn.ReLU()) self.sa = PAM_Module(inter_channels) self.conv51 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False), norm_layer(inter_channels), nn.ReLU()) self.conv6 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(512, out_channels, 1)) self.conv5c = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False), norm_layer(inter_channels), nn.ReLU()) self.sc = CAM_Module(inter_channels) self.conv52 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False), norm_layer(inter_channels), nn.ReLU()) self.conv7 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(512, out_channels, 1)) self.conv8 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(512, out_channels, 1)) def forward(self, x): # x: B,C,H,W: C 2048 feat1 = self.conv5a(x) # feat1: B,C//4,H,W sa_feat = self.sa(feat1) # sa_feat: B,C//4,H,W sa_conv = self.conv51(sa_feat) # sa_conv: B,C//4,H,W sa_output = self.conv6(sa_conv) # sa_output: B,C_out,H,W # x: B,C,H,W: C 2048 feat2 = self.conv5c(x) # feat2: B,C//4,H,W sc_feat = self.sc(feat2) # sc_feat: B,C//4,H,W sc_conv = self.conv52(sc_feat) # sc_conv: B,C//4,H,W sc_output = self.conv7(sc_conv) # sc_output: B,C_out,H,W feat_sum = sa_conv+sc_conv # feat_sum: B,C//4,H,W sasc_output = self.conv8(feat_sum) # sasc_output: B,C_out,H,W output = [sasc_output] output.append(sa_output) output.append(sc_output) # output:[sasc_output, sa_output, sc_output]: 3,B,C_out,H,W return tuple(output) 6.3 BaseNet以ResNet-50为例，相当于求得每一个layer的输出 [c1, c2, c3, c4] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class BaseNet(nn.Module): def __init__(self, nclass, backbone, aux, se_loss, dilated=True, norm_layer=None, base_size=576, crop_size=608, mean=[.485, .456, .406], std=[.229, .224, .225], root='./pretrain_models', multi_grid=False, multi_dilation=None): super(BaseNet, self).__init__() self.nclass = nclass self.aux = aux self.se_loss = se_loss self.mean = mean self.std = std self.base_size = base_size self.crop_size = crop_size # copying modules from pretrained models if backbone == 'resnet50': self.pretrained = resnet.resnet50(pretrained=True, dilated=dilated, norm_layer=norm_layer, root=root, multi_grid=multi_grid, multi_dilation=multi_dilation) elif backbone == 'resnet101': self.pretrained = resnet.resnet101(pretrained=True, dilated=dilated, norm_layer=norm_layer, root=root, multi_grid=multi_grid,multi_dilation=multi_dilation) elif backbone == 'resnet152': self.pretrained = resnet.resnet152(pretrained=True, dilated=dilated, norm_layer=norm_layer, root=root, multi_grid=multi_grid, multi_dilation=multi_dilation) else: raise RuntimeError('unknown backbone: &#123;&#125;'.format(backbone)) # bilinear upsample options self._up_kwargs = up_kwargs def base_forward(self, x): x = self.pretrained.conv1(x) x = self.pretrained.bn1(x) x = self.pretrained.relu(x) x = self.pretrained.maxpool(x) c1 = self.pretrained.layer1(x) c2 = self.pretrained.layer2(c1) c3 = self.pretrained.layer3(c2) c4 = self.pretrained.layer4(c3) return c1, c2, c3, c4 def evaluate(self, x, target=None): pred = self.forward(x) if isinstance(pred, (tuple, list)): pred = pred[0] if target is None: return pred correct, labeled = batch_pix_accuracy(pred.data, target.data) inter, union = batch_intersection_union(pred.data, target.data, self.nclass) return correct, labeled, inter, union 6.5 DANet相当于求这三种的预测：sa_conv+sc_conv-&gt;sasc_output, sa_conv-&gt;sa_output, sc_conv-&gt;sc_output 123456789101112131415161718192021222324252627282930313233343536373839404142434445class DANet(BaseNet): r"""Fully Convolutional Networks for Semantic Segmentation Parameters ---------- nclass : int Number of categories for the training dataset. backbone : string Pre-trained dilated backbone network type (default:'resnet50'; 'resnet50', 'resnet101' or 'resnet152'). norm_layer : object Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`; Reference: Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation." *CVPR*, 2015 """ def __init__(self, nclass, backbone, aux=False, se_loss=False, norm_layer=nn.BatchNorm2d, **kwargs): super(DANet, self).__init__(nclass, backbone, aux, se_loss, norm_layer=norm_layer, **kwargs) self.head = DANetHead(2048, nclass, norm_layer) def forward(self, x): # 具体的图片大小还是需要看图像分割的输入，这里以标准的224为例 # x: 3,H,W &amp;&amp; 224, 224 imsize = x.size()[2:] _, _, c3, c4 = self.base_forward(x) # c3, c4: ResNet-50 的 layer3 和 layer 4 的输出 # c3: 1024, H//16, W//16 &amp;&amp; 1024, 14, 14=224//16 # c4: 2018, H//32, W//32 &amp;&amp; 7, 7=224//32 x = self.head(c4) # x: [sasc_output, sa_output, sc_output]: 3,B,dataset.num_classes,H//32, W//32 &amp;&amp; 7, 7=224//32 x = list(x) x[0] = upsample(x[0], imsize, **self._up_kwargs) x[1] = upsample(x[1], imsize, **self._up_kwargs) x[2] = upsample(x[2], imsize, **self._up_kwargs) # 上采样 # x: [sasc_output, sa_output, sc_output]: 3,B,dataset.num_classes,H,W &amp;&amp; 224, 224 outputs = [x[0]] outputs.append(x[1]) outputs.append(x[2]) # x: [sasc_output, sa_output, sc_output]: 3,B,dataset.num_classes,H,W &amp;&amp; 224, 224 return tuple(outputs) 6.6 SegmentationMultiLosses希望 position+channel attetion, position attention, channel attention 三种预测都准确 1234567891011121314151617class SegmentationMultiLosses(CrossEntropyLoss): """2D Cross Entropy Loss with Multi-L1oss""" def __init__(self, nclass=-1, weight=None,size_average=True, ignore_index=-1): super(SegmentationMultiLosses, self).__init__(weight, size_average, ignore_index) self.nclass = nclass def forward(self, *inputs): *preds, target = tuple(inputs) pred1, pred2 ,pred3= tuple(preds) # sa_conv+sc_conv-&gt;sasc_output, sa_conv-&gt;sa_output, sc_conv-&gt;sc_output loss1 = super(SegmentationMultiLosses, self).forward(pred1, target) loss2 = super(SegmentationMultiLosses, self).forward(pred2, target) loss3 = super(SegmentationMultiLosses, self).forward(pred3, target) loss = loss1 + loss2 + loss3 return loss 6.7 其他其他的代码暂时就不看了，只是记录一个自己没有看到过的函数 Synchronized Cross-GPU Batch Normalization functions]]></content>
      <categories>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>segmentation</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SE block]]></title>
    <url>%2F2019%2F05%2F04%2FSE-block%2F</url>
    <content type="text"><![CDATA[0. 前言这篇文章主要是通过在神经网络中加入 SE-block 来加强通道之间的关系，提高性能，理论上讲可以加入任意网络任意任务，并且这篇文章获得了 ImageNet2017 的冠军。很牛逼。 paper: ECCV2018_Squeeze-and-Excitation Networks code: Caffe, TensorFlow, MatConvNet, MXNet, Pytorch, Chainer, pytorch 这篇文章清晰易懂，讲得很细(没用的话也比较多)，很work，一起拜读一下。 1. Introduction SE block: Squeeze and Excitation block 符号表达：$F_{tr}:X\to U, X\in R^{H’×W’×C’}, U\in R^{H×W×C}$ SE block 在底层时更偏向于提取任务之间的共享特征，在高层时更偏向于提取任务相关的特征。 3. Squeeze and Excitation Blocks3.1 Squeeze: Global Information Embeddingz_c=F_{sq}(u_c)=\frac{1}{H×W}\sum_{i=1}^H \sum_{j=1}^W u_c(i,j)其中，$z\in R^C$ 3.2 Excitation: Adaptive Recalibrations=F_{ex}(z,W)=\sigma(g(z,W))=\sigma(W_2\delta (W_1z))其中，$\delta$表示ReLU函数，$W_1\in R^{\frac{C}{r}×C}$ 并且 $W_2\in R^{C×\frac{C}{r}}$，也就是两个FC层。 \tilde{x}_c=F_{scale}(u_c, s_c)=s_c\cdot u_c3.3 Instantiations 4. Experiments主要是从分类等场景出发，说明了 SE block 在ResNet,Inception等各种网络和ImageNet, cifar-100等数据集上表现都好。 5. Ablation Study5.1 Reduction ratioReduction ratio r in Excitation 作者设置为r=16. 5.2 Squeeze Operator 作者只比较了max pooling 和avg pooling.这两种方法差不多。 5.3 Excitation Operator 作者比较了 ReLU, Tanh, Sigmoid三种函数，实验证明 Sigmoid 函数更好一些，这里指的是第二个激活函数。 5.4 Different stages 5.5 Integration strategy 6. code代码还是很简单的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# SE ResNet50from torch import nnclass SELayer(nn.Module): def __init__(self, channel, reduction=16): super(SELayer, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Sequential( nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid() ) def forward(self, x): b, c, _, _ = x.size() y = self.avg_pool(x).view(b, c) y = self.fc(y).view(b, c, 1, 1) return x * y.expand_as(x)class SEBasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16): super(SEBasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes, 1) self.bn2 = nn.BatchNorm2d(planes) self.se = SELayer(planes, reduction) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.se(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out]]></content>
      <categories>
        <category>attention</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>SE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reality Oriented Adaptation]]></title>
    <url>%2F2019%2F05%2F03%2FReality-Oriented-Adaptation%2F</url>
    <content type="text"><![CDATA[0. 前言0.0 前言这篇文章主要解决的问题是：当语义分割网络在合成数据集(有标签)上训练好，在真实数据集(没有标签)上性能下降比较多。作者认为有两个原因：对合成数据过拟合，合成数据与真实数据存在分布差异。(好吧，我认为这两是一个原因)。作者提出target guided distillation 和 spatial-aware adaptation 来改进性能，效果还挺好的。我主要看target guided distillaiton。 paper: CVPR2018_ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes code: 无 0.1 HydraPlus-Net顺便记录下刚看的论文 HydraPlus-Net，因为这篇论文是caffe代码且对我的帮助不大，所以只是简单地记录下其中的创新点。 paper: ICCV2017: HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis code: caffe 其主要创新点在于： attention不仅可以用于本block，也可以用于其他block 一个block可以生成多个attention map 1.Introduction针对合成数据集的模型在真实数据集上性能很差，作者提出原因可能是：过拟合和分布不一致，因此提出模型：ROAD-Net。下面对作者提出的名词做出解释 Reality Oriented Adaptation Networks(ROAD-Net) real style orientation &amp; target guided distillation: 为了避免过拟合合成数据集，使用 distillation 使 model 的输出和预训练的模型的输出一致，注意，这里 distillation 针对的是真实数据集，而不是合成数据集，这种方法称为 target guided distillation. real distribution orientation &amp; spatial-aware adaptation &amp; domain classifier: 因为合成数据集和真实数据集的分布不一致，提出 DANN 也就是 domain classifier，其实类似GAN的D，来使得合成数据集和真实数据集的特征分布一致。主要过程就是将合成数据集和真实数据集的特征图分割成几个区域，然后判断这几个区域是不是同一个domain。 2. Reality Oriented Adaptation Networks2.1 Target Guided Distillation 其中，pretrained model 在训练的时候不更新. distillation loss： L_{dist}=\frac{1}{N}\sum_{i,j}\parallel x_{i,j}-z_{i,j} \parallel _2其中，$x_{i,j}, z_{i,j}$分别表示 segmentation model 和 pretrained model 得到的 feature map 在位置 $(i,j)$ 的值，二范是简单的欧式距离。这个损失简单粗暴，后续的实验也证明了这种方法的确要更好一些。 我觉得这个是一个思路，这种 distillation 可以在一定程度上使得 segmentation model 学习到真实数据集的特征分布。 当然，除了 distillation loss 用来防止过拟合，也有其他方法用来防止过拟合，比如冻结一些层然后循环，或者用 source (合成数据集) 用来进行 distillation，是 learning without forgetting，这篇文章也很有用，等下简单地讲下这篇文章。 其实这里大有文章可做。 2.2 Spatial-Aware Adaptation 假设把特征图分割成了$m=1,…,M$块，每一块的区域坐标集合表示为$(u,v)\in R_m$，记点(u,v)对应的特征图为$x_{u,v}$，记区域对应的特征图为$X_m^s={x_{u,v}^s | (u,v)\in R_m}$和$X_m^t={x_{u,v}^t | (u,v)\in R_m}$，定义其loss为: L_{spt}=\sum_{m=1}^M L_{da}(X_m^s, X_m^t)其中，$L_{da}$表示 domain adaptation loss，其实就是domain classifier loss,具体表示如下。 L_H(X^s, X^t)=\frac{1}{|X|}\sum_{x\in X} l(h(x),d)其中$h:x\to {0,1}$，采用的DANN模型，应该是类似GAN中的D，$d\in {0,1}$. 这个domain classifer的目的是为了使生成的source和target生成的特征尽量相似。 2.3 Network OverviewL_{ROAD}=L_{seg}+\lambda_1 L_{dist}+\lambda_2 L_{spt}3. Experimental Results3.1 Experimental Results dst: target guided distillation spt: spatial-aware adaptation 通过结果可以看出，这两个创新点是有用的。 3.2 Analysis on Real Style Orientation 3.3 Analysis on Real Distribution Orientation 4. Others这里主要简单介绍下论文中提到的几篇参考文献的主要内容，并没有细读这几篇参考文献。 4.1 Domain AdaptationUnsupervised Domain Adaptation by Backpropagation Domain Adaptation 4.2 Distillationq_i=\frac{exp(z_i/T)}{\sum_j exp(z_j/T)}L(p,q)=\sum_i p_i * (-\log q_i)Awesome Knowledge Distillation中有相关的论文和一部分实现。 Learning without Forgetting 模型压缩总览 知识蒸馏 Knowledge Distillation 终于找到一个 KD (knowledge-distillation) loss 的代码. 这个我看懂了。 D_{KL}(P||Q)=\sum P(i)\frac{P(i)}{Q(i)}H(p,q)=E_p[-\log q]=H(p)+D_{KL}(P||Q)123456789101112# 标准的KD，利用的是交叉熵求KD# 但速度很慢x, target = x.to(device), target.to(device)with torch.no_grad(): out = teacher(x) soft_target = F.softmax(out/T, dim=1)hard_target = targetout = student(x) ## this is the input to softmaxlogp = F.log_softmax(out/T, dim=1)loss_soft_target = -torch.mean(torch.sum(soft_target * logp, dim=1))loss_hard_target = nn.CrossEntropyLoss()(out, hard_target)loss = loss_soft_target * T * T + alpha * loss_hard_target L_{KD}=\alpha T^2 KLdiv(Q_S^T,Q_T^T)+(1-\alpha)CrossEntrop(Q_s,y_{true})1234567891011121314151617# 利用KL散度求KD# 理论上和实际上KL散度等同于交叉熵，速度很快，结果相同def loss_fn_kd(outputs, labels, teacher_outputs, params): """ Compute the knowledge-distillation (KD) loss given outputs, labels. "Hyperparameters": temperature and alpha NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher and student expects the input tensor to be log probabilities! See Issue #2 """ alpha = params.alpha T = params.temperature KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \ F.cross_entropy(outputs, labels) * (1. - alpha) return KD_loss 12345678910111213141516171819202122232425262728293031323334# 这段代码证明了DL散度和交叉熵的反向传播是一样的import torchimport torch.nn as nnimport torch.nn.functional as F# sample numberN = 10# category numberC = 5# softmax output of teacherp = torch.softmax(torch.rand(N, C), dim=1)# logit output of students = torch.rand(N, C, requires_grad=True)# softmax output of student, T = 1q = torch.softmax(s, dim=1)# KL Diverse# this is the implementation of the author's# torch will do element mean because it is the default option# kl_loss = nn.KLDivLoss()(torch.log(q), p)# I think this should be the right solutionkl_loss = (nn.KLDivLoss(reduction='none')(torch.log(q), p)).sum(dim=1).mean()kl_loss.backward(retain_graph=True)print 'grad using KL DivLoss'print s.grad# clear the grads.grad.zero_()# bug2: should not do element wise mean operation# ce_loss = torch.mean(-torch.log(q) * p)ce_loss = torch.mean(torch.sum(-torch.log(q) * p, dim=1))ce_loss.backward()print 'grad using ce loss'print s.grad# the real gradient of s should be `(q - p) / batch_size`print 'real grad, should be (q-p) / batch_size'print (q - p) / N 或者也可以参考: code]]></content>
      <categories>
        <category>semantice segmentation</category>
      </categories>
      <tags>
        <tag>semantice segmentation</tag>
        <tag>domain guided distillation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Domain Guided Dropout]]></title>
    <url>%2F2019%2F04%2F30%2FDomain-Guided-Dropout%2F</url>
    <content type="text"><![CDATA[0.前言这篇文章的主要任务是，给定多个有标签行人重识别数据集，如何训练一个模型，在各个数据集上表现都好，并且这个模型能“隐式识别出”（Domain Guided Dropout）是哪个数据集。 paper: CVPR2016 Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification code: caffe 1. Introduction对于单个数据集，模型已经能达到很好的效果，对于多个数据集，训练一个模型，怎么才能达到也是很好的效果。 思路分为两步： 第一步，将多个数据集的训练集融合到一起，用单分类器训练一个模型baseline 第二步，用Domain Guided Dropout(DGD)替换标准的dropout，再训练几个epoch，相当于用DGD对模型有针对性的微调。 结论：fine-tune+DGD的组合作用更大一些。 DGD有两种方案：deterministic DGD 和 stochastic DGD. 等会儿看看能不能从caffe代码中找到一些代码，毕竟看不懂caffe代码很伤啊。 实验结果证明了：1. 多个数据集混合训练得到的baseline比单数据集的模型效果要好；2. deterministic DGD 对baseline有提升，而且stochastic DGD 提升效果更好。 2. Related Work现在对于多个数据集训练一个模型的方法集中在如何学习一个共同的域不变的特征表示空间，但是作者这篇论文提出的模型允许模型学习到与域相关的components和域无关的特征表示空间，我觉得这个思路要更好一些，因为测试是在各个数据集上分别进行测试的，那么如果能学到一些与域相关的components，将特征共享空间与component组合之后得到域相关空间。 3. Method 针对这张图，我是这么理解的，假设模型在训练时特征提取了2048维特征，那么这2048维特征可能前1500维特征是和 domain A 有关，而后1500维特征和 domain B 有关，这里的有关可以暂时理解成对后续的分类结果有比较大的影响，举个极端的例子，在 domain A 中，主要根据第100维特征分类，那么第100维特征的存在对分类结果有很大的影响，而其他维的特征对 domain A 中的数据集分类无关，那么模型只需要每次都把其他维的特征直接变成0即可，只是因为其他域的存在，使得模型不能把其他维的特征每次都变成0，从而影响了 domain A 的特征提取。 后面作者也专门做了研究特征的影响。 3.1 公式定义 符号 含义 D D个数据集 $\lbrace (x_i^j, y_i^j)_{j=1}^{N_i}\rbrace_{i=1}^D$ 数据集的表示，第i个数据集第j个图片 $g(\cdot)$ generic feature extractor 3.2 baselinebaseline 值得注意的是作者没有采用多个分类器的方法，而是用一个分类器，把所有数据集混在一起，重新排id，来训练。 多个分类器$f_1, f_2, …, f_D$ \sum_{i=1}^D \sum_{j=1}^{N_i} L(f_i(g(x_i^j)), y_i^j)单分类器$f$ \sum_{i=1}^D \sum_{j=1}^{N_i} L(f(g(x_i^j)), y_i^j)按照作者的说法，这种单分类器能同时学习到domain biases and person id，后续有实验分析证明这个理论。 网络架构： 其中M表示行人类别。其中 fc7 包括：Linear, ReLU, Dropout, 再结合后续的DGD，不难猜测测试特征就是fc7的输出，也就是g(x)，这里的dropout灵活处理，因为DGD就是取代的dropout. 3.3 Domain Guided Dropout定义：$g(x)\in R^d$，第i个元素的影响因子定义为: s_i=L(g(x)_{\i})-L(g(x))其中$g(x)_{\i}$表示将i个元素变成0. 对于每个域，遍历所有图片得到第i个元素的影响因子: \bar{s}_i=E_{x\in D}[s_i] 这张图片可以清晰地表明对于不同的 domain，第i个特征的影响程度不同，换句话说，有些特征对 domain A 影响大，对 domain B 影响小，如果能自动学习到哪些特征影响作用大，那么就可以有针对性地训练这些特征并减少其他特征对测试的影响。作者的思路很清晰。 作者实际是用的二阶泰勒展开逼近的$s_i$: s_i \approx -\frac{\partial L}{\partial g(x)_i}g(x)_i+\frac{1}{2}\frac{\partial^2 L}{\partial g(x)_i^2}g(x)_i^2实验证明，这种逼近的精度是可以满足需求的： 定义：mask m，根据上一步得到的s得到mask m，有两种方案： 方案一：deterministic m_i = \begin{cases} 1 &\text{if} s_i>0 \\ 0 &\text{if} s_i\eqslantless 0 \end{cases}方法二：stochastic p(m_i=1)=\frac{1}{1+e^{-s_i/T}}然后用mask m代替fc7的dropout，再训练10个epoch.在测试的时候，特征也是需要经过mask m的。 4. Experiments JSTL: jointly with a single-task learning objective DGD: Domain Guided Dropout JSTL+DGD: improve JSTL with deterministic Domain Guided Dropout: work on all the domains simultaneously. FT-JSTL+DGD: fine-tune the CNN separately on each domain with the stochastic Domain Guided Dropout FT-JSTL: fine-tune the CNN separately on each domain with standard dropout 从上图中可以看出，联合训练对大数据集的作用不明显，对小数据集的作用比较明显，从随机初始化开始训练的话，deterministic DGD的作用几乎没有，fine-tune的作用是有的，fine-tune和stochastic DGD 共同起的作用更大一些，也就是说fine-tune本身的作用最大，如果在fine-tune的过程中加入DGD，发挥的作用会更好一些， 其中a图表示对所有的域同时训练，b图表示分别对域微调得到不同的模型。]]></content>
      <categories>
        <category>person re-id</category>
      </categories>
      <tags>
        <tag>person re-id</tag>
        <tag>multi-domain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[One_Example_reID]]></title>
    <url>%2F2019%2F03%2F27%2FOne-Example-reID%2F</url>
    <content type="text"><![CDATA[0. 前言这篇文章主要的任务也是为了解决re-ID中需要全部标签的问题，核心思想只对一个摄像头下的全部行人各取一张图片（对于没有出现的行人，从其他摄像头下取一张），然后通过训练模型，聚类给假标签，逐步识别出所有图片。 paper: TIP 2019 Progressive Learning for Person Re-Identification with One Example author: Yu Wu, Yutian Lin, Yi Yang (University of Technology Sydney (UTS)) code: https://github.com/Yu-Wu/One-Example-Person-ReID project: https://zhuanlan.zhihu.com/p/54576174 关于这篇文章的作者也多提几句。 关于行人重识别(person re-ID)，我看到的多是悉尼科技大学的郑良团队，所以简单介绍一下郑良团队的成员。 郑良：主页，github，2015年清华博士毕业，有学生郑哲东，钟准，孙奕帆。 郑哲东：主页, github，悉尼科技大学博三。 钟准：主页, github（鸣人），厦门大学交换生博四。 孙奕帆：主页，github，清华博二。 林雨恬：主页，github，很漂亮也很厉害的一个小姐姐. Wu Yu：主页，github，知乎，悉尼科技大学博一。 人家牛逼，虚心学习。 [-] 2019-03-29 看到一个大神廖星宇的github,中国科学技术大学,应用数学研究生,旷世科技工程师,他对re-id的研究的浓缩,我觉得就已经可以秒杀90%的论文了. 廖星宇:主页, github 1. Introductionperson re-ID 有监督模型，半监督模型。作者采用的是 one-shot image-based setting，也就是每个行人只有一个样例。 具体来说，作者将数据集分为三部分：labeled data, selected pseudo-labeled data and index-labeled data，其中 labeled data 和 selected pseudo-labeled data 用分类损失，index-labeled data 用 exclusive loss。exclusive loss 的目标是尽可能地使图片之间都离得比较远。 作者主要的改进点在于提出了 exclusive loss 和 label estimation 作者的另一篇相关文章是在视频上做的 paper: CVPR 2018 Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning 知乎: https://www.leiphone.com/news/201806/o8a3H5um1H2zXrof.html code: https://github.com/Yu-Wu/Exploit-Unknown-Gradually 2. The progressive model2.1 Framework overview训练模型分为两步：在三个数据集上训练CNN模型—&gt;在 unlabeled data 上选择一些数据放到 pseudo-labeled data 上。 2.2 Preliminaries labeled data set $\mathcal{L}$ unlabeled data set $\mathcal{U}$ pseudo-labeled data set $\mathcal{S}^t$ index-labeled data set $\mathcal{M}^t$ 2.3 The joint learning methodThe Exclusive Loss: index-labeled data set $\mathcal{M}^t$, \max \limits_{\theta}\parallel \phi(\theta;x_i) - \phi(\theta;x_j)\parallel令$||v_i||=\tilde{\phi}(\theta;x_i)$是$x_i$的归一化后的特征，因此可以将最大化欧氏距离近似为最小化cos距离。 这个公式没有推导出来，还在询问作者的答案. 在Joint Detection and Identification Feature Learning for Person Search看到了类似的公式，并且有了新理解，在下面补充。 l(V)=-log \frac{exp(v_i^T\tilde{\phi}(x_i)/\tau)}{\sum_{j=1}^{|M^t|}exp(v_j^T\tilde{\phi}(x_i)/\tau)}其中，V是所有数据的归一化特征，更高的$\tau$导致softer probability distribution.在更新时，先计算当前数据与所有数据的cos距离，在反向传播时，$v_i=1\2*(v_i+\tilde{\phi}(x_i)$并且归一化。 The cross-entropy loss:labeled data set $\mathcal{L}$ and pseudo-labeled data set $\mathcal{S}^t$ 不再陈述 The total loss: \min \lambda l(\mathcal{L}) + \lambda l(\mathcal{S}^t) + (1-\lambda) l(V)(\mathcal{M}^t)2.4 The effective sampling criterion没有采用分类损失，而是利用最近邻赋予假标签并且假标签的真实性为与最近邻真值的距离，每次取$m_t = m_{t-1}+p\cdot n_u$ 验证集是另外一个re-ID的训练集。 整体算法如下： 2.5 补充公式Joint Detection and Identification Feature Learning for Person Search 定义：n个数据，每个数据表示成向量，即L-2归一化向量$V={v_i}_{i=1}^{n}$，则新向量$x$属于$v_i$类的概率定义为: p_i=\frac{exp(v_i^Tx/\tau)}{\sum_{j=1}^{n}exp(v_i^Tx/\tau)}其中，$\tau$更高，概率分布更平缓，设为0.1. 这里有两种含义。 第一种，$v_i$有标签，但$V$的类别多，属于同一类的数据较少，$x$不属于$V$，$x$有标签t，那么n个数据的概率最大值为x的标签，目标应该是最大化x属于t类的概率，其目的是为了分类更加准确。 第二种，$v_i$没有标签，$x=v_k$属于$V$，目标是最大化x属于第k个元素的概率，最大也就是1，此时$x=v_k$与其他向量$v_j$都正交，其目的是为了令各个向量都离得比较远，也就是作者的目的。作者这里没有用正交来做损失函数，而是用了softmax，很厉害。 3. code3.1 数据集加载两个文件 ./reid/datasets/duke.py ./reid/utils/data/dataset.py ./reid/datasets/duke.py 123class Duke(Dataset): def download(self): # 预处理数据集,使其变成统一形式,从而使用load进行加载 ./reid/utils/data/dataset.py 12345class Dataset(object): def load(self, verbose=True): def _check_integrity(self): # 检查文件的完整性 images meta.json splits.json]]></content>
      <categories>
        <category>re-ID</category>
      </categories>
      <tags>
        <tag>one_example</tag>
        <tag>re-ID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GANimation]]></title>
    <url>%2F2019%2F01%2F24%2FGANimation%2F</url>
    <content type="text"><![CDATA[0. 前言这篇文章是根据GANnotation的一个公式查过来的，感觉还挺厉害。 \hat{I}=(1-M) \circ C + M \circ I paper: GANimation Anatomically-aware Facial Animation from a Single Image github: https://github.com/albertpumarola/GANimation project: https://www.albertpumarola.com/research/GANimation/index.html 关键词：starGAN的改进、连续的表情变换、贴回去能够一致 1. Introduction在人脸转换中，StarGAN是最成功的GAN，但是只能生成离散的人脸。作者要做的就是生成连续的表情变化。 2. Problem Formulation 符号 含义 $\mathrm{I}_{y_r}\in \mathbb{R}^{H×W×3}$ 输入图片 $\mathrm{y}_r=(y_1,…,y_N)^T$ 其中，每一个$y_i$表示第i个action unit的程度，在0~1之间 $\mathrm{I}_{y_g}$ 输出图片 $\mathcal{M}$ 映射函数M: $(\mathrm{I}_{y_r},\mathrm{y}_g)$—&gt;$\mathrm{I}_{y_g}$ 非成对图片 3. Our Approach 3.1 Network Architechture3.1.1 Generator对于G的改进，为了能够使G只聚焦于对于新表情的生成，而保留其他元素，引入attention机制，也就是G生成的不是一整张图片，而是两个mask，color mask C 和 attention mask A.即： \mathrm{I}_{\mathrm{y}_f}=(1-A)\cdot C + A\cdot \mathrm{I}_{\mathrm{y}_o}其中，$\mathrm{A}=G_A(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f)\in {0,…,1}^{H×W}$，$\mathrm{C}=G_C(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f)\in {0,…,1}^{H×W×3}$ 3.1.2 Conditional CriticPatchGAN: 输入图像 $\mathrm{I}\dashrightarrow \mathrm{Y}_{\mathrm{I}}\in \mathbb{R}^{H/2^6×W/2^6}$ 并且对判别器进行改进，加入额外的回归判别类别。 3.2 Learning the model损失函数 3.2.1 Image Adversarial Loss判断图片是生成的还是真实的。 和StarGAN的损失一样。 L_I(G,D_I,I_{y_o},y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[D_\mathrm{I}(G(\mathrm{I}_{\mathrm{y}_o}|\mathrm{y}_f))]-\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[D_\mathrm{I}(\mathrm{I}_{\mathrm{y}_o})]+\lambda_{gp} \mathbb{E}_{\tilde{I}\thicksim \mathbb{P}_{\tilde{I}}}[(\parallel \nabla_{\tilde{I}}D_I(\tilde{I}) \parallel _2-1)^2]3.2.2 Attention Loss这个损失是针对attention mask A 和 color mask C. Total Variation Regularization L_A(G,I_{y_o},y_f)=\lambda_{TV}\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\sum_{i,j}^{H,W}[(A_{i+1,j}-A_{i,j})^2+(A_{i,j+1}-A_{i,j})^2]]+\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel A \parallel_2]这个公式的初步感觉是A要尽可能平缓，并且A中的元素尽可能小。 根据作者的说法，是为了保证A不变成全是1的矩阵，并且为了保证更加平滑的空间结合。以代码为准。 3.2.3 Conditional Expression Loss这个应该和starGAN的判断图片属性分类正确损失是一样的。 L_y(G,D_y,I_{y_o},y_o,y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel D_y(G(I_{y_o}|y_f))-y_f \parallel]+\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel D_y(I_{y_o})-y_o \parallel]3.2.4 Identity Loss这个应该就是starGAN的重构损失 L_{idg}(G,I_{y_o},y_o,y_f)=\mathbb{E}_{\mathrm{I}_{\mathrm{y}_o}\thicksim \mathbb{P}_o}[\parallel G(G(I_{y_o}|y_f)|y_o)-I_{y_o} \parallel _1]这个损失是为了保证生成前后图片的id是一样的。 3.2.5 Full LossL=L_I+\lambda_y L_y+\lambda_A (L_A(G,I_{y_g},y_r)+L_A(G,I_{y_r},y_g))+\lambda_{idt}L_{idt}\lambda_{gp}=10, \lambda_A=0.1, \lambda_{TV}=0.0001, \lambda_y=4000, \lambda_{idt}=104. Implementation DetailsThe attention mechanism guaranties a smooth transition between the morphed cropped face and the original image. 也就是说 attention mechanism 能够保证生成的图片很好地再贴回去。 4.1 Single Action Units Edition [x] 这里的AU是什么？ intensity怎么理解？ AU:https://www.cs.cmu.edu/~face/facs.htm intensity: https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units 4.2 Simultaneous Edition of Multiple AUs \alpha y_g+(1-\alpha)y_r4.3 Discrete Emotions Editing 作者生成的图片比StarGAN更清晰。 4.4 High Expressions Variability4.5 Images in the Wild 作者先检测到人脸，然后扣下来，做训练测试，然后再贴回去，与原图保持了一样的清晰度，个人猜测是因为表情的变化只在人脸的中央就可以完成，不涉及到背景的变换，如果涉及到背景的变换，那么是否还能保证贴回去与原图保持一致性。 4.6 Pushing the Limits of the Model 5. code5.1 生成器GeneratorGANimation的Generator的主体网络和starGAN的Generator的主体网络一致，只是多加了一个conv 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Generator(NetworkBase): """Generator. Encoder-Decoder Architecture.""" def __init__(self, conv_dim=64, c_dim=5, repeat_num=6): super(Generator, self).__init__() self._name = 'generator_wgan' layers = [] layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False)) layers.append(nn.InstanceNorm2d(conv_dim, affine=True)) layers.append(nn.ReLU(inplace=True)) # Down-Sampling curr_dim = conv_dim for i in range(2): layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False)) layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True)) layers.append(nn.ReLU(inplace=True)) curr_dim = curr_dim * 2 # Bottleneck for i in range(repeat_num): layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim)) # Up-Sampling for i in range(2): layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False)) layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True)) layers.append(nn.ReLU(inplace=True)) curr_dim = curr_dim // 2 self.main = nn.Sequential(*layers) layers = [] layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False)) layers.append(nn.Tanh()) self.img_reg = nn.Sequential(*layers) layers = [] layers.append(nn.Conv2d(curr_dim, 1, kernel_size=7, stride=1, padding=3, bias=False)) layers.append(nn.Sigmoid()) self.attetion_reg = nn.Sequential(*layers) def forward(self, x, c): # replicate spatially and concatenate domain information c = c.unsqueeze(2).unsqueeze(3) c = c.expand(c.size(0), c.size(1), x.size(2), x.size(3)) x = torch.cat([x, c], dim=1) features = self.main(x) return self.img_reg(features), self.attetion_reg(features) 5.2 DiscriminatorDiscriminator和StarGAN 的Discriminator完全一样 1234567891011121314151617181920212223242526class Discriminator(NetworkBase): """Discriminator. PatchGAN.""" def __init__(self, image_size=128, conv_dim=64, c_dim=5, repeat_num=6): super(Discriminator, self).__init__() self._name = 'discriminator_wgan' layers = [] layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1)) layers.append(nn.LeakyReLU(0.01, inplace=True)) curr_dim = conv_dim for i in range(1, repeat_num): layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1)) layers.append(nn.LeakyReLU(0.01, inplace=True)) curr_dim = curr_dim * 2 k_size = int(image_size / np.power(2, repeat_num)) self.main = nn.Sequential(*layers) self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False) self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=k_size, bias=False) def forward(self, x): h = self.main(x) out_real = self.conv1(h) out_aux = self.conv2(h) return out_real.squeeze(), out_aux.squeeze() 5.3 train D这里训练D的过程和starGAN有所不同，并且超参数也有所不同。 \lambda_{D-cond}=4000, \lambda_{gp}=10starGAN: \lambda_{cls}=1, \lambda_{gp}=10 [ ] 为什么和怎么使用的MSELoss 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# starGAN是把Image Adversarial Loss的三项一起反向传播，但GANimation是分开反向传播的，不确定这么做是否有影响。loss_D, fake_imgs_masked = self._forward_D()self._optimizer_D.zero_grad()loss_D.backward()self._optimizer_D.step()loss_D_gp= self._gradinet_penalty_D(fake_imgs_masked)self._optimizer_D.zero_grad()loss_D_gp.backward()self._optimizer_D.step()def _forward_D(self): # generate fake images fake_imgs, fake_img_mask = self._G.forward(self._real_img, self._desired_cond) fake_img_mask = self._do_if_necessary_saturate_mask(fake_img_mask, saturate=self._opt.do_saturate_mask) fake_imgs_masked = fake_img_mask * self._real_img + (1 - fake_img_mask) * fake_imgs # D(real_I) # 识别真图片为真，(Image Adversarial Loss) # 图片类别分类准确，这里的分类用的不是交叉熵，而是MSELoss，(Conditional Expression Loss) # 刚刚发现一个问题，如果是分类损失，MSELoss的输入必须是同样大小的，按照starGAN，D的输出是类别大小(batch*classification)，G的输入是(batch*1)，那这个样子肯定是没法进行MSELoss的，所以还需要看了数据的处理之后才能明白怎么回事。 # self._criterion_D_cond = torch.nn.MSELoss().cuda() d_real_img_prob, d_real_img_cond = self._D.forward(self._real_img) self._loss_d_real = self._compute_loss_D(d_real_img_prob, True) * self._opt.lambda_D_prob self._loss_d_cond = self._criterion_D_cond(d_real_img_cond, self._real_cond) / self._B * self._opt.lambda_D_cond # D(fake_I) # 识别假图片为假，(Image Adversarial Loss) d_fake_desired_img_prob, _ = self._D.forward(fake_imgs_masked.detach()) self._loss_d_fake = self._compute_loss_D(d_fake_desired_img_prob, False) * self._opt.lambda_D_prob # combine losses return self._loss_d_real + self._loss_d_cond + self._loss_d_fake, fake_imgs_maskeddef _compute_loss_D(self, estim, is_real): return -torch.mean(estim) if is_real else torch.mean(estim)def _gradinet_penalty_D(self, fake_imgs_masked): # (Image Adversarial Loss)的第三项 # interpolate sample alpha = torch.rand(self._B, 1, 1, 1).cuda().expand_as(self._real_img) interpolated = Variable(alpha * self._real_img.data + (1 - alpha) * fake_imgs_masked.data, requires_grad=True) interpolated_prob, _ = self._D(interpolated) # compute gradients grad = torch.autograd.grad(outputs=interpolated_prob, inputs=interpolated, grad_outputs=torch.ones(interpolated_prob.size()).cuda(), retain_graph=True, create_graph=True, only_inputs=True)[0] # penalize gradients grad = grad.view(grad.size(0), -1) grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1)) self._loss_d_gp = torch.mean((grad_l2norm - 1) ** 2) * self._opt.lambda_D_gp 5.4 train G这一部分和starGAN的训练类似，比starGAN多一个mask的平滑loss。 \lambda_{D-cond}=4000, \lambda_{cyc}=10, \lambda_{mask}=0.1, \lambda_{mask-smooth}=1*e^{-5}starGAN: \lambda_{cls}=1, \lambda_{cyc}=101234567891011121314151617181920212223242526272829303132def _forward_G(self, keep_data_for_visuals): # generate fake images fake_imgs, fake_img_mask = self._G.forward(self._real_img, self._desired_cond) fake_img_mask = self._do_if_necessary_saturate_mask(fake_img_mask, saturate=self._opt.do_saturate_mask) fake_imgs_masked = fake_img_mask * self._real_img + (1 - fake_img_mask) * fake_imgs # D(G(Ic1, c2)*M) masked # 生成图片为真 (Image Adversarial Loss) # 生成图片的属性为真 (Conditional Expression Loss) d_fake_desired_img_masked_prob, d_fake_desired_img_masked_cond = self._D.forward(fake_imgs_masked) self._loss_g_masked_fake = self._compute_loss_D(d_fake_desired_img_masked_prob, True) * self._opt.lambda_D_prob self._loss_g_masked_cond = self._criterion_D_cond(d_fake_desired_img_masked_cond, self._desired_cond) / self._B * self._opt.lambda_D_cond # G(G(Ic1,c2), c1) # 重构损失 (Identity Loss) rec_real_img_rgb, rec_real_img_mask = self._G.forward(fake_imgs_masked, self._real_cond) rec_real_img_mask = self._do_if_necessary_saturate_mask(rec_real_img_mask, saturate=self._opt.do_saturate_mask) rec_real_imgs = rec_real_img_mask * fake_imgs_masked + (1 - rec_real_img_mask) * rec_real_img_rgb # l_cyc(G(G(Ic1,c2), c1)*M) self._loss_g_cyc = self._criterion_cycle(rec_real_imgs, self._real_img) * self._opt.lambda_cyc # loss mask # (Attention Loss) 不仅对生成的mask进行了平滑，也对重构生成的mask进行了平滑损失 self._loss_g_mask_1 = torch.mean(fake_img_mask) * self._opt.lambda_mask self._loss_g_mask_2 = torch.mean(rec_real_img_mask) * self._opt.lambda_mask self._loss_g_mask_1_smooth = self._compute_loss_smooth(fake_img_mask) * self._opt.lambda_mask_smooth self._loss_g_mask_2_smooth = self._compute_loss_smooth(rec_real_img_mask) * self._opt.lambda_mask_smoothdef _compute_loss_smooth(self, mat): return torch.sum(torch.abs(mat[:, :, :, :-1] - mat[:, :, :, 1:])) + \ torch.sum(torch.abs(mat[:, :, :-1, :] - mat[:, :, 1:, :])) 5.5 保存图片这个保存图片在starGAN就没有太理解，在这里又看到了类似的，才理解这是对输入图片归一化的反向操作 123456789101112131415161718# starGANfrom torchvision.utils import save_imagedef denorm(self, x): """Convert the range from [-1, 1] to [0, 1].""" out = (x + 1) / 2 return out.clamp_(0, 1)save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)# GANimationimport numpy as npmean = [0.5, 0.5, 0.5]std = [0.5, 0.5, 0.5]for i, m, s in zip(img, mean, std): i.mul_(s).add_(m)image_numpy = img.numpy()image_numpy_t = np.transpose(image_numpy, (1, 2, 0))image_numpy_t = image_numpy_t*254.0image_numpy_t.astype(np.uint8) 5.6 其他没有实际跑这个代码，所以对于一些细节不是很清晰，尤其是数据处理那里，暂时根据查到的AU资料理解成17个AU(但1, 2, 4, 5, 6, 7, 9, 10, 12, 14, 15, 17, 20, 23, 25, 26, 28, and 45是18个AU)，每个AU是一个0~5的数字。 但是对于作者所说的能够生成连续的表情变换，这一点只能在测试代码中看出，但是在训练的时候并没有特意去表示连续的变化，暂时对于连续的变化存疑。 主要是openface这个库有点晕，等数据集下载之后试试。 https://github.com/albertpumarola/GANimation/issues/45https://github.com/albertpumarola/GANimation/issues/62https://github.com/albertpumarola/GANimation/issues/43https://github.com/albertpumarola/GANimation/issues/32https://github.com/albertpumarola/GANimation/issues/25]]></content>
      <categories>
        <category>GAN</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>mask and colour</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GANnotation]]></title>
    <url>%2F2019%2F01%2F17%2FGANnotation%2F</url>
    <content type="text"><![CDATA[0. 前言这是对GAN应用到人脸合成的改进，只是在arxiv上，先看看再说。 paper: https://arxiv.org/abs/1811.03492 github: https://github.com/ESanchezLozano/GANnotation youtube: https://youtu.be/-8r7zexg4yg key word: self-consistency loss, triple consistency loss, progressive image translation 1. Introduction作者对 self-consistency loss 的有效性进行了论证， self-consistency loss 可以有效地使图片转换前后 preserve identity，并且提出了新的loss triple consistency loss. 作者观察到当生成的图片再次经过生成另一种属性的图片时，生成的效果很差。记生成的图片再次经过网络生成新的图片的过程为 “progressive image translation”。比如输入一张图片a，先通过网络生成图片b，再将图片b送入网络，得到生成图片c。 并且作者提出了GAN-notation，即 unconstrained landmark guided face-to-face synthesis, 同时改变人脸的姿势和表情(simultaneous change in pose and expression)， 论证了 triple consistency loss 的有效性。 2. Proposed approach 2.1 Notation符号说明： 符号 解释 $I\in \mathcal{I}, w × h$ 人脸图片$I$ $s_i \in \mathbb{R}^{2n}$ $n$个有序点的集合 $H(s_t) \in \mathcal{H}, H(s_t) \in \mathbb{R}^{n×w×h}$ $s_i$编码成 heatmap，以均值为点的高斯分布的形式呈现 $G: \mathcal{I}×\mathcal{H} \to \hat{\mathcal{I}}$ $\hat{\mathcal{I}}$是生成图片的集合 $\hat{I} = G(I;H(s_t))$ 其中$I$和$H(s_t)$是在通道维拼接而成的 $\mathbb{P}_{\mathcal{I}}$ 图片$I$的分布 $\mathbb{P}_{\hat{\mathcal{I}}}$ 图片$\hat{I}$的分布 2.2 ArchitectureG: two spatial downsampling convolutions followed by a set of residual blocks two spatial upsampling blocks with 1=2 strided convolutions output: colour image $C$ and a mask $M$. \hat{I}=(1-M) \circ C + M \circ ID: PatchGAN, 128×128—&gt;4×4×512 2.3 Training2.3.1 Adversarial loss作者使用的是 hinge adversarial loss。 我在参考文献19中没有找到类似的loss，自己理解的是，对于判别器D而言，希望真图片的真值是$y \leqslant -1$，生成图片的真值是$y \geqslant 1$， 对于生成器G而言，希望生成的图片的真值是$y \geqslant 0$ \begin{align} L_{adv}(D) = &-\mathbb{E}_{\hat{I}\backsim \mathbb{P}_{\hat{\mathcal{I}}}}[min(0, -1+D(\hat{I}))]+ \\ &-\mathbb{E}_{I\backsim \mathbb{P}_{\mathcal{I}}}[min(0, -1+D(\hat{I}))] \end{align}L_{adv}(G)=-\mathbb{E}_{I \backsim \mathbb{P}_I } [D(\hat{I})]2.3.2 Pixel loss这个公式应该需要成对图片 L_{pix} = \parallel G(I;H(s_t)) - I_t \parallel2.3.3 Consistency lossL_{self}=\parallel G(G(I;H(s_t)); H(s_i)) - I \parallel2.3.4 Triple Consistency lossL_{triple}=\parallel G(\hat{I};H(s_n)) - G(I;H(s_n)) \parallel ^2其中，$ \hat{I} = G(I;H(s_t)) $. 2.3.5 Identity preserving lossL_{id}=\sum_{l=fc,p} \parallel \Phi_{CNN}^l(I) - \Phi_{CNN}^l(\hat{I}) \parallel其中，这个公式的目的是为了preserve the identity(???), 使用 Light CNN 的 fully connected layer 和 last pooling layer 提取出的特征。 不懂，为啥子呢？之前看StarGAN已经不使用这个公式了。 2.3.6 Perceptual loss这个应该就是之前见过的Vgg提取特征做损失。 \begin{align} L_{pp} =& \sum_{l} \parallel \Phi_{VGG}^l(I) \Phi_{VGG}^l(\hat{I}) \parallel \\ &+\parallel \Gamma(\Phi_{VGG}^{relu3_3}(I)) - \Phi_{VGG}^{relu3_3}(\hat{I}) \parallel_F \end{align}其中，$l=\lbrace relu1_2, relu2_2, relu3_3, relu4_3\rbrace$ 感觉用到的损失太多了。 2.3.7 Full loss\begin{align} L(G) =& \lambda_{adv}L_{adv} + \lambda_{pix}L_{pix}+\lambda_{self}L_{self}+\\ &\lambda_{triple}L_{triple}+\lambda_{id}L_{id}+\lambda_{pp}L_{pp}+\lambda_{tv}L_{tv} \end{align}其中，$\lambda_{adv}=1, \lambda_{pix}=10, \lambda_{self}=100, \lambda_{triple}=100, \lambda_{id}=1, \lambda_{pp}=10, \lambda_{tv}=10^{-4} $ 在作者给的参考文献14中，也没有明确找到$L_{tv}$的表达式。之后看代码再确定一下吧。 3. Experiments Adam, $\beta_1=0.5, \beta_2=0.999$ 3.1 On the use of a triple consistency loss 可以看到即使是 one-to-one ， with triple consistency loss 的效果也要更好一些。 3.2 GANnotationStarGAN只能对很粗的概念进行转换，但是GANnotation可以直接对比较细的属性pose进行转换，而且还可以连续变换，更厉害一些。 4. code4.1 生成器G生成器G输入是一张3通道的人脸图片和66通道的目标人脸关键点热力图，输出是3通道的colour image 和1通道的mask图。 生成器G共分为5部分：down_conv、bottleneck、feature_layer、colour_layer、mask_layer。 [x] colour_image 和 mask 怎么理解？ \hat{I}=(1-M) \circ C + M \circ I123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Generator(nn.Module): def __init__(self, conv_dim=64, c_dim=66, repeat_num=6): super(Generator, self).__init__() initial_layer = [nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1,padding=3, bias=False)] initial_layer += [nn.InstanceNorm2d(conv_dim, affine=True)] initial_layer += [nn.LeakyReLU(0.2, inplace=True)] curr_dim = conv_dim for i in range(2): initial_layer += [nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False)] initial_layer += [nn.InstanceNorm2d(curr_dim*2, affine=True)] initial_layer += [nn.LeakyReLU(0.2, inplace=True)] curr_dim = curr_dim * 2 self.down_conv = nn.Sequential(*initial_layer) bottleneck = [] for i in range(repeat_num): bottleneck += [ResidualBlock(curr_dim)] self.bottleneck = nn.Sequential(*bottleneck) features = [] for i in range(2): features += [nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False)] features += [nn.InstanceNorm2d(curr_dim//2, affine=True)] features += [nn.LeakyReLU(0.2,inplace=True)] curr_dim = curr_dim // 2 self.feature_layer = nn.Sequential(*features) colour = [nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False)] colour += [nn.Tanh()] self.colour_layer = nn.Sequential(*colour) mask = [nn.Conv2d(curr_dim, 1, kernel_size=7, stride=1, padding=3, bias=False)] mask += [nn.Sigmoid()] self.mask_layer = nn.Sequential(*mask) init_weights(self) def forward( self, x ): down = self.down_conv(x) bottle = self.bottleneck(down) features = self.feature_layer(bottle) col = self.colour_layer(features) mask = self.mask_layer(features) output = mask * ( x[:,0:3,:,:] - col ) + col return output 初始化 1234567891011121314151617181920212223def init_weights(net, init_type='normal', gain=0.02): def init_func(m): classname = m.__class__.__name__ if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1): if init_type == 'normal': weight_init.normal_(m.weight.data, 0.0, gain) elif init_type == 'xavier': weight_init.xavier_normal_(m.weight.data, gain=gain) elif init_type == 'kaiming': weight_init.kaiming_normal_(m.weight.data, a=0, mode='fan_in') elif init_type == 'orthogonal': weight_init.orthogonal_(m.weight.data, gain=gain) else: raise NotImplementedError('initialization method [%s] is not implemented' % init_type) if hasattr(m, 'bias') and m.bias is not None: weight_init.constant_(m.bias.data, 0.0) elif classname.find('BatchNorm2d') != -1: weight_init.normal_(m.weight.data, 1.0, gain) weight_init.constant_(m.bias.data, 0.0) print('initialize network with %s' % init_type) net.apply(init_func) np.loadtxt 1points = np.loadtxt('test_images/test_1.txt').transpose().reshape(66,2,-1) 4.2 其他作者现在只公布了demo代码。但不久就会公布训练代码，搓搓小手手期待中。 4.3 StarGAN-with-Triple-Consistency-Loss123456789101112# Generate target domain labels randomly.rand_idx = torch.randperm(label_org.size(0))label_trg = label_org[rand_idx]rand_idx = torch.randperm(label_org.size(0))label_third = label_org[rand_idx]# ================================================================================## Triple consistency loss. ## ================================================================================#x_third = self.G(x_fake, c_third)x_third_real = self.G(x_real, c_third)g_loss_triple = torch.mean(torch.abs(x_third - x_third_real)) 5. 答疑解惑5.1 colour_image 和 mask 怎么理解？这个公式来源于 paper: Ganimation: Anatomically-aware facial animation from a single image github: https://github.com/albertpumarola/GANimatio project: http://www.albertpumarola.com/#projects \hat{I}=(1-M) \circ C + M \circ I网络生成器没有直接回归整个图像，而是输出两个掩码，一个着色掩码C和一个注意力掩码A，其中，掩码A表示C的每个像素在多大程度上对输出图像有贡献，这样生成器就无需渲染与表情无关的元素，仅聚焦于定义了人脸表情的像素上。 这个公式貌似比StarGAN还牛逼。能不能放在StarGAN上呢？]]></content>
      <tags>
        <tag>GAN</tag>
        <tag>face synthesis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorboard]]></title>
    <url>%2F2019%2F01%2F15%2Ftensorboard%2F</url>
    <content type="text"><![CDATA[0. 前言新装pytorch之后，可视化工具visdom就不能用了，所以改用tensorboard，以下是tensorboard及其变体的使用命令。 1. 原理tensorboard与visdom不同，前者是直接读取文件进行展示，需要程序先把要展示的内容保存成文件，然后tensorboard再读取文件进行展示，后者是代码直接展示，在程序中就直接传递给visdom了。 相同点是两者都需要在程序外面启动。 2. API以及使用流程2.1 API tf.summary.FileWriter——用于将汇总数据写入磁盘 tf.summary.scalar——对标量数据汇总和记录 tf.summary.histogram——记录数据的直方图 tf.summary.image——将图像写入summary tf.summary.merge——对各类的汇总进行一次合并 tf.summary.merge_all——合并默认图像中的所有汇总 2.2 使用流程 添加记录节点：tf.summary.scalar/image/histogram()等 汇总记录节点：merged = tf.summary.merge_all() 运行汇总节点：summary = sess.run(merged)，得到汇总结果 日志书写器实例化：summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph)，实例化的同时传入 graph 将当前计算图写入日志 调用日志书写器实例对象summary_writer的add_summary(summary, global_step=i)方法将所有汇总日志写入文件 调用日志书写器实例对象summary_writer的close()方法写入内存，否则它每隔120s写入一次， close() 之后就无法再次写入了，需要重新打开reopen()，这里可以替代为summary_writer.flush()。 3. 启动1tensorboard --logdir /path/to/log 4. 初始化123456789101112import tensorflow as tf# tf.summary.FileWriter(logdir, graph=None, flush_secs=120, max_queue=10)writer = tf.summary.FileWriter("/path/to/log", sess.graph(), flush_secs = 2)# orwriter = tf.summary.FileWriter("/path/to/log")# orwriter = tf.summary.FileWriter("/path/to/log")writer.add_graph(sess.graph()) 其他常用API add_event(event)：Adds an event to the event file add_graph(graph, global_step=None)：Adds a Graph to the event file，Most users pass a graph in the constructor instead add_summary(summary, global_step=None)：Adds a Summary protocol buffer to the event file，一定注意要传入 global_step close()：Flushes the event file to disk and close the file flush()：Flushes the event file to disk add_meta_graph(meta_graph_def,global_step=None) add_run_metadata(run_metadata, tag, global_step=None) 备注：这个event还是没有看懂怎么使用。 5. scalar1234567891011121314151617import tensorflow as tf# 三个工具：writer用于指定路径，sess用于运行，summary_op用于执行。summary_writer = tf.summary.FileWriter("/path/to/log")sess = tf.Session()tf.summary.scalar('accuracy', accuracy) # tag, valuesummary_op = tf.summary.merge_all()summary_str = sess.run(summary_op)summary_writer.add_summary(summary_str, global_step) # y, x# 备注：global_step总是int型，即使输入0.4，在图像显示也是0.# or familytf.summary.scalar('loss1', 1, family='loss')tf.summary.scalar('loss2', 2, family='loss')tf.summary.scalar('loss3', 2)# 示意如下图 或者自定义数据 123456789101112131415import tensorflow as tf# 两个工具 summary_writer: 用于连接路径， summary: 用于指定y值summary_writer = tf.summary.FileWriter("/path/to/log")summary = tf.Summary(value=[ tf.Summary.Value(tag='test2', simple_value=0), tf.Summary.Value(tag='test3', simple_value=1),])summary_writer.add_summary(summary, global_step) # y, x# orsummary_writer = tf.summary.FileWriter("/path/to/log")summary = tf.Summary()summary.value.add(tag='test4', simple_value=0)summary.value.add(tag='test5', simple_value=1)sumamry_writer.add_summary(summary, global_step) 6. histogram1tf.summary.histogram('layer'+str(i+1)+'weights',weights) 这里的weights可以是list型，也可以是pytorch的tensor型，其他类型没试，但可以推断，一般情况下的都可以，猜测会统一转换为numpy型。 7. image1234567tf.summary.image('input', x_image, max_outputs=3)# ortf.summary.image('test', tf.reshape(images, [-1, 28, 28, 1]), 10)# ortf.summary.image('test1', torch.rand(3, 256, 256, 3)) 备注：x_image必须是uint8或者float32型的4-D Tensor[batch_size, height, width, channels]，其中channels为1， 3 或者4 对于numpy 12345678910from PIL import Imageimport tensorflow as tfimport torchfrom torchvision import transforms as Timage_ = Image.open('path')image_numpy = numpy.array(image_) # 271, 108, 3tf.summary.image('test1', image_numpy.reshape(1, 271, 108,3))summary_op = tf.summary.merge_all()summary_str = sess.run(summary_op)summary_writer.add_summary(summary_str, global_step) 对于pytorch 123456789from PIL import Imageimage_ = Image.open('path') # 3, 271, 108image_tensor = T.ToTensor()(image_)tf.summary.image('test9', image_tensor.permute(1,2,0).reshape(1, 271, 108,3))# 备注：将permute改为reshape之后不知道为什么会展示9空格的相同的黑白图片summary_op = tf.summary.merge_all()summary_str = sess.run(summary_op)summary_writer.add_summary(summary_str, global_step) uint8或者float32型的4-D Tensor[batch_size, height, width, channels]，其中channels为1， 3 或者4 8. 多个event的可视化如果 logdir 目录的子目录中包含另一次运行时的数据(多个 event)，那么 TensorBoard 会展示所有运行的数据(主要是scalar)，这样可以用于比较不同参数下模型的效果，调节模型的参数，让其达到最好的效果！ 这个还有待测试 9. tensorboard_loggertensorboard_logger可以暂时理解成简化版的tensorboard，或者是tensorboard的高级API。 tensorboard_logger依赖于tensorboard。 12345678910111213141516171819202122# 安装pip install tensorboard_logger# 启动tensorboard --logdir /path/to/log --port /port # 实际还是通过tensorboard来使用# 初始化from tensorboard_logger import Loggerlogger = Logger(logdir='./logs', flush_secs=2)# logger.logdir = './logs2' # 可以修改，尝试着修改了一下，发现没用，暂时用不着。# logger.flush_secs = 3 # 可以修改# APIlogger.log_value('loss', 10, step=3) # step 可以不写，推荐写# image_path = 'test.jpg'# image_ = Image.open(image_path)# image_numpy = np.array(image_)logger.log_images('images', image_numpy.reshape(1,271,168,3), step=2) # 规则同上logger.log_histogram('weights', torch.rand(2,3)+10, step=10)# 函数tensorboard_logger.configure, tensorboard_logger.log_value与tensorboard_logger.Logger可以达到一样的效果from tensorboard_logger import configure, log_valueconfigure("runs/run-1234", flush_secs=5)log_value('v1', v1, step) 10. tensorboardXhttps://github.com/lanpa/tensorboardX tensorboardX也依赖于tensorflow和tensorboard。 1234567891011121314# 安装pip install tensorboardX# 启动tensorboard --logdir /path/to/log --port /port# 初始化from tensorboardX import SummaryWriterwriter = SummaryWriter(logdir = 'tensorboard7', comment = 'test1', flush_secs = 1)# 当有logdir时，comment被忽略，# 当没有logdir时，只有comment(writer = SummaryWriter(comment = 'tensorboard7_test1', flush_secs = 1))会作为补充自动生成这样的文件目录：# ./runs/Jan17_11-24-12_zbp-PowerEdge-T630tensorboard7_test1/events....# 此外，任意tf.summary.FileWriter的参数都可以加上去。writer.close()# 没有writer.flush() 另一种写法 12with SummaryWriter(logdir = 'tensorboard7') as w: w.add_something() 不推荐with的写法，会自动重新创建一个文件，乱。 10.1 add_scalar12345# APIwriter.add_scalar('loss1', 2, global_step=2)writer.add_scalars('loss', &#123;'loss1':2, 'loss2':3&#125;, global_step=2)# 示例如下图，add_scalars可以将多个loss显示同一个图上，实现类似visdom的功能。# add_scalars(main_tag,tag_scalar_dict,global_step=None) 备注：: SummaryWriter以及之前的类似Writer都可以自动创建文件夹。 tensorboard —logdir /path/to/log会自动迭代该文件下的所有文件夹和文件，将event展示出来，并且将同名字的scalar放在一起，可以用于对比修改前后的结果。 10.2 add_image12345678910writer.add_image(tag, img_tensor, global_step=None)# img_tensor: 图像数据，shape（3，H，W) 配合torchvision.utils.make_grid使用# writer.add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW'),# dataformats: 'CHW', 'HWC', 'HW'image_path = 'test.jpg'image_ = Image.open(image_path)image_tensor = T.ToTensor()(image_)writer.add_image('test1', image_tensor, global_step=2)writer.add_image('test2', torchvision.utils.make_grid(torch.rand(16, 3, 256,256), nrow=8, padding=20), global_step=2) 10.3 add_histogram123456writer.add_histogram(tag,values,global_step=None,bins='tensorflow')# bins: (string): one of &#123;'tensorflow','auto', 'fd', ...&#125;, this determines# how the bins are made. You can find other options in:# https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.htmlwriter.add_histogram('test1', torch.rand(16, 3, 256,256), global_step=3)# 感觉 bins 这个参数也差不多么 10.4 add_graph123456789101112131415161718192021from torch import nnimport torchclass Net1(nn.Module): def __init__(self): super(Net1, self).__init__() self.conv1 = nn.Conv2d(3, 10, kernel_size=3, padding=0) self.fc1 = nn.Linear(10, 100) def forward(self, x ): x = self.conv1(x) x = x.view(-1, 10) x = self.fc1(x) x = torch.nn.functional.softmax(x, dim=1) return xinput = torch.rand(4, 3, 3, 3)model = Net1()writer.add_graph(model, (input,))add_graph(model, input_to_model, verbose=False)# verbose 用于是否print 终于pytorch也能显示graph了，不容易，应该提前熟悉这些API的。 11. 结论综上所述，我觉得： tensorboardX最适合非tensorflow的深度学习框架 tensorboard适合tensoflow tensorboard_logger也适合非tensorflow的深度学习框架，可以看成简化版的tensorboardX tensorboardX和tensorboard_logger可以互相替换 开始使用tensorboardX作为我的新的可视化工具。 不太懂tensorboardX和tensorboard_logger之间的区别，难道是开发的人不一样？]]></content>
      <categories>
        <category>tensorboard</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>tensorboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SyRI]]></title>
    <url>%2F2019%2F01%2F14%2FSyRI%2F</url>
    <content type="text"><![CDATA[0. 前言这也是一篇合成数据集的论文。 paper: Domain Adaptation through Synthesis for Unsupervised Person Re-identification github: 无 项目地址: 无 1. Introduction针对的问题是行人重识别数据缺乏在不同光照条件下的多样性，所以作者进行了数据合成。 HDR maps: high dynamic range environment maps. three-step domain adaption technique: Illumination inference Domain translation Fine-tuning 其他这篇文章看得我有点迷迷糊糊的，感觉其中用到了cycleGAN，优化了cycleGAN的一个损失函数，但是怎么进行的illuminaion inference、fine-tuning，没有充分理解。 罗浩对这篇文章的解读https://zhuanlan.zhihu.com/p/44212707 由于训练数据的缺乏，以及3D建模的技术增强，利用游戏等合成的逼真数据来进行视觉研究已经逐渐打入各个领域。这一篇就是利用电脑合成数据在进行ReID的论文。论文提出了新的合成数据集SyRI dataset，该数据集通过改变HDR参数等，一个行人ID可以拥有一百多个环境下图像数据。此外，为了在未见过的真实场景上实现更好的域适应效果，论文基于这个合成数据集提出了一种新的方法。 整个Pipeline包括三个环节。第一步是拿到了target domain $R_{M+1}$ 的一些未标注图片之后，要对光照进行一个推理，在合成数据集 $S_{k^*}$ 里面找到最接近样本 。然后利用CycleGAN将合成数据生成target domain style的数据。最后利用生成的数据对ReID网络进行fune-tuning。整体来说pipeline比较简单. 例如，在CycleGAN生成图环节，为了得到更高质量的图像，论文使用了和PTGAN一样的前景mask的思想。不同点在于PTGAN是借助于语义分割网络得到一个行人前景的mask，本文是直接使用了一个2D高斯核作为mask]]></content>
      <categories>
        <category>person re-id</category>
      </categories>
      <tags>
        <tag>person re-identification</tag>
        <tag>domain adaption</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InstaGAN]]></title>
    <url>%2F2019%2F01%2F04%2FInstaGAN%2F</url>
    <content type="text"><![CDATA[实例转换 0. 前言InstaGAN Instance-aware image-to-image translation Sangwoo Mo, Minsu Cho, Jinwoo Shin github: https://github.com/sangwoomo/instagan project: https://openreview.net/forum?id=ryxwJhC9YX 1. Introduction 整体分为三部分: an instance-augmented neural architecture a context preserving loss a sequential mini-batch inference/training technique an instance-augmented neural architecture: an image and the corresponding set of instance attributes. a context preserving loss: target instances and an identity function a sequential mini-batch inference/training technique: translating the mini-batches of instance attributes sequentially 2. InstaGAN符号说明： 符号 说明 $\mathcal{X}$, $\mathcal{Y}$ image domain $\mathcal{A}, \mathcal{B}$ a space of set of instance attributes $\boldsymbol{a} = \lbrace a_i \rbrace _{i=1}^N $ set of instance attributes instance segmentation mask $G_{XY}:\mathcal{X}-&gt;\mathcal{Y}, G_{YX}:\mathcal{Y}-&gt;\mathcal{X}$ tranlation function 2.1 InstaGAN architecture 符号 说明 $f_{GX}$ image feature extractor $f_{GA}$ attribute feature extractor $H_{GX}(x,a)=[f_{GX}(x);\sum_{i=1}^Nf_{GA}(a_i)]$ image representation $H_{GA}^n(x,a)=[f_{GX}(x);\sum_{i=1}^Nf_{GA}(a_i);f_{GA}(a_n)]$ image representation $h_{DX}(x,a)=[f_{DX}(x);\sum_{i=1}^Nf_{DA}(a_i)]$ image representation for discriminator $f_{GX},f_{GA},f_{DX},f_{DA},g_{GX},G_{GA},G_{DX}$ $(x,a)-&gt;(y’,b’)$ $(y,b)-&gt;(x’,a’)$ 作者为了能实现mask顺序不变性，采用相加的方式。 2.2 Training loss domain loss: GAN loss content loss: cycle-consistency loss and identity mapping loss and context preserving loss LSGAN: 判断图片是原始的还是生成的 L_{LSGAN}=(D_X(x,a)-1)^2+(D_X(G_{YX}(y,b)))^2+(D_Y(y,b)-1)^2+(D_Y(G_{XY}(y,b)))^2cycle-consistency loss: 循环一致性 L_{cyc}=\parallel G_{YX}(G_{XY}(x,a))-(x,a) \parallel \_1+\parallel G_{XY}(G_{YX}(y,b))-(y,b) \parallel _1identity mapping loss: 恒等映射 L_{idt}=\parallel G_{XY}(y,b)-(y,b) \parallel \_1 + \parallel G_{YX}(x,a)-(x,a) \parallel _1context preserving loss: 保留背景 L_{ctx}=\parallel w(a,b')\odot(x-y') \parallel _1 + \parallel w(b,a')\odot(y-x') \parallel _1其中，$w(a,b’), w(b,a’)$表示在原图片和生成图片都是背景的位置的权重是1. Total loss: \begin{align} L_{InstaGAN}&=L_{LSGAN} + \lambda_{cyc} L_{cyc}+ \lambda_{idt} L_{idt}+ \lambda_{ctx} L_{ctx} \\ &=L_{LSGAN}+L_{content} \end{align}2.3 sequential mini-batch translation考虑到在图片上的实例可能很多，而GPU的所需空间随之线性增长，可能不符合现实情况，所以需要考虑在图片上可以转化一小部分实例。 符号说明： 符号 说明 $a=\cup_{i=1}^Ma_i$ divide the set of instance masks a into mini-batch $a_1,a_2,…,a_M$ $(x_m, a_m)-&gt;(y’_m, b’_m) or (x_{m+1}, a_{m+1})$ mini-batch translation $(y’_m, b’_{1:m})=(y’_m, \cup_{i=1}^m b’_i)$ 用于判断真假 在这种情况下，不同的损失函数作用的范围发生改变，第m次时，content loss作用在$(x_m, a_m), (y’_m, b’_m)$，domain loss 作用在$(x,a), (y’_m, b’_{1:m})$，即 L_{InstaGAN-SM}=\sum_{m=1}^M \lbrace L_{LSGAN}((x,a),(y'_m, b'_{1:m}))+L_{content}((x_m, a_m),(y'_m, b'_m)) \rbraceL_{content}=\lambda_{cyc} L_{cyc}+\lambda_{idt} L_{idt}+\lambda_{ctx} L_{ctx} 每m个迭代detach一次，来使用固定大小的GPU。 划分mini-batch的原则：size of instances, 由大到小 3. experimental results3.1 image-to-image translation results 通过上述结果的展示，我可以认为在这方面InstaGAN要比CycleGAN的效果更好，更能得到想要的指定的结果。 第一个结果表明可以通过控制掩码来控制生成的图片。 第二个结果表明可以使用预测的掩码进行转换图片，从而减少获取掩码的成本。 3.2 ablation study Fig.9 主要使研究作者提出的三部分功能的作用，instance mask，损失函数，mini-batch的影响，从效果上看，还是最后一张图片效果更好一些。 Fig.10分别表示在training和inference中使用one-step还是sequential方法，我觉得都差不多，但是对于有限的GPU是个很好的方法。 4. Appendix4.1 architecture details PatchGAN discriminator is composed of 5 convolutional layers, including normalization and non-linearity layers. We used the first 3 convolution layers for feature extractors, and the last 2 convolution layers for classifier. 4.2 traning details $\lambda_{cyc}=10, \lambda_{idt}=10, \lambda_{ctx}=10$ Adam: $\beta_1=0.5, \beta_2=0.999$ batch_size=4 GPU = 4 learning rate: 0.0002 for G, 0.0001 for D, 前m个epoch保持不变，后n个epoch线性衰减为0.不同的数据集的m和n不同 size对于不同的数据集也不同。 4.3 trend of translation results 4.4 其他我觉得这是相当于对于CycleGAN，加上了指向性生成，不再是单独地生成目标域风格的图片，而是对指定区域生成目标域风格的图片。 刚刚想到一个问题，InstaGAN可以生成指定形状的图片，但是对于同一形状的不同物体，比如生成红色的裙子和黑色的裙子这样子的任务，可能不行。 4.5 video translation results 作者使用pix2pix作为分割。 感觉在视频上，裤子换成裙子后，能保持所有帧的裙子都是一样的，说明转换的稳定性很好。 5. code看细节还是需要看代码的实现过程。 5.1 文件目录 通过文件组织，可以发现cycleGAN尽可能地考虑了可扩展性。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859.|-- LICENSE|-- README.md|-- data| |-- __init__.py| |-- aligned_dataset.py| |-- base_data_loader.py| |-- base_dataset.py| |-- image_folder.py| |-- single_dataset.py| |-- unaligned_dataset.py| `-- unaligned_seg_dataset.py|-- datasets| |-- combine_A_and_B.py| |-- download_coco.sh| |-- download_cyclegan_dataset.sh| |-- download_pix2pix_dataset.sh| |-- generate_ccp_dataset.py| |-- generate_coco_dataset.py| |-- generate_mhp_dataset.py| |-- make_dataset_aligned.py| |-- pants2skirt_mhp|-- docs| `-- more_results.md|-- environment.yml|-- models| |-- __init__.py| |-- base_model.py| |-- cycle_gan_model.py| |-- insta_gan_model.py| |-- networks.py| |-- pix2pix_model.py| `-- test_model.py|-- options| |-- __init__.py| |-- base_options.py| |-- test_options.py| `-- train_options.py|-- requirements.txt|-- scripts| |-- conda_deps.sh| |-- download_cyclegan_model.sh| |-- download_pix2pix_model.sh| |-- install_deps.sh| |-- test_before_push.py| |-- test_cyclegan.sh| |-- test_pix2pix.sh| |-- test_single.sh| |-- train_cyclegan.sh| `-- train_pix2pix.sh|-- test.py|-- train.py`-- util |-- __init__.py |-- get_data.py |-- html.py |-- image_pool.py |-- util.py `-- visualizer.py 5.2 seg从下面的代码可以看出，需要读取固定数量的instance的segmentation。 123456789101112# self.max_instances = 20def read_segs(self, seg_path, seed): segs = list() for i in range(self.max_instances): path = seg_path.replace('.png', '_&#123;&#125;.png'.format(i)) if os.path.isfile(path): seg = Image.open(path).convert('L') seg = self.fixed_transform(seg, seed) segs.append(seg) else: segs.append(-torch.ones(segs[0].size())) return torch.cat(segs) 备注: 原始图片transforms之后,0~1变成了-1~1; 分割图片transforms之后-1表示背景,取值-1~1,这也是为什么补充的时候用-1补充的原因. 5.3 generator ResNet generator is composed of downsampling blocks, residual blocks, and upsampling blocks. We used downsampling blocks and residual blocks for encoders, and used upsampling blocks for generators. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879class ResnetSetGenerator(nn.Module): def __init__(self, input_nc=3, output_nc=3, ngf=64, norm_layer=nn.InstanceNorm2d, use_dropout=False, n_blocks=9, padding_type='reflect'): assert (n_blocks &gt;= 0) super(ResnetSetGenerator, self).__init__() self.input_nc = input_nc self.output_nc = output_nc self.ngf = ngf if type(norm_layer) == functools.partial: use_bias = norm_layer.func == nn.InstanceNorm2d else: use_bias = norm_layer == nn.InstanceNorm2d n_downsampling = 2 self.encoder_img = self.get_encoder(input_nc, n_downsampling, ngf, norm_layer, use_dropout, n_blocks, padding_type, use_bias) self.encoder_seg = self.get_encoder(1, n_downsampling, ngf, norm_layer, use_dropout, n_blocks, padding_type, use_bias) self.decoder_img = self.get_decoder(output_nc, n_downsampling, 2 * ngf, norm_layer, use_bias) # 2*ngf self.decoder_seg = self.get_decoder(1, n_downsampling, 3 * ngf, norm_layer, use_bias) # 3*ngf def get_encoder(self, input_nc, n_downsampling, ngf, norm_layer, use_dropout, n_blocks, padding_type, use_bias): model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias), norm_layer(ngf), nn.ReLU(True)] for i in range(n_downsampling): mult = 2 ** i model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias), norm_layer(ngf * mult * 2), nn.ReLU(True)] mult = 2 ** n_downsampling for i in range(n_blocks): model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)] return nn.Sequential(*model) def get_decoder(self, output_nc, n_downsampling, ngf, norm_layer, use_bias): model = [] for i in range(n_downsampling): mult = 2**(n_downsampling - i) model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias), norm_layer(int(ngf * mult / 2)), nn.ReLU(True)] model += [nn.ReflectionPad2d(3)] model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)] model += [nn.Tanh()] return nn.Sequential(*model) def forward(self, inp): # split data img = inp[:, :self.input_nc, :, :] # (B, CX, W, H) segs = inp[:, self.input_nc:, :, :] # (B, CA, W, H) mean = (segs + 1).mean(0).mean(-1).mean(-1) if mean.sum() == 0: mean[0] = 1 # forward at least one segmentation # run encoder enc_img = self.encoder_img(img) enc_segs = list() for i in range(segs.size(1)): # 第i个instance if mean[i] &gt; 0: # skip empty segmentation seg = segs[:, i, :, :].unsqueeze(1) enc_segs.append(self.encoder_seg(seg)) enc_segs = torch.cat(enc_segs) enc_segs_sum = torch.sum(enc_segs, dim=0, keepdim=True) # aggregated set feature # run decoder feat = torch.cat([enc_img, enc_segs_sum], dim=1) out = [self.decoder_img(feat)] idx = 0 for i in range(segs.size(1)): if mean[i] &gt; 0: enc_seg = enc_segs[idx].unsqueeze(0) # (1, ngf, w, h) idx += 1 # move to next index feat = torch.cat([enc_seg, enc_img, enc_segs_sum], dim=1) out += [self.decoder_seg(feat)] else: out += [segs[:, i, :, :].unsqueeze(1)] # skip empty segmentation return torch.cat(out, dim=1) 5.4 Discriminator On the other hand, PatchGAN discriminator is composed of 5 convolutional layers, including normalization and non-linearity layers. We used the first 3 convolution layers for feature extractors, and the last 2 convolution layers for classifier. In addition, we observed that applying Spectral Normalization (SN) (Miyato et al., 2018) for discriminators significantly improve the performance, although we used LSGAN (Mao et al., 2017), while the original motivation of SN was to enforce Lipschitz condition to match with the theory of WGAN (Arjovsky et al., 2017; Gulrajani et al., 2017). [x] SpectralNorm 这个是怎么运行的？ http://www.twistedwg.com/2018/10/13/SNGAN.html 虽然还是没有太搞懂其原理，但大致清楚了，是求矩阵的谱范数，因为难以求解，便用迭代的方式计算u、v。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# Define spectral normalization layer# Code from Christian Cosgrove's repository# https://github.com/christiancosgrove/pytorch-spectral-normalization-gan/blob/master/spectral_normalization.pydef l2normalize(v, eps=1e-12): return v / (v.norm() + eps)class SpectralNorm(nn.Module): def __init__(self, module, name='weight', power_iterations=1): super(SpectralNorm, self).__init__() self.module = module self.name = name self.power_iterations = power_iterations if not self._made_params(): self._make_params() def _update_u_v(self): u = getattr(self.module, self.name + "_u") v = getattr(self.module, self.name + "_v") w = getattr(self.module, self.name + "_bar") height = w.data.shape[0] for _ in range(self.power_iterations): v.data = l2normalize(torch.mv(torch.t(w.view(height, -1).data), u.data)) u.data = l2normalize(torch.mv(w.view(height, -1).data, v.data)) sigma = u.dot(w.view(height, -1).mv(v)) setattr(self.module, self.name, w / sigma.expand_as(w)) def _made_params(self): try: u = getattr(self.module, self.name + "_u") v = getattr(self.module, self.name + "_v") w = getattr(self.module, self.name + "_bar") return True except AttributeError: return False def _make_params(self): w = getattr(self.module, self.name) # shape: (64,3,4,4) height = w.data.shape[0] # int 64 width = w.view(height, -1).data.shape[1] # int 48 u = nn.Parameter(w.data.new(height).normal_(0, 1), requires_grad=False) # shape (64) v = nn.Parameter(w.data.new(width).normal_(0, 1), requires_grad=False) # shape (48) u.data = l2normalize(u.data) v.data = l2normalize(v.data) w_bar = nn.Parameter(w.data) del self.module._parameters[self.name] self.module.register_parameter(self.name + "_u", u) self.module.register_parameter(self.name + "_v", v) self.module.register_parameter(self.name + "_bar", w_bar) def forward(self, *args): self._update_u_v() return self.module.forward(*args)class NLayerSetDiscriminator(nn.Module): def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False): super(NLayerSetDiscriminator, self).__init__() self.input_nc = input_nc if type(norm_layer) == functools.partial: use_bias = norm_layer.func == nn.InstanceNorm2d else: use_bias = norm_layer == nn.InstanceNorm2d kw = 4 padw = 1 self.feature_img = self.get_feature_extractor(input_nc, ndf, n_layers, kw, padw, norm_layer, use_bias) self.feature_seg = self.get_feature_extractor(1, ndf, n_layers, kw, padw, norm_layer, use_bias) self.classifier = self.get_classifier(2 * ndf, n_layers, kw, padw, norm_layer, use_sigmoid) # 2*ndf def get_feature_extractor(self, input_nc, ndf, n_layers, kw, padw, norm_layer, use_bias): model = [ # Use spectral normalization SpectralNorm(nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw)), nn.LeakyReLU(0.2, True) ] nf_mult = 1 nf_mult_prev = 1 for n in range(1, n_layers): nf_mult_prev = nf_mult nf_mult = min(2 ** n, 8) model += [ # Use spectral normalization SpectralNorm(nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias)), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True) ] return nn.Sequential(*model) def get_classifier(self, ndf, n_layers, kw, padw, norm_layer, use_sigmoid): nf_mult_prev = min(2 ** (n_layers-1), 8) nf_mult = min(2 ** n_layers, 8) model = [ # Use spectral normalization SpectralNorm(nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw)), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True) ] # Use spectral normalization model += [SpectralNorm(nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw))] if use_sigmoid: model += [nn.Sigmoid()] return nn.Sequential(*model) def forward(self, inp): # split data img = inp[:, :self.input_nc, :, :] # (B, CX, W, H) segs = inp[:, self.input_nc:, :, :] # (B, CA, W, H) mean = (segs + 1).mean(0).mean(-1).mean(-1) if mean.sum() == 0: mean[0] = 1 # forward at least one segmentation # run feature extractor feat_img = self.feature_img(img) feat_segs = list() for i in range(segs.size(1)): # 第i个instance if mean[i] &gt; 0: # skip empty segmentation seg = segs[:, i, :, :].unsqueeze(1) feat_segs.append(self.feature_seg(seg)) feat_segs_sum = torch.sum(torch.stack(feat_segs), dim=0) # aggregated set feature # run classifier feat = torch.cat([feat_img, feat_segs_sum], dim=1) out = self.classifier(feat) return out 5.5 model 的输入12345678910111213def set_input(self, input): AtoB = self.opt.direction == 'AtoB' self.real_A_img = input['A' if AtoB else 'B'].to(self.device) self.real_B_img = input['B' if AtoB else 'A'].to(self.device) real_A_segs = input['A_segs' if AtoB else 'B_segs'] real_B_segs = input['B_segs' if AtoB else 'A_segs'] self.real_A_segs = self.select_masks(real_A_segs).to(self.device) # shape:(1,4,240,160) self.real_B_segs = self.select_masks(real_B_segs).to(self.device) self.real_A = torch.cat([self.real_A_img, self.real_A_segs], dim=1) # shape:(1,7,240,160) self.real_B = torch.cat([self.real_B_img, self.real_B_segs], dim=1) self.real_A_seg = self.merge_masks(self.real_A_segs) # merged mask self.real_B_seg = self.merge_masks(self.real_B_segs) # merged mask self.image_paths = input['A_paths' if AtoB else 'B_paths'] 前面说过，每次都生成20个mask，不足用-1补充，在输入网络时，只取面积最大的4个mask，然后对这4个进行或者从高到低排序或者随机排序。 1234567891011# ins_max = 4def select_masks_random(self, segs_batch): """Select masks in random order""" ret = list() for segs in segs_batch: mean = (segs + 1).mean(-1).mean(-1) m, i = mean.topk(self.opt.ins_max) num = min(len(mean.nonzero()), self.opt.ins_max) reorder = np.concatenate((np.random.permutation(num), np.arange(num, self.opt.ins_max))) ret.append(segs[i[reorder], :, :]) return torch.stack(ret) 这里的mask的合并没有太看懂，是为了去除(-1,1)之外的数字吗？ 跑了代码,觉得是的,或许是担心有其他干扰因素吧,反正剩下的都是-1~1之间的数字. 1234def merge_masks(self, segs): """Merge masks (B, N, W, H) -&gt; (B, 1, W, H)""" ret = torch.sum((segs + 1)/2, dim=1, keepdim=True) # (B, 1, W, H) return ret.clamp(max=1, min=0) * 2 - 1 其他 [ ] 这一步的意义是什么？？ 理解了，如果图片中没有instance，那么就不用进行下一步的转换了。 12self.forward_A = (self.real_A_seg_sng + 1).sum() &gt; 0 # check if there are remaining instancesself.forward_B = (self.real_B_seg_sng + 1).sum() &gt; 0 # check if there are remaining instances [x] fake_B_mul的意义是什么？ 因为在sequential mini-batch translation中，GAN_loss是全局的，所以每次需要把之前的fake_B_seg_sng保存起来一起计算，因此每次的临时的self.fake_B_mul，而self.fake_B_seg_list保存是mini-batch计算得到的。 12345678910111213if self.forward_A: self.real_A_sng = torch.cat([self.real_A_img_sng, self.real_A_seg_sng], dim=1) self.fake_B_sng = self.netG_A(self.real_A_sng) self.rec_A_sng = self.netG_B(self.fake_B_sng) self.fake_B_img_sng, self.fake_B_seg_sng = self.split(self.fake_B_sng) self.rec_A_img_sng, self.rec_A_seg_sng = self.split(self.rec_A_sng) fake_B_seg_list = self.fake_B_seg_list + [self.fake_B_seg_sng] # not detach for i in range(self.ins_iter - idx - 1): fake_B_seg_list.append(empty) self.fake_B_seg_mul = torch.cat(fake_B_seg_list, dim=1) self.fake_B_mul = torch.cat([self.fake_B_img_sng, self.fake_B_seg_mul], dim=1) [x] 怎么选取的背景 只要在A中且在B中都是背景的则都算是背景，否则只要有instance的区域不为背景。 12345def merge_masks(self, segs): """Merge masks (B, N, W, H) -&gt; (B, 1, W, H)""" # segs: shape(1,4,240,260)， 取值(-1~1) 训练集A中有两个instance，训练集B中有两个instance， ret = torch.sum((segs + 1)/2, dim=1, keepdim=True) # (B, 1, W, H) return ret.clamp(max=1, min=0) * 2 - 1 123456789def merge_masks(self, segs): """Merge masks (B, N, W, H) -&gt; (B, 1, W, H)""" ret = torch.sum((segs + 1)/2, dim=1, keepdim=True) # (B, 1, W, H) return ret.clamp(max=1, min=0) * 2 - 1def get_weight_for_ctx(self, x, y): """Get weight for context preserving loss""" z = self.merge_masks(torch.cat([x, y], dim=1)) return (1 - z) / 2 # [-1,1] -&gt; [1,0] [ ] 这里的empty的作用是什么 1empty = -torch.ones(self.real_A_seg_sng.size()).to(self.device) [ ] pix2pix 是怎么预测mask的，需要提前训练吗，数据集怎么提供？如果可以直接用，那么是否可以直接实现行人重识别的换人？ [x] 论文+代码，共4天]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep-learning-network]]></title>
    <url>%2F2019%2F01%2F03%2Fdeep-learning-network%2F</url>
    <content type="text"><![CDATA[0. 前言最近看了一些代码，发现大家的代码风格互相不一样，且没有一个统一的风格，所以抽象一个大致流程，用于更快地熟悉代码。 1. 神经网络从pytorch的代码中看，可以发现发现这么几个阶段。 1.1 数据dataset这一阶段根据具体需求可以分为离线预处理、在线预处理，主要是对数据预处理，包括不限于图片的处理，名字的处理等等。 代表代码： 1234class xxx(): __getitem__(index):data = data.Dataloader() 1.2 神经网络主要分为： 搭建网络 输入 损失函数 反向传播 其中关键的是搭建网络和损失函数。 有些代码会对神经网络的输入再次处理，有些代码会将输入、损失函数、反向传播重新写个方法或者类，类似trainer。 1.3 可视化可视化方法包括不限于visdom、tensorboard、html、输出重定向。 loss的获取，用一个dict的loss记录。]]></content>
      <categories>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SEIGAN]]></title>
    <url>%2F2018%2F12%2F27%2FSEIGAN%2F</url>
    <content type="text"><![CDATA[论文分享 0. 前言SEIGAN: Towards Compositional Image Generation by Simultaneously Learning to Segment, Enhance, and Inpaint. Pavel Ostyakov, Roman Suvorov, Elizaveta Logacheva1 Oleg Khomenko, Sergey I. Nikolenko 一作Pavel Ostyakov是莫斯科三星AI中心的人员，在 Kaggle Cdiscount’s Image Classification Challenge 比赛中获得第一名，YouTube 视频，github:https://github.com/PavelOstyakov?tab=repositories，他参加的 Kaggle 比赛多一点。 论文代码还没有公布。 1. Introduction 要解决的问题是图像合成，一共三件事：剪切、贴图、修复。即如何把一张图片的物体剪切下来，然后贴到另一张背景图上，并且补全被剪掉的区域。 并且实验证明，结果不仅合成的新图片逼真，而且分割也做得很好。 This process consists of three basic operations: (1) cut, extracting an object from image; (2) paste and enhance, making the pasted object appear natural in the new context; (3) inpaint, restoring the background after cutting out an object 关键词：语义分割、贴图、去目标、补全、修复、GAN、image generation、图像合成 备注：训练过程不需要成对图片。 相似工作：Learning to segment via cut-and-paste 通过作者的论述，思想相同的地方很多。 2. Methods2.1 Problem Setting and General Pipeline字符表示： 字符 含义 $x=$ 图片$x$，由目标$O$和背景$B_x$组成 $y=&lt;\emptyset,B_y&gt;$ 图片$y$，包含背景$B_y$，没有目标 $Y={&lt;\emptyset,B_y&gt;}_{y\in Y}$ 背景数据集 $X={}_{x\in X}$ 不同物体在不同背景的图片 $m$ 目标$O$在图片$x$上的mask $m$ $\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$ 去除目标只剩背景$B_x$并修复后的图片 $\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$ 将目标粘贴到背景$B_y$并增强后的图片 问题定义： 对于任意一组图片$x=$和$y=&lt;\emptyset,B_y&gt;$，利用模型将其变成$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$和$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$，其中$\hat{B}_x\approx B_x$，$\hat{B}_y\approx B_y$和$\hat{O}\approx O$. 步骤分解： 分割(segmentation)：预测图片$x=$的分割掩码$m=Mask(x)$，并且做简单的粘贴$z=m\odot x+(1-m)\odot y$，$\odot$我的理解是对应元素的乘法。 $\odot$ denotes componentwise multiplication; 增强(enhancement)：对于$z$，做进一步图片增强，使其更自然，得到$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$ 修复(inpainting)：对于去除目标后的图片$(1-m)\odot x$，修复成$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$ 其中 swap network 就是 segmentation network 和 enhancement network 的组合。 类似CycleGAN，网络架构应用两次，第一次得到$\hat{x}$和$\hat{y}$，第二次得到$\hat{\hat{x}}$和$\hat{\hat{y}}$ $G_{seg}$：Fig.3.right，输入$x=$，输出$m=Mask(x)$ $G_{inp}$：Fig.3.left，输入$(1-m)\odot x$，输出$\hat{x}=&lt;\emptyset,\hat{B}_x&gt;$ $G_{enh}$：Fig.3.left，输入$z=m\odot x+(1-m)\odot y$和噪声，输出$\hat{y}=&lt;\hat{O},\hat{B}_y&gt;$ $D_{bg}$：背景判别器，判断真背景($y$)和假背景($\hat{x}$、$\hat{\hat{y}}$)，真为1，假为0 $D_{obj}$：目标判别器，判断背景和目标是一体的($x$)和背景和目标是合成的($z$,$\hat{y}$)，一体的为1，合成的为0. 2.2 The inpainting Network $G_{inp}$背景对抗损失函数: l^{GAN}\_{inp}=(1-D_{bg}(\hat{x})),l^{GAN}\_{inp2}=(1-D_{bg}(\hat{\hat{y}}))l^{GAN}_{bg}=\lambda_1 l^{GAN}\_{inp} + \lambda_2 l^{GAN}\_{inp2}背景重构损失函数： l^{texture}_{bg}=|Gram(VGG_1(y))-Gram(VGG_1(\hat{\hat{y}}))|l^{perc}_{bg}=|VGG_2(y)-VGG_2(\hat{\hat{y}})|l^{MAE}\_{bg}=|y-\hat{\hat{y}}|l^{rec}_{bg}=\lambda_3 l^{texture}_{bg}+\lambda_4 l^{perc}_{bg}+\lambda_5l^{MAE}_{bg}其中，$VGG_1(y)$表示VGG19的前五层，$VGG_2(y)$表示VGG19的后五层 $G_{inp}$ 的网络架构: 其实没太看懂这个网络，可能还是需要有代码。 2.3 The Swap NetworkThe swap network = the segmentation network + the enhancement network 目标重构损失函数： l^{perc}_{obj}=|VGG_2(x)-VGG_2(\hat{\hat{x}})|l^{MAE}\_{obj}=|x-\hat{\hat{x}}|l^{rec}_{obj}=\lambda_6 l^{perc}_{obj}+\lambda_7 l^{MAE}_{obj}目标对抗损失函数： l^{GAN}\_{coarse}=(1-D_{obj}(z))^2l^{GAN}\_{enh}=(1-D_{obj}(\hat{y}))^2l^{GAN}_{obj}=\lambda_8 l^{GAN}_{coarse} + \lambda_9 l^{GAN}_{enh}identity loss： l^{id}\_{bg}=|G_{inp}((1-G_{seg}(y))\odot y)-y|l^{id}\_{obj}=|G_{enh}(x)-x|l^{id}=\lambda_{10}l^{id}\_{obj} + \lambda_{11}l^{id}_{bg}2.4 Total Loss Funcion, Remarks, and Network ArchitecturesThe Generator Loss: \begin{split} l=\lambda_1 l^{GAN}\_{inp} + \lambda_2 l^{GAN}\_{inp2} + \lambda_4 l^{perc}_{bg} + \lambda_5l^{MAE}_{bg} + \lambda_6 l^{perc}_{obj} + \\ \lambda_7 l^{MAE}_{obj} + \lambda_8 l^{GAN}_{coarse} + \lambda_9 l^{GAN}\_{enh} + \lambda_{10}l^{id}\_{obj} + \lambda_{11}l^{id}_{bg} \end{split}The Discriminator Loss: l^{disc}\_{bg}=(1-D_{bg}(y))^2+\frac{1}{2}D_{bg}(\hat{x})^2+\frac{1}{2}D_{bg}(\hat{\hat{y}})^2l^{disc}\_{obj}=(1-D_{obj}(x))^2+\frac{1}{2}D_{obj}(\hat{y})^2+\frac{1}{2}D_{obj}(z)^2Remarks： a pool of fake images，类似Cycle GAN也有。 对于不同大小和比例的图片$x,y$，作者使用了增强网络 texture loss $l^{rec}_{bg}$ 比 threshold 作用在 mask m 上的效果更好 In our setup this problem is addressed by a separate enhancement network, so we have fewer limitations when looking for appropriate training data. 3. Experimental evaluation实验性能主要分为两个： 生成图片的主观真实性 生成分割掩码的准确性 生成图片的主观真实性： 从Fig.4.left可以简单地看出，Full的效果还是很明显的。 生成分割掩码的准确性： 实验效果： 4. Future这个方法是否可以用在两个图片的目标对换呢？如果用在跨数据集的行人重识别上，那么是否可以将源数据集的行人粘贴在目标数据集上呢？好像有一个GAN就是类似的. Person Transfer GAN to Bridge Domain Gap for Person Re-Identification]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GPU]]></title>
    <url>%2F2018%2F12%2F26%2FGPU%2F</url>
    <content type="text"><![CDATA[主要记录从安装显卡驱动到cudnn的过程。 0. 前言这篇博客主要记录自己安装显卡驱动到cudnn的过程，只能当作工作记录，不可以作为参考手册。 整个过程主要分为： 显卡驱动 cuda cudnn ananconda pytorch tensorflow matlab 分辨率 cuda、cudnn卸载升级 1. 显卡驱动显卡驱动下载:https://www.nvidia.cn/Download/index.aspx?lang=cn 官方参考手册: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4RD7GVh1d 1.1 删除已经安装的nvidia驱动(对应官方参考手册2.7)如果已经安装过nvidia驱动没有成功或者接下来的过程中发生驱动安装错误的情况，则删除已经安装的nvidia驱动(对应官方参考手册2.7) 1234sudo apt-get --purge remove nvidia-*# orsudo apt-get autoremove --purge nvidia-*sudo reboot 1.2 禁止自带的 nouveau nvidia 驱动(官方参考手册4.3)1.2.1 判断 nouveau nvidia 驱动是否被禁止1lsmod | grep nouveau 有输出表示没有禁止，需要执行1.2.2；没有输出表示禁止成功，可以跳过1.2.2，执行1.3。 1.2.2 禁止 nouveau nvidia 驱动创建文件 1sudo vi /etc/modprobe.d/blacklist-nouveau.conf 在文件中写入 12blacklist nouveauoptions nouveau modeset=0 更新 1sudo update-initramfs -u 判断 1lsmod | grep nouveau 有输出表示没有禁止，需要重新执行1.2.2；没有输出表示禁止成功，重启电脑。 1sudo reboot 1.3 安装 nvidia 驱动根据机器型号下载相应的驱动，名称如：NVIDIA-Linux-x86_64-410.78.run。 重启电脑后或者在桌面环境后，切换到tty1文本模式:ctrl+alt+F1 1.3.1 关闭 x-windows 服务123sudo service lightdm stop# orsudo /etc/init.d/lightdm stop 1.3.2 安装驱动先按照 1.3.2.a 安装驱动，在安装驱动的过程中，如果出现循环登陆的情况，按照 1.3.2.b 安装驱动。 1.3.2.a 正常安装驱动1.3.2.a.1 安装并测试 安装: 123sudo sh ./NVIDIA-Linux-x86_64-410.78.run# orsudo bash ./NVIDIA-Linux-x86_64-410.78.run 在弹出来的选项中，选择默认的即可。好像是32-bit那个选否，core为否。 执行 1.3.3。 1.3.2.b 循环登陆时安装驱动一般而言我们安装的ubuntu 的显示器并没有接到nvidia的显卡上，而是使用了intel的集显。我们安装驱动其实只是想将我们运算的显卡的驱动更新，结果都给搞了，所以产生了冲突。当然，也可能是opengl产生的冲突。 12345sudo apt-get --purge remove nvidia-*# ctrl+alt+F1# 可以考虑适当关机sudo service lightdm stopsudo sh ./NVIDIA-Linux-x86_64-410.78.run –no-x-check –no-nouveau-check –no-opengl-files –no-x-check 安装驱动时关闭X服务 –no-nouveau-check 安装驱动时禁用nouveau –no-opengl-files 只安装驱动文件，不安装OpenGL文件 在弹出来的选项中，选择默认的即可。 执行1.3.3 1.3.3 测试测试: 1sudo nvidia-smi # 有输出表示成功，没有输出表示不成功，重新下载安装。 有输出表示成功，执行 启动 x-windows 服务 及其之后的命令。 没有输出表示失败，重新执行 1.1 、1.3.1 和 1.3.2.a.1.即： 1234sudo apt-get --purge remove nvidia-*# ctrl+alt+F1sudo service lightdm stopsudo sh ./NVIDIA-Linux-x86_64-410.78.run 如果几次都不成功，则验证下载的文件是否完整 12sudo md5sum sudo service lightdm stop# 有输出表示完整，没有输出表示不完整 启动 x-windows 服务: 123sudo service lightdm start# orsudo /etc/init.d/lightdm start 进入图形桌面: ctrl+alt+F7 如果正常登陆，继续下一个命令，如果出现重复登陆，则按照 1.3.2.b 安装驱动。 二次测试驱动(可以不需要): 12sudo nvidia-smi # 有输出表示成功，没有输出表示不成功sudo nvidia-settings # 有输出或者没有输出但是smi有输出都表示成功，反之表示不成功 2. CUDA2.1 安装文件提前安装某些文件: gcc 12345# 验证gccgcc --version# 如果有输出，表示gcc安装成功，进行下一步，如果没有输出，需要安装gcc# 安装gcc命令sudo apt-get install gcc 内核(官方文档2.4) 1sudo apt-get install linux-headers-$(uname -r) 安装CUDA: 1sudo sh cuda_9.0.176_384.81_linux.run 协议一路回车到底，然后会出现几个选项。 EULA: accept NVIDIA Acceerated Graphics Driver: n CUDA Toolkit: y Tookkit: 回车 symbolic link: y Samples: y 安装后，如果出现提示信息，表示缺少几个库，安装 1sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev 重新运行安装命令。 如果有补丁，则依次安装补丁，选择默认即可。 2.2 设置环境变量(官方文档7.1.1)123sudo gedit ~/.bashrc# orsudo gedit /etc/profile 写入 123456789# CUDA-9.0export CUDA_HOME=/usr/local/cuda-9.0export PATH=$PATH:$CUDA_HOME/binexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;# 如果有多个cuda，那么可以写成export CUDA_HOME=/usr/local/cuda-9.0:$CUDA_HOMEexport PATH=/usr/local/cuda-9.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64$:$LD_LIBRARY_PATH 刷新 123source ~/.bashrc# orsource /etc/profile 2.3 验证123cd /usr/local/cuda/samples/1_Utilities/deviceQuerysudo make./deviceQuery # 有输出表示成功，反之表示失败，重新安装cuda 1nvcc -V # 有输出表示成功，反之表示失败，重新安装cuda 3. cuDNN注册下载对应版本的cudnn: 解压: 123456tar -zxvf cudnn-9.0-linux-x86-v7.1.tgz# orcp cudnn-9.0-linux-x86-v7.1.solitairetheme8 cudnn-9.0-linux-x86-v7.1.tgztar -zxvf cudnn-9.0-linux-x86-v7.1.tgz# or# 直接右键解压缩 移动: 1234sudo cp cuda/include/cudnn.h /usr/local/cuda-9.0/include/sudo cp cuda/lib64/libcudnn* /usr/local/cuda-9.0/lib64/sudo chmod a+r /usr/local/cuda-9.0/include/cudnn.hsudo chmod a+r /usr/local/cuda-9.0/lib64/libcudnn* 建立软连接(可选) 1234sudo chmod +r libcudnn.so.7.0.5sudo ln -sf libcudnn.so.7.0.5 libcudnn.so.7 sudo ln -sf libcudnn.so.7 libcudnn.sosudo ldconfig 查看cudnn版本： 1cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 4. ananconda下载: Anaconda3-5.2.0-Linux-x86_64.sh 安装： 1234sh ./Anaconda3-5.2.0-Linux-x86_64.sh# 路径source ~/.bashrc 默认或者同意 清华源: 123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 测试： 12which pythonconda install numpy 5. pytorch安装：在官网找到安装命令即可，如果速度太慢，可以 ctrl+c 之后复制链接到其他电脑上手动下载并安装离线包。 安装离线包： 123conda install xxx# orpip install xxx GPU测试： 1234ipythonimport torch as tt.cuda.is_available()# True 6. Tensorflow安装：在官网找到安装命令即可，如果速度太慢，可以 ctrl+c 之后复制链接到其他电脑上手动下载并安装离线包。 安装离线包： 123conda install xxx# orpip install xxx 测试: 1234ipythonimport tensorflow as tfsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))# 如果输出有GPU，成功，没有的话重新开始装吧，我帮不了你，因为我成功了。 备注：tensorflow与cuda、cudnn的版本需要严格一致。可以通过建立虚拟环境的形式或者tensorflow会自动安装对应版本的cuda、cudnn来解决。参见官网:https://tensorflow.google.cn/install/source 7. matlab我使用的是分成两部分的matlab，参考https://blog.csdn.net/qq_36982160/article/details/78397514 matlab要比前面复杂一些，需要要有耐心。 挂载: 12mkdir /media/matlabsudo mount -t auto -o loop R2016b_glnxa64_dvd1.iso matlab/ 安装: 12cd /media/matlabsudo ./matlab/install 选项：第二个，使用文件安装密钥，不需要Internet连接 选项：是(y) 密钥：crack的readme的第一个数字串 文件路径：默认 下一步 安装 安装进行到80%左右的时候，会弹出一个提示框，说请插入dvd2，这时候我们需要重新开一个终端，把dvd2挂载到matlab文件夹中： 1sudo mount -t auto -o loop R2016b_glnxa64_dvd2.iso /media/matlab/ 点击OK继续 激活: 12cd /usr/local/MATLAB/R2016b/bin./matlab 在弹出的界面中选择: Activate manually without the internet，点击next 文件路径选择：./license_standalone.lic 把Crack文件夹中R2016b/Linux/R2016b/bin/glnxa64四个文件，复制到/usr/local/MATLAB/R2016b/bin/glnxa64目录下 1sudo cp Crack/R2016b/bin/glnxa64/lib* /usr/local/MATLAB/R2016b/bin/glnxa64 至此，Matlab已经安装并激活。 环境变量： 12345sudo gedit ~./bashrc# 输入# matlabexport PATH=/usr/local/MATLAB/R2016b/bin:$PATH# 以后直接在命令框中输入matlab即可启动 创建快捷方式： 1234567891011sudo gedit /usr/share/applications/Matlab.desktop# 写入[Desktop Entry]Type=ApplicationName=MatlabGenericName=Matlab 2016bComment=Matlab:The Language of Technical ComputingExec=sh /usr/local/MATLAB/R2016b/bin/matlab -desktopIcon=/usr/local/MATLAB/Matlab.pngTerminal=falseCategories=Development; Exec代表应用程序的位置 Icon代表应用程序图标的位置 Terminal为false表示启动时不启动命令行窗口，为true表示启动命令行窗口 此时会在/usr/share/applications中看到matlab（和文件Name对应）的快捷方式 8. 分辨率查看分辨率： 12xrandr# VGA1 自定义分辨率： 12cvt 1920 1080# "1920x1080_60.00" 173.00 1920 2048 2248 2576 1080 1083 1088 1120 -hsync +vsync 设置分辨率： 1234567sudo gedit /etc/profile# 末尾加入xrandr --newmode "1920x1080_60.00" 173.00 1920 2048 2248 2576 1080 1083 1088 1120 -hsync +vsyncxrandr --addmode VGA1 "1920x1080_60.00"# 刷新source /etc/profile 以后在分辨率选项里就有1920x1080，同时开机显示率也自动变成1920x1080. 9. cuda、cudnn的卸载1234567# 卸载cudasudo /usr/local/cuda-9.0/bin/uninstall_cuda_9.0.pl# 删除cudasudo rm -rf /usr/local/cuda-9.0# 卸载cudnnsudo rm -rf /usr/local/cuda/include/cudnn.hsudo rm -rf /usr/local/cuda/lib64/libcudnn 10. teamviewer for Linux 的安装与卸载主要针对teamviewer出现商业用途时使用。 参考: https://www.cnblogs.com/fxust/p/8040706.html 1234567891011121314151617181920# 1. 卸载dpkg -l |grep xxx#xxx就是安装的软件名sudo dpkg --purge xxxxrm -rf ~/.local/share/xxxx# 然后把所有teamviewer的文件，尤其是log文件# 2. 安装sudo dpkg -i teamviewer_13.0.5693_amd64.deb# 2.1 如果出现错误#添加架构依赖sudo dpkg --add-architecture i386#更新软件库sudo apt-get update#执行强制安装sudo apt-get -f install#再安装sudo dpkg -i teamviewer_13.0.5693_amd64.deb# 配置文件可以修改，可以不改]]></content>
      <categories>
        <category>GPU</category>
      </categories>
      <tags>
        <tag>driver</tag>
        <tag>cuda</tag>
        <tag>cudnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[starGAN]]></title>
    <url>%2F2018%2F12%2F19%2FstarGAN%2F</url>
    <content type="text"><![CDATA[0. 前言因为在person-reid论文HHL中涉及到了starGAN，所以做一个StarGAN的阅读记录，并比较与CycleGAN的区别。 StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo code-pytorch-official: https://github.com/yunjey/stargancode-tensorflow: &lt;https://github.com/taki0112/StarGAN-Tensorflow &gt; 1. Introduction解决多域之间图像转换一对多的问题，本文主要针对人脸进行改变。 关键词：multi-domain image-image translation 效果：转换效果如图所示 网络模型：CycleGAN和StarGAN模型对比 starGAN有一个生成器G，两个判别器。 备注： multi-domain：单数据集的不同属性作为了一个domain multi-datasets：不同数据集的不同属性 starGAN 分为multi-domain和multi-dataset两种。 2. Star Generative Adversarial Networks2.1 Multi-Domain Image-to-Image TranslationstarGAN: starGAN的训练模型 To achieve this, we train G to translate an input image x into an output image y conditioned on the target domain label c, G(x; c) -&gt; y. We randomly generate the target domain label c so that G learns to flexibly translate the input image. We also introduce an auxiliary classifier [22] that allows a single discriminator to control multiple domains. That is, our discriminator produces probability distributions over both sources and domain labels, D:x-&gt;{$D_{src}$(x); $D_{cls}$(x)} 符号说明：符号表 符号 含义 x input image c target domain label c’ source domain label y generate image Loss: training loss Adversarial Loss:(CycleGAN也有)对抗损失 L_{adv}=E_x[log D_{src}(x)]+E_{x,c}[log (1-D_{src}(G(x,c)))] \tag{1}Domain Classification Loss:(特有)分类损失 That is, we decompose the objective into two terms: a domain classification loss of real images used to optimize D, and a domain classification loss of fake images used to optimize G. 优化 D: L_{cls}^r=E_{x,c'}[-log D_{cls}(c'|x)] \tag{2} By minimizing this objective, D learns to classify a real image x to its corresponding original domain c’. 优化 G: L_{cls}^f=E_{x,c}[-log D_{cls}(c|G(x,c))] \tag{3} G tries to minimize this objective to generate images that can be classified as the target domain c. Reconstruction Loss: (共有)重构损失 L_{rec}=E_{x,c,c'}[\parallel x-G(G(x,c),c') \parallel _1] \tag{4}Full Objective: 共有 L_D=-L_{adv}+\lambda_{cls} L_{cls}^rL_G=L_{adv}+\lambda_{cls} L_{cls}^f+\lambda_{rec}L_{rec} \tag{5}\lambda_{cls}=1, \lambda_{rec}=102.2. Training with Multiple Datasets StarGAN也适用于多数据集间的转换，上述过程中的重构损失要求数据集之间的标签一致(？？？)。针对这个问题，作者引入Mask Vector. Mask Vector: 修改真值。 \tilde{c} = [c_1, ..., c_n, m] $c_i$ represents a vector for the labels of the i-th dataset. The vector of the known label $c_i$ can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n−1 unknown labels we simply assign zero values. 这样的话，所有的c都需要变成$\tilde{c}$ 3. ImplementationImproved GAN training: 为了稳定训练过程，替代方程1. L_{adv}=E_x[log D_{src}(x)]-E_{x,c}[log (D_{src}(G(x,c)))]-\lambda_{gp}E_{\hat{x}}[(\parallel \nabla_{\hat{x}} D_{src}(\hat{x}) \parallel_2-1)^2] \tag{6}\lambda_{gp}=10Network Architecture: 类似CycleGAN。 G: Leaky ReLU: 0.01 D: PatchGAN 现在网络架构可以看到的是作者使用的不是70x70的patchGAN，通过patchGAN的论文，也没有看到这种结构。 4. Experiments4.1 Baseline Models 通过结果可以看出，在Gender这个属性，ICGAN的转换效果要更好一些，但是损失了ID信息。 4.2 Training Adam: $\beta_1=0.5, \beta_2=0.999$ Updates: one generator update after five discriminator updates lr: For CelebA, 0.0001 for the first 100000 epochs, and linearly decay the lr to 0 over the next 100000 epochs. For the RaFD, 0.0001 for the first 100000 epochs, and linearly decay the lr to 0 over the next 100000 epochs.作者在论文写的是10和100，但是代码显示的是100000 batch: 16 input: For CelebA, crop: 178, resize: 128; For RaFD, 4.3 Results作者通过人脸的转换实验，不仅说明了StarGAN在单数据集的不同domian中效果好，而且在多数据集的不同domian中效果也好。 5. 代码在这里分析pytorch的代码，并对其中关键的代码进行解读。 如果不说明，则假设讨论单数据集的多域。 5.1 Model: G and DGenerator:生成器Generator，结构与前面提到的网络架构一致，这里需要注意两点： 当训练集是单数据集的多domain时，label需要扩充成图片大小，一起输入网络(这里有个疑问：网络真得能知道后面的通道是label吗) 当训练集是多数据集的多domain时，label的维度是c+c2+2，因为有mask，同样需要广播成图片大小，一起输入网络 Discriminator:判别器Discriminator，有个疑问是关于是感受野和计算损失的。 下面会提及到计算损失的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384class ResidualBlock(nn.Module): """Residual Block with instance normalization.""" def __init__(self, dim_in, dim_out): super(ResidualBlock, self).__init__() self.main = nn.Sequential( nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False), nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True), nn.ReLU(inplace=True), nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False), nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True)) def forward(self, x): return x + self.main(x)class Generator(nn.Module): """Generator network.""" def __init__(self, conv_dim=64, c_dim=5, repeat_num=6): super(Generator, self).__init__() layers = [] layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False)) layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True)) layers.append(nn.ReLU(inplace=True)) # Down-sampling layers. curr_dim = conv_dim for i in range(2): layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False)) layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True)) layers.append(nn.ReLU(inplace=True)) curr_dim = curr_dim * 2 # Bottleneck layers. for i in range(repeat_num): layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim)) # Up-sampling layers. for i in range(2): layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False)) layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True)) layers.append(nn.ReLU(inplace=True)) curr_dim = curr_dim // 2 layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False)) layers.append(nn.Tanh()) self.main = nn.Sequential(*layers) def forward(self, x, c): # Replicate spatially and concatenate domain information. # c: N*c_dim # 生成器直接将目标域c在通道维度进行拼接 c = c.view(c.size(0), c.size(1), 1, 1) c = c.repeat(1, 1, x.size(2), x.size(3)) x = torch.cat([x, c], dim=1) return self.main(x)class Discriminator(nn.Module): """Discriminator network with PatchGAN.""" def __init__(self, image_size=128, conv_dim=64, c_dim=5, repeat_num=6): super(Discriminator, self).__init__() layers = [] layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1)) layers.append(nn.LeakyReLU(0.01)) curr_dim = conv_dim for i in range(1, repeat_num): layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1)) layers.append(nn.LeakyReLU(0.01)) curr_dim = curr_dim * 2 kernel_size = int(image_size / np.power(2, repeat_num)) self.main = nn.Sequential(*layers) self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False) self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False) def forward(self, x): h = self.main(x) # True or False out_src = self.conv1(h) # classes onehot out_cls = self.conv2(h) return out_src, out_cls.view(out_cls.size(0), out_cls.size(1)) 5.2 input对于任一张图片，其target label是随机取其他图片的label，而没有刻意去指定 1234567891011# label_org和label_trg可以认为是单个图片的真实label，形式可以是[0,1,1,0]或者4，根据不同的数据集形式进行处理，前者是多分类label，后者是单分类label，用于计算损失# c_org，c_trg是与图片一起输入网络的&#123;0,1&#125;向量，形式是[0,1,1,0]或者是[0,0,0,1]的形式，用于网络的输入x_real, label_org = next(data_iter)rand_idx = torch.randperm(label_org.size(0))label_trg = label_org[rand_idx]if self.dataset == 'CelebA': c_org = label_org.clone() c_trg = label_trg.clone()elif self.dataset == 'RaFD': c_org = self.label2onehot(label_org, self.c_dim) c_trg = self.label2onehot(label_trg, self.c_dim) 5.3 train G and D5.3.1 train D这里需要对上述提到的损失函数做进一步处理。 判断图片真假损失:由方程6得： L_{adv}=-D_{src}(x)+D_{src}(G(x,c))+\lambda_{gp}(\parallel \nabla_{\hat{x}} D_{src}(\hat{x}) \parallel_2-1)^2 \tag{7}\lambda_{gp}=10判断原图片属性正确:由方程2得： L_{cls}^r=D_{cls}(c'|x) \tag{8}总损失： L_D=-L_{adv}+\lambda_{cls} L_{cls}^r\lambda_{cls}=1备注： 在计算真假损失的时候，是直接求输出的均值，这一点不是很理解。 方程7的第三项的计算见gradient_penalty，对整个图片的梯度求和。 方程8的的求解见classification_loss，就是一个简单的分类损失。 不理解方程2为什么要加个符号？方程7也是符号正好相反？ 12345678910111213141516171819202122232425# =================================================================================== ## 2. Train the discriminator ## =================================================================================== ## Compute loss with real images.out_src, out_cls = self.D(x_real) # out_src：N,1,2,2; out_cls: N,c_dimd_loss_real = - torch.mean(out_src) # 方程7的第一项d_loss_cls = self.classification_loss(out_cls, label_org, self.dataset) # 方程8# Compute loss with fake images.x_fake = self.G(x_real, c_trg)out_src, out_cls = self.D(x_fake.detach())d_loss_fake = torch.mean(out_src) # 方程7的第二项# Compute loss for gradient penalty.alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device)x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)out_src, _ = self.D(x_hat)d_loss_gp = self.gradient_penalty(out_src, x_hat) # 方程7的第三项# Backward and optimize.d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp # 总损失self.reset_grad()d_loss.backward()self.d_optimizer.step() 12345678910111213def gradient_penalty(self, y, x): """Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.""" weight = torch.ones(y.size()).to(self.device) dydx = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=weight, retain_graph=True, create_graph=True, only_inputs=True)[0] dydx = dydx.view(dydx.size(0), -1) dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1)) return torch.mean((dydx_l2norm-1)**2) 123456def classification_loss(self, logit, target, dataset='CelebA'): """Compute binary or softmax cross entropy loss.""" if dataset == 'CelebA': return F.binary_cross_entropy_with_logits(logit, target, size_average=False) / logit.size(0) elif dataset == 'RaFD': return F.cross_entropy(logit, target) 5.3.2 train G与以往训练几个G之后才训练D不同，这里是训练几个D之后才训练G。 同样对上述提到的损失做进一步处理。 生成图片为真：由方程6得，与方程7正好相反： L_{adv}=-D_{src}(G(x,c)) \tag{9}生成图片的属性正确：由方程3得： L_{cls}^f=D_{cls}(c|G(x,c)) \tag{10}Reconstruction Loss: 重构损失 L_{rec}=\parallel x-G(G(x,c),c') \parallel _1 \tag{4}总损失 L_G=L_{adv}+\lambda_{cls} L_{cls}^f+\lambda_{rec}L_{rec} \tag{5}\lambda_{cls}=1, \lambda_{rec}=101234567891011121314151617181920# =================================================================================== ## 3. Train the generator ## =================================================================================== #if (i+1) % self.n_critic == 0: # Original-to-target domain. x_fake = self.G(x_real, c_trg) out_src, out_cls = self.D(x_fake) g_loss_fake = - torch.mean(out_src) # 方程9 g_loss_cls = self.classification_loss(out_cls, label_trg, self.dataset) # 方程10 # Target-to-original domain. x_reconst = self.G(x_fake, c_org) g_loss_rec = torch.mean(torch.abs(x_real - x_reconst)) # Backward and optimize. g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls self.reset_grad() g_loss.backward() self.g_optimizer.step() 5.4 valCelebA数据集：这里制作target domain label的方法分为头发属性(互相排斥)和其他属性(不排斥):对于选中的头发属性’Black_Hair’, ‘Blond_Hair’, ‘Brown_Hair’,则把5张图片的’Black_Hair’全部设为1,’Blond_Hair’, ‘Brown_Hair’设为0,作为第一个target domain label, 再把5张图片的’Blond_Hair’全部设为1,’Black_Hair’, ‘Brown_Hair’设为0,作为第二个target domain label, 再把5张图片的’Brown_Hair’全部设为1,’Black_Hair’,’Blond_Hair’ 设为0,作为第三个target domain label,对于其他属性,则直接取相反数做为第三个target domain label和第四个target domain label. RaFD数据集:属于排斥属性，也和头发类似，对某一列全部设为1，其余设为0. 1234567891011121314151617181920212223242526272829303132333435c_orgOut[24]: tensor([[ 0., 0., 0., 1., 0.], [ 0., 0., 0., 1., 1.], [ 0., 0., 0., 1., 0.], [ 1., 0., 0., 0., 1.], [ 1., 0., 0., 0., 1.],c_trg_listOut[25]: [tensor([[ 1., 0., 0., 1., 0.], [ 1., 0., 0., 1., 1.], [ 1., 0., 0., 1., 0.], [ 1., 0., 0., 0., 1.], [ 1., 0., 0., 0., 1.]], device='cuda:0'), tensor([[ 0., 1., 0., 1., 0.], [ 0., 1., 0., 1., 1.], [ 0., 1., 0., 1., 0.], [ 0., 1., 0., 0., 1.], [ 0., 1., 0., 0., 1.]], device='cuda:0'), tensor([[ 0., 0., 1., 1., 0.], [ 0., 0., 1., 1., 1.], [ 0., 0., 1., 1., 0.], [ 0., 0., 1., 0., 1.], [ 0., 0., 1., 0., 1.]], device='cuda:0'), tensor([[ 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 1.], [ 0., 0., 0., 0., 0.], [ 1., 0., 0., 1., 1.], [ 1., 0., 0., 1., 1.]], device='cuda:0'), tensor([[ 0., 0., 0., 1., 1.], [ 0., 0., 0., 1., 0.], [ 0., 0., 0., 1., 1.], [ 1., 0., 0., 0., 0.], [ 1., 0., 0., 0., 0.]], device='cuda:0')] 12345678910111213141516171819202122232425262728293031def create_labels(self, c_org, c_dim=5, dataset='CelebA', selected_attrs=None): """Generate target domain labels for debugging and testing.""" # Get hair color indices. if dataset == 'CelebA': hair_color_indices = [] for i, attr_name in enumerate(selected_attrs): if attr_name in ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']: hair_color_indices.append(i) c_trg_list = [] for i in range(c_dim): if dataset == 'CelebA': c_trg = c_org.clone() if i in hair_color_indices: # Set one hair color to 1 and the rest to 0. c_trg[:, i] = 1 for j in hair_color_indices: if j != i: c_trg[:, j] = 0 else: c_trg[:, i] = (c_trg[:, i] == 0) # Reverse attribute value. elif dataset == 'RaFD': c_trg = self.label2onehot(torch.ones(c_org.size(0))*i, c_dim) c_trg_list.append(c_trg.to(self.device)) return c_trg_list# Fetch fixed inputs for debugging.data_iter = iter(data_loader)x_fixed, c_org = next(data_iter)x_fixed = x_fixed.to(self.device)c_fixed_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs) 5.5 多数据集在多数据集的情况下，损失函数大体不变，略微不同。 5.5.1 input多数据集顺序输入 1for dataset in ['CelebA', 'RaFD']: 123456789101112131415161718celeba_iter = iter(self.celeba_loader)x_real, label_org = next(celeba_iter)rand_idx = torch.randperm(label_org.size(0))label_trg = label_org[rand_idx]if dataset == 'CelebA': c_org = label_org.clone() c_trg = label_trg.clone() zero = torch.zeros(x_real.size(0), self.c2_dim) mask = self.label2onehot(torch.zeros(x_real.size(0)), 2) c_org = torch.cat([c_org, zero, mask], dim=1) c_trg = torch.cat([c_trg, zero, mask], dim=1)elif dataset == 'RaFD': c_org = self.label2onehot(label_org, self.c2_dim) c_trg = self.label2onehot(label_trg, self.c2_dim) zero = torch.zeros(x_real.size(0), self.c_dim) mask = self.label2onehot(torch.ones(x_real.size(0)), 2) c_org = torch.cat([zero, c_org, mask], dim=1) c_trg = torch.cat([zero, c_trg, mask], dim=1) 5.5.2 train D and G123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# =================================================================================== ## 2. Train the discriminator ## =================================================================================== ## Compute loss with real images.out_src, out_cls = self.D(x_real) out_cls = out_cls[:, :self.c_dim] if dataset == 'CelebA' else out_cls[:, self.c_dim:] # 属性损失只考虑一半d_loss_real = - torch.mean(out_src) # 方程7的第一项d_loss_cls = self.classification_loss(out_cls, label_org, dataset) # 方程8# Compute loss with fake images.x_fake = self.G(x_real, c_trg)out_src, _ = self.D(x_fake.detach())d_loss_fake = torch.mean(out_src) # 方程7的第二项# Compute loss for gradient penalty.alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device)x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)out_src, _ = self.D(x_hat)d_loss_gp = self.gradient_penalty(out_src, x_hat) # 方程7的第三项# Backward and optimize.d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gpself.reset_grad()d_loss.backward()self.d_optimizer.step()# Logging.loss = &#123;&#125;loss['D/loss_real'] = d_loss_real.item()loss['D/loss_fake'] = d_loss_fake.item()loss['D/loss_cls'] = d_loss_cls.item()loss['D/loss_gp'] = d_loss_gp.item()# =================================================================================== ## 3. Train the generator ## =================================================================================== #if (i+1) % self.n_critic == 0: # Original-to-target domain. x_fake = self.G(x_real, c_trg) out_src, out_cls = self.D(x_fake) out_cls = out_cls[:, :self.c_dim] if dataset == 'CelebA' else out_cls[:, self.c_dim:] # 生成图片的属性只考虑一半 g_loss_fake = - torch.mean(out_src) g_loss_cls = self.classification_loss(out_cls, label_trg, dataset) # Target-to-original domain. x_reconst = self.G(x_fake, c_org) g_loss_rec = torch.mean(torch.abs(x_real - x_reconst)) # Backward and optimize. g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls self.reset_grad() g_loss.backward() self.g_optimizer.step() # Logging. loss['G/loss_fake'] = g_loss_fake.item() loss['G/loss_rec'] = g_loss_rec.item() loss['G/loss_cls'] = g_loss_cls.item() 5.5.3 val and test对当前图片生成两个数据集下不同属性的图片，也就是说，具有跨数据集生成图片的能力。 val: 123456789if (i+1) % self.sample_step == 0: with torch.no_grad(): x_fake_list = [x_fixed] for c_fixed in c_celeba_list: c_trg = torch.cat([c_fixed, zero_rafd, mask_celeba], dim=1) x_fake_list.append(self.G(x_fixed, c_trg)) for c_fixed in c_rafd_list: c_trg = torch.cat([zero_celeba, c_fixed, mask_rafd], dim=1) x_fake_list.append(self.G(x_fixed, c_trg)) test: 12345678910111213141516171819202122232425for i, (x_real, c_org) in enumerate(self.celeba_loader): # Prepare input images and target domain labels. x_real = x_real.to(self.device) c_celeba_list = self.create_labels(c_org, self.c_dim, 'CelebA', self.selected_attrs) c_rafd_list = self.create_labels(c_org, self.c2_dim, 'RaFD') zero_celeba = torch.zeros(x_real.size(0), self.c_dim).to(self.device) # Zero vector for CelebA. zero_rafd = torch.zeros(x_real.size(0), self.c2_dim).to(self.device) # Zero vector for RaFD. mask_celeba = self.label2onehot(torch.zeros(x_real.size(0)), 2).to(self.device) # Mask vector: [1, 0]. mask_rafd = self.label2onehot(torch.ones(x_real.size(0)), 2).to(self.device) # Mask vector: [0, 1]. # Translate images. x_fake_list = [x_real] for c_celeba in c_celeba_list: c_trg = torch.cat([c_celeba, zero_rafd, mask_celeba], dim=1) x_fake_list.append(self.G(x_real, c_trg)) for c_rafd in c_rafd_list: c_trg = torch.cat([zero_celeba, c_rafd, mask_rafd], dim=1) x_fake_list.append(self.G(x_real, c_trg)) # Save the translated images. x_concat = torch.cat(x_fake_list, dim=3) result_path = os.path.join(self.result_dir, '&#123;&#125;-images.jpg'.format(i+1)) save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0) print('Saved real and fake images into &#123;&#125;...'.format(result_path)) 6. 其他通过代码,我们可以猜出,对于starGAN,每一个domain都是一个二值属性,这些属性可以是互相排斥的,例如头发颜色,可以是不互相排斥的,并且这里和CycleGAN还是有一些区别的,CycleGAN的domain是数据集,source domain 和 target domain是风马牛不相及的,source domain和target domain有自己的风格,例如map数据集,是没有真值的,有的只是深度网络提取出的特征和70*70patchGAN.但是starGAN中,生成的图片和原始图片是一个数据集的,并且这两张图片不是要求风格一样,感觉这能应用到person-reid中也是神奇. 在图片真假的分类损失中，之前的GAN都是使用True和False来表示，这次换了一个新公式直接mean，还有点难理解。]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>starGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CASN]]></title>
    <url>%2F2018%2F12%2F17%2FCASN%2F</url>
    <content type="text"><![CDATA[CASN: Re-Identification with Consistent Attentive Siamese Networks Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J. Radke 1. Introduction这篇论文； 采用了Grad-CAM作为attention机制 attention consistency: 采用Siamese-Net来使同一个人的attention位置是一样的 2. The Consistent Attentive Siamese Network整体网络架构如图所示： 整体网络架构以IDE为基准网络，分为两部分: Identification Module Siamese Module Identification 和 Siamese 的特征提取网络共享，不同的只是fc层 2.1 The Identification Module通过Grad-CAM的学习，已经知道了Grad-CAM的作用。 Identification loss: L_{ide}=-\sum_{n=1}^N log \frac{exp(y_{c_n})}{\sum_j exp(y_j)}Identification loss 更偏向于不同行人之间的判别信息。 Identification attention loss: L_{ia}=\overline{y_{c_n}}其中，给定一张图片$I_n$和类别$c_n$，Grad-CAM得到attention map $M_n$，做归一化操作，令$\Sigma(M_n)=sigmoid(\alpha(M_n-\beta))$，从而得到去掉attention区域的新图片$\overline{I_n}=I_n*(1-\Sigma(M_n))$，$\overline{y_{c_n}}$是$\overline{I_n}$的预测值。 Identification attention loss 更偏向于行人的全部信息。 我的理解是$\overline{I_n}$中尽可能包含少的ID信息，所以预测出是$c_n$的概率更小，得到的attention区域尽可能地包括全部信息。 两种loss的效果对比图 2.2 The Siamese ModuleSiamese Module 结构图 Siamese loss L_{bce}=-\sum_p log(\frac{exp(z_{c_p})}{exp(z_0)+exp(z_1)})Siamese attention loss \alpha_i= \begin{cases} 1, \mbox{if} f_i^->0 \\ 0, otherwise \end{cases}s_1=, s_2=(dot products)\alpha_1^k=GAP(\frac{\partial s_1}{\partial A_1}), \alpha_2^k=GAP(\frac{\partial s_2}{\partial A_2}) \tag{5}M_1=ReLU(\sum_k \alpha_1^k A_1^k),M_2=ReLU(\sum_k \alpha_2^k A_2^k)L_{sa}=L_{bce}+\alpha \parallel M_{m1}^{resize}-M_{m2}^{resize} \parallel _2\alpha=0.2其中，$M_{m1}$是$M_1$中超过阈值t的元素，$M_{m1}^{resize}$是$M_{m1}$resize成相同大小的元素，主要是为了解决对齐问题。 通过与作者的沟通，作者认为$s_1$表示了$f_1$中对 BCE prediction 有用的信息。但是我没有见过这么表示对预测有用的信息的方法，之前只见过通过类别进行反向传播的(Grad-CAM)，但是作者这么坚持，说明应该是有效的。 Sorry I didn’t get your first question. By finding neurons in fi which are larger than zero, we find features in fi which have positive influence on BCE prediction. s1 is not I1’s class. We here call s1 the importance score, which collect scores for every neuron which contributes to BCE prediction. 2.3 Overall Design of the CASNCASN的整体架构 The overall loss L=L_{ide}+\lambda_1 L_{ia}+\lambda_2 L_{sa}3. Experiments and ResultsImplementation Details input: 288x144 SGD: momentum=0.9 lr=0.03 epoch=40 lr decay=0.1 after 30 baseline: IDE and PCB( input: 384x128) batch=16 test: we send the query and gallery as pair inputs to obtain attention maps $\parallel M_{m1}^{resize}-M_{m2}^{resize} \parallel _2$ Results 通过 Ablation Study , 对比CASN(IDE)、PCB，可以看出IA或者SA的作用和简单地分成6块达到的效果是类似的，这是不是说明了这种attention机制没有很大的作用，或者说分成6块就已经是一种很好的attention机制。 另外，+IA、+SA、CASN的对比，感觉IA或者SA一种机制就已经足够了，两者达到的效果是一样的，只使用一种就可以了。 4. Others这篇论文不懂的地方： $L_{ia}$为什么可以直接这么写，不需要经过softmax之类的，或者不应该是每类的概率差不多么， $L_{ia}$还是经过相同的网络得到的吗？反向求导要怎么写？ 根据Grad-CAM的以类别反向求导，方程5给我的感觉更像是$f$的特征和作为输入图片的分类预测值，合理性站不住脚。 在测试时，需要每次输入一对图片，是不是太慢了。 如果实验结果可以复现的话，那么IA我觉得还是很有用的，解释性也强。 参考： 对于 Identification attention loss 的流程，需要参考Grad-CAM和GAIN, GAIN也可以在Grad-CAM中找到详解。]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>person-reid</tag>
        <tag>Grad-CAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Grad-CAM]]></title>
    <url>%2F2018%2F12%2F14%2FGrad-CAM%2F</url>
    <content type="text"><![CDATA[Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization Gradient-weighted Class Activation Mapping Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam code-torch: https://github.com/ramprs/grad-cam/ code-pytorch: https://github.com/jacobgil/pytorch-grad-cam code-keras: https://github.com/jacobgil/keras-grad-cam 参考链接： https://www.jianshu.com/p/b2f7efe10ad8 https://www.jianshu.com/p/1d7b5c4ecb93 http://spytensor.com/index.php/archives/20/ https://www.jianshu.com/p/e4fa1348e5bc 1. 声明最近在看到一篇person-reid的文章Re-Identification with Consistent Attentive Siamese Networks，其中涉及到了Grad-CAM，所以简单学习一下Grad-CAM，但不作为重点。 2018-12-18 在使用的过程中，发现自己写的这篇博客不太容易让自己一目了然，所以根据链接来进行更新一次。 2018-12-20 在重新看论文person-reid的过程中，发现其中涉及到的网络架构师GAIN，所以补充GAIN的说明及其代码。此时，这篇的重点变成了Grad-CAM和GAIN。 2. 前言对于深度模型的可解释性和可视化，现在已经研究出了一些方法，包括不限于Deconvolution, Guided-Backpropagation, CAM, Grad-CAM. 其中 Deconvolution 和 Guided-Backpropagation 得到更偏向于细粒度图， CAM 和 Grad-CAM 得到更偏向于类区分的热力图。 各种可视化方法及其效果图参见：https://github.com/utkuozbulak/pytorch-cnn-visualizations 参考链接: https://blog.csdn.net/geek_wh2016/article/details/81060315 2.1 DeconvolutionDeconvolution: Visualizing and Understanding Convolutional Networks code: https://github.com/kvfrans/feature-visualization 综述: 这篇paper是CNN可视化的开山之作(由 Lecun 得意门生 Matthew Zeiler 发表于2013年)，主要解决了两个问题: why CNN perform so well? how CNN might be improved? 实现: 对于CNN，可视化就是整个过程的逆过程，即Unpooling+ReLU+Deconv. Unpooling: 记录max-pool的位置，即Switches表格，unpooling时，最大值放回该位置，其他位置放0. ReLU: 继续使用ReLU. Deconv: 使用相同卷积核的转置作为新的卷积核，对特征进行卷积. 参考链接： http://kvfrans.com/visualizing-features-from-a-convolutional-neural-network/ https://blog.csdn.net/Julialove102123/article/details/78292807 https://blog.csdn.net/gm_margin/article/details/79335140 2.2 Guided-BackpropagationGuided-Backpropagation: Striving for Simplicity: The All Convolutional Net 反向传播、反卷积和导向反向传播都是反向传播，区别在于经过 ReLU 层时对梯度的不同处理策略。在这篇论文中有详细的解释。 计算公式如下： 文中提出使用 stride convolution 代替 pooling，研究这种结构的有效性。 效果显示如下： 可以看出 Guided-Backpropagation 主要提取对分类有效果的特征，但是与是哪类没有关系。 2.3 CAMCAM: Learning Deep Features for Discriminative Localization 综述：论文重新审视了global average pooling (GAP) 的有效性，并详细阐述了GAP如何使得CNN有优异的目标定位能力。介绍：摒弃FC，使用GAP。实现： 2.4 Grad-CAMGrad-CAM 是CAM的改进版， 与 CAM 的不同点在于前者的特征加权系数是反向传播得到的，后者的特征加权系数是分类器的权重。 Grad-CAM 可以加载到任意网络架构上，而不需要修改网络架构，而CAM必须使用GAP。 下面会详细介绍。 2.5 GAINGAIN: Tell Me Where to Look: Guided Attention Inference Network code: https://github.com/alokwhitewolf/Guided-Attention-Inference-Network GAIN 是 Grad-CAM 的改进版，Grad-CAM只能可视化解释现有的网络结构的结果，却不能指导网络架构，GAIN可以指导网络修正错误，关注更正确的位置。 问题：在船识别的过程中，网络的关注点是水面而不是船。 实现：通过最小化遮挡图像的物体来训练。 整体网络架构中，只有一个网络，两个处理流都是共享同一个网络。 公式：损失函数 w_{l,k}^c=GAP(\frac{\partial s^c}{\partial f_{l,k}})A^c=ReLU(conv(f_l, w^c))T(A^c)=\frac{1}{1+exp(-\omega(A^c-\sigma))}I^{*c}=I-(T(A^c)\odot I)L_{am}=\frac{1}{n}\sum_cs^c(I^{*c})L_{self}=L_{cl}+\alpha L_{am}\alpha=1扩展：如果有额外的监督真值，比如分割，那么可以进行扩充网络 L_e=\frac{1}{n}\sum_c (A^c-H^c)^2L_{ext}=L_{cl}+\alpha L_{am}+\omega L_e\alpha=1, \omega=102.5.1 GAIN-code第一步：训练分类网络 FCN: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133self.conv1_1 = L.Convolution2D(3, 64, 3, 1, 1)self.conv1_2 = L.Convolution2D(64, 64, 3, 1, 1)self.conv2_1 = L.Convolution2D(64, 128, 3, 1, 1)self.conv2_2 = L.Convolution2D(128, 128, 3, 1, 1)self.conv3_1 = L.Convolution2D(128, 256, 3, 1, 1)self.conv3_2 = L.Convolution2D(256, 256, 3, 1, 1)self.conv3_3 = L.Convolution2D(256, 256, 3, 1, 1)self.conv4_1 = L.Convolution2D(256, 512, 3, 1, 1)self.conv4_2 = L.Convolution2D(512, 512, 3, 1, 1)self.conv4_3 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_1 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_2 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_3 = L.Convolution2D(512, 512, 3, 1, 1)self.fc6 = L.Convolution2D(512, 4096, 7, 1, 0)self.fc7 = L.Convolution2D(4096, 4096, 1, 1, 0)self.score_fr = L.Convolution2D(4096, n_class, 1, 1, 0)def segment(self, x, t=None): # conv1 self.conv1_1.pad = (100, 100) h = F.relu(self.conv1_1(x)) conv1_1 = h h = F.relu(self.conv1_2(conv1_1)) conv1_2 = h h = _max_pooling_2d(conv1_2) pool1 = h # 1/2 # conv2 h = F.relu(self.conv2_1(pool1)) conv2_1 = h h = F.relu(self.conv2_2(conv2_1)) conv2_2 = h h = _max_pooling_2d(conv2_2) pool2 = h # 1/4 # conv3 h = F.relu(self.conv3_1(pool2)) conv3_1 = h h = F.relu(self.conv3_2(conv3_1)) conv3_2 = h h = F.relu(self.conv3_3(conv3_2)) conv3_3 = h h = _max_pooling_2d(conv3_3) pool3 = h # 1/8 # conv4 h = F.relu(self.conv4_1(pool3)) h = F.relu(self.conv4_2(h)) h = F.relu(self.conv4_3(h)) h = _max_pooling_2d(h) pool4 = h # 1/16 # conv5 h = F.relu(self.conv5_1(pool4)) h = F.relu(self.conv5_2(h)) h = F.relu(self.conv5_3(h)) h = _max_pooling_2d(h) pool5 = h # 1/32 # fc6 h = F.relu(self.fc6(pool5)) h = F.dropout(h, ratio=.5) fc6 = h # 1/32 # fc7 h = F.relu(self.fc7(fc6)) h = F.dropout(h, ratio=.5) fc7 = h # 1/32 # score_fr h = self.score_fr(fc7) score_fr = h # 1/32 # score_pool3 h = self.score_pool3(pool3) score_pool3 = h # 1/8 # score_pool4 h = self.score_pool4(pool4) score_pool4 = h # 1/16 # upscore2 h = self.upscore2(score_fr) upscore2 = h # 1/16 # score_pool4c h = score_pool4[:, :, 5:5 + upscore2.shape[2], 5:5 + upscore2.shape[3]] score_pool4c = h # 1/16 # fuse_pool4 h = upscore2 + score_pool4c fuse_pool4 = h # 1/16 # upscore_pool4 h = self.upscore_pool4(fuse_pool4) upscore_pool4 = h # 1/8 # score_pool4c h = score_pool3[:, :, 9:9 + upscore_pool4.shape[2], 9:9 + upscore_pool4.shape[3]] score_pool3c = h # 1/8 # fuse_pool3 h = upscore_pool4 + score_pool3c fuse_pool3 = h # 1/8 # upscore8 h = self.upscore8(fuse_pool3) upscore8 = h # 1/1 # score h = upscore8[:, :, 31:31 + x.shape[2], 31:31 + x.shape[3]] score = h # 1/1 self.score = score if t is None: assert not chainer.config.train return loss = F.softmax_cross_entropy(score, t, normalize=True) if np.isnan(float(loss.data)): raise ValueError('Loss is nan.') chainer.report(&#123;'loss': loss&#125;, self) self.conv1_1.pad = (1, 1) return loss FCN-v1.0: 普通的分类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364self.conv1_1 = L.Convolution2D(3, 64, 3, 1, 1)self.conv1_2 = L.Convolution2D(64, 64, 3, 1, 1)self.conv2_1 = L.Convolution2D(64, 128, 3, 1, 1)self.conv2_2 = L.Convolution2D(128, 128, 3, 1, 1)self.conv3_1 = L.Convolution2D(128, 256, 3, 1, 1)self.conv3_2 = L.Convolution2D(256, 256, 3, 1, 1)self.conv3_3 = L.Convolution2D(256, 256, 3, 1, 1)self.conv4_1 = L.Convolution2D(256, 512, 3, 1, 1)self.conv4_2 = L.Convolution2D(512, 512, 3, 1, 1)self.conv4_3 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_1 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_2 = L.Convolution2D(512, 512, 3, 1, 1)self.conv5_3 = L.Convolution2D(512, 512, 3, 1, 1)self.fc6_cl = L.Linear(512, 4096)self.fc7_cl = L.Linear(4096, 4096)self.score_cl = L.Linear(4096, n_class-1) # Disregard 0 class for classificationself.final_conv_layer = 'conv5_3'self.grad_target_layer = 'prob'self.freezed_layers = ['fc6_cl', 'fc7_cl', 'score_cl']def classify(self, x, is_training=True): with chainer.using_config('train',False): # conv1 h = F.relu(self.conv1_1(x)) h = F.relu(self.conv1_2(h)) h = _max_pooling_2d(h) # conv2 h = F.relu(self.conv2_1(h)) h = F.relu(self.conv2_2(h)) h = _max_pooling_2d(h) # conv3 h = F.relu(self.conv3_1(h)) h = F.relu(self.conv3_2(h)) h = F.relu(self.conv3_3(h)) h = _max_pooling_2d(h) # conv4 h = F.relu(self.conv4_1(h)) h = F.relu(self.conv4_2(h)) h = F.relu(self.conv4_3(h)) h = _max_pooling_2d(h) # conv5 h = F.relu(self.conv5_1(h)) h = F.relu(self.conv5_2(h)) h = F.relu(self.conv5_3(h)) h = _max_pooling_2d(h) h = _average_pooling_2d(h) with chainer.using_config('train',is_training): h = F.relu(F.dropout(self.fc6_cl(h), .5)) h = F.relu(F.dropout(self.fc7_cl(h), .5)) h = self.score_cl(h) # 1*20 return h loss: 1234# cl_output=classify(image)# cl_output: 1*20# target: 1*20 ~ [0,1] 1表示有这个label，0表示没有这个label，用的是多分类损失函数，且类别之间不排斥，类似对每个类别做二元分类。loss = F.sigmoid_cross_entropy(cl_output, target, normalize=True) 第二步：训练GAIN 12345678910111213141516171819202122232425262728293031323334self.GAIN_functions = collections.OrderedDict([ ('conv1_1', [self.conv1_1, F.relu]), ('conv1_2', [self.conv1_2, F.relu]), ('pool1', [_max_pooling_2d]), ('conv2_1', [self.conv2_1, F.relu]), ('conv2_2', [self.conv2_2, F.relu]), ('pool2', [_max_pooling_2d]), ('conv3_1', [self.conv3_1, F.relu]), ('conv3_2', [self.conv3_2, F.relu]), ('conv3_3', [self.conv3_3, F.relu]), ('pool3', [_max_pooling_2d]), ('conv4_1', [self.conv4_1, F.relu]), ('conv4_2', [self.conv4_2, F.relu]), ('conv4_3', [self.conv4_3, F.relu]), ('pool4', [_max_pooling_2d]), ('conv5_1', [self.conv5_1, F.relu]), ('conv5_2', [self.conv5_2, F.relu]), ('conv5_3', [self.conv5_3, F.relu]), ('pool5', [_max_pooling_2d]), ('avg_pool', [_average_pooling_2d]), ('fc6_cl', [self.fc6_cl, F.relu]), ('fc7_cl', [self.fc7_cl, F.relu]), ('prob', [self.score_cl, F.sigmoid])])self.final_conv_layer = 'conv5_3'self.grad_target_layer = 'prob'self.freezed_layers = ['fc6_cl', 'fc7_cl', 'score_cl'] 通过分类结果获取mask： 12345678910111213141516171819202122232425262728293031def stream_cl(self, inp, label=None): # h: 1*3*281*500 # label: 真值 array([0, 14]) # return: gcam: mask,size(1,3,281,500); h: size(1,20), class_id: 一个数字 h = inp for key, funcs in self.GAIN_functions.items(): for func in funcs: h = func(h) if key == self.final_conv_layer: activation = h if key == self.grad_target_layer: break gcam, class_id = self.get_gcam(h, activation, (inp.shape[-2], inp.shape[-1]), label=label) return gcam, h, class_iddef get_gcam(self, end_output, activations, shape, label): # end_output: size: 1,20 # activations: size: 1*512*18*32 # shape: (281, 500) # label: 真值 self.cleargrads() class_id = self.set_init_grad(end_output, label) end_output.backward(retain_grad=True) grad = activations.grad_var grad = F.average_pooling_2d(grad, (grad.shape[-2], grad.shape[-1]), 1) grad = F.expand_dims(F.reshape(grad, (grad.shape[0]*grad.shape[1], grad.shape[2], grad.shape[3])), 0) weights = activations weights = F.expand_dims(F.reshape(weights, (weights.shape[0]*weights.shape[1], weights.shape[2], weights.shape[3])), 0) gcam = F.resize_images(F.relu(F.convolution_2d(weights, grad, None, 1, 0)), shape) return gcam, class_id $L_{cl}$ 12gcam, cl_scores, class_id = self._optimizers['main'].target.stream_cl(image, gt_labels)cl_loss = F.sigmoid_cross_entropy(cl_scores, target, normalize=True) $L_{am}$ 123masked_output = self._optimizers['main'].target.stream_am(masked_image)masked_output = F.sigmoid(masked_output)am_loss = masked_output[0][class_id][0] 备注: $L_{cl}$和$L_{am}$完全共享网络。 3. Introduction可视化即应该满足高分辨率，也应该满足类别定位能力。 示例图像 4. ApproachCAM在CAM中，一个全连接层替换成GAP，参见上面的CAM图，则分类任务可以表示成 y^c = \sum_k w_k^c \frac{1}{Z} \sum_i \sum_j A_{ij}^k其中，$y^c$表示分类结果，$w_k^c$表示第k个特征图(kxhxw)对第c个类别的贡献，即全连接层的系数，$Z$表示特征图的大小，$Z=h\cdot w$，$A_{ij}^k$表示第k个特征图。 则 CAM 的输出图表示为： L_{CAM}^c=\sum_k w_k^c A^kGrad-CAM在Grad-CAM中，权重系数是通过反向传播得到的。 \alpha_k^c=\frac{1}{Z}\sum_i \sum_j \frac{\partial y^c}{\partial A_{ij}^k}则Grad-CAM的输出图表示为： L_{Grad-CAM}^c=ReLU(\sum_k \alpha_k^c A^k)可以证明，Grad-CAM与CAM的公式是同一个公式的变形。 Guided Grad-CAMGuided Grad-CAM 是将 Grad-CAM 与 Guided Backpropagation 得到的输出图简单地点乘，从而获得类区分定位的高分辨率细节图。 同时作者还分析了CNN分类错误的样本。 5. 代码对于pytorch代码进行分析 5.1 Grad-CAM计算Grad-CAM: 反向传播：先计算出当前图片的分类结果output(size:1*5)(假设共5类)，选出最优分类结果，假设是第2类，然后令one-hot=[0,1,0,0,0]，求得sum-one-hot=得到一个数字，然后反向传播。 cam: (H,W), ~(0,1) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134class FeatureExtractor(object): """ Class for extracting activations and registering gradients from targetted intermediate layers """ """ 调用方式: outputs, x = FeatureExtractor(x) gradients = FeatureExtractor.gradients """ def __init__(self, model, target_layers): self.model = model self.target_layers = target_layers self.gradients = [] def save_gradient(self, grad): self.gradients.append(grad) def __call__(self, x): """ :param x: N*C*H*W, a picture :return: outputs: list, activations layer output, A in equation x : feature map, feature of model output n*c*h*w """ outputs = [] self.gradients = [] for name, module in self.model._modules.items(): x = module(x) if name in self.target_layers: x.register_hook(self.save_gradient) outputs += [x] return outputs, xclass ModelOutputs(object): """ Class for making a forward pass, and getting: 1. The network output. 2. Activations from intermeddiate targetted layers. 3. Gradients from intermeddiate targetted layers. """ """ 调用方式： target_activations, output = ModelOutputs(x) gradients = ModelOutputs.get_gradients() """ def __init__(self, model, target_layers): self.model = model self.feature_extractor = FeatureExtractor(self.model.features, target_layers) def get_gradients(self): return self.feature_extractor.gradients def __call__(self, x): """ :param x: N*C*H*W, a picture :return: target_activations: list, activations layer output, A in equation output : tensor, classification output. N*c. y in equation. """ target_activations, output = self.feature_extractor(x) output = output.view(output.size(0), -1) output = self.model.classifier(output) return target_activations, outputclass GradCam(object): """ Class for making Grad-CAM, and getting: 1. Grad-CAM """ """ 调用方式： mask=GradCam(input) """ def __init__(self, model, target_layer_names, use_cuda): self.model = model self.model.eval() self.cuda = use_cuda if self.cuda: self.model = model.cuda() self.extractor = ModelOutputs(self.model, target_layer_names) def forward(self, input): return self.model(input) def __call__(self, input, index=None): """ :param input: N*C*H*W, a picture :param index: int :return: cam: N*C*H*W ~(0,1) L_&#123;Grad-CAM&#125;^c """ if self.cuda: features, output = self.extractor(input.cuda()) else: features, output = self.extractor(input) if index == None: index = np.argmax(output.cpu().data.numpy()) one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32) one_hot[0][index] = 1 # After test, requires_grad could be False one_hot = Variable(torch.from_numpy(one_hot), requires_grad=True) if self.cuda: one_hot = torch.sum(one_hot.cuda() * output) else: one_hot = torch.sum(one_hot * output) self.model.features.zero_grad() self.model.classifier.zero_grad() # After test, requires_grad could be False one_hot.backward(retain_graph=True) grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy() target = features[-1] target = target.cpu().data.numpy()[0, :] weights = np.mean(grads_val, axis=(2, 3))[0, :] cam = np.zeros(target.shape[1:], dtype=np.float32) for i, w in enumerate(weights): cam += w * target[i, :, :] cam = np.maximum(cam, 0) cam = cv2.resize(cam, (224, 224)) cam = cam - np.min(cam) cam = cam / np.max(cam) return cam 显示Grad-CAM 1234567def show_cam_on_image(img, mask): heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET) heatmap = np.float32(heatmap) / 255 cam = heatmap + np.float32(img) cam = cam / np.max(cam) cv2.imwrite("cam.jpg", np.uint8(255 * cam)) 5.2 GuidedBackpropReLUModelgb: (C,H,W) 任意值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class GuidedBackpropReLU(Function): def forward(self, input): positive_mask = (input &gt; 0).type_as(input) output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask) self.save_for_backward(input, output) return output def backward(self, grad_output): input, output = self.saved_tensors grad_input = None positive_mask_1 = (input &gt; 0).type_as(grad_output) positive_mask_2 = (grad_output &gt; 0).type_as(grad_output) grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input), torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output, positive_mask_1), positive_mask_2) return grad_inputclass GuidedBackpropReLUModel: def __init__(self, model, use_cuda): self.model = model self.model.eval() self.cuda = use_cuda if self.cuda: self.model = model.cuda() # replace ReLU with GuidedBackpropReLU for idx, module in self.model.features._modules.items(): if module.__class__.__name__ == 'ReLU': self.model.features._modules[idx] = GuidedBackpropReLU() def forward(self, input): return self.model(input) def __call__(self, input, index=None): if self.cuda: output = self.forward(input.cuda()) else: output = self.forward(input) if index == None: index = np.argmax(output.cpu().data.numpy()) one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32) one_hot[0][index] = 1 # After test, requires_grad could be False one_hot = Variable(torch.from_numpy(one_hot), requires_grad=True) if self.cuda: one_hot = torch.sum(one_hot.cuda() * output) else: one_hot = torch.sum(one_hot * output) # self.model.features.zero_grad() # self.model.classifier.zero_grad() one_hot.backward(retain_graph=True) output = input.grad.cpu().data.numpy() output = output[0, :, :, :] return output 5.3 Guided Grad-CAM123456cam_mask = np.zeros(gb.shape)for i in range(0, gb.shape[0]): cam_mask[i, :, :] = maskcam_gb = np.multiply(cam_mask, gb)utils.save_image(torch.from_numpy(cam_gb), 'cam_gb.jpg') 6. 效果显示原图 Grad-CAM Guided-Backpropagation Guided Grad-CAM]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RotationNet-paper]]></title>
    <url>%2F2018%2F12%2F11%2FRotationNet-paper%2F</url>
    <content type="text"><![CDATA[1. RotationNet-paper paper: RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints(CVPR2018) Asako Kanezaki, Yasuyuki Matsushita2, and Yoshifumi Nishida. Asako Kanezaki 是日本东京研究所专门研究3D的一个老师。 code-pytorch: https://github.com/kanezaki/pytorch-rotationnetcode-caffe: https://github.com/kanezaki/rotationnetproject: https://kanezaki.github.io/rotationnet/ MIMO data: https://github.com/kanezaki/MIRO 作者是使用caffe版本提交的论文，我也只是看了看代码，作为理解作者论文的辅助，实际没有跑过代码。 这篇博客以代码和论文混杂，因为是借助代码理解论文的，又因为不主要做这个方向，所以并没有在意精度什么的。 1.1 出发点作者不仅想要预测出图片的类别label，还想预测出图片的view-points. 我觉得作者的创新点在于对view的状态顺序编码成view-rotation，限定了view的取值空间，使预测的结果变成了哪种view-rotaion的view准确率高。 因为在一般情况下，想到的是直接预测view，而不是view-rotation. 1.2 网络架构 1.2.1 训练过程：以MIRO数据集、case=3为例，nview=160，vcand=(16, 160),view-rotation=16，num-classes=12. 这里的view-roration，我的理解是view的排列方式，但是还是不太顺。 输入的图片个数batch-size必须是nview的倍数，以输入一个样本的160个角度的图片为例，即batch-size=160，nsamp=1，不影响后续的分析，因为每个样本没有任何关系。 输出是output=batch-size x ((num_classes+1) * nview)= 160 x (13 x 160). 可以理解成对每一个图片，输出网络架构的一行，可以理解成160张图片在160个view下属于13个类的概率。 预测view rotation: 利用下面的预测view公式，求log并相减得到output- = (160x160) x 12 x 1，可以理解成一个矩阵： (160x160) x 12，每行表示当前图片当前view下的属于各类的概率，第一个160行表示第一张图片在160个view下的概率分布，第二个160行表示第二张图片在160个view下的概率分布。scores = (16 x 12 x 1)。scores可以理解成当前view-rotation下这160张图片一起属于各类的概率。 123456789101112131415output = model(input_var)num_classes = int( output.size( 1 ) / nview ) - 1output = output.view( -1, num_classes + 1 )# compute scores and decide target labelsoutput_ = torch.nn.functional.log_softmax( output )output_ = output_[ :, :-1 ] - torch.t( output_[ :, -1 ].repeat( 1, output_.size(1)-1 ).view( output_.size(1)-1, -1 ) )output_ = output_.view( -1, nview * nview, num_classes )output_ = output_.data.cpu().numpy()output_ = output_.transpose( 1, 2, 0 )scores = np.zeros( ( vcand.shape[ 0 ], num_classes, nsamp ) )for j in range(vcand.shape[0]): for k in range(vcand.shape[1]): scores[ j ] = scores[ j ] + output_[ vcand[ j ][ k ] * nview + k ] 生成动态真值target-：已知这160张图片的真值target[ n * nview ]，假设是第3类，j-max表示第j-max个view-rotation下，预测为第3类的概率最大，继而生成动态真值target-=(target.size(0) x nview)=160 x 160=25600，可以理解成160张图片在160个view下的真值，在j-max个view-rotation对应的view设置为类别3，其余的设置为13. 123456target_ = torch.LongTensor( target.size(0) * nview )for n in range( nsamp ): j_max = np.argmax( scores[ :, target[ n * nview ], n ] ) # assign target labels for k in range(vcand.shape[1]): target_[ n * nview * nview + vcand[ j_max ][ k ] * nview + k ] = target[ n * nview ] 1.2.2 验证过程与训练类似，可以得到output- = (160x160) x 12 x 1，可以理解成一个矩阵： (160x160) x 12，每行表示当前图片当前view下的属于各类的概率，第一个160行表示第一张图片在160个view下的概率分布，第二个160行表示第二张图片在160个view下的概率分布。scores = (16 x 12 x 1)。scores可以理解成每个view-rotation下这160张图片一起属于各类的概率。 j-max表示在第j-max个view-rotation下，scores可以找到最大概率。 output[n] 表示每连续的160张图片一起属于某类的概率。 123456789101112131415161718192021222324252627output = model(input_var)num_classes = int( output.size( 1 ) / nview ) - 1output = output.view( -1, num_classes + 1 )output = torch.nn.functional.log_softmax( output )output = output[ :, :-1 ] - torch.t( output[ :, -1 ].repeat( 1, output.size(1)-1 ).view( output.size(1)-1, -1 ) )output = output.view( -1, nview * nview, num_classes )# measure accuracy and record lossprec1, prec5 = my_accuracy(output.data, target, topk=(1, 5))# def my_accuracytarget = target[0:-1:nview]batch_size = target.size(0)num_classes = output_.size(2)output_ = output_.cpu().numpy()output_ = output_.transpose( 1, 2, 0 )scores = np.zeros( ( vcand.shape[ 0 ], num_classes, batch_size ) )output = torch.zeros( ( batch_size, num_classes ) )for j in range(vcand.shape[0]): for k in range(vcand.shape[1]): scores[ j ] = scores[ j ] + output_[ vcand[ j ][ k ] * nview + k ]for n in range( batch_size ): j_max = int( np.argmax( scores[ :, :, n ] ) / scores.shape[ 1 ] ) output[ n ] = torch.FloatTensor( scores[ j_max, :, n ] )output = output.cuda() 1.2.3 测试过程因为caffe代码没有看懂，所以根据作者的论文和代码猜一下，当输入的图片没有160张，假设只有100张图片，那么又该怎么做？ 模型的输出是output=batch-size x ((num_classes+1) * nview)= 100 x (13 x 160)，那么怎么求scores？ 求scores是需要全部view的信息的。这里不会了，尽管已经给出了公式，但是公式只能算出output-，没有score，不会了。 坐等作者回复。 Our method is available only when the relative poses of test images are known. For example, if you captured three images where the second image is 22.5 degrees forward from the first image and the third image is 45 degrees forward from the second image, then the images should be indexed as (0, 1, 3). Then you would get 3x160x12 output values. An easy way to proceed is to create a 160x160x12 “output2” which has zero values, and then insert the output values as “output2[0] = output[0]; output2[1] = output[1]; output2[3] = output[2];”. (In our paper, we used LSD-SLAM to calculate relative poses of test images.) 根据作者的回复，不难理解，给定的测试图片是需要预先知道测试图片序列的相对位置的。 1.3 预测公式\max_{(v_i)\_{i=1}^M}\prod_{i=1}^M(\log p_{v_{i},y}^{(i)}-\log p_{v_{i},N+1}^{(i)})]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>RotationNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-2.0]]></title>
    <url>%2F2018%2F12%2F10%2Fpytorch-2.0%2F</url>
    <content type="text"><![CDATA[1. eval pytorch 的eval()只是改变一些模块的状态，并不影响backward过程。 https://blog.csdn.net/u012436149/article/details/78281553 https://www.jianshu.com/p/6cb1fd785540 2. 扩展 torch.autograd参考链接： https://zhuanlan.zhihu.com/p/27783097 https://pytorch-cn.readthedocs.io/zh/latest/notes/extending/ https://blog.csdn.net/Hungryof/article/details/78346304 2.1 Function和Module的差异和应用场景 Function一般只定义一个操作，因为无法保存参数，例如激活函数、pool操作等； Module可以保存参数，适合于定义一个层 Function需要定义三个方法：__init__, forward, backward; Module: 需要定义__init__和forward，而backward由自动求导构成； 可以简单地认为，Module就是由Function构成的 Module中不仅包括了Function，还包括了对应的参数、其他变量等等。 Function: __init__ (optional) - 如果你的operation包含非Variable参数，那么就将其作为__init__的参数传入到operation中。例如：AddConstant Function加一个常数，Transpose Function需要指定哪两个维度需要交换。如果你的operation不需要额外的参数，你可以忽略__init__。 2.2 定义一个ReLU函数2.2.1 定义一个ReLU函数123456789101112131415161718192021import torchfrom torch.autograd import Variableclass MyReLU(torch.autograd.Function): # 输入和输出相互对应 def forward(self, input_): # 在forward中，需要定义MyReLU这个运算的forward计算过程 # 同时可以保存任何在后向传播中需要使用的变量值 self.save_for_backward(input_) # 将输入保存起来，在backward时使用 output = input_.clamp(min=0) # relu就是截断负数，让所有负数等于0 return output def backward(self, grad_output): # 根据BP算法的推导（链式法则），dloss / dx = (dloss / doutput) * (doutput / dx) # dloss / doutput就是输入的参数grad_output、 # 因此只需求relu的导数，在乘以grad_output input_, = self.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 # 上诉计算的结果就是左式。即ReLU在反向传播中可以看做一个通道选择函数，所有未达到阈值（激活值&lt;0）的单元的梯度都为0 return grad_input 12345678910111213141516171819class GuidedBackpropReLU(Function): def forward(self, input): positive_mask = (input &gt; 0).type_as(input) output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask) self.save_for_backward(input, output) return output def backward(self, grad_output): input, output = self.saved_tensors grad_input = None positive_mask_1 = (input &gt; 0).type_as(grad_output) positive_mask_2 = (grad_output &gt; 0).type_as(grad_output) grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input), torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output, positive_mask_1), positive_mask_2) return grad_input 123456789101112131415161718192021222324252627282930313233# Inherit from Functionclass Linear(Function): # bias is an optional argument def forward(self, input, weight, bias=None): self.save_for_backward(input, weight, bias) output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return output # This function has only a single output, so it gets only one gradient def backward(self, grad_output): # This is a pattern that is very convenient - at the top of backward # unpack saved_tensors and initialize all gradients w.r.t. inputs to # None. Thanks to the fact that additional trailing Nones are # ignored, the return statement is simple even when the function has # optional inputs. input, weight, bias = self.saved_tensors grad_input = grad_weight = grad_bias = None # These needs_input_grad checks are optional and there only to # improve efficiency. If you want to make your code simpler, you can # skip them. Returning gradients for inputs that don't require it is # not an error. if self.needs_input_grad[0]: grad_input = grad_output.mm(weight) if self.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and self.needs_input_grad[2]: grad_bias = grad_output.sum(0).squeeze(0) return grad_input, grad_weight, grad_bias 2.2.2 Wrap成一个ReLU函数可以把刚刚自定义的ReLU类封装成一个函数，方便调用 123456789def relu(input_): # MyReLU()是创建一个MyReLU对象， # Function类利用了Python __call__操作，使得可以直接使用对象调用__call__制定的方法 # __call__指定的方法是forward，因此下面这句MyReLU（）（input_）相当于 # return MyReLU().forward(input_) return MyReLU()(input_)input_ = Variable(torch.linspace(-3, 3, steps=5))print input_print relu(input_) 在某些版本中，Function类的使用方法是.apply方法，其和__call__方法类似。 在某些版本中，forward和backward方法需要是@staticmethod 在某些版本中，forward中Variable参数会自动转为tensor，backward参数是Variable 在某些版本中，forward和backward中self会写成ctx save_for_backward只能传入Variable或是Tensor的变量，如果是其他类型的，可以用self.xyz = xyz，使其在backward中可以用。 2.2.3 检查backward是否正确12345678from torch.autograd import gradcheck# gradchek takes a tuple of tensor as input, check if your gradient# evaluated with these tensors are close enough to numerical# approximations and returns True if they all verify this condition.input = (Variable(torch.randn(20,20).double(), requires_grad=True),)test = gradcheck.gradcheck(Linear(), input, eps=1e-6, atol=1e-4)print(test) 备注： 自动求导竟然是在backward的操作中创建的图！暂时没懂。 3. 扩展torch.nnnn包括两种街扩-modules和functional版本。其中，functional版本已经在上面介绍过，不再重复，这一小节主要介绍modules。 扩展module需要预先实现一个执行计算和梯度的Function。 扩展module需要实现两个方法： __init__ (optional) - 输入参数，例如kernel sizes, numbers of features, 等等。同时初始化 parameters和buffers。 forward() - 实例化一个执行operation的Function，使用它执行operation。和functional wrapper(上面实现的那个简单的wrapper)十分类似。 123456789101112131415161718192021222324252627282930class Linear(nn.Module): def __init__(self, input_features, output_features, bias=True): self.input_features = input_features self.output_features = output_features # nn.Parameter is a special kind of Variable, that will get # automatically registered as Module's parameter once it's assigned # as an attribute. Parameters and buffers need to be registered, or # they won't appear in .parameters() (doesn't apply to buffers), and # won't be converted when e.g. .cuda() is called. You can use # .register_buffer() to register buffers. # nn.Parameters can never be volatile and, different than Variables, # they require gradients by default. self.weight = nn.Parameter(torch.Tensor(input_features, output_features)) if bias: self.bias = nn.Parameter(torch.Tensor(output_features)) else: # You should always register all possible parameters, but the # optional ones can be None if you want. self.register_parameter('bias', None) # Not a very smart way to initialize weights self.weight.data.uniform_(-0.1, 0.1) if bias is not None: self.bias.data.uniform_(-0.1, 0.1) def forward(self, input): # See the autograd section for explanation of what happens here. return Linear()(input, self.weight, self.bias) #注意这个Linear是之前实现过的Linear parameter和buffer是一类特殊的Variable。 1234567def __init__(self):self._parameters = OrderedDict()self._modules = OrderedDict()self._buffers = OrderedDict()self._backward_hooks = OrderedDict()self._forward_hooks = OrderedDict()self.training = True _parameters: 是一个字典，保存的有：nn.Parameter()和register_parameter()，而nn.Linear(3,4)的parameter不会保存于此。 _modules:nn.Linear(3,4)会保存于此。 _buffers:register_buffer()会保存于此. named_parameters():会保存_parameters和_modules的parameters。 __getattr__()方法会从_modules,_parameters和_buffers这三个字典中获取。 3. retain_graph and create_graph3.1 retain_graphretain_graph 是保存前向传播过程中的buffer。是因为如果有两个backward,那么需要retain_graph=true,因为默认一个前向对应一个反向,用于实现多次反向传播 1234567891011121314151617import torchx = torch.ones(1)b = torch.rand(1, requires_grad=True)w = torch.rand(1, requires_grad=True)y = w * xz = y + bz2 = y-bz.backward()w.grad1z2.backward()RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.改为z.backward(retain_graph=True) # 只能保证当前backward后的buffer不被清除.w.grad1z2.backward() 发现一个神奇的操作,如果y=w+x,那么不需要retain_graph 也可以多次反向传播,说明问题出现在y上.当y中存储了buffer w或者x后,就需要retain_graph,反之,不需要也能多次反向传播.但是现在找不到这个buffer保存在哪里 3.2 create_graphcreate_graph用于实现高阶导数,相当于对于叶子节点的导数dx与x是否建立计算图,即是否需要保存f’(x)作为一个计算图 12345678910import torchx = torch.tensor([5.0], requires_grad=True)y = x ** 2grad_x = torch.autograd.grad(y, x, create_graph=True)grad_x[0].grad_fn&lt;MulBackward1 at 0x7feb27c9e160&gt;grad_grad_x = torch.autograd.grad(grad_x[0], x)print(grad_grad_x[0].grad_fn)grad_grad_grad_x = torch.autograd.grad(grad_grad_x, x)RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SPGAN-tensorflow]]></title>
    <url>%2F2018%2F12%2F10%2FSPGAN-tensorflow%2F</url>
    <content type="text"><![CDATA[在阅读SPGAN代码源码的过程中，学习到的关于tensorflow的一些知识。 1. tf.ConfigProto参考链接:https://blog.csdn.net/dcrmg/article/details/79091941 tf.ConfigProto用于对sessison会话的参数配置。 log_device_placement=True: 可以获取到 operations 和 Tensor 被指派到哪个设备(几号CPU或几号GPU)上运行,会在终端打印出各项操作是在哪个设备上运行的 allow_soft_placement=True: 允许tf自动选择一个存在并且可用的设备来运行操作.在tf中，通过命令 “with tf.device(‘/cpu:0’):”,允许手动设置操作运行的设备 config.gpu_options.allow_growth = True: 动态申请显存 config.gpu_options.per_process_gpu_memory_fraction = 0.4: 占用40%显存,限制GPU使用率. 123config = tf.ConfigProto(allow_soft_placement=True)config.gpu_options.allow_growth = Truesess = tf.Session(config=config) 12345678# 限制GPU使用率config = tf.ConfigProto()config.gpu_options.per_process_gpu_memory_fraction = 0.4 #占用40%显存session = tf.Session(config=config)等同于gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.4)config=tf.ConfigProto(gpu_options=gpu_options)session = tf.Session(config=config) 123456# 设置使用哪块GPU方法一： 在python中设置os.environ['CUDA_VISIBLE_DEVICES'] = '0' #使用 GPU 0os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # 使用 GPU 0，1方法二： 在执行时设置CUDA_VISIBLE_DEVICES=0,1 python yourcode.py 2. tf读取数据集图片的方式参考链接: https://www.jb51.net/article/134550.htm https://www.jb51.net/article/134547.htm tf的流程是文件系统—&gt;文件名队列—&gt;内存队列 推荐使用方法一 方法一：使用WholeFileReader输入queue，decode输出是Tensor，eval后是ndarray123456789101112131415161718192021222324252627282930313233343536373839404142import tensorflow as tfimport osimport matplotlib.pyplot as pltdef file_name(file_dir): #来自//www.jb51.net/article/134543.htm for root, dirs, files in os.walk(file_dir): #模块os中的walk()函数遍历文件夹下所有的文件 print(root) #当前目录路径 print(dirs) #当前路径下所有子目录 print(files) #当前路径下所有非目录子文件def file_name2(file_dir): #特定类型的文件 L=[] for root, dirs, files in os.walk(file_dir): for file in files: if os.path.splitext(file)[1] == '.jpg': L.append(os.path.join(root, file)) return Lpath = file_name2('test')#以下参考//www.jb51.net/article/134547.htm (十图详解TensorFlow数据读取机制)#path2 = tf.train.match_filenames_once(path)file_queue = tf.train.string_input_producer(paths, shuffle=True, num_epochs=2) #创建输入队列image_reader = tf.WholeFileReader()key, image = image_reader.read(file_queue)image = tf.image.decode_jpeg(image, channerls=3)with tf.Session() as sess:# coord = tf.train.Coordinator() #协同启动的线程# threads = tf.train.start_queue_runners(sess=sess, coord=coord) #启动线程运行队列# coord.request_stop() #停止所有的线程# coord.join(threads) tf.local_variables_initializer().run() threads = tf.train.start_queue_runners(sess=sess) #print (type(image)) #print (type(image.eval())) #print(image.eval().shape) for _ in path+path: plt.figure plt.imshow(image.eval()) plt.show() 方法二：使用gfile读图片，decode输出是Tensor，eval后是ndarray12345678910111213141516171819202122232425import matplotlib.pyplot as pltimport tensorflow as tfimport numpy as npprint(tf.__version__)image_raw = tf.gfile.FastGFile('test/a.jpg','rb').read() #bytesimg = tf.image.decode_jpeg(image_raw) #Tensor#img2 = tf.image.convert_image_dtype(img, dtype = tf.uint8)with tf.Session() as sess: print(type(image_raw)) # bytes print(type(img)) # Tensor #print(type(img2)) print(type(img.eval())) # ndarray !!! print(img.eval().shape) print(img.eval().dtype)# print(type(img2.eval()))# print(img2.eval().shape)# print(img2.eval().dtype) plt.figure(1) plt.imshow(img.eval()) plt.show() 方法三：使用read_file，decode输出是Tensor，eval后是ndarray12345678910111213141516171819202122232425import matplotlib.pyplot as pltimport tensorflow as tfimport numpy as npprint(tf.__version__)image_raw = tf.gfile.FastGFile('test/a.jpg','rb').read() #bytesimg = tf.image.decode_jpeg(image_raw) #Tensor#img2 = tf.image.convert_image_dtype(img, dtype = tf.uint8)with tf.Session() as sess: print(type(image_raw)) # bytes print(type(img)) # Tensor #print(type(img2)) print(type(img.eval())) # ndarray !!! print(img.eval().shape) print(img.eval().dtype)# print(type(img2.eval()))# print(img2.eval().shape)# print(img2.eval().dtype) plt.figure(1) plt.imshow(img.eval()) plt.show() 3. tf.train.shuffle_batch参考链接: https://www.jianshu.com/p/9cfe9cadde06 https://blog.csdn.net/ying86615791/article/details/73864381 12345img_batch = tf.train.shuffle_batch([img],batch_size=batch_size, capacity=capacity,min_after_dequeue=min_after_dequeue,num_threads=num_threads,allow_smaller_final_batch=allow_smaller_final_batch) tf不是像pytorch一样全局打乱，而是每一次在较短的队列中打乱。其中，队列的最长长度是capacity，最短长度是min_after_dequeue。 4. tf.summary参考链接：https://blog.csdn.net/hongxue8888/article/details/78610305 1summary_writer = tf.summary.FileWriter('./summaries/' + dataset + '_spgan' , sess.graph) 5. tf.train.Saver参考链接：http://www.cnblogs.com/denny402/p/6940134.html 1saver = tf.train.Saver(max_to_keep= 30) 6. saver.restore参考链接：https://blog.csdn.net/changeforeve/article/details/80268522 12345678910111213def load_checkpoint(checkpoint_dir, sess, saver): print(" [*] Loading checkpoint...") ckpt = tf.train.get_checkpoint_state(checkpoint_dir) print(ckpt) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) ckpt_path = os.path.join(checkpoint_dir, ckpt_name) saver.restore(sess, ckpt_path) print(" [*] Loading successful!") return ckpt_path else: print(" [*] No suitable checkpoint!") return None 7. tf.train.Coordinator参考链接: https://blog.csdn.net/weixin_42052460/article/details/80714539 https://www.jianshu.com/p/d063804fb272 12coord = tf.train.Coordinator()threads = tf.train.start_queue_runners(sess=sess, coord=coord) 8. tf.identity参考链接: https://stackoverflow.com/questions/34877523/in-tensorflow-what-is-tf-identity-used-for https://blog.csdn.net/hu_guan_jie/article/details/78495297 tf.idenity的逻辑就是等于号，区别是前者在计算图上加了个节点，使得可以多个设备之间可以通信，但是等于号为什么不行呢？ 9. tf.reuse参考链接：https://blog.csdn.net/UESTC_C2_403/article/details/72329786]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>SPGAN</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github-markdown-mathjax]]></title>
    <url>%2F2018%2F12%2F03%2Fgithub-markdown-mathjax%2F</url>
    <content type="text"><![CDATA[前言：在github上传自己的论文记录后发现，github上的显示和在vscode上的显示不同，主要体现在公式在github上不能正确地显示，并且换行也不能正确地显示。 参考链接：https://blog.csdn.net/phdsky/article/details/81431279 搜索之后发现github的markdown不支持mathjax的渲染。Github issue - github’s markdown mathjax rendingStackoverflow - How to show math equations in general github’s markdown 解决方案或者是公式转图片，或者是使用github内嵌的公式编辑器，或者是适用于chrome的github with MathJax插件。 我最后采用的是github with MathJax插件。GitHub with MathJax 插件]]></content>
      <categories>
        <category>github-markdown</category>
      </categories>
      <tags>
        <tag>github</tag>
        <tag>markdown</tag>
        <tag>mathjax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown-math]]></title>
    <url>%2F2018%2F12%2F03%2Fmarkdown-math%2F</url>
    <content type="text"><![CDATA[前言: 因为最近经常用到markdown写数学公式，每次都查感觉有点啰嗦，所以做个简单小结，把平常用的做个记录。这个博客根据平时自己常用到的进行动态增加。 [x] 2019-01-18: hexo更换渲染引擎，详见下文 [x] 2018-12-27: 使用Katex [x] 2018-12-27：一份不太简单的LATEX2e介绍 [x] 2019-05-24: \mathop {argmin}_{a} $\mathop {argmin} _{a}$ 参考链接： https://wangcong.info/article/MarkdownWithMath.html https://blog.csdn.net/deepinC/article/details/81103326 https://blog.csdn.net/HaleyPKU/article/details/80341932 https://www.zybuluo.com/fyywy520/note/82980 1. 公式使用参考1.1 插入公式插入公式分为行中公式，独立公式和自动编号公式 1.行中公式 $ a=b $ 1$ 数学公式 $ 2.独立公式 a=b 1$$ 数学公式 $$ 3.编号公式 a=b \tag {1}1$$ 数学公式 \tag &#123;1&#125; $$ 由公式$(1)$可以得出结论 1由公式$(1)$可以得出结论 4.自动编号公式自动编号公式在github上显示不出来，原则上是可以的，推荐使用手动编号。 \begin{equation} x^n+y^n=z^n \label{eq:afa} \end{equation}1234\begin&#123;equation&#125;数学公式\label&#123;eq:当前公式名&#125;\end&#123;equation&#125; 5.自动编号公式的引用方法 在公式 \eqref{eq:wwqr} 中，我们看到了这个被自动编号的公式。貌似没有成功 6.单个公式换行 单个公式很长的时候需要换行，但仅允许生成一个编号时，可以用split标签包围公式代码，在需要转行的地方使用\，每行需要使用1个&amp;来标识对齐的位置，结束后可使用\tag{…}标签编号。 \begin{split} a &= b \\ c &= d \\ e &= f \end{split}\tag{1.2}1234567$$\begin&#123;split&#125;a &amp;= b \\c &amp;= d \\e &amp;= f \end&#123;split&#125;\tag&#123;1.3&#125;$$ 7.多行的独立公式 有时候需要罗列多个公式，可以用eqnarray*标签包围公式代码，在需要转行的地方使用\，每行需要使用2个&amp;来标识对齐位置，两个&amp;…&amp;号之间的是公式间对齐的位置，每行公式后可使用\tag{…}标签编号： github貌似对于多行公式显示不出来。 \begin{eqnarray*} x^n+y^n &=& z^n \tag{1.4} \\ x+y &=& z \tag{1.5} \end{eqnarray*}123456$$\begin&#123;eqnarray*&#125;x^n+y^n &amp;=&amp; z^n \tag&#123;1.4&#125; \\x+y &amp;=&amp; z \tag&#123;1.5&#125;\end&#123;eqnarray*&#125;$$ 1.2 符号 输入 显示 输入 显示 x^y $x^y$ x_y $x_y$ \sideset{\^1_2}{\^3_4}\bigotimes $\sideset{^1_2}{^3_4}\bigotimes$ \langle &lt; \lceil $\lceil$ \rceil $\rceil$ \lfloor $\lfloor$ \frac{a}{b} $\frac{a}{b}$ \sqrt[2]{3} $\sqrt[2]{3}$ \alpha,\gamma $\alpha$ $\gamma$ \frac{a}{b} $\frac{a}{b}$ \sum_{n=1}^N{3n} $\sum_{n=1}^N{3n}$ \prod_{n=1}^N{3n} $\prod_{n=1}^N{3n}$ \sqrt[2]{5} $\sqrt[2]{5}$ \int^5_1{f(x)}{\rm d}x $\int^5_1{f(x)}{\rm d}x$ \iint^5_1{f(x)}{\rm d}x $\iint^5_1{f(x)}{\rm d}x$ +\infty $+\infty$ -\infty $-\infty$ \lim_{n\rightarrow+\infty} n $\lim_{n\rightarrow+\infty} n$ \in $\in$ \geq\,\leq $\geq,\leq$ \subset,\supset $\subset,\supset$ \pm,\cdot $\pm,\cdot$ \times,\div $\times,\div$ \not=,\not&lt; $\not=,\not&lt;$ \not\supset $\not\supset$ \log_2{18} $\log_2{18}$ \parallel $\parallel$ \sum_{n=1}^N{n} $\sum_{n=1}^N{n}$ \prod{n} $\prod{n}$ \pm,\cdot,\times,\div $\pm,\cdot,\times,\div$ 1.3 希腊字母 输入 显示 输入 显示 \alpha,\beta,\gamma $\alpha,\beta,\gamma$ \delta,\epsilon, \varepsilon $\delta,\epsilon, \varepsilon$ \theta,\lambda,\mu $\theta,\lambda,\mu$ \phi,\varphi,\sigma $\phi,\varphi,\sigma$ \Sigma $\Sigma$ 1.4 空心字母与Fraktur字母 输入 显示 输入 显示 \mathbb{A} $\mathbb{A}$ \mathfrak{B} $\mathfrak{B}$ 1.5 分段函数 P_{r-j}= \begin{cases} 0 &\mbox{if $r-j$ is odd}\\ r!\,(-1)^{(r-j)/2} &\mbox{if $r-j$ is even} \end{cases}1234567$$P_&#123;r-j&#125;= \begin&#123;cases&#125; 0 &amp;\mbox&#123;if $r-j$ is odd&#125;\\ r!\,(-1)^&#123;(r-j)/2&#125; &amp;\mbox&#123;if $r-j$ is even&#125; \end&#123;cases&#125;$$ 1.6 多行对齐公式123456$$\begin&#123;align&#125;h(x) =&amp; \frac&#123;1&#125;&#123;\int_xt(x)\mathrm&#123;d&#125;x&#125; \tag&#123;1&#125;\\f(x) =&amp; \frac&#123;1&#125;&#123;\int_x\eta(x)\mathrm&#123;d&#125;x&#125;g(x)\tag&#123;2&#125;\end&#123;align&#125;$$ 2. LaTex公式渲染引擎参考链接：https://www.jianshu.com/p/a9f26f4cd4e6 针对Hexo渲染LaTex公式的时候，下划线总是容易被渲染成斜体，所以更换Hexo默认的Markdown渲染引擎。 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 更换渲染引擎只能保证块内公式的下划线的问题，对于行内公式，需要修改\node_modules\kramed\lib\rules\inline.js。 12// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 对于两个连续的花括号，需要中间加个空格。 对于2019-01-18之前已经发过的博客文章，如公式不进行主动修复。]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[person-reid-transfer-learning]]></title>
    <url>%2F2018%2F11%2F29%2Fperson-reid-transfer-learning%2F</url>
    <content type="text"><![CDATA[transfer learning 这个博客主要是因为最近看了几篇关于无监督迁移学习在行人重识别领域的论文，发现隔了几天，自己对论文就忘记得差不多了，所以对论文的关键内容做个简单记录。 参考链接: Transfer Learning 因为在某些情况下，图片或者公式无法正常显示，所以，我基本会同步到我的博客https://tjjtjjtjj.github.io/2018/11/29/person-reid-transfer-learning/#more 现有方法在transfer learning方向的性能对比 1. ARNAdaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification Yu-Jhe Li, Fu-En Yang, Yen-Cheng Liu, Yu-Ying Yeh, Xiaofei Du, and Yu-Chiang Frank Wang, CVPR 2018 Workshop 这篇论文主要分离了数据集的特有特征和行人特征，从而使不同数据集的行人特征投射到统一特征空间中。 作者是台湾人，没有公布代码。有其他人复现了代码，但是效果很差。 我下一步也会尝试复现一下。 1.1 网络架构 根据作者的描述， $E_I$是resnet50的前四个layer,输入是3X256X256,输出$X^s$是2048X7X7 $E_T,E_C,E_S$,是相同的网络架构，来自FCN的三层，通过查阅FCN的网络设置，初步猜想是FCN的conv6，conv7，conv8，相应的Decoder暂时按照反卷积来设置。这一部分还需要参考FCN的网络设置。 $E_T,E_C,E_S$ conv6:7X7X2048,relu6,drop6(0.5),conv7:1X1X2048,relu6,drop6(0.5),conv8:1X1X2048,至于conv6,7的bn和conv8的bn，relu要不要，还需要实验的验证 在FCN中，逆卷积的使用方式是 deconv(k=64, s=32, p=0)+crop(offset=19)，参考资料:FCN学习:Semantic Segmentation,经典网络复现系列（一）：FCN 反卷积的时候一般都是k=2n, s=n, 参考FCN和pytorch的入门与实践第六章的生成器，我们的Decoder使用deconv(k=1,s=1), deconv(k=1, s=1), deconv(k=7, s=1) encoder和decoder都使用bn和relu 分类层有dropout 学习率，$E_I=10^{-7}, E_T E_C E_S D_C = 10^{-3}, C_S = 2*10^{-3} $，并且在前几个epoch只更新$E_I$ 优化器：SGD 1.2 损失函数分类损失 L_{class}=-\sum_{i=1}^{N_s}y_i^s.log\hat{y}_i^s \tag {1}对比损失 L_{ctrs}=\sum_{i,j}{\lambda}(e_{c,i}^s-e_{c,j}^s)^2+ ({1-\lambda}) [max(0, m-(e_{c,i}^s-e_{c,j}^s))]^2 \tag {2}重构误差 L_{rec} = \sum_{i=1}^{N_s} ||X_i^s-\hat{X_i^s}||^2 + \sum_{i=1}^{N_t} ||X_i^t-\hat{X_i^t}||^2 \tag 3差别损失 L_{diff} = || {H_c^s}^T H_p^s ||_F^2 + || {H_c^t}^T H_p^t ||_F^2 \tag 4总损失 L_{total} = L_{class} + \alpha L_{ctrs} + \beta L_{rec} + \gamma L_{diff} \tag {5}其中 \alpha=0.01, \beta= 2.0, \gamma=15001.3 模块分析三个模块: $ L_{rec} $ $ L_{class} $和$ L_{ctrs} $ $ E_T$和$E_S$ 1.3.1 半监督$ L_{rec} $这里不是很懂这个重构误差损失函数的作用，下面的这个解释也不行。重构损失是半监督损失函数。暂时理解成重构损失保证在获取特征的过程中尽可能减少信息损失。或者说，类似PCA，保留主成分，这个主成分只能保证尽可能地把样本分开。至于这个主成分是否重要，是否有利于分类，不得而知。 参考链接：深度学习中的“重构” 作者在这里提示，当只有重构损失函数的时候，应该保持$E_I$不更新，只更新$E_C$. S: Market, T: Duke; S: Duke, T: Market method rank-1 mAP rank-1 mAP $L_{rec}$ 44.5 20.3 31.2 18.4 1.3.2 监督$ L_{rec} $, $ L_{class} $和$ L_{ctrs} $半监督和监督 监督损失使得共享空间捕获到行人语义信息。 S: Market, T: Duke; S: Duke, T: Market method rank-1 mAP rank-1 mAP w/o $ L_{class} $, $ L_{ctrs} $ 52.2 23.7 36.7 19.6 w $ L_{class} $, $ L_{ctrs} $ 70.3 39.4 60.2 33.4 $L_{rec}$ 44.5 20.3 31.2 18.4 $L_{rec}$, $ L_{class} $和$ L_{ctrs} $ 60.5 28.7 48.4 26.8 1.3.3 无监督$ L_{rec} $, $ E_T $和$ E_S $特有特征的提取是为了去除共享空间的噪声。 假设共享空间存在，且特有特征空间存在，如果没有特有特征的提取，那么得到的行人特征或多或少地都会包含特征空间的基向量。 当然，这里也隐含了一些假设，共享空间和特有空间一定是线性无关的。空间的基向量是2048维。 S: Market, T: Duke; S: Duke, T: Market method rank-1 mAP rank-1 mAP w/o $ E_T $, $ E_S $ 60.5 28.7 48.4 26.8 w $ L_{class} $, $ L_{ctrs} $ 70.3 39.4 60.2 33.4 $L_{rec}$ 44.5 20.3 31.2 18.4 $ L_{rec} $, $ E_T $和$ E_S $ 52.2 23.7 36.7 19.6 2. HHLGeneralizing A Person Retrieval Model Hetero- and Homogeneously Zhun Zhong, Liang Zheng, Shaozi Li, Yi Yang, ECCV 2018 code: https://github.com/zhunzhong07/HHL web: http://zhunzhong.site/paper/HHL.pdf 中文: http://www.cnblogs.com/Thinker-pcw/p/9787440.html preson-reid中主要面临的问题： 数据集之间的差异 数据集内部摄像头的差异 解决方法： 相机差异：利用StarGAN进行风格转化 数据集差异：将源域/目标域图片视为负匹配 数据集之间的三元组损失有把不同数据集的行人特征映射到同一特征空间的效果。 创新点在于使用straGAN和复杂的三元组损失。 2.1 网络架构 网络的简要介绍 CNN是resnet50，网络包括两个分支，一个计算源数据集的分类损失，一个计算相似度学习的triplet损失。 FC-2014的组成：linear(2048，1024)—&gt;bn(1024)—&gt;relu—&gt;dropout(0.5),相当于一个embedding。 FC-#ID是linear(1024,751), FC-128是linear(1024, 128), 两个分支的具体情况是： x1—&gt;linear(2048, 1024)—&gt;x2—&gt;bn(1024)—&gt;x3—&gt;relu—&gt;x4—&gt;dropout(0.5)—&gt;x5—&gt;linear(102, 751)—&gt;x6 x1—&gt;linear(2048, 1024)—&gt;x2—&gt;bn(1024)—&gt;x3—&gt;relu—&gt;x4—&gt;linear(1024, 128) 网络的triplet损失是Batch Hard Triplet Loss 网络的输入设置：在每一个batch中，对于分类损失，source domain随机选取batchsize=128张图片，对于triplet损失，source domain随机选取8个人的共batchsize=64张图片，其中连续的8张图片属于同一个人，target domain随机选取batchsize=16个人的共16X9=144张图片，假设这16个人都是不同的人。实验发现，当source domain的分类损失的图片比较少的时候，无法实现预期效果，其他情况下没有测试。当batchsize是这样的配比时，可以达到作者的效果。理由未知． starGAN是离线训练 学习率设置：base：$10^{-1}$，其他：$10^{-2}$，并且每过40个epoch，学习率阶梯性地乘以0.1.一共训练60个epoch就可以达到预期效果，这部分设置和PCB很类似。不知道是经验还是怎么。 关于StarGAN待自己复现之后再做进一步解释，现在只复现过StyleGAN。 triplet损失的margin=0.3 2.2 损失函数分类损失 L_{cross}=-\sum_{i=1}^{N_s}y_i^s.log\hat{y}_i^striplet损失 L_T=\sum_{x_a, x_p, x_n}[D_{x_a, x_p}+m-D_{x_a, x_n}]_+相机不变性的triplet损失 目标域中一张原始图片作为anchor，StarGAN图片为positive，其他图片为negative L_C=L_T((x_t^i)^{n_t}\bigcup(x_{t^\*}^i)^{n_t^*})域不变性的triplet损失 源域中一张图片为anchor，同一id的其他图片作为positive，目标域的任一图片为negative L_D=L_T((x_s^i)^{n_s}\bigcup(x_t^i)^{n_t})相机不变性和域不变性的triplet损失 是将相机不变性和域不变性合为一体，源域的positive不变，negative为源域的其他图片和目标域的图片，目标域的positive不变，negative为源域的图片和目标域的其他行人图片 L_{CD}=L_T((x_s^i)^{n_s}\bigcup(x_t^i)^{n_t}\bigcup(x_{t*}^i)^{n_t^*})总损失： L_{HHL}=L_{cross}+\beta*L_{CD}其中： \beta=0.52.3 模块分析 starGAN sample方法 2.3.1 starGAN在源数据集上训练，在目标数据集上测试不同图像增强方法下的图片距离，通过表格可以得出，预训练的模型对于目标数据集的随机翻转等等有很好的鲁棒性，但是，对于不同摄像头的同一个人，其距离还是很大。因此，利用StarGAN和相机不变性的triplet损失来减少由于摄像头带来的偏差。 Source Target Random Crop Random Flip CamStyle Transfer Duke Market 0.049 0.034 0.485 Market Duke 0.059 0.044 0.614 2.3.2 sample方法对于目标域的取样方法，对比了三种方法的性能，分别是随机取样、聚类取样、有监督取样，通过下图可以看出，这三种方法的性能是一样的，最后，作者给的代码是随机取样。 2.4 实验设置2.4.1 Camera style transfer model：StarGAN使用StarGAN进行对于摄像头风格转化。 2 conv + 6 residual + 2 transposed input 128X64 Adam $\beta_1=0.5, \beta_2=0.999$ 数据初始化:随机翻转和随机裁剪 学习率：前100个epoch为0.0001，后100个epoch线性衰减到0 2.4.2 Re-ID model training 设置可以参考Zhong, Z., Zheng, L., Zheng, Z., Li, S., Yang, Y.: Camera style adaptation for person re-identification input 256*128 数据初始化：随机裁剪和随机翻转 dropout=0.5 学习率：新增的层：0.1，base：0.01，每隔40个epoch乘以0.1 mini-batch：源域上对于IDE为128，对于tripletloss是64.目标域上对于triplet loss是16. epoch=60 测试：2048-dim计算欧式距离 2.5 超参数设置 triplet loss的权重$\beta$ 一个batch中目标域上$n_t$的个数 2.5.1 参数的设置$\beta$ $\beta$应该设置成0.4-0.8 2.5.2 参数的设置$n_t$ $n_t$在当前设置(源域上对于IDE为128，对于tripletloss是64)下，应该$n_t&gt;16$ 通过上述参数的设置，结合自己实验时的错误，不妨这么理解，在固定mini-batch=128的情况下 首先引入源域的triplet_loss，并调整batch和$\beta$，使效果达到最优，,batch的选取2倍数的等间隔，$\beta$可以取等间隔，最后batch=64，即128/2=64，$\beta$则可以先固定成某个值. 然后引入目标域的triplet_loss，并且要先考虑只有目标域的性能，再考虑结合的性能，每次都需要重新考虑$\beta$和batch的大小 这么一想，这篇论文做的实验还是很多的。 2.6 实验结果 通过结果我们看出来，其实提升的效果主要来源于$L_C$，说明预训练的模型对于目标域不同摄像头的图片鲁棒性很差。 是否说明预训练的模型只学习到了源数据集的跨摄像头的不变行人特征，而对于目标域的摄像头下的不同风格很敏感，而对目标域的同一摄像头下的行人特征很鲁棒。 $L_T$的提升效果很小是否可以说明目标数据集与源数据集的行人特征空间本身就已经很好地重合了，假如tripl_loss真得具有将不同数据集的行人特征映射到同一特征空间的效果的话。 通过这篇论文，我们能学到的东西很多，比如对比实验，参数设置实验，想法验证实验等等。 附录triplet_loss发现triplet_loss很厉害的样子，不妨看看是个什么情况。 参考链接：Triplet Loss and Online Triplet Mining in TensorFlow Re-ID with Triplet Loss In Defense of the Triplet Loss for Person Re-Identification code Triplet Loss and Online Triplet Mining in TensorFlow这个博客讲述了triplet_loss的起源、发展和具体使用的几种形式。最后的结论是应该使用在线的batch hard策略。 Re-ID with Triplet Loss这篇博客则逻辑性地介绍了各种triplet_loss的变体。最后的结论是batch hard+soft margin效果更好。 也有提及到，triplet_loss总是不如分类损失强。 下一步工作已经理解源代码 3. SPGANImage-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification Weijian Deng, Liang Zheng, Guoliang Kang, Yi Yang, Qixiang Ye, Jianbin Jiao, CVPR 2018 这篇论文主要是构建”Learning via Translation”的框架来进行迁移学习，利用SPGAN(CycleGAN+Simaese net)从源数据集迁移到目标数据集，然后在目标数据集上训练。 论文的重点是怎么改进CycleGAN。 web:http://www.sohu.com/a/208231404_642762 code:https://github.com/Simon4Yan/Learning-via-Translation CycleGAN Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks code:https://github.com/zhunzhong07/CamStyle 自己对代码的分析https://tjjtjjtjj.github.io/2018/11/19/cycleGAN/#more 3.1 前言一般的无监督迁移方法都是假设源域和目标域上有相同ID的图片，不太适用于跨数据集的行人重识别。 3.2 网络架构GAN网络 LMP网络 行人重识别整体网络 网络的简要介绍 整理网络由两部分组成，第一部分是SPGAN，第二部分是常见的行人重识别网络的修改版LMP，重点是第一部分。 整个网络是用Caffe搭建。 因为自己没有仔细看caffe的代码，后期有需要的还是要看看超参数设置的。 SPGAN基本沿用了CycleGAN的设置，epoch=5，更多的epoch没有用。 SPGAN的正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$,负样本是$G(x_S)$和$x_T$或者$F(x_T)$和$x_S$，作者给的代码中用的正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$，负样本是$x_S$和$x_T$。 SPGAN的训练分为生成器、判别器、SiaNet。 $L_{ide}$可以保持转换前后图片颜色保持一致。 LMP网络直接generated domain上训练。 在论文的tabel2的注释中，可以看到是分成了7份，与PCB的6份差不多。 3.3 损失函数3.3.1 CycleGANL_{T_{adv}}(G,D_T,p_x,p_y)=E_{y\sim p_y}[(D_T(y)-1)^2]+E_{x\sim p_x}[(D_T(G(x))-1)^2]L_{S_{adv}}(F,D_S,p_x,p_y)=E_{x\sim p_x}[(D_S(x)-1)^2]+E_{y\sim p_y}[(D_S(F(y)))^2]L_{cyc}(G,F)=E_{x\sim p_x}\parallel F(G(x))-x \parallel_1+E_{y\sim p_y}\parallel G(F(y))-y\parallel_1L_{ide}(G,F,p_x,p_y)=E_{x\sim p_x}\parallel F(x)-x\parallel_1+E_{y\sim p_y}\parallel G(y)-y\parallel_13.3.2 SPGANSiameses Net: L_{con}(i,x_1,x_2)=(1-i)(max(0,m-d))^2+id^2其中，$m\in [0,2]$，$d=1-cos(\theta)\in [0,2]$表示归一化后的欧式距离.正样本是$x_S$和$G(x_S)$或者$x_T$和$F(x_T)$,负样本是$G(x_S)$和$x_T$或者$F(x_T)$和$x_S$。 Overall objective loss: L_{sp}=L_{T_{adv}}+L_{S_{adv}}+\lambda_1 L_{cyc}+\lambda_2 L_{ide}+\lambda_3 L_{con}其中，$\lambda_1=10，\lambda_2=5，\lambda_3=2, m=2$ 3.3.3 行人重识别网络以resnet50为基础网络，和PCB类似，分割成两块。 3.4 实验设置3.4.1 SPGANSPGAN的整体训练过程与CycleGAN基本是一致的，建议先参考CycleGAN，再学习SPGAN。 $\lambda_1=10，\lambda_2=5，\lambda_3=2, m=2$，学习率为0.0002，batch=1，total_epoch=5 SiaNet: 4个conv+4个max pool+1个FC。 x(3,256,256)-&gt;conv(3,64,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2) -&gt;conv(64,128,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2) -&gt;conv(128,256,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2) -&gt;conv(256,512,k=(4,4),s=2)-&gt;max pool(k=(2,2),s=2)(1,1,512) -&gt;FC(512, 128)-&gt;leak_relu(0.2)-&gt;dropout(0.5)-&gt;FC(128,64) 输入预处理：随机左右翻转、resize(286)、crop(256)、img/127.5-1。 激活函数全部使用leak_relu(0.2)，没有使用bn 1234567891011121314151617181920212223242526272829def metric_net(img, scope, df_dim=64, reuse=False, train=True): bn = functools.partial(slim.batch_norm, scale=True, is_training=train, decay=0.9, epsilon=1e-5, updates_collections=None) with tf.variable_scope(scope + '_discriminator', reuse=reuse): h0 = lrelu(conv(img, df_dim, 4, 2, scope='h0_conv')) # h0 is (128 x 128 x df_dim) pool1 = Mpool(h0, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') h1 = lrelu(conv(pool1, df_dim * 2, 4, 2, scope='h1_conv')) # h1 is (32 x 32 x df_dim*2) pool2 = Mpool(h1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') h2 = lrelu(conv(pool2, df_dim * 4, 4, 2, scope='h2_conv')) # h2 is (8 x 8 x df_dim*4) pool3 = Mpool(h2, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') h3 = lrelu(conv(pool3, df_dim * 8, 4, 2, scope='h3_conv')) # h3 is (2 x 2 x df_dim*4) pool4 = Mpool(h3, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') shape = pool4.get_shape() flatten_shape = shape[1].value * shape[2].value * shape[3].value h3_reshape = tf.reshape(pool4, [-1, flatten_shape], name = 'h3_reshape') fc1 = lrelu(FC(h3_reshape, df_dim*2, scope='fc1')) dropout_fc1 = slim.dropout(fc1, 0.5, scope='dropout_fc1') net = FC(dropout_fc1, df_dim, scope='fc2') #print_activations(net) #print_activations(pool4) return net 3.4.2 LMPbatch_size=16, total_epoch=50, SGD, momentum=0.9, gamma=0.1, lr_ini=0.001, decay to 0.0001 after 40 epochs. 这部分的设置和IDE基本类似。 3.5 对比实验模块的对比实验 通过对比实验可以看到，以mAP为指标，CycleGAN增加了3个点，SiaNet(m=2)增加了3个点，LMP增加了4个点。说明作者尝试的3个模块都在一定程度上起到了作用。但是个人感觉还是差点什么。比如，为什么会有效？ 假设目标都是为了使源域与目标域的行人特征映射到同一特征空间。这里的CycleGAN做到了这一点。LMP可以认为是加在哪里都有效的一种方式。那SiaNet其实更像是在保证生成的图片不仅要保留源图片的内容，更要保留源图片的行人特征。这种保留是以一种隐空间的形式在保存，而不是明显的分类损失这样子。 $\lambda_3 $对比实验 pool 和 part的对比实验 也就是说，pool的方式和parts的取法是实验得到的，不是凭空想出来的。 通过上述实验超参数的设置对比实验，与HHL论文比较，都是固定其他，变化一个参数，然后选取最优的参数，是基于局部最优就是全局最优的思想。感觉到作者的实验做得很足。 不同base model的对比实验 附录IDE and $IDE^+$IDE https://github.com/zhunzhong07/IDE-baseline-Market-1501 We name the descriptor as ID-discriminative Embedding (IDE).感觉还是没有很好地理解IDE。 对于IDE+没有找到对应的原文，因为不是重点，暂且跳过。 IDE的pytorch代码 https://github.com/Simon4Yan/Person_reID_baseline_pytorch IDE和$IDE^+$的网络模型是一样的： resnet50(layer4)+avgpool+Linear(2048,512)+bn1d(512)+LeakReLU(0.1)+Dropout(0.5)+Linear(512, num_class) 区别在于训练时bn层是否更新： 123456789101112131415161718# model.model = resnet50(layer4)+avgpool+Linear(2048,512)+bn1d(512)+LeakReLU(0.1)+Dropout(0.5)# model.classifier = Linear(512, num_class)# IDEfor phase in ['train', 'val']: if phase == 'train': scheduler.step() model.train(True) # Set model to training mode else: model.train(False) # Set model to evaluate mode# IDE+for phase in ['train', 'val']: if phase == 'train': scheduler.step() model.eval() # Fix BN of ResNet50 model.model.fc.train(True) model.classifier.train(True) else: model.train(False) # Set model to evaluate mode 新增 2018-12-17参考论文: Re-Identification with Consistent Attentive Siamese Networks IDE的网络架构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 一般情况下，cut_at_pooling=False，num_features=256, has_embedding为true# 一般情况下，新增了feat、feat_bn、relu、drop、classifierclass ResNet(nn.Module): __factory = &#123; 18: torchvision.models.resnet18, 34: torchvision.models.resnet34, 50: torchvision.models.resnet50, 101: torchvision.models.resnet101, 152: torchvision.models.resnet152, &#125; def __init__(self, depth, pretrained=True, cut_at_pooling=False, num_features=0, norm=False, dropout=0, num_classes=0, triplet_features=0): super(ResNet, self).__init__() self.depth = depth self.pretrained = pretrained self.cut_at_pooling = cut_at_pooling # Construct base (pretrained) resnet if depth not in ResNet.__factory: raise KeyError("Unsupported depth:", depth) self.base = ResNet.__factory[depth](pretrained=pretrained) if not self.cut_at_pooling: self.num_features = num_features self.norm = norm self.dropout = dropout self.has_embedding = num_features &gt; 0 self.num_classes = num_classes self.triplet_features = triplet_features out_planes = self.base.fc.in_features # Append new layers if self.has_embedding: self.feat = nn.Linear(out_planes, self.num_features) self.feat_bn = nn.BatchNorm1d(self.num_features) init.kaiming_normal_(self.feat.weight, mode='fan_out') init.constant_(self.feat.bias, 0) init.constant_(self.feat_bn.weight, 1) init.constant_(self.feat_bn.bias, 0) else: # Change the num_features to CNN output channels self.num_features = out_planes if self.dropout &gt; 0: self.drop = nn.Dropout(self.dropout) if self.num_classes &gt; 0: self.classifier = nn.Linear(self.num_features, self.num_classes) init.normal_(self.classifier.weight, std=0.001) init.constant_(self.classifier.bias, 0) if self.triplet_features &gt; 0: self.triplet = nn.Linear(self.num_features, self.triplet_features) init.normal_(self.triplet.weight, std=0.001) init.constant_(self.triplet.bias, 0) if not self.pretrained: self.reset_params() def forward(self, x, output_feature=None): for name, module in self.base._modules.items(): if name == 'avgpool': break x = module(x) if self.cut_at_pooling: return x x = F.avg_pool2d(x, x.size()[2:]) x = x.view(x.size(0), -1) if output_feature == 'pool5': x = F.normalize(x) return x if self.has_embedding: x = self.feat(x) x = self.feat_bn(x) if self.norm: x = F.normalize(x) elif self.has_embedding: x = F.relu(x) # triplet feature if self.triplet_features &gt; 0: x_triplet = self.triplet(x) if self.dropout &gt; 0: x = self.drop(x) if self.num_classes &gt; 0: x_class = self.classifier(x) # two outputs if self.triplet_features &gt; 0: return x_class, x_triplet return x_class def reset_params(self): for m in self.modules(): if isinstance(m, nn.Conv2d): init.kaiming_normal(m.weight, mode='fan_out') if m.bias is not None: init.constant(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): init.constant(m.weight, 1) init.constant(m.bias, 0) elif isinstance(m, nn.Linear): init.normal(m.weight, std=0.001) if m.bias is not None: init.constant(m.bias, 0) Caffe and pytorchCaffe和pytorch中的bn层的计算方式不一样。 在caffe中，bn层在训练时是eval状态，也是只使用Imagenet的mean和variance The eval mode for BN layer during training, corresponding to Caffe’s batch_norm_param {use_global_stats: true}, means using ImageNet BN mean and variance during training. 在pytorch中，bn层在训练时如果设置成eval装填，才可以达到caffe的精度。 疑惑IDE和IDE+的效果区别为什么会这么大? 下一步工作 [x] 已经理解源代码 尝试在pytorch上复现结果，现在根据作者提供的代码，感觉并不是很难。主要是SPGAN。 4. 基于GAN的类似论文类似的采取GAN做person-reid方向的论文还有好多，上面两篇是现在最新的，下面就简单地介绍几篇类似的文章，其中涉及到的原理和前文提到的GAN的方法类似。 4.1 PTGANPerson Transfer GAN to Bridge Domain Gap for Person Re-Identification Longhui Wei1, Shiliang Zhang1, Wen Gao1, Qi Tian 这篇论文对Cycle-GAN进行了改进，保留ID信息的损失函数如下： L_{ID}=E_{a \sim p_{data}(a)} [||(G(a)-a) \odot M(a)||_2] + E_{b \sim p_{data}(b)} [||(F(b)-b) \odot M(b)||_2]其中，$M(b)$表示使用PSPNet分割后的结果。 转化效果如下图所示 这里的Cycle-Gan生成图片的效果和SPGAN生成的效果还是有一些区别的，不是很理解。 其他的不是本次的重点，不做介绍。 4.2 DCGAN+CNNUnlabeled Samples Generated by GAN Improve the Person Re-Identification Baseline in Vitro Zhedong Zheng Liang Zheng Yi Yang 这篇论文主要是利用DCGAN生成新的数据集进行数据集扩充。 网络架构如图所示： 生成效果图 生成图片的标签LSRO q_{LSR}=\begin{cases} \frac{\epsilon}{K},&k\neq y\\\\ 1-\epsilon+\frac{\epsilon}{K},&k=y \end{cases}l_{LSR}=-(1-\epsilon)log(p(y))-\frac{\epsilon}{K}\sum_{k=1}^{K}log(p(k))q_{LSRO}=\frac{1}{K}l_{LSRO}=-(1-Z)log(p(y))-\frac{Z}{K}\sum_{k=1}^Klog(p(k))其中，真实图片的Z=0，生成图片的Z=1. 5. MMFAMulti-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification Shan Lin, Haoliang Li, Chang-Tsun Li, Alex Chichung Kot, BMVC 2018 5.1 前言其想法也是将源域与目标域映射到同一特征空间。创新点是： 利用MMD缩小源域与目标域的分布差异 考虑了属性 MMD的参考代码 5.2 网络架构 网络架构的说明: 每一个batch中包括$n_s$张源域图片，$n_t$张目标域图片。batch=32 backbone是resnet50，并且修改resnet50的avg_pool为max_pool $H_S$是pool层的输出向量，$H_S^{id}$是ID-FC层的输出相邻，$H_S^{attr_m}$Attr-FC-m的输出向量。 input (256,128,3) FC=fc+bn+dropout(0.5)+leaky ReLU+fc SGD:momentum=0.9,weight decay=5x10e-4 lr=0.01,每20个epoch乘以0.1 测试使用max pool的2048维向量的欧式距离 Market有27个属性，Duke有23个属性 5.3 损失函数Identity Loss: L_{id}=-\frac{1}{n_s}\sum_{i=1}^{n_S}log(p_{id}(h_{S,i}^{id},y_{S,i}))Attribute Loss: L_{attr}=-\frac{1}{M}\frac{1}{n_S}\sum_{m=1}^{M}\sum_{i=1}^{n_S}(a_{S,i}^{m}\cdot log(p_{attr}(h_{S,i}^{attr_m}, m)) -\\\\ (1-a_{S,i}^{m})\cdot log(1-p_{attr}(h_{S,i}^{attr_m}, m)))Attribute Feature Adaptation L_{AAL}=\frac{1}{M}\sum_{m=1}^{M}MMD(H_{S}^{attr_m}, H_{T}^{attr_m})^2\\\\ =\frac{1}{M}\sum_{m=1}^{M}\parallel \frac{1}{n_S}\sum_{i=1}^{n_S}\phi(h_{S,i}^{attr_m}) - \frac{1}{n_T}\sum_{i=1}^{n_T}\phi(h_{T,j}^{attr_m}) \parallel \_{H}^2 \\\\ =\frac{1}{M}\sum_{m=1}^{M}[ \frac{1}{(n_S)^2}\sum_{i=1}^{n_S}\sum_{i'=1}^{n_S}k(h_{S,i}^{attr_m}, h_{S,i'}^{attr_m})\\\\ +\frac{1}{(n_T)^2}\sum_{i=1}^{n_T}\sum_{i'=1}^{n_T}k(h_{T,i}^{attr_m}, h_{T,i'}^{attr_m})\\\\ -\frac{2}{n_S\cdot n_T}\sum_{i=1}^{n_S}\sum_{j=1}^{n_T}k(h_{S,i}^{attr_m}, h_{T,j}^{attr_m}) ]k(h_{S,i}^{attr_m}, h_{T,j}^{attr_m})=exp(-\frac{1}{2\alpha}\parallel h_{S,i}^{attr_m} - h_{T,j}^{attr_m}\parallel ^2)\alpha=1,5,10Mid-level Deep Feature Adaptation L_{MDAL}=MMD(H_S,H_T)^2Overall loss L_{all}=L_{id}+\lambda_1 L_{attr}+\lambda_2 L_{AAL}+\lambda_3 L_{MDAL}\lambda_1=0.1,\lambda_2=1,\lambda_3=15.4 实验分析5.4.1 实验结果 5.4.2 实验模块实验模块对比实验结果 5.5 附录通过实验结果可以看出，在MMFA模型中，ID+Mid-level Deep Feature Adaptation的贡献最大。 下一步可以尝试考虑Mid-level Deep Feature Adaptation。 作者把avg pool 换成max pool。 6. TJ-AIDLTransferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li, ECCV 2018 6.1 前言这篇论文的创新点在于： 根据属性和id的关系，提出了Identity Inferred Attribute Space。 6.2 网络架构Attribute-Identity Transferable Joint Learning Unsupervised Target Domain Adaptation 网络架构的简要说明： (a) Identity Branch (b) Attribute Branch (c) Identity Inferred Attribute (IIA) space 训练过程分为两步: (I) 源域训练: Attribute-Identity Transferable Joint Learning (II) 目标域微调: Unsupervised Target Domain Adaptation 一般情况下Identity Branch和Attribute Branch是共享网络，但是本论文中特意分成两个非共享网络 重点在于对$e_{IIA}$的处理 IIA-encoder 是3个fc层，512/128/m，decoder是encoder的镜像。 基准网络是MobileNet Adam优化器，lr=0.002，mementum$\beta_1=0.5, \beta_2=0.999$ batch size=8 We started with training the identity branch by 100,000 iterations on the source identity labels and then the whole model by 20,000 iterations for both transferable joint learning on the labelled source data and unsupervised domain adaptation on the unlabelled target data 6.3 损失函数Identity Branch (a) softmax L_{id}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}log(p_{id}(I_i^S,y_i^S)) \tag{1}Attribute Branch(b) sigmoid L_{att}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}\sum_{j=1}^{m}(a_{i,j}log(p_{att}(I_i,j))+(1-a_{i,j})log(1-p_{att}(I_i,j))) \tag{2}Identity Inferred Attribute (IIA) space (c) L_{rec}=\parallel x_{id}-f_{IIA}(x_{id}) \parallel ^2 \tag{3}L_{ID-transfer}=\parallel e_{IIA}-\tilde{p}\_{att} \parallel ^2 \tag{4}L_{att,IIA}=-\frac{1}{n_{bs}}\sum_{i=1}^{n_{bs}}\sum_{j=1}^{m}(a_{i,j}log(p_{IIA}(I_i,j))+(1-a_{i,j})log(1-p_{IIA}(I_i,j))) \tag{5}L_{IIA}=L_{att,IIA}+\lambda_1 L_{rec}+\lambda_2 L_{ID-transfer} \tag{6}\lambda_1=10, \lambda_2=10Impact of IIA on Identity and Attribute Branches L_{att-total}=L_{att}+\lambda_2 L_{ID-transfer} \tag{7}6.4 训练与部署流程 6.5 模块分析6.5.1 ID和Attribute模块分析 通过ID only的mAP和HHL的baseline，可以看出MobileNet和Resnet50对mAP的影响不受很大。 另外，可以看出，依然是ID占据了很大比重。 6.5.2 Adapation的作用 从表格中可以看出，Adaptation的作用很小。说明，预训练的模型已经很好地能保持属性的一致性，即不同角度得到的属性是一样的。 6.6 补充还是难以理解作者这么做的出发点，感觉有点凭空就设计出这么多损失函数，可能是哪里还缺点什么东西。 训练更新的时候方程(7)的出现原因是什么？更新(6)的时候应该就已经对attr进行了影响吧？ 在step(II)中，是怎么更新方程(6)的。 Identity Inferred Attribute Space的合理性是怎么体现的？]]></content>
      <categories>
        <category>ind1</category>
      </categories>
      <tags>
        <tag>person-reid</tag>
        <tag>transfer learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cycleGAN]]></title>
    <url>%2F2018%2F11%2F19%2FcycleGAN%2F</url>
    <content type="text"><![CDATA[这篇博客主要记录在跟随cycleGAN作者的代码复现学到的东西。title: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(ICCV2017) paper: https://arxiv.org/abs/1703.10593code: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pixmycode: https://github.com/TJJTJJTJJ/pytorch_cycleGANcycle_gan的整体框架写得很漂亮，frame可以参考github的frame 1.动态导入模块以及文件内的类类似这种文件结构.models|— init.py|— base_model.py|— cycle_gan_model.py|— networks.py|— pix2pix_model.py`— test_model.py 在init.py这样写两个函数 1234567891011121314151617181920212223242526def find_model_using_name(model_name): """ 根据model_name导入具体模型'models/model_name_model.py' :param model_name: eg. cycle_gan :return: mdoel class eg.cycle_gan_model.CycleGANModle """ # step1 import 'models/model_name_model' model_filename = 'models.'+model_name+'_model' modellib = importlib.import_module(model_filename) # step2 get model_name model = None target_model_name = model_name.replace('_','')+'model' for name, cls in modellib.__dict__.items(): if name.lower() == target_model_name.lower() \ and issubclass(cls, BaseModel): model = cls if model is None: print_str = "In &#123;model_filename&#125;.py, there should be a subclass of BaseModel with class name " \ "that matches &#123;target_model_name&#125; in lowercase.".format(model_filename=model_filename, \ target_model_name=target_model_name) print(print_str) exit(0) return model 12modellib.__dict__ == vars(modellib)vars().keys() == dir() 12345678910111213141516171819import importlibmodellib = importlib.import_module(model_filename)for k in dir(modellib): print(k)CycleGANModel__builtins____cached____doc____file____loader____name____package____spec__print(modellib.__dict__)&#123;'__name__': 'cycle_gan_model',...'CycleGANModel': cycle_gan_model.CycleGANModel&#125; 12exit(0)无错误退出exit(1)有错误退出 2.学习率直线下降123456for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):def lambda_rule(epoch): lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1) return lr_lscheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule) 3.NotImplemented &amp;&amp; NotImplementedError参考:http://www.php.cn/python-tutorials-160083.htmlhttps://stackoverflow.com/questions/1062096/python-notimplemented-constant return NotImplementedraise NotImplementedError(‘initialization method [%s] is not implemented’ % init_type) 4.parser的修改这里既有外界传入的参数,也有自己的参数isTrain,在主函数里调用的时候调用方式是一致的,只是一个可以通过外界传参,一个不能通过外界传参 12345678910111213141516class TrainOptions(): def initialize(self): parser = argparse.ArgumentParser( formatter_class=argparse.ArgumentDefaultsHelpFormatter ) parser.add_argument('--batch_size', type=int, default=1, help='input batch size') parser.set_defaults(dataset_mode='single') opt, _ = parser.parse_known_args() self.isTrain = True return opt def parse(self): opt = self.initialize() opt.isTrain = self.isTrainopt = TrainOptions().parse() 5.eval()和test()函数的结合12345678910111213141516171819202122def eval(self): """ make models eval mode during test time :return: None """ for name in self.model_names: if isinstance(name, str): net = getattr(self, 'net'+name) net.eval() return Nonedef test(self): """ don't need backprop during test time :return: """ with torch.no_grad(): self.forward()model.eval()model.test() 6.多GPU结论12345678910111213141516171819202122232425262728293031323334353637modelb = torch.nn.DataParallel(modela, device_ids=[0,1,2])# savetorch.save(modelb.module.cpu().state_dict(),path)modelb.cuda(gpu_ids[0])# loaddef load_networks(self, epoch): for name in self.model_names: if isinstance(name, str): load_filename = '%s_net_%s.pth' % (epoch, name) load_path = os.path.join(self.save_dir, load_filename) net = getattr(self, 'net' + name) if isinstance(net, torch.nn.DataParallel): net = net.module print('loading the model from %s' % load_path) # if you are using PyTorch newer than 0.4 (e.g., built from # GitHub source), you can remove str() on self.device state_dict = torch.load(load_path, map_location=str(self.device)) if hasattr(state_dict, '_metadata'): del state_dict._metadata # patch InstanceNorm checkpoints prior to 0.4 for key in list(state_dict.keys()): # need to copy keys here because we mutate in loop self.__patch_instance_norm_state_dict(state_dict, net, key.split('.')) net.load_state_dict(state_dict)# ortorch.nn.DataParallel加载预训练模型import torchclass ModelA(torch.nn.Module): def __init__(self): super(ModelA, self).__init__() self.base1 = torch.nn.Conv2d(2,2,2) def forward(self,x): passaa = ModelA()bb = torch.nn.DataParallel(aa, device_ids=[0])bb.module.load_state_dict(torch.load('aa.pth')) 对于单gpu和Module对于普通的model.cuda,在保存模型会自动变成cpu,需要再次cuda一次对于DataParallel,在保存模型会自动变成cpu,需要再次cuda一次通过源码可以得知,DataParallel的device_ids初始化就已经确定,所以不用担心cuda到第一个GPU上而导致DataParallel忘记自己可以复制到哪些GPU上,会自动复制的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124import torchclass ModelA(torch.nn.Module): def __init__(self): super(ModelA, self).__init__() self.base = torch.nn.Conv2d(2,2,2,2) def forward(self,x): passaa = ModelA()print(aa)ModelA( (base): Conv2d(2, 2, kernel_size=(2, 2), stride=(2, 2)))print(aa.state_dict())OrderedDict([('base.weight', tensor([[[[ 0.0119, 0.2522], [-0.0682, 0.2366]], [[ 0.2013, 0.2106], [ 0.2242, 0.1711]]], [[[-0.2777, 0.2446], [ 0.3494, 0.1552]], [[ 0.0270, 0.1272], [-0.1878, -0.3501]]]])), ('base.bias', tensor([ 0.1433, 0.1061]))])aa.cuda()print(aa.state_dict())OrderedDict([('base.weight', tensor([[[[ 0.0119, 0.2522], [-0.0682, 0.2366]], [[ 0.2013, 0.2106], [ 0.2242, 0.1711]]], [[[-0.2777, 0.2446], [ 0.3494, 0.1552]], [[ 0.0270, 0.1272], [-0.1878, -0.3501]]]], device='cuda:0')), ('base.bias', tensor([ 0.1433, 0.1061], device='cuda:0'))]print(aa.cpu().state_dict())OrderedDict([('base.weight', tensor([[[[ 0.1570, -0.2992], [-0.2927, -0.2748]], [[-0.0097, 0.0346], [-0.3125, 0.2615]]], [[[-0.2506, -0.2632], [ 0.1302, -0.2223]], [[ 0.1422, 0.0427], [ 0.3453, 0.0219]]]])), ('base.bias', tensor([ 0.1974, -0.1549]))])print(aa.state_dict())OrderedDict([('base.weight', tensor([[[[ 0.1570, -0.2992], [-0.2927, -0.2748]], [[-0.0097, 0.0346], [-0.3125, 0.2615]]], [[[-0.2506, -0.2632], [ 0.1302, -0.2223]], [[ 0.1422, 0.0427], [ 0.3453, 0.0219]]]])), ('base.bias', tensor([ 0.1974, -0.1549]))])bb = torch.nn.DataParallel(aa, device_ids=[0])print(bb)DataParallel( (module): ModelA( (base): Conv2d(2, 2, kernel_size=(2, 2), stride=(2, 2)) ))print(bb.state_dict())OrderedDict([('module.base.weight', tensor([[[[ 0.0119, 0.2522], [-0.0682, 0.2366]], [[ 0.2013, 0.2106], [ 0.2242, 0.1711]]], [[[-0.2777, 0.2446], [ 0.3494, 0.1552]], [[ 0.0270, 0.1272], [-0.1878, -0.3501]]]], device='cuda:0')), ('module.base.bias', tensor([ 0.1433, 0.1061], device='cuda:0'))])print(bb.module.cpu().state_dict())OrderedDict([('base.weight', tensor([[[[ 0.1570, -0.2992], [-0.2927, -0.2748]], [[-0.0097, 0.0346], [-0.3125, 0.2615]]], [[[-0.2506, -0.2632], [ 0.1302, -0.2223]], [[ 0.1422, 0.0427], [ 0.3453, 0.0219]]]])), ('base.bias', tensor([ 0.1974, -0.1549]))])print(bb)DataParallel( (module): ModelA( (base): Conv2d(2, 2, kernel_size=(2, 2), stride=(2, 2)) ))print(bb.state_dict())OrderedDict([('base.weight', tensor([[[[ 0.0119, 0.2522], [-0.0682, 0.2366]], [[ 0.2013, 0.2106], [ 0.2242, 0.1711]]], [[[-0.2777, 0.2446], [ 0.3494, 0.1552]], [[ 0.0270, 0.1272], [-0.1878, -0.3501]]]])), ('base.bias', tensor([ 0.1433, 0.1061]))])bb.cuda(gpu_ids[0])print(bb.state_dict())OrderedDict([('module.base.weight', tensor([[[[ 0.0119, 0.2522], [-0.0682, 0.2366]], [[ 0.2013, 0.2106], [ 0.2242, 0.1711]]], [[[-0.2777, 0.2446], [ 0.3494, 0.1552]], [[ 0.0270, 0.1272], [-0.1878, -0.3501]]]], device='cuda:0')), ('module.base.bias', tensor([ 0.1433, 0.1061], device='cuda:0'))]) 7.Norm参考：https://blog.csdn.net/liuxiao214/article/details/81037416 输入图像：[N,C,H,W]BatchNorm: [1,C,1,1]InstanceNorm: [N,C,1,1] 经过实验,instanceNorm层的weight, bias, running_mean, running_var总是None代码中加载模型的时候对instanceNorm层进行了删除操作,为什么对于pytorch之前的版本instanceNorm层是有running_mean和running_var的,之后的版本修正了之后,就不再需要了 123456789101112def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0): key = keys[i] if i + 1 == len(keys): # at the end, pointing to a parameter/buffer if module.__class__.__name__.startswith('InstanceNorm') and \ (key == 'running_mean' or key == 'running_var'): if getattr(module, key) is None: state_dict.pop('.'.join(keys)) if module.__class__.__name__.startswith('InstanceNorm') and \ (key == 'num_batches_tracked'): state_dict.pop('.'.join(keys)) else: self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1) 8.functools偏函数：适合为多个调用函数提供一致的函数接口 123456789101112from functools import partialdef f(m,n,p): return m*n*pre=partial(f,3,4)print(re(5)# 60if type(norm_layer) == functools.partial: use_bias = norm_layer.func == nn.InstanceNorm2delse: use_bias = norm_layer == nn.InstanceNorm2d 9.论文与代码ndf模型的定义与论文有一个地方不一致,论文写的第一个conv之后通道数是32,但实现是64.与作者沟通得知,第一层不是32,而是64,剩下的也依次递增. 下采样的时候没有使用reflect进行补充,而是使用了0填充.与作者沟通后，提出的是都可以尝试一下 unet modelUnet model与网上的不是很一致3-&gt;1-&gt;2-&gt;4-&gt;8-&gt;8-&gt;83&lt;-2&lt;-4&lt;-8&lt;-16&lt;-16&lt;-16 参数 no_lsgan12345678910111213141516self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan).to(self.device)# GAN lossclass GANLoss(nn.Module): def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0): super(GANLoss, self).__init__() self.register_buffer('real_label', torch.tensor(target_real_label)) self.register_buffer('fake_label', torch.tensor(target_fake_label)) if use_lsgan: self.loss = nn.MSELoss() else: self.loss = nn.BCELoss()use_sigmoid = opt.no_lsganself.netD_A = define_D(opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids) 也就是opt.no_lsgan为True时, netD使用sigmoid, GANloss使用BCELoss()opt.no_lsgan为False时, netD不使用sigmoid, GANloss使用MSELoss() MSELoss:均方误差 (x-y)*2BCELoss:二分类的交叉熵:使用前需要使用sigmoid函数,input和target的输入维度是一样的.(N,) 根据作者提供的运行代码,猜测作者使用的是opt.no_lsgan为False,均方误差 L1loss: |x-y| G and D 的反向传播过程回顾一下G和D的反向传播train G 123456set_requires_grad(D, False)fake_A = G(real_A)loss = criterion(D(fake_A), True)optimizers_G.zero_grad()loss.backward()optimizers_G.step() train D 12345678set_requires_grad(D, False)#fake_A = fake_A.detach() # 取消G的gradloss1 = criterion(fake_A.detach(), False)loss2 = criterion(realA, True)loss = loss1 + loss2optimizers_D.zero_grad()loss.backward()optimizers_D.step() 10.ConTransposed的计算方法逆卷积后的图像大小和之前的能对应上，需要output_padding 1234nn.ConvTranspose2d(ngf*mult, int(ngf*mult/2), kernel_size=3,stride=2, padding=1, output_padding=1, bias=use_bias)]-k+2p+s-out_padding是s的整数k=3,s=2,p=1,则out_padding=1k=3,s=4,p=1,则out_padding=3 11.初始化参数123456789101112131415161718192021222324def init_weights(net, init_type='normal', gain=0.02): def init_func(m): # conv, contranspose ,linear, bn # type(m) == nn.Conv2d classname = m.__class__.__name__ if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1): if init_type == 'normal': init.normal_(m.weight.data, 0.0, gain) elif init_type == 'xavier': init.xavier_normal_(m.weight.data, gain=gain) elif init_type == 'kaiming': init.kaiming_normal_(m.weight.data, a=0, mode='fan_in') elif init_type == 'orthogonal': init.orthogonal_(m.weight.data, gain=gain) else: raise NotImplementedError('initialization method [%s] is not implemented' % init_type) if hasattr(m, 'bias') and m.bias is not None: init.constant_(m.bias.data, 0.0) elif classname.find('BatchNorm2d') != -1: init.normal_(m.weight.data, 1.0, gain) init.constant_(m.bias.data, 0.0) print('initialize network with %s' % init_type) net.apply(init_func) 12.disriminator PatchGAN and GANLossPatchGAN的kernel是4. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class GANLoss(nn.Module): def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0): super(GANLoss, self).__init__() self.register_buffer('real_label', torch.tensor(target_real_label)) self.register_buffer('fake_label', torch.tensor(target_fake_label)) if use_lsgan: self.loss = nn.MSELoss() else: self.loss = nn.BCELoss() def get_target_tensor(self, input, target_is_real): if target_is_real: target_tensor = self.real_label else: target_tensor = self.fake_label return target_tensor.expand_as(input) def __call__(self, input, target_is_real): target_tensor = self.get_target_tensor(input, target_is_real) return self.loss(input, target_tensor)# Defines the PatchGAN discriminator with the specified arguments.class NLayerDiscriminator(nn.Module): def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False): super(NLayerDiscriminator, self).__init__() if type(norm_layer) == functools.partial: use_bias = norm_layer.func == nn.InstanceNorm2d else: use_bias = norm_layer == nn.InstanceNorm2d kw = 4 padw = 1 sequence = [ nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True) ] nf_mult = 1 nf_mult_prev = 1 for n in range(1, n_layers): nf_mult_prev = nf_mult nf_mult = min(2**n, 8) sequence += [ nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True) ] nf_mult_prev = nf_mult nf_mult = min(2**n_layers, 8) sequence += [ nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True) ] sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)] if use_sigmoid: sequence += [nn.Sigmoid()] self.model = nn.Sequential(*sequence) def forward(self, input): return self.model(input)pred_real = netD(real)loss_D_real = self.criterionGAN(pred_real, True) GANLoss的备注使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.call(input)，在call函数中，主要调用的是 layer.forward(x)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。 13.PatchGAN的感受野论文使用的是70X70 PatchGANPatchGAN:paper:Image-to-Image Translation with Conditional Adversarial Networkshttps://arxiv.org/abs/1611.07004 自动计算网址：https://fomoro.com/tools/receptive-fields/ 1234567891011121314151617181920212223242526感受野的计算规则对于第m层,m=0,1,2,...,N. hm表示第m层应该有的视野,假设mN=1km, sm, pm,表示第m-1层到第m层的conv的kernel第一层对于第0层的感受野h1 = 1, (h0-k1)/s1+1=h1第二层对于第0层的感受野h2 = 1(h1-k2)/s2+1=h2(h0-k1)/s1+1=h1依次类推def f(output_size, ksize, stride): return (output_size - 1) * stride + ksizelast_layer = f(output_size=1, ksize=4, stride=1)# Receptive field: 4fourth_layer = f(output_size=last_layer, ksize=4, stride=1)# Receptive field: 7third_layer = f(output_size=fourth_layer, ksize=4, stride=2)# Receptive field: 16second_layer = f(output_size=third_layer, ksize=4, stride=2)# Receptive field: 34first_layer = f(output_size=second_layer, ksize=4, stride=2)# Receptive field: 70print(first_layer) 14.torch.tensor.clone()clone()梯度受影响,clone之后的新的tensor的梯度也会影响到原tensor,但是新tensor本身是没有梯度的.clone之后的新tensor的改变不会影响原有的tensor应该这么理解,clone也是计算图中的一个操作,这样的话就可以解释通了. 1234567891011121314151617import torchinput = torch.ones(3,3,3,3)input.requires_grad = Trueinput2 = input.clone()print(input2.requires_grad)y = input.sum()y.backward()print(input.grad)# 1,1,1...print(input2.grad)# Noney = input2.sum()y.backward()print(input.grad)# 2,2,2...print(input2.grad)# None 1234567891011121314151617181920212223import torchinput = torch.ones(3,3)input.requires_grad = Trueinput2 = input.clone()input2[1,1] = 6print(input2)print(input)tensor([[ 2., 2., 2.], [ 2., 6., 2.], [ 2., 2., 2.]])tensor([[ 2., 2., 2.], [ 2., 2., 2.], [ 2., 2., 2.]])input2[1,1] = 6y = (input2*input2).sum()y.backward()print(input.grad)print(input2.grad)tensor([[ 8., 8., 8.], [ 8., 4., 8.], [ 8., 8., 8.]])None 12print(input2.grad_fn)&lt;CopySlices object at 0x7fe89dc841d0&gt; clone的用法tensor保留梯度的交换 123456789tmp = tensor1.clone()tensor2 = tmptensor1 = tensor3# ortensor1, tensor2 = tensor3, tensor1.clone()# ortmp = self.images[random_id].clone()self.images[random_id] = imagereturn_images.append(tmp) 15.from XX import这里还有一些不太对的地方 12345from .base_model import BaseModel # 同一个文件夹from util.image_pool import # 父级文件夹# 建议from .base_model import BaseModel # 同一个文件夹from ..util.image_pool import # 父级文件夹 16.register_bufferregister_bufferself.register_buffer可以将tensor注册成buffer，在forward中使用self.mybuffer, 而不是self.mybuffer_tmp.定义Parameter和buffer都只需要传入Tensor即可。也不需要将其转成gpu。这是因为，当网络进行.cuda()时候，会自动将里面的层的参数,buffer等转换成相应的GPU上。网络存储时也会将buffer存下，当网络load模型时，会将存储的模型的buffer也进行赋值。buffer的更新在forward中，optim.step只能更新nn.Parameter类型的参数。用法self.register_buffer(‘running_mean’, torch.zeros(num_features)) 17. itertools无限迭代器itertools，用于创建高效迭代器的函数,itertools.chain 连接多个列表或者迭代器。将多个网络写在一起,使用一个优化器 1234self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999)) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 自然数无限迭代器# itertools.count&gt;&gt;&gt; import itertools&gt;&gt;&gt; natuals = itertools.count(1)&gt;&gt;&gt; for n in natuals:... print n...123...# 序列无限重复&gt;&gt;&gt; import itertools&gt;&gt;&gt; cs = itertools.cycle('ABC') # 注意字符串也是序列的一种&gt;&gt;&gt; for c in cs:... print c...'A''B''C''A''B''C'...# 单元素无限重复&gt;&gt;&gt; ns = itertools.repeat('A', 10)&gt;&gt;&gt; for n in ns:... print n...打印10次'A'# 无限迭代器中截取有限序列&gt;&gt;&gt; natuals = itertools.count(1)&gt;&gt;&gt; ns = itertools.takewhile(lambda x: x &lt;= 10, natuals)&gt;&gt;&gt; for n in ns:... print n...打印出1到10# 迭代对象的串联for c in itertools.chain('ABC', 'XYZ'): print c# 迭代效果：'A' 'B' 'C' 'X' 'Y' 'Z'# 迭代器中相邻的重复元素挑出来放在一起&gt;&gt;&gt; for key, group in itertools.groupby('AAABBBCCAAA'):... print key, list(group) # 为什么这里要用list()函数呢？...A ['A', 'A', 'A']B ['B', 'B', 'B']C ['C', 'C']A ['A', 'A', 'A']# imap, ifilter&gt;&gt;&gt; for x in itertools.imap(lambda x, y: x * y, [10, 20, 30], itertools.count(1)):... print x...104090 18.visdom123self.vis = visdom.Visdom(server=opt.display_server, port=opt.display_port, env=opt.display_env, raise_exceptions=True, use_incoming_socket=False)# env根据_自动分层。e.g. cycle_gan--&gt;cycle,cycle_gan 19.三引号12345678910三引号的作用&gt;&gt;&gt; str1 = """List of name:... Hua Li... Chao Deng... &#123;&#125;... """.format('hhh')&gt;&gt;&gt; print(str1)List of name:Hua LiChao Deng 20.异常1234567891011121314151617181920212223242526272829# 1.触发异常def mye( level ): if level &lt; 1: raise Exception("Invalid level!") # 触发异常后，后面的代码就不会再执行try: mye(0) # 触发异常except Exception as err: print(1,err)else: print(2)1 Invalid level!# 2.自定义异常class MyException(Exception): def __init__(self,message): Exception.__init__(self) self.message=message print('This is MyException')a=7if a&lt;10: try: raise MyException("my excepition is raised ") except MyException as e: print('*****************') print(e.message) This is MyExceptionmy excepition is raised 21.自定义类的iter12345678910111213141516171819202122232425# 自定义类的iterclass cl1(): def __init__(self): self.N = 10 def __iter__(self): for i in range(10): print(i) if i &lt; 5: yield i else: breakcc = cl1()for i in cc: print('hhh',i)0hhh 01hhh 12hhh 23hhh 34hhh 45]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>cycleGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow]]></title>
    <url>%2F2018%2F11%2F05%2Ftensorflow%2F</url>
    <content type="text"><![CDATA[这是一篇关于tensorflow的博客，这里面很多东西都是很杂碎的，不在此做处理，等积累的多了，理解才能正确。 TensorFlow入门教程 1.TensorFlow深度学习应用实践评价不好 TensorFlow：实战Google深度学习框架（第2版）8.6分，可以用来实践 Tensorflow：实战Google深度学习框架8.4分 4.莫烦的tensorlfow教程https://github.com/MorvanZhou适合实践 5.某个网友的自己实现的教程https://www.jianshu.com/p/27a2fb320934https://github.com/zhaozhengcoder/Machine-Learning/tree/master/tensorflow_tutorials 6.官网APIhttps://tensorflow.google.cn/api_docs/python/tf 7.深度学习之TensorFlow入门、原理与进阶实战7.6分22章，内容更加详实，偏向理论，可以用来只看不实践 8.TensorFlow实战7.3分适合看看，内容不深，实践性不强，理论也很浅在github上也没有代码 不应该总是要求全部，所以应该这样的顺序来学习先学：TensorFlow：实战Google深度学习框架（第2版）再学：莫烦：https://github.com/MorvanZhou+网页的教程基本就可以了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758print(a.graph == tf.get_default_graph())tf.get_variable() name 必须双引号name的作用 https://blog.csdn.net/xiaohuihui1994/article/details/81022043可以理解成sess需要指定，不能自动加入.run,.eval能执行的两种方式with tf.Session(graph=g1) as sess: tf.global_variables_initializer().run()或者sess = tf.Session()tf.global_variables_initializer().run(session=sess)或者sess = tf.InteractiveSession() # 会自动注册为默认会话result.eval()或者sess = tf.Session()with sess.as_default(): result.eval()####初始化init = tf.global_variables_initializer()w1.initializer『TensorFlow』使用集合collection控制variableshttps://www.cnblogs.com/hellcat/p/9006904.htmlcollectionimport tensorflow as tfg1 = tf.Graph()with g1.as_default(): v = tf.get_variable("v", [1], initializer = tf.zeros_initializer()) # 设置初始值为0 gv= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES # gv = tf.global_variables() for var in gv: print(var)现在有了几个概念需要理清楚：计算图： 不同计算图中的变量是独立的collection： 不同类型的variable放在不同的collection中，主要是tf.GraphKeys.GLOBAL_VARIABLES和tf.GraphKeys.TRAINABLE_VARIABLES会话： 会话需要与计算图相连接，完成相应计算图的执行，一个会话对应一个计算图及其执行结果tf.add_to_collectionhttps://www.jianshu.com/p/6612f368e8f4这样就不需要传入weighs和biases，这里的reuse实现了定义和使用的一体化，不需要专门对weights定义和调用。def inference(input_tensor, reuse=False): with tf.variable_scope('layer1', reuse=reuse): weights = tf.get_variable("weights") biases = tf.get_variable("biases") with tf.variable_scope('layer2', reuse=reuse): weights = tf.get_variable("weights") biases = tf.get_variable("biases")TFRecord数据格式https://blog.csdn.net/u012759136/article/details/52232266]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-cuda]]></title>
    <url>%2F2018%2F11%2F05%2Fpytorch-cuda%2F</url>
    <content type="text"><![CDATA[关于pytorch中模型的多GPU 1.cudnn.benchmark = True12from torch.backends import cudnncudnn.benchmark = True 而且大家都说这样可以增加程序的运行效率。那到底有没有这样的效果，或者什么情况下应该这样做呢？总的来说，大部分情况下，设置这个 flag 可以让内置的 cuDNN 的 auto-tuner 自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。 一般来讲，应该遵循以下准则： 如果网络的输入数据维度或类型上变化不大，设置 torch.backends.cudnn.benchmark = true 可以增加运行效率； 如果网络的输入数据在每次 iteration 都变化的话，会导致 cnDNN 每次都会去寻找一遍最优配置，这样反而会降低运行效率。 这下就清晰明了很多了。 其实看完这段还是很蒙蔽，不知道具体什么情况下使用，暂且算加速过程好了。 2. nn.DataParallel12345from torch import nnmodel2 = nn.DataParallel(model1)model2.cuda()` 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from torch import nnimport torch as tclass Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(" In Model: input size", input.size(), "output size", output.size()) return output----model1 = Model(3,4)print(model1)for var in model1.parameters(): print(var)Model( (fc): Linear(in_features=3, out_features=4, bias=True))Parameter containing:tensor([[ 0.1964, 0.4389, -0.2216], [-0.1046, -0.2055, -0.5383], [ 0.0673, 0.0949, 0.5205], [ 0.5473, -0.3700, -0.4179]])Parameter containing:tensor([ 0.2416, 0.4188, -0.0096, 0.1569])----model2 = nn.DataParallel(model1)print(model2)for var in model2.parameters(): print(var)DataParallel( (module): Model( (fc): Linear(in_features=3, out_features=4, bias=True) ))Parameter containing:tensor([[ 0.1964, 0.4389, -0.2216], [-0.1046, -0.2055, -0.5383], [ 0.0673, 0.0949, 0.5205], [ 0.5473, -0.3700, -0.4179]], device='cuda:0')Parameter containing:tensor([ 0.2416, 0.4188, -0.0096, 0.1569], device='cuda:0')----model2.cuda()print(model2)for var in model2.parameters(): print(var)DataParallel( (module): Model( (fc): Linear(in_features=3, out_features=4, bias=True) ))Parameter containing:tensor([[ 0.1964, 0.4389, -0.2216], [-0.1046, -0.2055, -0.5383], [ 0.0673, 0.0949, 0.5205], [ 0.5473, -0.3700, -0.4179]], device='cuda:0')Parameter containing:tensor([ 0.2416, 0.4188, -0.0096, 0.1569], device='cuda:0')]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beyond-part-models]]></title>
    <url>%2F2018%2F11%2F05%2Fbeyond-part-models%2F</url>
    <content type="text"><![CDATA[PCB:https://github.com/huanghoujing/beyond-part-models1234567891011121314151617181920212223242526272829303132333435363738.├── bpm│ ├── dataset│ │ ├── Dataset.py│ │ ├── __init__.py│ │ ├── Prefetcher.py│ │ ├── PreProcessImage.py│ │ ├── TestSet.py│ │ └── TrainSet.py│ ├── __init__.py│ ├── model│ │ ├── __init__.py│ │ ├── PCBModel.py│ │ └── resnet.py│ └── utils│ ├── dataset_utils.py│ ├── distance.py│ ├── __init__.py│ ├── metric.py│ ├── re_ranking.py│ ├── utils.py│ └── visualization.py├── example_rank_lists_on_Market1501│└── script ├── dataset │ ├── combine_trainval_sets.py │ ├── mapping_im_names_duke.py │ ├── mapping_im_names_market1501.py │ ├── transform_cuhk03.py │ ├── transform_duke.py │ └── transform_market1501.py └── experiment ├── train_pcb.py └── visualize_rank_list.pybpm:正式的模型训练script:主要用于数据的预处理和训练的对外借口 以market1501为例 数据的预处理第一个表示id，第二个表示cam，第三个表示同id同cam的第几张图片，对zip_file中的*.jpg移动到save_dir+images中，并且重命名，将所有图片重命名保存到save_dir+images 分为 ——trainval name+label |——train name+label |——val name+mask |——query 0 |——gallery 1 ——test name+mask |——query 0 |——multi-query 2 |——gallery 1 保存到save_dir, ‘partitions.pkl’中 trainval提取val的时候，val中的id只提取100个id，并且会自动跳过只在一个cam下的id。123456789partitions: dict &#123;'trainval_im_names': train_test_split['trainval_im_names'], 'trainval_ids2labels': trainval_ids2labels, 'train_im_names': train_im_names, 'train_ids2labels': train_ids2labels, 'val_im_names': val_im_names, 'val_marks': val_marks, 'test_im_names': test_im_names, 'test_marks': test_marks&#125; 数据集对于数据集是怎么加载、转化的，还是没有头绪，写法之前没有遇到过，这一部分有待提高 模型模型是一致的 模型训练test好像是re-ranking了val没有re-ranking lr: 0.1 0.01factor: 0.1epochs: 60staircase_decay_at_epochs: 41]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-2.0]]></title>
    <url>%2F2018%2F11%2F05%2Fpython-2.0%2F</url>
    <content type="text"><![CDATA[python2.0这也是关于python的一些记录，与其他python不矛盾，也不一定相互独立。 1.python+matlabpython和matlab关于.mat数据的交换 123scipy.io.loadmat(file_name, mdict=None, appendmat=True, **kwargs)scipy.io.savemat(file_name, mdict, appendmat=True, format='5', long_field_names=False, do_compression=False, oned_as='row') 2.python与文件IO主要参考：python cookbook 2.1 文本.txt1234567891011121314151617181920212223242526# t:rt模式下，python在读取文本时会自动把\r\n转换成\n.# read# read the entire file as a single stringwith open('some.txt', 'rt') as f: data = f.read()# read lineswith open('some.txt', 'rt') as f: for line in f:#writewith open('some.txt', 'wt') as f: f.write(text1)with open('some.txt', 'wt') as f: print(line1, file=f)# 换行模式，默认情况下，python会自动识别，或者传入newline,# newline可以取的值有None, \n, \r, ‘\r\n'，用于区分换行符，但是这个参数只对文本模式有效；with open('some.txt', 'rt', newline='') as f: pass# 编码错误问题 errors: replace, ignorewith open('sample.txt', 'rt', encoding='ascii', errors='replace') as f: pass 2.2 print的分隔符与行尾符1234567# print seq endprint('ACME', 50, seq=',', end='!!\n')ACME,50!!row = ('ACME', 50)print(*row, seq=',', end='!!\n')ACME,50!! 2.3 二进制数据1234567# 二进制数据，比如图片、声音等with open('some.bin', 'rb') as f: data = f.read()with open('some.bin', 'wb') as f: f.write(b'hello') 2.4 模拟普通文件123456789101112131415# 模拟文本文件import ios = io.StringIO()# s = io.StringIO('hello world\n')s.write('hello world\n')print('this is a test', file=s)s.getvalue()'hello world\nthis is a test's.read(4)s.read()# 模拟二进制文件s = io.BytesIO()s.write(b'binary data')s.getvalue()b'binary data' 2.5 压缩文件12345678910111213141516# gzip,bz2import gzipwith gzip.open('some.gz', 'rt') as f: text = f.read()with gzip.open('some.gz', 'wt') as f: f.write(text)-------------------------------------import bz2with bz2.open('some.bz2', 'rt') as f: text = f.read()with bz2.open('some.bz2', 'wt') as f: f.write(text)------------------------------------from zipfile import ZipFilewith ZipFile(zip_file) as z: z.extractall(path=save_dir) 2.6 csv1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# csv# 其实namedtuple继承自OrderedDict有序字典# read------import csvfrom collections import namedtuplewith open('stock.csv') as f: # 第一种：row是列表，访问：row[0] f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: pass # 第二种：命名元组，访问：Row.Symbol f_csv = csv.reader(f) headers = next(f_csv) # headers = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) ] Row = namedtuple('Row', headers) for r in f_csv: row = Row(*r) pass # 第三种：字典序列，访问：row['Sysbol'] f_csv = csv.DictReader(f) for row in f_csv: pass# write-------# 第一种：类表headers = ['Symbol','Price','Date','Time','Change','Volume']rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ]with open('stock.csv', 'w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows)# 第二种：字典headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']rows = [&#123;'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800&#125;, &#123;'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500&#125;, &#123;'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000&#125;, ]with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows)# 以tab分割-----with open('stock.tsv') as f: f_tsv = csv.reader(f, delimiter='\t') for row in f_tsv: pass# 类型转换---------# 第一种：tuplecol_types = [str, float, str, str, float, int]with open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: # Apply conversions to the row items row = tuple(convert(value) for convert, value in zip(col_types, row))# 第二种：dictprint('Reading as dicts with type conversion')field_types = [ ('Price', float), ('Change', float), ('Volume', int) ]with open('stocks.csv') as f: for row in csv.DictReader(f): row.update((key, conversion(row[key])) for key, conversion in field_types) print(row)# 高级操作--------# pandas.read_csv() 2.7 json12345678910111213141516# json# write &amp; read--------------# 第一种 dict----strimport jsondata = &#123; 'name' : 'ACME','shares' : 100, 'price' : 542.23 &#125;json_str = json.dumps(data)daa = json.loads(json_str)# 第二种 dict----filewith open('data.json', 'w') as f: json.dump(data, f)with open('data.json', 'r') as f: data = json.load(f) 3. sys.pathsys.path:动态地改变Python搜索路径 123456import syssys.path.append(’引用模块的地址')#或者import syssys.path.insert(0, '引用模块的地址') 4. os.path12345678910111213os.path.abspath(path) #返回绝对路径os.path.basename(path) #返回文件名os.path.exists(path) #路径存在则返回True,路径损坏返回Falseos.path.dirname(path) #返回文件路径os.path.expanduser(path) #把path中包含的"~"和"~user"转换成用户目录os.path.isabs(path) #判断是否为绝对路径os.path.isfile(path) #判断路径是否为文件os.path.isdir(path) #判断路径是否为目录os.path.join(path1[, path2[, ...]]) #把目录和文件名合成一个路径os.path.samefile(path1, path2) #判断目录或文件是否相同os.path.split(path) #把路径分割成dirname和basename，返回一个元组os.path.splitext(path) #分割路径，返回路径名和文件扩展名的元组os.path.walk(path, visit, arg) 5. glob.glob12345import globlistglob = []listglob = glob.glob(r"/home/xxx/picture/*.png")listglob.sort()print(listglob) 6. argparse参考链接： https://blog.csdn.net/guoyajie1990/article/details/76739977 [x] 2018-01-15 自定义类型 [x] 2018-12-23 完善nargs 不是很适合交互式调试 命令行参数分为位置参数和选项参数 位置参数12345678910import argparseparser = argparse.ArgumentParser(description="say something about this application !!")parser.add_argument('name', type=int, help="i can tell you how to set a name argument")result = parser.parse_args()print(result.name)$python main.py taylortaylor 选项参数12345678910import argparseparser = argparse.ArgumentParser(description="say something about this application !!")parser.add_argument("-a","--age", help="this is an optional argument")result = parser.parse_args()print(result.age)$python main.py --age 888888$python main.py --age=888888 特殊的选项参数起着开关的作用:action=”store_true” 12345678import argparseparser = argparse.ArgumentParser(description="say something about this application !!")parser.add_argument("-a", "--age", help="this is an optional argument", action="store_true")result = parser.parse_args()print(result.age)$python main.py -aTrue 指定选项：choices=[0, 1, 2] 123456import argparseparser = argparse.ArgumentParser(description="say something about this application !!")parser.add_argument("-a", "--age", help="this is an optional argument", type=int, choices=[0, 1, 2])result = parser.parse_args()print(result.age)1 nargs N (整数) N个命令行参数被保存在一个list中 ‘?’ 如果存在该参数且给出了参数值，则从命令行取得该参数，如果存在该参数但未给出参数值，则从const关键字中取得参数值，如果不存在该参数，则将生成默认值。 ‘*’命令行参数被保存在一个list中 ‘+’命令行参数被保存在一个list中，要求至少要有一个参数，否则报错 argparse.REMAINDER 其余的命令行参数保存到一个list中 123456789101112131415161718192021222324252627282930313233343536373839404142# N&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument('--foo', nargs=2)&gt;&gt;&gt; parser.add_argument('bar', nargs=1)&gt;&gt;&gt; parser.parse_args('c --foo a b'.split())Namespace(bar=['c'], foo=['a', 'b'])# ?&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument('--foo', nargs='?', const='c', default='d')&gt;&gt;&gt; parser.add_argument('bar', nargs='?', default='d')&gt;&gt;&gt; parser.parse_args('XX --foo YY'.split())Namespace(bar='XX', foo='YY')&gt;&gt;&gt; parser.parse_args('XX --foo'.split())Namespace(bar='XX', foo='c')&gt;&gt;&gt; parser.parse_args(''.split())Namespace(bar='d', foo='d')# *&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument('--foo', nargs='*')&gt;&gt;&gt; parser.add_argument('--bar', nargs='*')&gt;&gt;&gt; parser.add_argument('baz', nargs='*')&gt;&gt;&gt; parser.parse_args('a b --foo x y --bar 1 2'.split())Namespace(bar=['1', '2'], baz=['a', 'b'], foo=['x', 'y'])# +&gt;&gt;&gt; parser = argparse.ArgumentParser(prog='PROG')&gt;&gt;&gt; parser.add_argument('foo', nargs='+')&gt;&gt;&gt; parser.parse_args('a b'.split())Namespace(foo=['a', 'b'])&gt;&gt;&gt; parser.parse_args(''.split())usage: PROG [-h] foo [foo ...]PROG: error: too few arguments# argparse.REMAINDER&gt;&gt;&gt; parser = argparse.ArgumentParser(prog='PROG')&gt;&gt;&gt; parser.add_argument('--foo')&gt;&gt;&gt; parser.add_argument('command')&gt;&gt;&gt; parser.add_argument('args', nargs=argparse.REMAINDER)&gt;&gt;&gt; print(parser.parse_args('--foo B cmd --arg1 XX ZZ'.split()))Namespace(args=['--arg1', 'XX', 'ZZ'], command='cmd', foo='B') 计数 12parser.add_argument("-v", "--verbosity", action="count", default=0, help="increase output verbosity") 自定义函数我本来想实现一个直接list生成tuple的函数，后来才发现是每个str进行转换，所以对list进行了单独处理。 123456789101112def str2int(v): print(v, type(v))parser = argparse.ArgumentParser()parser.add_argument('--image_size', type=str2int, nargs='+', default=256, help='image resolution, eg. 256 128')config = parser.parse_args('--image_size 256 128'.split())print(config)256 str128 str--image_size ['256', '128'] 7. defaultdict遍历生成字典 12345678910# 第一种counts = &#123;&#125;for k in strings: counts[k]=counts.setdault(k, 0)+1# 第二种from collections import defaultdictdd = defaultdict(int)for k in strings: counts[k]=counts[k]+1 8. shuti1234567891011shutil.copy 复制文件shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉shutil.move( src, dst) 移动文件或重命名shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间shutil.copy( src, dst) 复制一个文件到一个文件或一个目录shutil.copy2( src, dst) 在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，类似于cp –p的东西shutil.copy2( src, dst) 如果两个位置的文件系统是一样的话相当于是rename操作，只是改名；如果是不在相同的文件系统的话就是做move操作shutil.copytree( olddir, newdir, True/Flase)把olddir拷贝一份newdir，如果第3个参数是True，则复制目录时将保持文件夹下的符号连接，如果第3个参数是False，则将在复制的目录下生成物理副本来替代符号连接shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容 9. pickle12picklepython codebook 注释：序列化对象，将对象obj保存到文件file中去。参数protocol是序列化模式，默认是0（ASCII协议，表示以文本的形式进行序列化），protocol的值还可以是1和2（1和2表示以二进制的形式进行序列化。其中，1是老式的二进制协议；2是新二进制协议）。file表示保存到的类文件对象，file必须有write()接口，file可以是一个以’w’打开的文件或者是一个StringIO对象，也可以是任何可以实现write()接口的对象。 10.重定向1234567891011121314151617181920212223242526272829303132333435363738394041424344from __future__ import absolute_importimport osimport sysclass Logger(object): def __init__(self, fpath=None): self.console = sys.stdout self.file = None if fpath is not None: mkdir_if_missing(os.path.dirname(fpath)) self.file = open(fpath, 'w') def __del__(self): self.close() def __enter__(self): pass def __exit__(self, *args): self.close() def write(self, msg): self.console.write(msg) if self.file is not None: self.file.write(msg) def flush(self): self.console.flush() if self.file is not None: self.file.flush() os.fsync(self.file.fileno()) def close(self): self.console.close() if self.file is not None: self.file.close()def mkdir_if_missing(path): if not os.path.exists(path): os.makedirs(path)if __name__ == '__main__': sys.stdout = Logger(fpath='./log.txt') print('2222222222') 11. 其他12# id2labelstrainval_ids2labels = dict(zip(trainval_ids, range(len(trainval_ids)))) 12. path123456seg_path.mkdir()args.data_root = Path(args.data_root)args.img_root = args.data_root / 'images'args.ann_root = args.data_root / 'annotations'for ann_path in args.ann_root.iterdir():ann_path.name.split('_') 13. list dict tuple set list dict tuple set 索引 支持 不支持 支持 不支持 有无序 有序 无序 有序 无序 +* 支持 支持 增 append, extend update 不支持 add(item), update(iter) 单个元素删除 不支持 discard, remove, pop, 单个元素修改 支持 支持 不支持 整体删除 支持 clear del del, clear 内置函数 items(), setdefault(k,v),fromkeys() cmp,len,max,tuple 初始化 set(),{} 其他 元素可以为list 不能有可变元素list,dict,set等 其他 拆包 其他 tuple一旦创建，增删改都需要以一个整体为单位进行 集合：子集(&lt;, issubset)，并集 |, 交集 &amp;，差集 -，对称差 ^ 14. 函数嵌套引起的变量引用问题问题： 12345678910def fun1(): res = [] count = 0 def fun2(): res.append(1) print(count) count += 1 fun2()fun1()# res.append(1) print(count) 可以执行，但是count += 1无法执行，会报错 python引用变量的顺序：当前作用域局部变量，外层作用域变量，当前模块中的全局变量，python内置变量。每个变量只能有一种身份。 第一种情况（直接修改全局变量和外层变量 报错）： 123456# 直接修改全局变量 报错gcount = 0def global_test(): gcount+=1 print (gcount)global_test() 12345678# 直接修改外层变量 报错def fun1(): gcount = 0 def global_test(): gcount+=1 print (gcount) global_test()fun1() 报错原因：可以将python引用变量的顺序记为身份由低到高，并且python允许访问修改有局部变量身份的变量，但是只允许访问更高等级身份的变量。 第二种情况（global修改全局变量 成功，修改外层变量 报错）： 123456789# 修改全局变量 成功# global 修饰后，使函数对全局变量有修改权限。gcount = 0def global_test(): global gcount gcount+=1 print (gcount)global_test() 或者global可以定义一个还没有出现的变量，而nonlocal不行 12345678# 成功# 此时，gcount 是全局变量，且函数对其有修改权限def global_test(): global gcount gcount = 1 print (gcount)global_test()print(gcount) 1234567891011# 修改外层变量 报错# 此时有两个 gcount，一个全局变量 global gcount，一个外层作用域 gcount.def fun1(): gcount = 0 def global_test(): global gcount gcount+=1 print (gcount) global_test()fun1() 注意，global 只能在变量第一次声明时使用 其中global声明gcount为全局变量，可以修改 第三种情况（nonlocal修改全局变量 报错，修改外层变量 成功）： 123456789101112# nonlocal修改全局变量 报错# nonlocal 只用用于修饰外层作用域出现过的变量。count = 0def fun1(): def make_counter(): def counter(): nonlocal count count += 1 print(count) counter() make_counter()fun1() 12345678910111213# nonlocal修改外层变量 成功# 多个外层的都可以访问到，从内而外def fun1(): count = 0 def make_counter(): count = 1 def counter(): nonlocal count count += 1 print(count) counter() make_counter()fun1() 或者nonlocal不可以定义一个还没有出现的变量，而global可以 123456789101112# 报错def fun1(): def make_counter(): def counter(): nonlocal count count = 1 print(count) counter() print(count) make_counter() print(count)fun1() nonlocal的特殊用法，应该是属于装饰器那一块 1234567891011121314151617def make_counter(): count = 0 def counter(): nonlocal count count += 1 return count return counterdef make_counter_test(): mc = make_counter() print(mc()) print(mc()) print(mc())make_counter_test()123 第四种情况（直接访问 成功）： 12345# 直接访问全局变量 成功gcount = 0def global_test(): print(gcount)global_test() 1234567# 直接访问外层变量 成功def fun1(): gcount = 0 def global_test(): print(gcount) global_test()fun1() 允许访问高级身份的变量 第五种情况： 12345678910111213# 成功def add_b(): global b b = 42 def do_global(): global b b = b + 10 print(b) do_global() print(b)add_b()# 52 52# b都是一个b，全局变量b，并且add_b域和go_global域对其具有访问和修改权 123456789101112# 报错def add_b(): global b b = 42 def do_global(): nonlocal b b = b + 10 print(b) do_global() print(b)add_b()# global b之后b是全局变量，add_b 中没有局部变量b，也就是说 do_global 中使用 non_local 进行修饰的时候，外层作用域没有变量b，因此报错 更多函数嵌套的变量访问:https://www.cnblogs.com/z360519549/p/5172020.html 123456789101112def add_b(): #global b #b = 42 def do_global(): global b b = 10 print(b) do_global() print(b)add_b()print(b)# 10 10 10 12345678910111213def add_b(): #global b #b = 42 def do_global(): global b b = 10 print(b) do_global() b = b + 20 # 报错 print(b)add_b()b = b + 30print(" b = %s " % b) 在https://blog.csdn.net/xcyansun/article/details/79672634找到了新的答案 再回到最原始的问题，为什么res可以，而count不行，因为res是list，res.append()之后不会改变其id(res)，但是count+=1会改变id(count) 123456count = 0print(id(count))# 1572891680count+=1print(id(count))# 1572891712]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-optim]]></title>
    <url>%2F2018%2F11%2F05%2Fpytorch-optim%2F</url>
    <content type="text"><![CDATA[pytorch-optim optim的学习率设置问题1.不同的学习率1234567891011121314151617# 第一种optimizer = t.optim.Adam(model.parameters(), lr = 0.1)# 第二种optim_group = [&#123;'params':model.net1.parameters(),'lr':0.4&#125;, &#123;'params':model.net2.parameters(),'lr':0.1&#125;]optimizer = t.optim.Adam(optim_group,lr=0.04)# 第三种ignored_params = list(map(id, model.model.fc.parameters() )) + list(map(id, model.classifier.parameters() ))base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())optimizer_ft = optim.SGD([ &#123;'params': base_params, 'lr': 0.01&#125;, &#123;'params': model.model.fc.parameters(), 'lr': 0.1&#125;, &#123;'params': model.classifier.parameters(), 'lr': 0.1&#125; ], weight_decay=5e-4, momentum=0.9, nesterov=True) 2. 学习率衰减12345678exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)def train_model(model, criterion, optimizer, scheduler, num_epochs=25): for epoch in range(num_epochs): scheduler.step() for data in dataloaders[phase]: optimizer.zero_grad() 3. iterools.chain123456# initialize optimizersself.optimizer_G = torch.optim.Adam(filter(lambda p: p.requires_grad, itertools.chain(self.netG_A.parameters(), self.netG_B.parameters())), lr=opt.lr, betas=(opt.beta1, 0.999))self.optimizer_D = torch.optim.Adam(filter(lambda p: p.requires_grad, itertools.chain(self.netD_A.parameters(), self.netD_B.parameters())), lr=opt.lr, betas=(opt.beta1, 0.999))self.optimizers = []self.optimizers.append(self.optimizer_G)self.optimizers.append(self.optimizer_D)]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-init]]></title>
    <url>%2F2018%2F11%2F05%2Fpytorch-init%2F</url>
    <content type="text"><![CDATA[pytorch-init pytorch模型的初始化pytorch模型的初始化的常用方法。 1.apply+typeapply可以理解成从children开始遍历可以用于init，可以用于model定义之后，与type配合。 Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import torch as tfrom torch import nn# define modelclass Net(nn.Module): def __init__(self): super(Net,self).__init__() self.pre = nn.Sequential(nn.Linear(2,2), nn.Conv2d(3,3,3)) self.two = nn.Sequential(nn.Linear(3,3)) self.apply(init_weights) def forward(self,x): passdef init_weights(m): print(m) print(type(m)) print(nn.Linear) print(m.__class__) print(m.__class__.__name__) if type(m) == nn.Linear: m.weight.data.fill_(0.0) print(m.weight.data) print("_______________________")net2 = Net()Linear(in_features=2, out_features=2, bias=True)&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;Lineartensor([[ 0., 0.], [ 0., 0.]])_______________________Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))&lt;class 'torch.nn.modules.conv.Conv2d'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.conv.Conv2d'&gt;Conv2d_______________________Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)))&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;Sequential_______________________Linear(in_features=3, out_features=3, bias=True)&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;Lineartensor([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]])_______________________Sequential( (0): Linear(in_features=3, out_features=3, bias=True))&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;Sequential_______________________Net( (pre): Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) ) (two): Sequential( (0): Linear(in_features=3, out_features=3, bias=True) ))&lt;class '__main__.Net'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class '__main__.Net'&gt;Net_______________________ 2.apply+m.class.nameweights_init_kaiming还要一种初始化函数1234567891011121314151617def weights_init_kaiming(m): classname = m.__class__.__name__ # print(classname) if classname.find('Conv') != -1: init.kaiming_normal(m.weight.data, a=0, mode='fan_in') elif classname.find('Linear') != -1: init.kaiming_normal(m.weight.data, a=0, mode='fan_out') init.constant(m.bias.data, 0.0) elif classname.find('BatchNorm1d') != -1: init.normal(m.weight.data, 1.0, 0.02) init.constant(m.bias.data, 0.0)def weights_init_classifier(m): classname = m.__class__.__name__ if classname.find('Linear') != -1: init.normal(m.weight.data, std=0.001) init.constant(m.bias.data, 0.0)]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cuda安装教程]]></title>
    <url>%2F2018%2F11%2F05%2Fcuda%2F</url>
    <content type="text"><![CDATA[安装教程参考链接https://www.jianshu.com/p/35c7fde85968]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_reptilian]]></title>
    <url>%2F2018%2F10%2F05%2Fpython-reptilian%2F</url>
    <content type="text"><![CDATA[前言这是第一次爬虫，以理论为主，以实现为辅。因为是到处看的，所以不是很有逻辑性。参考链接：python爬虫原理Python爬虫的两套解析方法和四种爬虫实现爬虫基本原理 一、工具 两个解析库：BeautifulSoup, lxml 两个请求库：urllib, requests 法法 二、爬虫流程用户获取网络数据的方式方式1:浏览器提交请求——&gt;下载网页代码——&gt;解析成页面方式2:模拟浏览器发送请求(获取网页代码)——&gt;提取有用数据——&gt;存放在数据库或者文件中爬虫就是指方式2. 1.发起请求使用http库向目标站点发送请求，即发送一个Request。Request包含：请求头，请求体等。Request模块缺点：不能执行JS和CSS代码。 2.获取响应内容服务器正常响应，得到一个Response。Response包含：html，json，图片，视频等。 3.解析内容解析html数据：正则表达式(RE模块)，第三方解析库如BeautifulSoup,pyquery等解析json数据：json模块解析二进制数据：以wb形式写入文件 4.保存数据数据库（MySQL, Mongdb, Redis)文件 三、Request&amp;Response1.Request1.1.请求方式常见的有：GET/POST 1.2.请求的URLurl是全球容易资源定位符，用来丁意思互联网上一个唯一的资源，例如：一张图片、一个文件、一段视频。 1.3.请求头User-agen：访问的浏览器请求头没有user-agent客户端配置，会被当成非法用户hostcookies：cookie用来保存登录信息Referrer：访问源至哪里来 1.4.请求体get：请求体没有内容post：请求体是format data 2.Response2.1 响应状态码200：代表成功301：代表调转404：文件不存在403：无权限访问502：服务器错误 2.2 响应头Set-Cookie:BDSVRTM=0; path=/：可能有多个，是来告诉浏览器，把cookie保存下来Content-Location：服务端响应头中包含Location返回浏览器之后，浏览器就会重新访问另一个页面 2.3preview网页源代码，包括：Json数据、html、图片、二进制数据 接下来开始尝试写一些基本的爬虫代码，并做记录```python 发起请求，并获取请求内容from urllib import requestresp = request.urlopen(‘https://movie.douban.com/nowplaying/hangzhou/‘) # http.client.HTTPResponsehtml_data = resp.read().decode(‘utf-8’) # str 这里的print是最好看的 解析内容from bs4 import BeautifulSoup as bssoup = bs(html_data, ‘html.parser’) # bs4.BeautifulSoupnowplaying_movie = soup.find_all(‘div’, id=’nowplaying’) # bs4.element.ResultSet list的形式，可以暂时看成是多个组成的list，需要先[0]的进行访问。tmp = nowplaying_movie[0] # bs4.element.Tagnowplaying_movie_list = nowplaying_movie[0].find_all(‘li’, class_=’list-item’) # bs4.element.ResultSet list形式， bs4.element.Tagnowplaying_list = [] # 此时就是直接获取数据了，find_all是对相应片段的截取for item in nowplaying_movie_list: nowplaying_dict = {} nowplaying_dict[‘id’] = item[‘data-subject’] for tag_img_item in item.find_all(‘img’): nowplaying_dict[‘name’] = tag_img_item[‘alt’] nowplaying_list.append(nowplaying_dict) requrl = ‘https://movie.douban.com/subject/&#39;+nowplaying_list[0][&#39;id‘] + ‘/comments’ +’?’ +’start=0’ + ‘&amp;limit=20’ 三句一体resp = request.urlopen(requrl)html_data = resp.read().decode(‘utf-8’)soup = bs(html_data, ‘html.parser’) comment_div_lists[0].find_all(‘span’, class_=”short”)[0].string # .string 可以暂时理解成中间的字符串]]></content>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2F2018%2F10%2F04%2Fgit%2F</url>
    <content type="text"><![CDATA[learning git 自己之前已经学过一次git了，但是最近在用的时候，仍然感觉不顺手，所以今天趁这个机会，再学一遍，这一次，以命令为主，以原理为辅。 初始化仓库1$ git init 文件到Git仓库12$ git add readme.txt$ git commit -m "wrote a readme file" 查看状态1git status 查看修改内容1git diff readme.txt 查看提交日志1git log 版本回到过去和将来12345678910# 回到过去git reset --hard HEAD^# HEAD~100# 此时git log已经没有了最新版本的提交信息# 回到将来git reset --hard 1094a# 或者git reflog # 命令历史git reset --hard 1094a 撤销修改12345# 当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时git checkout -- readme.txt# 当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，git reset HEAD readme.txtgit checkout -- readme.txt 删除文件12git rm test.txtgit commit -m "remove" 远程仓库克隆1git clone git@github.com:michaelliao/gitskills.git 添加远程仓库123456# git://不支持pushgit remote add origin git://github.com/TJJTJJTJJ/ticgit.git# git@只是pushgit remote add origin git@github.com:TJJTJJTJJ/ticgit.git# removegit remote remove origin 查看远程版本1git remote -v 提交到远程1git push -u origin master 获取远程仓库内容12git fetch origingit merge origin/master 提交远程仓库1git push origin master 暂时到这里，剩下的分支，自己暂时还不会用到，等用到了再去学就可以了。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-chapter10-ImageCaption]]></title>
    <url>%2F2018%2F09%2F30%2Fpytorch-chapter10-ImageCaption%2F</url>
    <content type="text"><![CDATA[前言本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。这是我的代码大神链接：https://github.com/anishathalye/neural-style这是论文作者写的 问题以及思考这一次感觉写起来很顺利，数据的处理+基本模型的走读基本只用了两天，剩下的两天主要是耗在了beam_searching上，原理的解析和代码的思考。现在记录一下这次走读的过程中学习到的东西，如果是和之前的记录有联系，那么则尽量记在一起。 局部反向传播管理部分参考第八章，基本来自官网文档一共是四种 @torch.no_grad() with torch.no_grad(): torch.set_grad_enabled(bool) with torch.set_grad_enabled(False): 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# 第一种：固定上下文管理器，torch.no_grad()和torch.enable_grad()又分为@torch.no_grad()和with torch.no_grad()x = torch.tensor([1], requires_grad=True)with torch.no_grad(): print(x.requires_grad) y = x*2 print(y.requires_grad) TrueFalsey.requires_gradFalse# 以上说明了上下文管理器内和外是一致的# 下面说明上下文管理器的作用域只在局部有效with torch.no_grad(): print(x.requires_grad) z = x*2 print(z.requires_grad) with torch.enable_grad(): print(x.requires_grad) y = x*2 print(y.requires_grad)TrueFalseTrueTruewith torch.enable_grad(): print(x.requires_grad) z = x*2 print(z.requires_grad) with torch.no_grad(): print(x.requires_grad) y = x*2 print(y.requires_grad)TrueTrueTrueFalse&gt;&gt;&gt; @torch.no_grad()... def dddd():... x = torch.tensor([2.2],requires_grad=True)... y = 2*x... print(y.requires_grad)... &gt;&gt;&gt; dddd()False# 第二种：条件的上下文管理器 torch.set_grad_enabled(bool)又分为with torch.set_grad_enabled(bool)和 torch.set_grad_enabled(bool)with torch.set_grad_enabled(False): print(x.requires_grad) y = x*2 print(y.requires_grad)TrueFalse&gt;&gt;&gt; torch.set_grad_enabled(False)&gt;&gt;&gt; y = x * 2&gt;&gt;&gt; y.requires_gradFalse# 在测试@torch.enable_grad()的时候没有成功，问题应该是版本问题，0.4.0的版本就不行，但是0.4.1的版本就可以了 预训练模型的修改备注：感觉这一块应该是很条理才对，但是没有找到类似的说明只能等以后见得多了，再做补充，网上有一些对特定模型的修改，但是都不全面，也没有具体说明各个方法的优劣。应该是这样的，层必须和forward对应，参数的加载可以放在模型定义时，也可以放在模型定义之后。 不修改原模型的forward流程常用于对特定层的修改123456789101112131415161718import torchvision.models as modelsmodel = models.resnet50(pretrained=True)# 只修改最后一层# 第一种fc_features = model.fc.in_featuresmodel.fc = nn.Linear(fc, 9)# 第二种resnet50 = tv.models.resnet50(pretrained=True)del resnet50.fcresnet50.fc = lambda x: x# 如果直接修改out_features是没有用的model.fc.out_features = 9resnet50.fc.weight.shapetorch.Size([1000, 2048])即如果修改某一层，要重新定义这一层 在模型内修改forward流程常用于中间层的增加1234567891011# 需要自己先定义类似的网络，注意定义的名字必须一致和方式需要一致，利用state_dict来更新参数import torchvision.models as modelsresnet50 = models.resnet50(pretrained=True)cnn = CNN(Bottleneck, [3, 4, 6, 3])pretrained_dict = resnet50.state_dict()model_dict = cnn.state_dict()# 选取相同名字参数pretrained_dict = &#123;k: v for k, v in pretrained_dict.items() if k in model_dict&#125;model_dict.update(pretrained_dict)cnn.load_state_dict(model_dict)print(cnn) 在模型外增加forward流程常用与开头或者末尾层的增加1234model.add_module('layer_name',layer)可以理解成self.layer_name = layerx = model.layer_name(x) 取特定模块，利用children()和nn.Sequential()也可以实现特定层的修改这个方法比较啰嗦，不是很推荐，或者不如第一种方法，或者不如最后一种方法1234567891011121314151617model = models.vgg16(pretrained=True)removed = list(model.classifier.children())[:-1]model.classifier = torch.nn.Sequential(*removed)model.add_module('fc', torch.nn.Linear(4096, out_num)) # out_num是你希望输出的数量 # 直接list(model)是不行的，但是list(model.children())就可以list(ResNet34.children())In [23]: for i in ResNet34.children(): ...: print(type(i)) ...: &lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt; 取特定模块利用list和modulelist，可用于对于特定模块的特定操作，可修改forward流程12345678# 第八章的方法 定义新模型， 在模型定义时，加载原模型参数， 修改forward 对于单向的还好，对于有分支的还没有尝试 用了list和modulelist 直接在定义模型的地方取features = list(vgg16(pretrained=True).features)[:23]self.features = nn.ModuleList(features).eval()for ii, model in enumerate(self.features): x = model(x) if ii in &#123;3,8,15,22&#125;: results.append(x) 12345678910111213141516for k,v in resnet34.named_children(): print(k,v)conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)relu ReLU(inplace)maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)layer1 Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) )... tensor.new和fill_和copy_在第九章，创建同类型的tensor用new,保证类型和cuda一致，不保证requires_grad，保证了和源类型一致，不共享内存1234567891011121314import torchx = torch.Tensor([2.2],requires_grad=True).cuda()xtensor([ 3.2000], device='cuda:0')y = x.new([4,5])yy.requires_gradFalsetensor([ 4., 5.], device='cuda:0')z = x.data.new([6,7])z.requires_gradFalseztensor([ 6., 7.], device='cuda:0') 在第十章，创建同类型同样大小同cuda的tensor，用fill_，fill_也保证了类型和cuda一致，保证了和目标类型一致1234567891011121314151617181920212223242526272829303132333435363738In [36]: x = t.Tensor(3,4).cuda()In [37]: xOut[37]: tensor([[ 1.1395e-19, 4.5886e-41, 3.4482e+25, 3.0966e-41], [ 5.7353e-31, 4.5886e-41, -1.2545e+37, 1.3914e+25], [ 2.9680e-31, 4.5886e-41, 5.7344e-31, 4.5886e-41]], device='cuda:0')In [38]: x.fill_(0)Out[38]: tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], device='cuda:0')# 测试requires_grad提示，不能In [43]: x.requires_grad= TrueIn [44]: xOut[44]: tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], device='cuda:0', requires_grad=True)In [45]: x.fill_(1)---------------------------------------------------------------------------RuntimeError Traceback (most recent call last)&lt;ipython-input-45-0c255de765ba&gt; in &lt;module&gt;()----&gt; 1 x.fill_(1)RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.# 强行修改值，则grad_fn也发生了变化。In [46]: x[0]=1In [47]: xOut[47]: tensor([[1., 1., 1., 1.], [0., 0., 0., 0.], [0., 0., 0., 0.]], device='cuda:0', grad_fn=&lt;CopySlices&gt;) 第十章的copy_，类型不变，可以作为计算图进行保留123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 不共享内存In [49]: x = t.Tensor(2,2).fill_(0)In [50]: xOut[50]: tensor([[0., 0.], [0., 0.]])In [51]: y = t.Tensor(1,2).fill_(1)In [52]: yOut[52]: tensor([[1., 1.]])In [53]: x[0].copy_(y[0])Out[53]: tensor([1., 1.])In [54]: xOut[54]: tensor([[1., 1.], [0., 0.]])In [55]: yOut[55]: tensor([[1., 1.]])In [56]: y[0][0]=2In [57]: yOut[57]: tensor([[2., 1.]])In [58]: xOut[58]: tensor([[1., 1.], [0., 0.]])# 类型不变In [60]: y = t.IntTensor(1,2).fill_(1)In [61]: yOut[61]: tensor([[1, 1]], dtype=torch.int32)In [62]: x = t.Tensor(2,2).fill_(0)In [63]: xOut[63]: tensor([[0., 0.], [0., 0.]])In [64]: x[0].copy_(y[0])Out[64]: tensor([1., 1.])In [65]: xOut[65]: tensor([[1., 1.], [0., 0.]])# requires_grad，会作为一个计算图保留In [66]: xOut[66]: tensor([[1., 1.], [0., 0.]])In [67]: x.requires_grad=TrueIn [68]: xOut[68]: tensor([[1., 1.], [0., 0.]], requires_grad=True)In [69]: x[1].copy_(y[0])Out[69]: tensor([1., 1.], grad_fn=&lt;AsStridedBackward&gt;)# cuda，可以保留In [76]: y[0]=2In [77]: yOut[77]: tensor([[2, 2]], dtype=torch.int32)In [78]: x[1].copy_(y[0])Out[78]: tensor([2., 2.], device='cuda:0', grad_fn=&lt;AsStridedBackward&gt;)In [79]: xOut[79]: tensor([[1., 1.], [2., 2.]], device='cuda:0', grad_fn=&lt;CopySlices&gt;) tensor赋值操作 只复制值，不共享内存123456789101112131415161718192021222324252627282930# 第一种 利用tensor 只复制值In [6]: x = t.tensor([3,4])In [7]: xOut[7]: tensor([3, 4])In [8]: y = t.tensor(x)In [9]: yOut[9]: tensor([3, 4])In [10]: x[0] = 1In [11]: yOut[11]: tensor([3, 4])# 第二种 利用切片， 只复制值In [12]: y = t.Tensor(4)In [13]: y[0:2]=xIn [14]: yOut[14]: tensor([1.0000e+00, 4.0000e+00, 1.1395e-19, 4.5886e-41])In [15]: x[0]=6In [16]: yOut[16]: tensor([1.0000e+00, 4.0000e+00, 1.1395e-19, 4.5886e-41]) t.save12345678910111213141516# 单个变量 不保留名字t.save(x, 'a.pth')y = t.load('a.pth') # 这个时候已经和x没有任何关系了# 多个变量 或者保留名字dic = dict(aa=x, bb=y)t.save(dic, 'a.pth')y = t.load('a.pth') # 这个时候已经和dic没有任何关系了，但是aa,bb还保留着y&#123;'aa': tensor([[ 100.0000, 100.0000, 100.0000, 100.0000], [ -0.0000, 0.0000, 0.0000, 0.0000], [ -0.0000, 0.0000, -0.0000, 0.0000]]), 'bb': tensor(1.00000e-11 * [[-0.0000, 0.0000, -0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000], [-3.9650, 0.0000, -0.0000, 0.0000]])&#125; 第十章的诡异装饰器作者在这里实现了batcha_size的拼接的方式。具体的函数闭包可以参考python1234# def create_collate_fn():# def collate_fn():# pass# return collate_fn 来，猜一下这里为什么这么写，函数闭包，根据昨天看的，函数闭包和类函数有的一拼，或者说可以用于创建多个类似的函数，暂时先这么理解，因为还没有太多的用到，在这里的函数闭包是为了实现对作为函数的参数进行传递变量，也就是把函数作为变量传递，这种思想要注意一下。设想几种情况。假设函数h的定义是这样的：1234567891011121314In [1]: def h(x, f): ...: """ ...: Args: ...: x: int ...: f: function ...: """ ...: out = f(x) ...: return outIn [2]: def f(x): ...: return 2*xIn [3]: h(2,f)Out[3]: 4 第一种情况，函数f的所有输入都是h可以给的，那么这时候如上所示，直接定义一个函数，然后把函数名或者其他等于函数的变量传进去就可以。第二种情况，函数f的有一部分变量，需要是外界给的，即f的定义中，引用到了不属于h的输入的变量。就像这样。1234567891011In [4]: def g(i): ...: def f(x): ...: return i*x ...: return f ...: ...:In [5]: ff = g(3)In [6]: h(4,ff)Out[6]: 12 那么这个时候函数闭包就可以很好地实现这种想法。这是暂时对于函数闭包的理解，但我知道这种想法肯定是有问题的。 rnn的pack和padfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequenceli_ = [[3,4,2,1],[3,4,2,1],[3,4,2,1],[3,4,2,0],[3,4,0,0]]ten = t.Tensor(li_).long()pad_variable = tenembedding = nn.Embedding(5,2)pad_embeddings = embedding(pad_variable)lengths = [5,5,4,3]pad_embeddingspad_embeddingstensor([[[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [ 0.5581, 0.7382]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [ 0.5581, 0.7382], [ 0.5581, 0.7382]]])packed_variable = pack_padded_sequence(pad_embeddings, lengths)PackedSequence(data=tensor([[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621], [ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621], [ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621], [ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [ 0.0256, -1.6445], [-0.0939, -0.4070]]), batch_sizes=tensor([ 4, 4, 4, 3, 2]))packed_variable.data.shapetorch.Size([17, 2])rnn = nn.LSTM(2,3)output, hn = rnn(packed_variable)outputPackedSequence(data=tensor([[-0.1698, -0.1311, 0.2030], [-0.0984, -0.0693, 0.1601], [-0.0791, -0.1195, 0.2111], [-0.0175, 0.0069, 0.0978], [-0.2580, -0.1868, 0.3193], [-0.1392, -0.0959, 0.2441], [-0.1221, -0.1489, 0.3270], [-0.0223, 0.0109, 0.1334], [-0.3011, -0.2100, 0.3821], [-0.1544, -0.1061, 0.2877], [-0.1452, -0.1551, 0.3886], [-0.0232, 0.0129, 0.1460], [-0.3222, -0.2195, 0.4168], [-0.1593, -0.1098, 0.3109], [-0.1575, -0.1556, 0.4222], [-0.3325, -0.2233, 0.4370], [-0.1603, -0.1111, 0.3235]]), batch_sizes=tensor([ 4, 4, 4, 3, 2]))hn[1].shapetorch.Size([1, 4, 3])pad_packed_sequence(packed_variable) (tensor([[[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [-0.7936, 0.9621]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [-1.4719, -0.4871], [ 0.0000, 0.0000]], [[ 0.0256, -1.6445], [-0.0939, -0.4070], [ 0.0000, 0.0000], [ 0.0000, 0.0000]]]), tensor([ 5, 5, 4, 3])) 12345678910111213141516# embedding_dim=3, seq_len=4,3 batch_size=2 即把两句话a,b作为一个batch,空余补0from torch.nn.utils.rnn import pad_sequencea = t.ones(4, 3)b = t.ones(3,3)pad_sequence([a,b])tensor([[[ 1., 1., 1.], [ 1., 1., 1.]], [[ 1., 1., 1.], [ 1., 1., 1.]], [[ 1., 1., 1.], [ 1., 1., 1.]], [[ 1., 1., 1.], [ 0., 0., 0.]]]) 123456# a,b两句话，分别有3,2个词，batch_size=2, 共有3个batch_size，大小分别是2,2,1from torch.nn.utils.rnn import pad_sequencea = t.tensor([1,2,3])b = t.tensor([4,5])pack_sequence([a,b])PackedSequence(data=tensor([ 1, 4, 2, 5, 3]), batch_sizes=tensor([ 2, 2, 1])) beam_searching[参考链接]https://blog.csdn.net/xljiulong/article/details/51554780[参考链接]http://jhave.org/algorithms/graphs/beamsearch/beamsearch.shtml网上讲的大部分都有各自的问题，不是很清晰，只有那篇英文才是标准的，这哥们应该是翻译的，还不错作者使用的是beam_searching的变种，原理类似，但是条件不一致，具体的在代码注释中，不再陈述。 第十章和第九章关于生成语句的流程的区别第十章和第九章在模型生成的地方有两个点不一样，第九章的模型本身可以进行正常的输入与输出，所以第九章也写成这个样子输入(LongTensor) 11 输出 tensor 1vocabsize123456789101112131415161718192021222324252627282930313233results = list(start_words)start_word_len = len(start_words)for i in range(opt.max_gen_len): output, hidden = model(input, hidden) if i &lt; start_word_len: w = results[i] input = input.data.new([word2ix[w]]).view(1,1) else: # output size 1×vocab_size [[1,2,3,...]] # 这里应该看一下，输出output是个什么东西 top_index = output.data[0].topk(1)[1][0].item() w = ix2word[top_index] results.append(w) input = input.data.new([word2ix[w]]).view(1,1) if w == '&lt;EOP&gt;': del results[-1] breakreturn results# 简写为# model: embedder rnn classifierresults = []for i in range(opt.max_gen_len): output, hidden = model(input, hidden) # input(tensor) 1*1 output(tensor) 1*vocabsize hidden(tensor) 1*1*hidden_dim top_index = output.data[0].topk(1)[1][0].item() w = ix2word(top_index) results.append(w) input = input.data.new(word2ix(w)).view(1,1)return results 第十章因为使用了pack_padded_sequence来加速训练，那么训练的模型就不能直接拿来像第九章进行生成，另外第一个字是图片特征的转化而成的，不需要embedding层，而是需要fc层，其实也可以直接拿来用，把captions设置为空就好了,在这里作者没有直接用，直接用好像比较麻烦。而是采用beam_search中把rnn和classifier层传进去，写了一个标准的beam_search函数，即输入是第一个字，输出是beam_size句话，因为设计到其他选词保留的问题，所以直接传入的的是各个分函数，进行自行拼接。也可能是为了复用logprobs = nn.functional.log_softmax(output, dim=1) ## 暂时不清楚这里为什么用log_softmax，是负数啊，大哥，不过大小好像不变在rnn中有一个问题，就是能不能用t.no_grad,会不会影响其向前传播。 对数据的预处理第九章是把对数据的预处理写在了data里面，但事实上，这个数据预处理应该与主模型分开，是属于前一个过程。有什么需要交互的，也是通过文件进行，包括配置。 新建立的数据结构的对比大小123456789101112131415161718192021222324252627282930313233343536373839404142434445class Caption(object): """ 现在不太确定这个集合是hash_table还是set，感觉是hash_tale,是因为set不需要专门的存储结构。再看看吧 这里应该不是那三个集合，而是集合中的每一个元素，比如G(i),这种，作者应该是重新创建了一种数据结构来用，来进行存储 Args: sentence: list(int) state: tuple(hn, cn) hn:1*1*hidden_dim logprob: probability score: 等于logprb或者logprb/len(sentence) """ def __init__(self, sentence, state, logprob, score, metadata=None): """ Args: sentence(list): """ self.sentence = sentence self.state = state self.logprob = logprob self.score = score self.metadata = metadata # 这里我猜是为了实现堆排序的比较。尽管知道是，但是还是不知道为什么 def __cmp__(self, other): """Compares Captions by score.""" assert isinstance(other, Caption) if self.score == other.score: return 0 elif self.score &lt; other.score: return -1 else: return 1 # For Python 3 compatibility (__cmp__ is deprecated). def __lt__(self, other): assert isinstance(other, Caption) return self.score &lt; other.score # Also for Python 3 compatibility. def __eq__(self, other): assert isinstance(other, Caption) return self.score == other.score 作者在固定长度的列表中，使用了小顶堆的方式，来保证每次取出去的都是最小的。 存在问题现在还有一个问题就是当一个.py文件里的函数或者类超过2、3个时，应该以什么的方式注释才能更好地让别人知道这个文件里的函数和怎么干的。 总结第十章的代码在难度上其实已经感觉下降了好多，当然自己又忘了写requires.txt。但是在调试改bug自己就用了三天。其中的bug有的时候自己已经忘记当初是怎么写的了，尴尬。自己训练出来的模型也没有作者声称的那么好，暂时不知道]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[html]]></title>
    <url>%2F2018%2F09%2F24%2Fhtml%2F</url>
    <content type="text"><![CDATA[HTML 教程 html 基础因为在visdom中的text支持html标签，所以来简单学学html。参考链接:菜鸟教程[菜鸟工具在线编辑工具]https://c.runoob.com/front-end/6112345678910111213141516&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;我的第一个标题&lt;/h1&gt; &lt;p&gt;我的第一个段落。&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 实例解析 \&lt;!DOCTYPE html&gt; 声明为 HTML5 文档 &lt;\html&gt; 元素是 HTML 页面的根元素 &lt;\head&gt; 元素包含了文档的元（meta）数据，如 定义网页编码格式为 utf-8。 &lt;\title&gt; 元素描述了文档的标题 元素包含了可见的页面内容 &lt;\h1&gt; 元素定义一个大标题 &lt;\p&gt; 元素定义一个段落 网页结构1234567891011&lt;html&gt; &lt;head&gt; &lt;title&gt; 页面标题&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;这是一个标题&lt;/h1&gt; &lt;p&gt;这是一个段落。&lt;/p&gt; &lt;p&gt;这是另一个段落&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 只有与之间的元素才会显示 标题 - 这是一个标题段落 这是一个段落。 段前段后有空行链接 这是一个链接图片 html元素空元素标签html属性 html设置属性，常以键值对的形式出现 这是一个链接 常用属性： class id style titlehtml水平线 换行 注释 格式化标签123456&lt;b&gt;加粗文本&lt;/b&gt;&lt;br&gt;&lt;i&gt;斜体文本&lt;/i&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;电脑自动输出&lt;/code&gt;&lt;br&gt;&lt;br&gt;这是 &lt;sub&gt; 下标&lt;/sub&gt; 和 &lt;sup&gt; 上标&lt;/sup&gt;&lt;small&gt;这个文本是缩小的&lt;/small&gt;&lt;big&gt;这个文本字体放大&lt;/big&gt; 123456789&lt;pre&gt;此例演示如何使用 pre 标签对空行和 空格进行控制&lt;/pre&gt;此例演示如何使用 pre 标签对空行和 空格进行控制 1234567891011&lt;code&gt;计算机输出&lt;/code&gt;&lt;br /&gt;&lt;kbd&gt;键盘输入&lt;/kbd&gt;&lt;br /&gt;&lt;tt&gt;打字机文本&lt;/tt&gt;&lt;br /&gt;&lt;samp&gt;计算机代码样本&lt;/samp&gt;&lt;br /&gt;&lt;var&gt;计算机变量&lt;/var&gt;&lt;br /&gt; 地址1234567&lt;address&gt;Written by &lt;a href="mailto:webmaster@example.com"&gt;Jon Doe&lt;/a&gt;.&lt;br&gt; Visit us at:&lt;br&gt;Example.com&lt;br&gt;Box 564, Disneyland&lt;br&gt;USA&lt;/address&gt; 创建电子邮件标签 首字母缩写和缩写123&lt;abbr title="etcetera"&gt;etc.&lt;/abbr&gt;&lt;br /&gt;&lt;acronym title="World Wide Web"&gt;WWW&lt;/acronym&gt; 文字显示方向1&lt;p&gt;&lt;bdo dir="rtl"&gt;该段落文字从右到左显示。&lt;/bdo&gt;&lt;/p&gt; 块引用123&lt;p&gt;WWF's goal is to: &lt;q&gt;Build a future where people live in harmony with nature.&lt;/q&gt;We hope they succeed.&lt;/p&gt; 删除字和插入字的效果1&lt;p&gt;My favorite color is &lt;del&gt;blue&lt;/del&gt; &lt;ins&gt;red&lt;/ins&gt;!&lt;/p&gt; html 链接属性 target:定义文档在哪个窗口打开 id属性1&lt;a href="http://www.runoob.com/" target="_blank"&gt;访问菜鸟教程!&lt;/a&gt; 12345&lt;p&gt;&lt;a href="#C4"&gt;查看章节 4&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a id="C4"&gt;章节 4&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这边显示该章节的内容……&lt;/p&gt; html , , , , , , and . 定义了浏览器工具栏的标题 当网页添加到收藏夹时，显示在收藏夹中的标题 显示在搜索引擎结果页面的标题 标签描述了基本的链接地址/链接目标，该标签作为HTML文档中所有的链接标签的默认链接 1&lt;base href="http://www.runoob.com/images/" target="_blank"&gt; 标签定义了文档与外部资源之间的关系。1&lt;link rel="stylesheet" type="text/css" href="mystyle.css"&gt; 标签定义了HTML文档的样式文件引用地址.1234&lt;style type="text/css"&gt;body &#123;background-color:yellow&#125;p &#123;color:blue&#125;&lt;/style&gt; 元素 META 元素通常用于指定网页的描述，关键词，文件的最后修改时间，作者，和其他元数据。123&lt;meta name="keywords" content="HTML, CSS, XML, XHTML, JavaScript"&gt;&lt;meta name="description" content="免费 Web &amp; 编程 教程"&gt;&lt;meta http-equiv="refresh" content="30"&gt; HTML 样式- CSSCSS (Cascading Style Sheets) 用于渲染HTML元素标签的样式.123456789101112131415161718192021# 使用添加到 &lt;head&gt; 部分的样式信息对 HTML 进行格式化 内部样式表 应用于单个文件&lt;head&gt;&lt;meta charset="utf-8"&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;style type="text/css"&gt;h1 &#123;color:red;&#125;p &#123;color:blue;&#125;&lt;/style&gt;&lt;/head&gt;# 使用 style 属性制作一个没有下划线的链接 内联样式 应用于个别元素&lt;a href="http://www.runoob.com/" style="text-decoration:none;"&gt;访问 runoob.com!&lt;/a&gt;# 标签链接到一个外部样式表 外部引用&lt;head&gt;&lt;meta charset="utf-8"&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;link rel="stylesheet" type="text/css" href="styles.css"&gt;&lt;/head&gt;font-family（字体），color（颜色），和font-size（字体大小）， text-align（文字对齐）# 图像 1# 表格 Header 1 Header 2 row 1, cell 1 row 1, cell 2 row 2, cell 1 row 2, cell 2 1# 列表 Coffee Tea Milk Coffee Tea Milk Coffee Tea Milk ``` 区块 块级和 内联级表单 2018-10-05 前言：主要是为了爬虫做一些补充，此次主要是为了爬虫做补充，不涉及html的理论用于加载脚本文件，指一段javascript代码，暂时不影响后续操作。 &amp;区块 块级和 内联级，本身没有太强的含义，前者以新行显示，后者不以新行显示。主要是作为容器存在，用于布局。 id&amp;classid具有唯一性，在一个网页内同一个命名只能使用一次，定义以#开头class命名的类可以在一个网页使用无数次，定义以.开头但两者都是定义的样式而已。]]></content>
      <categories>
        <category>html</category>
      </categories>
      <tags>
        <tag>html</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-picture]]></title>
    <url>%2F2018%2F09%2F24%2Fpython-picture%2F</url>
    <content type="text"><![CDATA[针对图像的操作 cv2, matplotlib.pylab, PIL.Image一般用PIL.Image或者cv2来打开或者保存，用matplotlib.pylab来显示在pytorch中也可以用tv.utils.save_image()专门来保存图片。 cv2参考链接注意：pylab.imread和PIL.Image.open读入的都是RBG顺序，而cv2.imread读入的是BGR顺序，混合使用的时候要特别注意123456789101112131415161718192021222324252627282930313233# 读取图片import cv2import numpy as npimg = cv2.imread('examples.png') # # 默认是读入为彩色图，即使原图是灰度图也会复制成三个相同的通道变成彩色图img_gray = cv2.imread('examples.png',0) # 第二个参数为0的时候读入为灰度图，即使原图是彩色图也会转成灰度图print(type(img), img.dtype, np.min(img), np.max(img))print(img.shape)print(img_gray.shape)(&lt;type 'numpy.ndarray'&gt;, dtype('uint8'), 0, 255) # opencv读进来的是numpy数组，类型是uint8，0-255(824, 987, 3) # 彩色图3通道(824, 987) # 灰度图单通道## 显示import pylab as plt import cv2 import numpy as np img = cv2.imread('examples.png') plt.imshow(img[..., -1::-1]) # 因为opencv读取进来的是bgr顺序呢的，而imshow需要的是rgb顺序，因此需要先反过来 plt.show()## 灰度与RGB转化import cv2 import pylab as plt img = cv2.imread('examples.png') img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # BGR转灰度 img_bgr = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR) # 灰度转BRG img_rgb = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB) # 也可以灰度转RGB## 保存图片import cv2 img = cv2.imread('examples.png') # 这是BGR图片 cv2.imwrite('examples2.png', img) # 这里也应该用BGR图片保存，这里要非常注意，因为用pylab或PIL读入的图片都是RGB的，如果要用opencv存图片就必须做一个转换 img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.imwrite('examples_gray.png', img_gray) matplotlib.pylab12345678910111213141516171819202122# 读取图片import matplotlib.pylab as pltimport numpy as npimg = plt.imread('examples.png')print(type(img), img.dtype, np.min(img), np.max(img))[out](&lt;type 'numpy.ndarray'&gt;, dtype('float32'), 0.0, 1.0) # matplotlib读取进来的图片是float，0-1# 显示plt.imshow(img)plt.show()# 保存# 有两种# 其实产生这个现象的原因很简单：在 plt.show() 后调用了 plt.savefig() ，在 plt.show() 后实际上已经创建了一个新的空白的图片（坐标轴），这时候你再 plt.savefig() 就会保存这个新生成的空白图片# ref:(https://blog.csdn.net/u010099080/article/details/52912439)# 第一种:在plt.show之前保存plt.savefig('test.png')# 第二种:画图的时候保存句柄fig = plt.gcf()plt.show()fig.savefig('test.png') PIL.Image123456789101112131415161718192021222324252627282930313233343536373839404142434445# 读取图片from PIL import Imageimport numpy as npimg = Image.open('examples.jpg')print(type(img), img.dtype, np.min(img), np.max(img))img = np.array(img) # 将PIL格式图片转为numpy格式image_pil = Image.fromarray(image_numpy)print(type(img), img.dtype, np.min(img), np.max(img))(&lt;class 'PIL.PngImagePlugin.PngImageFile'&gt;, 0, 255) # 注意，PIL是有自己的数据结构的，但是可以转换成numpy数组 (&lt;type 'numpy.ndarray'&gt;, dtype('uint8'), 0, 255) # 和用matplotlib读取不同，PIL和matlab相同，读进来图片和其存储在硬盘的样子是一样的，uint8，0-255# 灰度和RGB转化from PIL import Image img = Image.open('examples.png') img_gray = img.convert('L') # RGB转换成灰度图像 img_rgb = img_gray.convert('RGB') # 灰度转RGB print(img) print(img_gray) print(img_rgb) [out] &lt;PIL.PngImagePlugin.PngImageFile image mode=RGB size=987x824 at 0x7FC2CCAE04D0&gt; &lt;PIL.Image.Image image mode=L size=987x824 at 0x7FC2CCAE0990&gt; &lt;PIL.Image.Image image mode=RGB size=987x824 at 0x7FC2CCAE0250&gt;# 显示import pylab as plt from PIL import Image import numpy as npimg = Image.open('examples.png') img_gray = img.convert('L') #转换成灰度图像 img = np.array(img) img_gray = np.array(img_gray) plt.imshow(img) # or plt.imshow(img / 255.0)，matplotlib和matlab一样，如果是float类型的图像，范围是0-1才能正常imshow，如果是uint8图像，范围则需要是0-255 plt.show() plt.imshow(img_gray, cmap=plt.gray()) # 显示灰度图要设置cmap参数 plt.show() plt.imshow(Image.open('examples.png')) # 实际上plt.imshow可以直接显示PIL格式图像 plt.show()# 保存img = Image.open('examples.png') img.save('examples2.png') img_gray = img.convert('L') img_gray.save('examples_gray.png') # 不管是灰度还是彩色，直接用save函数保存就可以，但注意，只有PIL格式的图片能够用save函数]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python,picture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMC]]></title>
    <url>%2F2018%2F09%2F24%2FCMC%2F</url>
    <content type="text"><![CDATA[rank-1,rank-5,mAP 第一种，网上的标准计算公式https://blog.csdn.net/u013698770/article/details/60776102https://blog.csdn.net/zkp_987/article/details/79969512https://blog.csdn.net/kaixinjiuxing666/article/details/812727961234567891011121314151617181920212223242526272829303132333435363738import numpy as npindexxx = np.array([[8,9,4,7,5,6,3,2,1,0],[9,6,7,2,1,4,3,5,8,0],[7,9,5,3,1,2,4,6,8,0]])good_index = np.array([1,3,5,7,9])CMC = np.array([0,0,0,0,0,0,0,0,0,0])mAP = 0.0for i in indexxx: cmc = np.array([0,0,0,0,0,0,0,0,0,0]) index = i ngood = len(good_index) mask = np.in1d(index, good_index) rows_good = np.argwhere(mask==True) rows_good = rows_good.flatten() cmc[rows_good[0]:] = 1 print('cmc:',cmc) CMC += cmc ap = 0.0 for i in range(ngood): d_recall = 1.0/ngood precision = (i+1)*1.0/(rows_good[i]+1) ap = ap + d_recall*precision print('ap:&#123;:.2f&#125;%:'.format(100*ap)) mAP += apCMC = CMC/3mAP = mAP/3print('top1:&#123;:.2f&#125;% top5:&#123;:.2f&#125;% mAP:&#123;:.2f&#125;%'.format(100*CMC[0],100*CMC[4],100*mAP))cmc: [0 1 1 1 1 1 1 1 1 1]ap:54.54%cmc: [1 1 1 1 1 1 1 1 1 1]ap:69.26%cmc: [1 1 1 1 1 1 1 1 1 1]ap:100.00%top1:66.67% top5:100.00% mAP:74.60% 第二种：baseline的计算公式12345678910111213141516171819202122232425262728293031323334353637383940414243def compute_mAP(index, good_index, junk_index): ap = 0 cmc = torch.IntTensor(len(index)).zero_() if good_index.size==0: # if empty cmc[0] = -1 return ap,cmc # remove junk_index mask = np.in1d(index, junk_index, invert=True) index = index[mask] # find good_index index ngood = len(good_index) mask = np.in1d(index, good_index) rows_good = np.argwhere(mask==True) rows_good = rows_good.flatten() cmc[rows_good[0]:] = 1 for i in range(ngood): d_recall = 1.0/ngood precision = (i+1)*1.0/(rows_good[i]+1) if rows_good[i]!=0: old_precision = i*1.0/rows_good[i] else: old_precision=1.0 ap = ap + d_recall*(old_precision + precision)/2 return ap, cmcCMC = torch.IntTensor(len(gallery_label)).zero_()ap = 0.0#print(query_label)for i in range(len(query_label)): ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam) if CMC_tmp[0]==-1: continue CMC = CMC + CMC_tmp ap += ap_tmp print(i, CMC_tmp[0])CMC = CMC.float()CMC = CMC/len(query_label) #average CMCprint('top1:%f top5:%f top10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label))) 第二种的简化版本，只计算CMC1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889def evaluate(qf,ql,qc,gf,gl,gc): """ qf: list [1,2,3] ql: 1 qc: 1 gf: list [[1,2,3],[1,2,3]] gl: [1,2,3] gc: [1,2,3] len(gf)==len(gl)==len(gc) """ query = qf score = np.dot(gf,query) # predict index index = np.argsort(score) #from small to large # 表示位置，[4,3,1,0,2] index = index[::-1] # 表示 #index = index[0:2000] # good index query_index = np.argwhere(gl==ql) # list [[1],[2]] # 表示位置，即galley中的第几个样本是相同的id camera_index = np.argwhere(gc==qc) # list [[1],[2]] # 表示位置 good_index = np.setdiff1d(query_index, camera_index, assume_unique=True) # [2,3]表示同一个id不同摄像头的图片的位置 junk_index1 = np.argwhere(gl==-1) # [[1],[2]] 表示id为-1的图片的位置 junk_index2 = np.intersect1d(query_index, camera_index) # [1,2] 表示同一个id同一个摄像头的图片的位置 junk_index = np.append(junk_index2, junk_index1) #.flatten()) CMC_tmp = compute_mAP(index, good_index, junk_index) return CMC_tmpdef compute_cmc(index, good_index, junk_index): """ index: list [4,3,1,0,2]，已经排序，数字表示第几张图片 good_index: [3,1] list 位置 数字表示第几张图片 junk_index: [4,2]list 位置 数字表示第几张图片 """ cmc = torch.IntTensor(len(index)).zero_() if good_index.size==0: # if empty cmc[0] = -1 return ap,cmc # remove junk_index mask = np.in1d(index, junk_index, invert=True) index = index[mask] # [3,1,0] # find good_index index ngood = len(good_index) mask = np.in1d(index, good_index) # [t,t,f] rows_good = np.argwhere(mask==True) # rows_good = rows_good.flatten() # [0,1] cmc[rows_good[0]:] = 1 return cmcdef main(): result = scipy.io.loadmat('pytorch_result.mat') query_feature = result['query_f'] # list [[1,2,3],[1,2,3],[1,2,3]] query_cam = result['query_cam'][0] # list [1,2,3] query_label = result['query_label'][0] # list [1,2,3] gallery_feature = result['gallery_f'] # list [[1,2,3],[1,2,3],[1,2,3]] gallery_cam = result['gallery_cam'][0] # list [1,2,3] gallery_label = result['gallery_label'][0] # list [1,2,3] CMC = torch.IntTensor(len(gallery_label)).zero_() for i in range(len(query_label)): CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam) if CMC_tmp[0]==-1: continue CMC = CMC + CMC_tmp print(i, CMC_tmp[0]) CMC = CMC.float() CMC = CMC/len(query_label) #average CMC print('top1:%f top5:%f top10:%f'%(CMC[0],CMC[4],CMC[9])def get_id(img_path): camera_id = [] labels = [] for path, v in img_path: filename = path.split('/')[-1] label = filename[0:4] camera = filename.split('c')[1] if label[0:2]=='-1': labels.append(-1) else: labels.append(int(label)) camera_id.append(int(camera[0])) return camera_id, labelsgallery_cam,gallery_label = get_id(gallery_path)query_cam,query_label = get_id(query_path)]]></content>
      <categories>
        <category>person-reid</category>
      </categories>
      <tags>
        <tag>CMC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch chapter9 CharRNN]]></title>
    <url>%2F2018%2F09%2F23%2Fpytorch-chapter9-CharRNN%2F</url>
    <content type="text"><![CDATA[前言本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。这是我的代码大神链接：https://github.com/anishathalye/neural-style这是论文作者写的 问题及其思考data是鸭子类型因为data作为tensor，已经实现了__getitem__和__len__方法，可以被DataLoader加载。或者说，只要能类似鸭子就可以，这方面掌握得还不熟悉。 LSTM的输入作者明确提出，LSTM的输入类型是(seq_len, batch_size, embedding_dim)，除去embedding_dim，就是(seq_len, batch_size)，原因很简单，LSTM是每次输入一个字，输出一个字，那么输入就是x[0]，对于图像，x[0]就是一张图片，那么对于文字，x[0]也应该就是一个字。好吧，还是说不通，等以后看了相关资料说不定才能理解。 代码编写12345678910111213141516171819202122232425262728293031323334# 变成列表，方便后续的操作，因为start_words的每个字用过之后就没用了，# 用pop不行,因为对于空列表会报错,用None作为结尾标志。可以看出，如果我们想让某个序列正常退出，可以通过设置特殊的结尾来实现。# 这一段的逻辑有点乱，因为prefix_words可能没有，所以对于start_words，必须先进行一个模型生成。# 对于或有或无的perfix_words，为了消除其存在对代码和思路的影响，应该保证prefix_words前后的代码状态不变，即"""第一种这种保证了output,hidden的状态不变output, hidden = model(input, hidden)# step： 对prefix_words进行输入prefix_words = '' if prefix_words==None else prefix_words for word in prefix_words: input = input.data.new(word2ix[word]).view(1,1) output, hidden = model(input, hidden) for i in range(opt.max_gen_len-1): top_index = output[0].topk(1)[1][0].item() ... output, hidden = model(input, hidden)第二种这种保证了input的状态不变for word in prefix_words: output, hidden = model(input, hidden) input = (input.data.new([word2ix[word]])).view(1, 1)for i in range(opt.max_gen_len): output, hidden = model(input, hidden) top_index = output.data[0].topk(1)[1][0].item() 决定采用第二种，因为代码的主体思路是for i in range(opt.max_gen_len)，prefix_word是插入部分，是可有可无部分。第一种会造成 top_index与model的切分，不利于后期分析。或者说，以后碰到这种类型的代码，可以直接跳过中间部分，对后面进行分析。 os.walk() &amp;&amp; os.listdir()os.walk()12345678910111213141516171819202122232425In [6]: for i,j,k in os.walk('./'): ...: print(i)././b./a./a/aa2./a/aa1In [7]: for i,j,k in os.walk('./'): ...: print(j) ['b', 'a'][]['aa2', 'aa1'][][]In [8]: for i,j,k in os.walk('./'): ...: print(k)['1']['cc', 'bb']['aa']['bbb'][]i+k即可 os.walk()返回的是当前文件夹下所有的可遍历的文件夹，生成的生成器，i表示文件夹，j表示i文件夹下的文件夹，k表示i文件夹下的文件。以上是os.walk的for用法，下面是直接的用法。对于文件访问，直接i+k，对于文件夹访问，i+j即可。 os.listdir()1234567891011In [17]: aa = os.walk('./')In [18]: bb = list(aa)In [19]: bbOut[19]: [('./', ['b', 'a'], ['1']), ('./b', [], ['cc', 'bb']), ('./a', ['aa2', 'aa1'], ['aa']), ('./a/aa2', [], ['bbb']), ('./a/aa1', [], [])] 不知道为什么这里不能直接用 aa,bb,cc = os.walk(‘./‘)12345678910In [22]: for ii in os.listdir('./'): ...: print(ii) ...: 1baIn [23]: aa = os.listdir('./')In [24]: aaOut[24]: ['1', 'b', 'a'] os.listdir()返回的是当前文件夹下的文件夹或者文件。现在碰到的情况是文件夹排列有序，直接访问文件，所以os.list()就可以了。对应的就是12for filename in os.listdir(src): path = os.path.join(src,filename) 小发现 刚刚发现github上的chinese中的某个文件夹是一个新的github文件。这是个啥情况 python 可以在函数内部定义函数，是局部域，不能被外界访问，很好，这样就相当于说明了哪些函数是为哪些函数服务的。 json数据格式的读取12345678910111213141516In [25]: import jsonIn [26]: s = &#123;"name": "ACME", "shares": 50, "price": 490.1&#125;In [27]: json_str = json.dumps(s)In [28]: json_str.__class__Out[28]: strIn [29]: json_strOut[29]: '&#123;"name": "ACME", "shares": 50, "price": 490.1&#125;'In [33]: ss = json.loads(json_str)In [34]: ssOut[34]: &#123;'name': 'ACME', 'shares': 50, 'price': 490.1&#125; json文件的读取第一种:此时data里是该文件内的全部数据12with open(file,'r') as f: data = json.load(f) 第二种：此时data也是该文件内的全部数据，open(file).read()表示读取数据1data = json.loads(open(file).read()) 显然第一种安全，第二种还需要显示地关闭文件可以使用pprint来打印data，好看12import pprint import pprint pprint data 正则表达式普通字符和11个元字符：. 匹配任意除换行符”\n”外的字符(在DOTALL模式中也能匹配换行符 a.c\ 转义字符，使后一个字符改变原来的意思* 匹配前一个字符0或多次+ 匹配前一个字符1次或无限次? 匹配一个字符0次或1次^ 匹配字符串开头。在多行模式中匹配每一行的开头$ 匹配字符串末尾，在多行模式中匹配每一行的末尾| 或。匹配|左右表达式任意一个，从左到右匹配，如果|没有包括在()中，则它的范围是整个正则表达式{} {m}匹配前一个字符m次，{m,n}匹配前一个字符m至n次，若省略n，则匹配m至无限次[] 字符集。对应的位置可以是字符集中任意字符。字符集中的字符可以逐个列出，也可以给出范围，如[abc]或[a-c]。abc表示取反，即非abc。所有特殊字符在字符集中都失去其原有的特殊含义。用\反斜杠转义恢复特殊字符的特殊含义。()表达式作为一个整体，可以后接数量词。表达式中的|仅在该组中有效。print re.split(r”;|,|\?”, line1)print re.split(r”[;,?]”, line1)print re.split(r”\W+”, line)不知道为什么12345para = '-181-欲出未出光辣达，[千山]万山[如火]发（哈哈哈大笑）。须臾[走向][天上]'re.subn('[\d-]','',para)('欲出未出光辣达，[千山]万山[如火]发（哈哈哈大笑）。须臾[走向][天上]', 5)re.subn('[\d-]*','',para)('欲出未出光辣达，[千山]万山[如火]发（哈哈哈大笑）。须臾[走向][天上]', 38) list越界与切片list不能越界索引访问，但是对于切片，切片是会自动匹配长度的，所以使用slice不需要担心越界问题。12345s = [1,2,3]s[8] # 报错s[1] == 1s[:1] == [1]s[1:10] # return s[1:3] 索引位置返回的是元素的副本切片返回的是list的副本 嵌套列表压平func = lambda x: [y for l in x for y in l] if type(x) is list else [x] tuple的连接123(1,2)+(3,4) == (1,2,3,4)# 定义只有一个数字的tuple，避免函数歧义t = (1,) 求list的size123li = [[1,2],[3,4]]tu = np.asarray(li).shape# shape返回的是tuple型，可以直接拼接 异常触发参考链接http://www.runoob.com/python/python-exceptions.html等总结的时候尝试一下分为捕捉异常和触发异常 捕捉异常12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 捕捉异常第一种 try/except语句try:&lt;语句&gt; #运行别的代码except &lt;名字&gt;：&lt;语句&gt; #如果在try部份引发了'name'异常except &lt;名字&gt;，&lt;数据&gt;:&lt;语句&gt; #如果引发了'name'异常，获得附加的数据else:&lt;语句&gt; #如果没有异常发生# 捕捉异常第二种 try/finallytry:&lt;语句&gt;finally:&lt;语句&gt; #退出try时总会执行raise# 实例1import sys try: f = open('myfile.txt') s = f.readline() i = int(s.strip())except OSError as err: print("OS error: &#123;0&#125;".format(err))except ValueError: print("Could not convert data to an integer.")except: print("Unexpected error:", sys.exc_info()[0]) raise# 实例2如果你只想知道这是否抛出了一个异常，并不想去处理它，那么一个简单的 raise 语句就可以再次把它抛出。try: raise NameError('HiThere')except NameError: print('An exception flew by!') raise# 实例3处理有参数的异常def temp_convert(var): try: return int(var) except (ValueError) as Argument: print ("参数没有包含数字\n", Argument)# 调用函数temp_convert("xyz");# 捕捉异常共同使用try: fh = open("testfile", "w") try: fh.write("这是一个测试文件，用于测试异常!!") finally: print "关闭文件" fh.close()except IOError: print "Error: 没有找到文件或读取文件失败"# 注except (RuntimeError, TypeError, NameError): 触发异常1234567891011121314151617# 函数触发异常def functionName( level ): if level &lt; 1: raise Exception("Invalid level!", level) # 触发异常后，后面的代码就不会再执行# 捕捉异常和触发异常的配合def mye( level ): if level &lt; 1: raise Exception("Invalid level!") # 触发异常后，后面的代码就不会再执行try: mye(0) # 触发异常except Exception: print 1else: print 2 自定义异常12345678910&gt;&gt;&gt;class MyError(Exception): def __init__(self, value): self.value = value def __str__(self): return repr(self.value) &gt;&gt;&gt; try: raise MyError(2*2) except MyError as e: print('My exception occurred, value:', e.value) list的更新更新分为逐元素更新和逐列表更新 逐元素更新12345678910111213# list的逐元素更新li = [] for i in range(5): li.append(i)# 对于空列表，等价于li = [i for i in range(5)]# 或者等价于generatorli = (i for i in range(5))# 或者转化为逐列表更新,常用与列表头和列表尾同时更新for i in range(4): li=[i]+li+[i+11]# 或者对于已经得到的元素[a,b,c] 即对于一个空列表的append，我们总是可以将其转化成列表推导式，并且对于dict和set，只需要将中括号换成大括号即可 逐列表更新123456789# list的逐列表更新li = []ll = [[1,2,3],[4,5,6]]for i in ll: li.extend(i)li[1,2,3,4,5,6]# 或者对于有限个列表c = a+b 注：append()和extend()和+=都是在原有列表增加，+是生成一个新的列表感觉逐列表更新应该可以更优美. os.path.join()12345678910111213141516171819202122232425262728293031323334353637In [42]: import osIn [43]: print("1:",os.path.join('aaaa','bbbb','ccccc.txt'))1: aaaa/bbbb/ccccc.txtIn [44]: print("1:",os.path.join('/aaaa','bbbb','ccccc.txt'))1: /aaaa/bbbb/ccccc.txtIn [45]: print("1:",os.path.join('aaaa','/bbbb','ccccc.txt'))1: /bbbb/ccccc.txtIn [46]: print("1:",os.path.join('aaaa','bbbb','/ccccc.txt'))1: /ccccc.txtIn [47]: print("1:",os.path.join('aaaa','/bbbb','/ccccc.txt'))1: /ccccc.txtIn [48]: print("1:",os.path.join('aaaa','./bbbb','ccccc.txt'))1: aaaa/./bbbb/ccccc.txtIn [49]: print("1:",os.path.join('aaaa','../bbbb','ccccc.txt'))1: aaaa/../bbbb/ccccc.txtIn [50]: print("1:",os.path.join('aaaa','bbbb','./ccccc.txt'))1: aaaa/bbbb/./ccccc.txtIn [51]: print("1:",os.path.join('aaaa','bbbb','../ccccc.txt'))1: aaaa/bbbb/../ccccc.txtIn [54]: print("1:",os.path.join('aaaa','/bbbb','....../ccccc.txt'))1: /bbbb/....../ccccc.txtIn [55]: print("1:",os.path.join('aaaa','bbbb/','ccccc.txt'))1: aaaa/bbbb/ccccc.txtIn [57]: print("1:",os.path.join('aaaa','bbbb\\','ccccc.txt'))1: aaaa/bbbb\/ccccc.txt 对os.path.join()总结如下：从后往前，遇到绝对路径，则绝对路径前面的元素丢弃，遇到类似’…/‘，则将其看成一个普通的路径名字，而对于/在末尾的，会自动根据情况补充。 list()和[]的区别，字符串分割成单个字符123456789101112131415161718192021In [58]: list('abcd')Out[58]: ['a', 'b', 'c', 'd']In [59]: ['abcd']Out[59]: ['abcd']In [61]: aa = (1,2)In [62]: list(aa)Out[62]: [1, 2]In [63]: [aa]Out[63]: [(1, 2)]In [65]: bb = ['abc']In [66]: list(*bb)Out[66]: ['a', 'b', 'c']for i in 'abcd': print(i) 即，list()会拆解输入值，拼接成list，可以用在’abcd’这样的字符串直接拆成’a’,’b’,’c’,’d’这样的形式，因为re.split不支持这种拆分法。当然，如果只是单纯地逐元素访问并逐元素地进行操作，我们可以使用for i in ‘abcd’:这样的访问。也可以认为是’’.join()的逆操作1234567In [65]: bb = ['abc']In [66]: list(*bb)Out[66]: ['a', 'b', 'c']In [71]: ''.join(list(bb))Out[71]: 'abc' list、dict和numpy的互相转换123456789101112131415161718In [74]: c = np.array(&#123;'a':1,'b':2&#125;)In [75]: c[0]---------------------------------------------------------------------------IndexError Traceback (most recent call last)&lt;ipython-input-75-71463270cd6c&gt; in &lt;module&gt;()----&gt; 1 c[0]IndexError: too many indices for arrayIn [77]: cOut[77]: array(&#123;'a': 1, 'b': 2&#125;, dtype=object)In [79]: c.tolist()Out[79]: &#123;'a': 1, 'b': 2&#125;In [80]: d = c.tolist()In [81]: d.__class__Out[81]: dict shell的基础教程12345678for fff in `ls *.json`docconv -f utf8-tw -t UTF8-CN $fff -o simplified/$fffdonefor skill in Ada Coffe Action Java; do echo "I am good at $&#123;skill&#125;Script"done shell 传递参数1234567891011121314151617#!/bin/bash# author:菜鸟教程# url:www.runoob.comecho "Shell 传递参数实例！";echo "执行的文件名：$0";echo "第一个参数为：$1";echo "第二个参数为：$2";echo "第三个参数为：$3";$ chmod +x test.sh $ ./test.sh 1 2 3Shell 传递参数实例！执行的文件名：./test.sh第一个参数为：1第二个参数为：2第三个参数为：3 shell 流程控制123456789if condition1then command1elif condition2 then command2else commandNfi Shell 输入/输出重定向命令 说明command &gt; file 将输出重定向到 file。command &lt; file 将输入重定向到 file。command &gt;&gt; file 将输出以追加的方式重定向到 file。n &gt; file 将文件描述符为 n 的文件重定向到 file。n &gt;&gt; file 将文件描述符为 n 的文件以追加的方式重定向到 file。n &gt;&amp; m 将输出文件 m 和 n 合并。n &lt;&amp; m 将输入文件 m 和 n 合并。&lt;&lt; tag 将开始标记 tag 和结束标记 tag 之间的内容作为输入。需要注意的是文件描述符 0 通常是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。12345678910111213141516171819202122# 如果希望 stderr 重定向到 file，可以这样写：$ command 2 &gt; file# 如果希望将 stdout 和 stderr 合并后重定向到 file$ command &gt; file 2&gt;&amp;1$ command &gt;&gt; file 2&gt;&amp;1# 如果希望对 stdin 和 stdout 都重定向$ command &lt; file1 &gt;file2#Here Document 是 Shell 中的一种特殊的重定向方式，用来将输入重定向到一个交互式 Shell 脚本或程序。$ wc -l &lt;&lt; EOF 欢迎来到 菜鸟教程 www.runoob.comEOF3 # 输出结果为 3 行# 如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null：$ command &gt; /dev/null# 如果希望屏蔽 stdout 和 stderr，可以这样写：$ command &gt; /dev/null 2&gt;&amp;1 str.split(‘’)以列表形式返回1234label_dim = '16803+100'cc = label_dim.split('+')cc['16803','100'] map123456scales_tr = '20,20--20,20'scale = [map(int, x.split(',')) for x in scales_tr.split('--')]scale[&lt;map at 0x7fd798714748&gt;, &lt;map at 0x7fd798714588&gt;]list(scale[0])[20,20] tensor的拼接 t.cat t.stack12345result = []for ii in index:# tensor的截取与合并 cat, stack,cat+view=stack,stack 新增维度进行合并 result.append(fake_img.data[ii])tv.utils.save_image(t.stack(result), opt.gen_img, normalize=True, range=(-1,1)) tensor.view()123x = x.view(x.size(0),-1)# 或者x = x.view(x.size()[0],-1) Inception-V3参考链接https://www.jianshu.com/p/3bbf0675cfcehttps://blog.csdn.net/loveliuzz/article/details/79135583其中都有一些错误，还需要看着源码纠正一下。 python 文件IOpython中的三个读read(),readline()和readlines().read() 每次读取整个文件，它通常用于将文件内容放到一个字符串变量中，然而 .read() 生成文件内容最直接的字符串表示，但对于连续的面向行的处理，它却是不必要的.readlines()之间的差异是后者一次读取整个文件，象 .read()一样。.readlines()自动将文件内容分析成一个行的列表，该列表可以由 Python 的 for… in … 结构进行处理.readline()每次只读取一行 python打开多个文件123with open('a.txt', 'r') as a, open('b.txt', 'r') as b: print(a.read()) print(b.read()) 123456789101112131415161718192021222324252627282930313233343536373839404142#!/usr/bin/env python# coding: utf-8class open_many: def __init__(self, files=None, mode='r'): if files is None: self._files = [] else: self._files = files self.mode = mode self.fds = [] # fd is short for file descriptor def __enter__(self): print('--&gt;enter') for f in self._files: print('--&gt;opening file') self.fds.append(open(f, self.mode)) return self.fds def __exit__(self, exc_type, exc_val, traceback): print('--&gt;exit') for fd in self.fds: print('--&gt;closing file') fd.close() if exc_type == ValueError: print('--&gt;exception: ' + str(exc_val)) return Trueif __name__ == '__main__': print('') with open_many(['a.txt', 'b.txt'], 'r') as files: for f in files: print f.read() print('') with open_many() as files: raise ValueError('captured') print('') with open_many() as files: raise Exception('uncaptureable') python csv在使用常规的读取文件的方法的时候，出现了问题，每一个数字包括小数点都被当成了一个字符，这样明显是不对的，对于数字的csv，要考虑下这个方法，我感觉应该和写的方式有关，待续。参考链接https://www.cnblogs.com/dmir/p/5009075.html1234with open(path) as f: f_csv = csv.reader(f) headers = next(f_csv) for i in f_csv: 12import pandas as pdcsv_data = pd.read_csv(path) 优化器优化器与模型参数完全共享内存，一个改变，另一个会立即跟着改变。不能重复加载同一个参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138# 第一种optimizer = t.optim.Adam(model.parameters(), lr = 0.1)optimizerAdam (Parameter Group 0 amsgrad: False betas: (0.9, 0.999) eps: 1e-08 lr: 0.1 weight_decay: 0)for i in optimizer.param_groups: print(i) print('______') &#123;'params': [Parameter containing:tensor([[[[ 0.0493, 0.1696, 0.0647], [ 0.1935, 0.3102, -0.0871], [-0.2787, 0.0894, -0.0438]]], [[[-0.2671, 0.2079, 0.2474], [ 0.2068, -0.1825, 0.1427], [-0.0853, -0.1799, -0.2465]]]]), Parameter containing:tensor([-0.3158, 0.1429]), Parameter containing:tensor([[[[ 0.2063, 0.0771, 0.1579], [ 0.1543, 0.1374, -0.1951], [-0.1221, 0.0099, -0.1331]], [[-0.1899, 0.1978, 0.1065], [ 0.1400, -0.0740, 0.0397], [-0.2165, -0.0180, 0.1072]]], [[[ 0.0692, -0.1296, 0.0524], [ 0.0577, -0.1184, 0.0697], [ 0.0859, -0.2086, 0.0419]], [[-0.0270, 0.1836, -0.0649], [ 0.1680, -0.1061, -0.2357], [-0.0408, 0.0799, 0.0065]]]]), Parameter containing:tensor([ 0.1269, -0.1582]), Parameter containing:tensor([[[[ 0.0185, -0.2579, -0.1185], [ 0.1269, 0.0274, 0.1019], [ 0.0329, -0.1229, -0.1922]]]]), Parameter containing:tensor([-0.2731])], 'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False&#125;______ # 第二种optimizer = t.optim.Adam([&#123;'params':model.net1.parameters(),'lr':0.4&#125;, &#123;'params':model.net2.parameters(),'lr':0.1&#125;],lr=0.04)optimizerAdam (Parameter Group 0 amsgrad: False betas: (0.9, 0.999) eps: 1e-08 lr: 0.4 weight_decay: 0Parameter Group 1 amsgrad: False betas: (0.9, 0.999) eps: 1e-08 lr: 0.1 weight_decay: 0)for i in optimizer.param_groups: print(i) print('______')&#123;'params': [Parameter containing:tensor([[[[ 0.0493, 0.1696, 0.0647], [ 0.1935, 0.3102, -0.0871], [-0.2787, 0.0894, -0.0438]]], [[[-0.2671, 0.2079, 0.2474], [ 0.2068, -0.1825, 0.1427], [-0.0853, -0.1799, -0.2465]]]]), Parameter containing:tensor([-0.3158, 0.1429])], 'lr': 0.4, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False&#125;______&#123;'params': [Parameter containing:tensor([[[[ 0.2063, 0.0771, 0.1579], [ 0.1543, 0.1374, -0.1951], [-0.1221, 0.0099, -0.1331]], [[-0.1899, 0.1978, 0.1065], [ 0.1400, -0.0740, 0.0397], [-0.2165, -0.0180, 0.1072]]], [[[ 0.0692, -0.1296, 0.0524], [ 0.0577, -0.1184, 0.0697], [ 0.0859, -0.2086, 0.0419]], [[-0.0270, 0.1836, -0.0649], [ 0.1680, -0.1061, -0.2357], [-0.0408, 0.0799, 0.0065]]]]), Parameter containing:tensor([ 0.1269, -0.1582]), Parameter containing:tensor([[[[ 0.0185, -0.2579, -0.1185], [ 0.1269, 0.0274, 0.1019], [ 0.0329, -0.1229, -0.1922]]]]), Parameter containing:tensor([-0.2731])], 'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False&#125;______optimizer.state_dict()&#123;'state': &#123;&#125;, 'param_groups': [&#123;'lr': 0.4, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [140617843939944, 140617843755120]&#125;, &#123;'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [140617843755192, 140617843755264, 140617843755336, 140617843755408]&#125;]&#125;# 第三种optimizer = t.optim.Adam([&#123;'params':model.net1.parameters(),'lr':0.4&#125;])optimizer.add_param_group(&#123;'params':model.net2.parameters(),'lr':0.3&#125;)# 第四种for param_group in optimizer.param_groups: param_group['lr']=lr_new 模型保存123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 只保留模型参数并且加载t.save(model.state_dict(),'model_state_dict')model.state_dict()OrderedDict([('net1.weight', tensor([[[[-0.3164, -0.2508, -0.3294], [ 0.2388, -0.1582, 0.0678], [ 0.0194, 0.1120, 0.2794]]], [[[-0.2425, 0.0833, -0.0842], [ 0.0687, -0.0637, -0.3034], [-0.3268, -0.1049, -0.0286]]]])), ('net1.bias', tensor([ 0.2742, 0.2194])), ('net2.weight', tensor([[[[ 0.2241, 0.2280, -0.0597], [-0.1045, -0.1610, 0.0445], [-0.1772, -0.0639, -0.0172]], [[ 0.0975, -0.0081, 0.0690], [-0.1273, 0.0693, 0.1792], [ 0.0773, 0.1652, -0.1688]]], [[[-0.2314, 0.0494, -0.0648], [-0.1919, 0.2145, 0.0369], [-0.1336, -0.1077, -0.0743]], [[ 0.1510, -0.0868, -0.1766], [-0.1764, 0.0398, 0.2146], [-0.0269, 0.1241, -0.2304]]]])), ('net2.bias', tensor(1.00000e-02 * [-7.3981, -0.8345]))])temp = t.load('model_state_dict.pth')OrderedDict([('net1.weight', tensor([[[[-0.3164, -0.2508, -0.3294], [ 0.2388, -0.1582, 0.0678], [ 0.0194, 0.1120, 0.2794]]], [[[-0.2425, 0.0833, -0.0842], [ 0.0687, -0.0637, -0.3034], [-0.3268, -0.1049, -0.0286]]]])), ('net1.bias', tensor([ 0.2742, 0.2194])), ('net2.weight', tensor([[[[ 0.2241, 0.2280, -0.0597], [-0.1045, -0.1610, 0.0445], [-0.1772, -0.0639, -0.0172]], [[ 0.0975, -0.0081, 0.0690], [-0.1273, 0.0693, 0.1792], [ 0.0773, 0.1652, -0.1688]]], [[[-0.2314, 0.0494, -0.0648], [-0.1919, 0.2145, 0.0369], [-0.1336, -0.1077, -0.0743]], [[ 0.1510, -0.0868, -0.1766], [-0.1764, 0.0398, 0.2146], [-0.0269, 0.1241, -0.2304]]]])), ('net2.bias', tensor(1.00000e-02 * [-7.3981, -0.8345]))])model2.state_dict()OrderedDict([('net3.weight', tensor([[[[ 0.2793, -0.2330, 0.3270], [-0.1419, 0.1562, 0.1875], [-0.0249, 0.1297, 0.1642]]], [[[ 0.2770, 0.1016, -0.1096], [ 0.1929, 0.0210, 0.1722], [ 0.1304, 0.0820, 0.1205]]]])), ('net3.bias', tensor([-0.3235, -0.1770])), ('net4.weight', tensor([[[[-0.2043, -0.1492], [ 0.1728, -0.1069]], [[-0.2903, 0.3385], [ 0.2778, 0.1589]]], [[[-0.1423, -0.0439], [ 0.2849, -0.0840]], [[ 0.0354, 0.1711], [-0.0274, -0.2220]]]])), ('net4.bias', tensor([-0.0264, -0.1094]))])model2.load_state_dict(temp)Missing key(s) in state_dict: "net3.weight", "net3.bias", "net4.weight", "net4.bias". Unexpected key(s) in state_dict: "net1.weight", "net1.bias", "net2.weight", "net2.bias". # 印证了在保存模型参数的时候是根据名字进行保存，# 保留模型t.save(model,'model.pth')temp2 = t.load('model.pth')Nettest( (net1): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (net2): Sequential( (0): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)) ))加载进来就是一个模型，包括forward什么的都还在。 numpy和list和tensor对于size的访问区别 list: 只有len()方法，返回的是最外层的个数，reshape方法 numpy: b.size是全部个数，b.shape是(2,3) b = np.arange(6).reshape(2,3), b.resize(2,3) np.arange(1,6,2) tensor: c.shape==c.size() len(c)==c.size(0) 返回torch.size([3,2]), c.resize_(4,4)(可以改变自身尺寸) c.resize(1,4)（来源于torchvision，可以忽略）==c.reshape(1,4)(对于连续地址共享内存，不连续地址则复制)==c.view(1,4)(共享内存) t.arange(1,6,2) t_.unsqueeze(1)tensor的普通索引基本共享内存，而高级索引基本不共享内存。 numpy—&gt;tensor t_ = t.from_numpy(numpy_)(共享内存）或者 t_ = t.tensor(numpy_)(返回新对象) tensor—&gt;numpy np_ = t_.numpy()(共享内存) 或者 np_ = np.array(t_) numpy—&gt;list list_ = np_.tolist()(不共享内存） list—&gt;numpy np_ = np.array(list_)(不共享内存） tensor—&gt;list list_ = t_.tolist() (不共享内存) 或者 item_ = t_.item() (不共享内存） list—&gt;tensor t_ = t.tensor(list_)(不共享内存）也就是说numpy和tensor可以做到互相共享内存，而list只是一个对外的和Python相连接的一个形式.补充：基于numpy和tensor，推荐使用shape属性， 修改形状则分别使用reshape()和view(), 123456789101112131415161718192021222324import numpy as npimport torch as t# a,b,c共享内存a = np.ones([2,3])b = t.from_numpy(a)c = b.numpy()# a,b,c不共享内存a = np.ones([2,3])b = t.tensor(a)c = np.array(b)import numpy as np# 不共享内存a = np.ones([2,3])b = a.tolist()c = np.array(b)import tensor as ta = t.tensor([2,3])b = a.item()b = a.tolist()c = t.tensor(b) torch.Tensor 和 torch.tensor的区别暂时还没有组织好的语言，先以代码的形式记录下来主要是类型和对0维元素的区别。1234567891011121314151617181920212223242526In [12]: import torch as tIn [13]: t.Tensor(3)Out[13]: tensor([-4.8232e+13, 4.5581e-41, -1.8931e-03])In [14]: t.Tensor([3,4])Out[14]: tensor([ 3., 4.])In [15]: t.tensor(3)Out[15]: tensor(3)In [16]: t.tensor([3,4])Out[16]: tensor([ 3, 4])In [17]: a = t.tensor([3,4])In [18]: type(a)Out[18]: torch.TensorIn [19]: a.dtypeOut[19]: torch.int64# Tensor只是tensor(dtype=float)的别名。x = torch.tensor([3.0],requires_grad=True)# torch.Tensor不接收requires_grad参数# torch.tensor只有是float型参数的情况下才接受requires_grad参数 tensor.new()和tensor.data.new()暂时不知道这两者的区别，但是书上的代码多数都是tensor.data.new() topk的用法output.data[0].topk(1)[1][0].item()123456789101112131415161718In [20]: x = t.arange(1,6)In [21]: y = t.topk(x,2)In [22]: type(y)Out[22]: tupleIn [23]: yOut[23]: (tensor([ 5., 4.]), tensor([ 4, 3]))In [24]: y[1]Out[24]: tensor([ 4, 3])In [25]: y[1][1]Out[25]: tensor(3)In [26]: y[1][1].item()Out[26]: 3 ipython和jupyter和pycharm在写代码的前期，用jupyter好使，因为对于不确定的比较多的代码是可以直接看到结果，对某一段进行调试，检查某一段的基本语法错误，或者对于某个想法的实现，是简单直接的甚至对于中型代码，用代码框可以实现视觉上的分离，逻辑清晰，并且支持markdown的记录与注释，对于不了解的代码有很好的支持性。但是在写代码的后期，jupyter的弊端逐渐显现，不能使用模板，init.py的生成不好使，文件与文件夹的关系不清晰。甚至一个简单的文件或者文件夹挪动位置都很麻烦，需要桌面的辅助。而pycharm对于文件管理，init.py等有很好的支持性。更适合写已经成熟的代码。这一下难住我了，作为新手，肯定每次都要实验好些代码，看输入输出的效果，如果是pycharm则比较麻烦，对于调试很啰嗦。命令行窗口做为补充，也不好使，因为每次能看到的东西有限，重复性差，只能用于单句代码的验证。所以不妨这样，前期开发还是用jupyter，等开发的差不多了，甚至等单个文件已经开发完毕，这样的话开发就可以先不管文件夹的事，等各个文件开发完毕，再转成pycharm，来实现文件夹、文件的组织管理和后期的调试，这是因为现在多数不使用jupyter直接运行，而是使用py进行运行。我觉得应该有很多人用.ipynb进行运行，但是我不知道怎么才能更好的运行。 anaconda虚拟环境不错 python如果在 Python 脚本文件首行输入#!/usr/bin/env python，那么可以在命令行窗口中执行/path/to/script-file.py以执行该脚本文件。使用三引号(‘’’或”””)可以指定一个多行字符串。]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2018%2F09%2F13%2Fnumpy%2F</url>
    <content type="text"><![CDATA[简介参考链接NumPy Ndarray对象这只是简单的入门，以后接触得越多，对于其中的理解也才会更加全面，并做补充。12345678910111213141516171819# 1.简单的array对象a = np.array([1, 2, 3], dtype = complex)# 2.结构数组形式的array对象student = np.dtype([('name','S20'), ('age', 'i1'), ('marks', 'f4')]) a = np.array([('abc', 21, 50),('xyz', 18, 75)], dtype = student) print(a)[('abc', 21, 50.0), ('xyz', 18, 75.0)]a[0]['name']# 3.其他形式a.shapea.reshape(2,4,3) x.itemsize 每个元素的字节单位长度numpy.frombuffer() 怎么理解？？np.fromiter(iter(list))numpy.arange(start, stop, step, dtype)numpy.linspace(start, stop, num, endpoint, retstep, dtype)numpy.logscale(start, stop, num, endpoint, base, dtype) numpy的保存和读取，这里还是有点东西的，待续12345678910# numpy在save和load的时候没有显式保存变量名np.save("A.npy",A)B=np.load("A.npy")Bnp.savez("files.npz",A,B,C_array=C)np.savez_compressed("files.npz",A,B,C_array=C)D=np.load("files.npz")D['arr_1']D['C_array'] numpy savetxt() loadtxt12np.savetxt('a.txt',a,fmt='%d', delimiter=',')b= np.loadtxt('a.txt',delimiter=',') 12np.savetxt('a.csv',a,fmt='%d', delimiter=',')b= np.loadtxt('a.csv',delimiter=',') 索引ndarray对象中的元素遵循基于零的索引。 有三种可用的索引方法类型： 字段访问，基本切片和高级索引，可以使用省略号当全部切片只是返回一个视图，高级索引返回数据的副本，并且切片是全取，而高级索引是取对应的位置的元素y = x[1:3,1:2]:两行元素,y = x[[1,2],[1,3]]:两个元素布尔索引是数据的复制广播法则数组上的迭代 for i in np.nditer(a,order=’C’ or order=’F’), np.nditer(a).next()广播迭代 reshape：不改变数据的条件下修改形状flat：数组上的一维迭代器， for i in a.flat , a.flat[2:4], 暂时看不出来flat和nditer的区别flatten: 返回折叠为一维的数组副本ravel： 返回连续的展开数组 flattten和ravel的区别暂时不知道在哪numpy.rollaxis：这里的转轴有问题，比较好的理解是[[[000,001],[010,011]],[[100,101],[110,111]]]，[参考：]https://blog.csdn.net/liaoyuecai/article/details/80193996，还有一种理解是从页，行，列的方式，每次都在原数组上以固定页，固定行的方式进行读取，保证所有的数字以一列的方式，即总是表示成000,001,010,011,100,101,110,111，再想一个更简单的方法。跨括号法，不扯numpy.swapaxes(arr, axis1, axis2) 修改维度序号 维度和描述 broadcast 产生模仿广播的对象 b = np.broadcast(x,y) c = np.empty(b.shape) c.flat = [u + v for (u,v) in b],并且np.nditer()也可以达到相同的效果 broadcast_to 将数组广播到新形状 numpy.broadcast_to(array, shape, subok) expand_dims 扩展数组的形状numpy.expand_dims(arr, axis) squeeze 从数组的形状中删除单维条目 y = np.squeeze(x,axis=(0,1)) 数组的连接序号 数组及描述 concatenate 沿着现存的轴连接数据序列，不产生新的轴 stack 沿着新轴连接数组序列，产生新的轴 hstack 水平堆叠序列中的数组(列方向) vstack 竖直堆叠序列中的数组(行方向)数组分割序号 数组及操作 split 将一个数组分割为多个子数组 hsplit 将一个数组水平分割为多个子数组(按列) vsplit 将一个数组竖直分割为多个子数组(按行) 添加/删除元素序号 元素及描述 resize 返回指定形状的新数组 append 将值添加到数组末尾 insert 沿指定轴将值插入到指定下标之前 delete 返回删掉某个轴的子数组的新数组 unique 寻找数组内的唯一元素 切片的新表达式： np.s_[::2]位运算：跳过 字符串函数：对dtype为numpy.string_或numpy.unicode_的数组执行向量化字符串操作 add() 返回两个str或Unicode数组的逐个字符串连接 multiply() 返回按元素多重连接后的字符串 center() 返回给定字符串的副本，其中元素位于特定字符串的中央 capitalize() 返回给定字符串的副本，其中只有第一个字符串大写 title() 返回字符串或 Unicode 的按元素标题转换版本 lower() 返回一个数组，其元素转换为小写 upper() 返回一个数组，其元素转换为大写 split() 返回字符串中的单词列表，并使用分隔符来分割 splitlines() 返回元素中的行列表，以换行符分割 strip() 返回数组副本，其中元素移除了开头或者结尾处的特定字符 join() 返回一个字符串，它是序列中字符串的连接 replace() 返回字符串的副本，其中所有子字符串的出现位置都被新字符串取代 decode() 按元素调用str.decode encode() 按元素调用str.encode dtype???add()，subtract()，multiply()和divide()排序quicksort, mergesort, heaqsortdt = np.dtype([(‘name’, ‘S10’),(‘age’, int)])a = np.array([(“raju”,21),(“anil”,25),(“ravi”, 17), (“amar”,27)], dtype = dt)print(np.sort(a, order = ‘name’))numpy.argsort()numpy.lexsort()np.argmax() np.argmin()np.nonzero()np.where()np.extract() 改变形状： b.shape = 3,2无复制： b = a 值和形状都是共享，id相同浅复制: b = a.view() 值共享，形状不共享，id不同 切片也是浅复制深复制：ｂ＝ a.copy() numpy.matlib 矩阵库，返回的是矩阵matrix对象，而不是ndarray对象 .empty(), .zeros(), .ones(), eye(), identity(), rand() 只能是二维的np.matrix(‘1,2;3,4’) np.matirx([[1,2],[3,4]]) array和asarray都可以将结构数据转化为ndarray，但是主要区别就是当数据源是ndarray时，array仍然会copy出一个副本，占用新的内存，但asarray不会。matrix和array互换： np.matrix(np.array) np.array(np.matrix),此时两者值和形状没有关系，使用asmatrix和asarray时，值共享，形状不共享暂时没有想到matrix的意义 线性代数numpy.linalg貌似可以直接作用于列表 dot 两个数组的点积，也就是矩阵式乘法 np.dot(a,b)，对于多维数组的乘法，可以同样以XXX0.和XXX.0的方法对于两个1维数组，是点积，对于两个矩阵，是矩阵乘法，对于1维数组和矩阵，则对1维数组适当地转置，然后进行矩阵乘法，需要满足倒数第一维和倒数第二维相等。 vdot 两个向量的点积，也就是矩阵式对应元素的乘积和 inner 两个数组的内积 matmul 两个数组的矩阵积 determinant 数组的行列式 numpy.linalg.det() solve 求解线性矩阵方程 inv 寻找矩阵的乘法逆矩阵 Matplotlibfrom matplotlib import pyplot as plt numpy转化数据类型123456789101112131415In [11]: arr = np.array([1,2,3,4,5])In [12]: arrOut[12]: array([1, 2, 3, 4, 5])# 该命令查看数据类型In [13]: arr.dtypeOut[13]: dtype('int64')In [14]: float_arr = arr.astype(np.float64)# 等价于，即str和data-type是一样的float_arr = arr.asdtype('float64')#该命令查看数据类型In [15]: float_arr.dtypeOut[15]: dtype('float64') 字符串数组转化为数值型123456789In [4]: numeric_strings = np.array(['1.2','2.3','3.2141'], dtype=np.string_)In [5]: numeric_stringsOut[5]: array(['1.2', '2.3', '3.2141'], dtype='|S6')# 此处写的是float 而不是np.float64, Numpy很聪明，会将python类型映射到等价的dtype上# # 这里的float是Python的数据类型，NumPy会自动的将其映射到等价的dtype上，即np.float64In [6]: numeric_strings.astype(float)Out[6]: array([ 1.2, 2.3, 3.2141]) 所以astype一共可以接受三种参数 第一种是dtype，即np.int32这种， 第二种是字符串，即’int32’这样，与第一种相呼应， 第三种是Python的数据类型，会自动转化。 numpy中的数据类型转换，不能直接改原数据的dtype! 只能用函数astype()。[参考链接]https://www.cnblogs.com/hhh5460/p/5129032.html123456789101112131415161718192021# 如果直接修改dtype，会导致长度发生改变&gt;&gt;&gt; a.dtype = 'float16'&gt;&gt;&gt; aarray([ -9.58442688e-05, 7.19000000e+02, 2.38159180e-01, 1.92968750e+00, nan, -1.66034698e-03, -2.63427734e-01, 1.96875000e+00, -1.07519531e+00, -1.19625000e+02, nan, 1.97167969e+00, -1.60156250e-01, -7.76290894e-03, 4.07226562e-01, 1.94824219e+00], dtype=float16)&gt;&gt;&gt; a.shape(16,)&gt;&gt;&gt; a.dtype = 'float16'&gt;&gt;&gt; aarray([ -9.58442688e-05, 7.19000000e+02, 2.38159180e-01, 1.92968750e+00, nan, -1.66034698e-03, -2.63427734e-01, 1.96875000e+00, -1.07519531e+00, -1.19625000e+02, nan, 1.97167969e+00, -1.60156250e-01, -7.76290894e-03, 4.07226562e-01, 1.94824219e+00], dtype=float16)&gt;&gt;&gt; a.shape(16,) 对于字符串数组还没有找到合理的说明，sad np.in1d(x,y)123456789101112&gt;&gt;&gt; test = np.array([0, 1, 2, 5, 0])&gt;&gt;&gt; states = [0, 2]&gt;&gt;&gt; mask = np.in1d(test, states)&gt;&gt;&gt; maskarray([ True, False, True, False, True], dtype=bool)&gt;&gt;&gt; test[mask]array([0, 2, 0])&gt;&gt;&gt; mask = np.in1d(test, states, invert=True)&gt;&gt;&gt; maskarray([False, True, False, True, False], dtype=bool)&gt;&gt;&gt; test[mask]array([1, 5]) np.argsort()123456789101112x = np.array([3, 1, 2])y = np.argsort(x)array([1, 2, 0])In [48]: xxOut[48]: array([3, 1, 2])In [49]: yyOut[49]: array([1, 2, 0])In [50]: xx[yy]Out[50]: array([1, 2, 3]) np.setdiff1d()这种是以集合的方式，会把列表先压平，@return: sorted 1D array1234&gt;&gt;&gt; a = np.array([1, 2, 3, 2, 4, 1])&gt;&gt;&gt; b = np.array([3, 4, 5, 6])&gt;&gt;&gt; np.setdiff1d(a, b)array([1, 2]) np.argwhere()@return: index_array12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; x = np.arange(6).reshape(2,3)&gt;&gt;&gt; xarray([[0, 1, 2], [3, 4, 5]])&gt;&gt;&gt; np.argwhere(x&gt;1)array([[0, 2], [1, 0], [1, 1], [1, 2]])In [32]: x[0,2]Out[32]: 2In [26]: x[[0,1],[2,0]]Out[26]: array([2, 3])In [29]: z = zip([0,1],[2,0])In [30]: for i in z: ...: print(i) ...: (0, 2)(1, 0)In [53]: x[(0,1),(2,0)]Out[53]: array([2, 3])In [73]: z = list(zip(*y))In [74]: x[z]Out[74]: array([2, 3, 4, 5]) np.setdiff1d(x,y),intersect1d(x,y)集合的减法运算,交集运算 np.random.choice(5,3)123a = np.random.choice(5,3)aarray([0, 3, 2])]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch chapter8 neural style]]></title>
    <url>%2F2018%2F09%2F11%2Fpytorch-chapter8-neural-style%2F</url>
    <content type="text"><![CDATA[1 前言本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。这是我的代码大神链接：(https://github.com/anishathalye/neural-style) 2 问题及其解决我在第六章和第七章的时候还是基于pytorch 0.4.0，而第八章的时候我开始基于pytorch 0.4.1，所以以下的内容介绍都是基于0.4.1 2.1 文件组织形式123456789101112131415161718192021├─checkpoints/├─content_img/│ ├─input.jpg│ ├─output.jpg│ └─style.jpg├─data/│ ├─coco/a.jpg├─dataset/│ ├─__init__.py│ └─dataset.py├─models/│ ├─__init__.py│ └─PackedVGG.py│ └─transformer_net.py└─utils/│ ├─__init__.py│ └─utils.py│ └─visualize.py├─config.py└─main.py 其中，上半部分是对数据和模型的保存组织形式，我们只需要能对应起来即可，其中，checkpoints是为了保存模型，content_img中的style.jpg是训练时候的风格图片，input.jpg是测试的输入，output.jpg是测试的输出，data中的数据是训练数据，主要是因为这个训练数据太整齐，是用ImageFolder读取的，为了避免麻烦，也为了在测试的时候方便观察图片，所以style.jpg我们暂时放在了content中。下半部分是重点，我们需要写的代码，每次都是先从dataset.py和models开始写起，然后导入visualize.py，这个文件基本不会发生改变，然后同时写main.py和config.py，边写边扩展utils中的其他文件，例如main中用到的函数等等。 2.2 modelsPackedVGG.py这里我们主要是取已有的网络，得到中间层的输出models.named_parameters():返回的是一个生成器，每次返回一个参数的关键字和值models.state_dict():返回的是一个字典，记录了参数的关键字和值models.parameters():返回的是变量，没有名字，可以在requires_grad中用到models.features返回的是相对应的模型1234567891011121314151617181920212223242526272829In [7]: from torchvision.models import vgg16In [8]: models = vgg16(pretrained=True)In [9]: model = models.features[:1]In [10]: modelOut[10]: Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))In [11]: models.parameters()Out[11]: &lt;generator object Module.parameters at 0x7f8fad26b3b8&gt;In [12]: models.named_parameters()Out[12]: &lt;generator object Module.named_parameters at 0x7f8f29e99d58&gt;In [13]: model.named_parameters()Out[13]: &lt;generator object Module.named_parameters at 0x7f8fad26b2b0&gt;In [14]: model.parameters()Out[14]: &lt;generator object Module.parameters at 0x7f8fad26b4c0&gt;In [15]: model.state_dict()Out[15]: OrderedDict([('0.weight', tensor([[[[-0.5537, 0.1427, 0.5290], [-0.5831, 0.3566, 0.7657], [-0.6902, -0.0480, 0.4841]], 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107from torchvision.models import vgg16models = vgg16(pretarined = True)In [19]: modelsOut[19]: VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace) (2): Dropout(p=0.5) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace) (5): Dropout(p=0.5) (6): Linear(in_features=4096, out_features=1000, bias=True) ))In [20]: models.featuresOut[20]: Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))In [21]: models.features[1]Out[21]: ReLU(inplace)# listIn [27]: models4 = models2[0:2]In [28]: models4Out[28]: Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace))In [32]: models4listOut[32]: [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace)]In [36]: models4list[1]Out[36]: ReLU(inplace)In [37]: models4list[1].named_parametersOut[37]: &lt;bound method Module.named_parameters of ReLU(inplace)&gt; sequencial是支持索引操作的list(module)会变成一个list，可以通过索引来获取层，注意，nn.ModuleList, nn.Sequential, nn.Conv等都是Module,都可以通过named_parameters来获取参数。为了能够提取出中间层的输出，作者换了一个方法，用的nn.ModuleList,nn.ModuleList和nn.Sequential的区别在此才真正显现，nn.Sequential更有利于直接把输入传给Module，计算是一个整体，写起来更方便，而nn.Modulist则不能直接把输入传给Module，需要用循环传输入，更有利于在层中做一些保留，提取中间层的输出。后面我们会讲到hook。或者说提取中间层的输出我们可以选择在定义网络的forward中进行，另外，就是需要注意的是，这里的输入是一个batch_size大小的矩阵，所以即便像作者这样，用一个列表保存输出，但实际输出的列表中的元素都是(b,n,h,w)大小的。后面我会验证。 提取中间层的输出有两种方法：第二种方法参考链接：https://www.jianshu.com/p/0a23db1df55a12345678910# 第一种方法，这种方法是在前向网络中提取输出，好像也是在反向传播网络中，但这种提取中间层是永久性的，也适合用这些层的做其他运算，这些运算是计算在整体网络框架中的def forward(self, x): results = [] for ii, model in enumerate(self.features): x = model(x) if ii in &#123;3, 8, 15, 22&#125;: results.append(x) vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3']) return vgg_outputs(*results) 12345# 第二种方法，适合在在不影响整体网络的情况下拿出一个分支进行单独计算，现在还不清楚这样子会不会影响backward，个人感觉会，因为也是相当于一个变量对其进行计算，导数为1。def forward(self, x): x= self.model(x) self.fea = x x = self.main(x) transformer.py可参考链接 padding的操作是边界反射补充 放大方法是双线性插值，而不是ConvTransposed2d，即unsample或者说是interpolate， 但是其中的一个参数align_corners一直没有理解，既然是双线性插值，那结果就是固定的，怎么还会因为其他参数发生变化。 其中，写的时候必要的时候可以写写子网络这里我对residualblock提出了疑问，事实上left+right后面可以没有relu层，这一点我们可以从以下链接找到说明。https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.pyhttp://torch.ch/blog/2016/02/04/resnets.html The above result seems to suggest that it’s important to avoid changing data that passes through identity connections only. We can take this philosophy one step further: should we remove the ReLU layers at the end of each residual block? ReLU layers also perturb data that flows through identity connections, but unlike batch normalization, ReLU’s idempotence means that it doesn’t matter if data passes through one ReLU or thirty ReLUs. When we remove ReLU layers at the end of each building block, we observe a small improvement in test performance compared to the paper’s suggested ReLU placement after the addition. However, the effect is fairly minor. More exploration is needed. 对于其他的出现的网络架构，其实都是有理可循的，但暂时不是本篇的重点，所以只做一个记录。上卷积简单地看了看这篇论文，unsample要比ConvTransposed2D要好，但是没有看懂。留作后续。 dataset.py &amp; visualize.py因为加载数据是用的tv.datasets.ImageFolder，所以dataset.py不需要写，visualize.py是第六章的时候写好的，这里只写几个改进的 self.vis = Visdom(env=env,use_incoming_socket=False, **kwargs)，这里的use_incoming_socket是不需要从浏览器接受数据到软件中，如果没有的话会提示 ‘&gt;’ not supported between instances of ‘float’ and ‘NoneType’ 在一个函数前提示输入的大小和类型是一件很重要的事情，必要的时候需要输入分布， 这里的plot用了一个很巧的方法，用字典记录不同的点123self.index = &#123;&#125;x = self.index.get(win,0)self.index[win] = x+1 其他的细节可以看代码中的记录，应该比较清晰了。 main.py &amp; utils.py &amp; config.py其中utils主要为main提供一些用到的函数，config提供参数，main作为主函数，里面主要就是train(),val(),test(),help(),下面记录一些写main函数的一些疑问。 cuda这里写几种怎么从cpu到gpu的方法以及应用场景。123456789101112131415161718192021# 第一种device = t.device('cuda') if opt.use_gpu else t.device('cpu')models.to(device)tensor = tensor.to(device)此时使用默认的cuda，一般是cuda:0，适用于全局# 第二种torch.cuda.current_device() # 查询当前GPUtorch.cuda.set_device(1)device = torch.device('cuda')models.to(device)此时用的是cuda:1，使用于全局#第三种#上下文管理器with torch.cuda.device(1): models.to(device)#第四种import osos.environ["CUDA_VISIBLE_DEVICES"]="2"没用过 tqdmhttps://blog.csdn.net/langb2014/article/details/54798823?locationnum=8&amp;fps=1进度条，但是只在jupyter和终端中用的时候效果很明显，在代码中用的效果没有那么好，tqdm试了试，用在enumerate()中时，需要写成这样：123456elements = ('a', 'b', 'c')for count, ele in tqdm(enumerate(elements)): print(count, i)# two argumentsfor count, ele in tqdm(enumerate(elements), total=len(train_ids), leave=False): print(count, i) 包括zip也是一样，因为他们返回的是一个生成器，并不知道长度。 tqdm的进一步用法 123456from tqdm import tqdmpb = tqdm(total=len(idxs))pb.set_description('&#123;&#125;&#123;&#125;'.format(phase, domain))for i in range(5): pb.update(1)pb.close() 反向传播和梯度下降参考链接https://blog.csdn.net/qq_16234613/article/details/80025832这里主要是针对第七章和第八章出现的反向传播和梯度下降出现的问题进行记录。在第七章，是这么实现分别训练的1234567891011121314fake_img = netg(noises).detach() fake_output = netd(fake_img)error_d_fake = criterion(fake_output, fake_labels)error_d_fake.backward()optimizer_d.step()optimizer_g.zero_grad()noises.data.copy_(t.randn(opt.batch_size, opt.nz, 1, 1))fake_img = netg(noises)output = netd(fake_img)error_g = criterion(output, true_labels)error_g.backward()optimizer_g.step() y = x.detach()：表示将生成一个新的叶子节点，值与当前节点的值相同，但是y.requires_grad = False, y.grad_fn=None，此时x和y共享内存，对y数据的操作也会影响x，可以理解为冻结了通过y进行反向传播的路。如果在网络的输出detach，即y= models(x).detach()，可以理解成，models只进行前向传播，grad=None。1234567891011121314151617181920In [17]: a = torch.ones(3,3)In [18]: a.requires_grad=TrueIn [19]: b = a*2In [20]: b.requires_gradOut[20]: TrueIn [21]: b.grad_fnOut[21]: &lt;MulBackward at 0x7f8fac6e40f0&gt;In [22]: c = b.detach()In [23]: c.requires_gradOut[23]: FalseIn [24]: print(c.grad_fn)NoneIn [25]: c.is_leaf 123456789101112131415161718192021222324252627282930313233343536373839In [2]: a = torch.ones(3,3)In [14]: bOut[14]: tensor([[2., 2., 2.], [2., 2., 2.], [2., 2., 2.]], grad_fn=&lt;MulBackward&gt;)In [15]: c = b.detach()In [16]: cOut[16]: tensor([[2., 2., 2.], [2., 2., 2.], [2., 2., 2.]])In [17]: c[0,0]=1In [18]: cOut[18]: tensor([[1., 2., 2.], [2., 2., 2.], [2., 2., 2.]])In [19]: bOut[19]: tensor([[1., 2., 2.], [2., 2., 2.], [2., 2., 2.]], grad_fn=&lt;MulBackward&gt;)In [20]: c.requires_gradOut[20]: FalseIn [21]: b.grad_fnOut[21]: &lt;MulBackward at 0x7f764429ffd0&gt;In [22]: b.grad_fn.next_functionsOut[22]: ((&lt;AccumulateGrad at 0x7f7644428358&gt;, 0),)In [23]: a.grad_fn 在第八章，是这么表示的12for param in vgg16.parameters(): param.requires_grad = False 这种表示可以使得某一个网络不参与梯度下降这个过程，但是对于网络的输入和输出还是支持梯度下降的。requires_grad只是表示当前的变量不再需要梯度下降，综上所述，对于中间变量，需要使用x.detach()，使其变成默认的叶子节点，对于叶子节点，使用x.requires_grad。并且对于中间变量使用requires_grad会报错。 在第八章，还有一种表示方法：1234567with t.no_grad(): features = vgg16(style_img) gram_style = [gram_matrix(feature) for feature in features]@t.no_grad()def stylize(**kwargs): pass 这种方法会使得任何计算得到的结果都是requires_grad = False,暂时不清楚和detach()的区别。也是一种表示只前向传播的方法，不参与反向传播和梯度下降。 train()图片分为两种：风格图片，只需要一张，内容图片，很多，用于训练，这一点没有暂时没有理解为什么这么设置。其中，对输入的图片进行了乘以255，我觉得是因为为了使模型的输出直接就是255，不需要再进行处理，没有验证。ensor.item() tensor.tolist()content_image = tv.datasets.folder.default_loader(opt.content_path)在训练过程中，会发现对于整个训练过程，不仅有神经网络，而且还有自己定义的函数，nn.functional，还有两个损失函数，这是之前没有预料到的。 保存图片1234567# 保存图片的几种方法，第七章的是 # 0-1tv.utils.save_image(fix_fake_imgs,'%s/%s.png' % (opt.img_save_path, epoch),normalize=True, range=(-1,1))# vis.save竟然没找到 我的神 # 0-1vis.img('input')vis.save([opt.env]) utils.py这里的疑问是得到gram矩阵的时候，为什么要除以c*h*w,而不是h*w，虽然源码都是这么写的。 写到这里也还是还要很多疑问，暂时保留。昨天发现训练的过程不对，今天在对比代码的过程中，发现了自己写代码的一些漏洞，主要有 命名不规范：表示同一个东西出现了两个命名，导致了自己在写代码的过程中传参出现了问题，或者是一类东西没有一个规则进行命名，导致自己在写代码的过程中用到之前的变量的时候必须返回去去查找这个变量，效率低且容易出错。 对源码的修改不是很恰当，导致在写上卷积层的输出和源码完全不一致，这个是自己之前没有遇到的。 visdom的运用，我用不同的environment导致结果也不一样，default是之前一直用的，这次换成了test1之后显示的结果就对了。这个暂时还不清楚原因，如果是会保留信息的话，但是plot是重新开始画的，等会测试测试vis的问题。是网络的问题。但是vis.save()的介绍是序列化信息，暂时还没有理解。 对单张图片进行加载验证content_image = tv.datasets.folder.default_loader(opt.content_path)可以理解成Image.open，看源码就可以知道的 贴两个成果图看看效果。 遗留的问题Gram矩阵为什么可以代表图片风格，这里有个解释(https://arxiv.org/pdf/1701.01036.pdf)，还没来得及看。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>neural style transfer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[data]]></title>
    <url>%2F2018%2F09%2F06%2Fdata%2F</url>
    <content type="text"><![CDATA[Market-1501参考链接http://blog.fangchengjin.cn/reid-market-1501.htmlhttps://github.com/RSL-NEU/person-reid-benchmark 6个摄像头 1501个人，其中751个人、12936张图片用于训练，750个人、19732张图片用于测试， 3368张查询图片 目录说明 bounding_box_test 19732张测试图片 0000_c1s1_000151_01.jpg 前缀为 0000 表示在提取这 750 人的过程中DPM检测错的图（可能与query是同一个人），-1 表示检测出来其他人的图（不在这 750 人中） DPM检测出的 gallery样本 bounding_box_train 12936张训练图片 0002_c1s1_000451_03.jpg train样本 query 3368张图片，与test的750人对应 但是是人工绘制的 与bounding_box_test中的图片略微有所不同 与gt_bbox中的图片是一样的 0001_c1s1_001051_00.jpg 为 750 人在每个摄像头中随机选择一张图像作为query，因此一个人的query最多有 6 个，共有 3,368 张图像 query样本 gt_bbox 25259张图片 手工绘制 包含1501个行人 0001_c1s1_001051_00.jpg 手工标注的bounding box，用于判断DPM检测的bounding box是不是一个好的box gt_query 是对3368张图片的查询图片的判定，好坏， 0001_c1s1_001051_00_good.mat matlab格式，用于判断一个query的哪些图片是好的匹配（同一个人不同摄像头的图像）和不好的匹配（同一个人同一个摄像头的图像或非同一个人的图像） 命名规则以 0001_c1s1_000151_01.jpg 为例1） 0001 表示每个人的标签编号，从0001到1501；2） c1 表示第一个摄像头(camera1)，共有6个摄像头；3） s1 表示第一个录像片段(sequece1)，每个摄像机都有数个录像段；4） 000151 表示 c1s1 的第000151帧图片，视频帧率25fps；5） 01 表示 c1s1_001051 这一帧上的第1个检测框，由于采用DPM检测器，对于每一帧上的行人可能会框出好几个bbox。00 表示手工标注框 DukeMTMC-reIDDukeMTMC是多目标多摄像机行人跟踪数据集，8个摄像头，2700多个人物，DukeMTMC-reID是DukeMTMC的行人重识别子集，并且提供了人工标注的bounding box。从视频中每 120 帧采样一张图像，得到了 36,411 张图像。一共有 1,404 个人出现在大于两个摄像头下，有 408 个人 (distractor ID) 只出现在一个摄像头下 目录结构 bounding_box_test 0002_c1_f0044158.jpg 用于测试集的 702 人 包含 17,661 张图像（随机采样，702 ID + 408 distractor ID） bounding_box_train 0001_c2_f0046182.jpg 用于训练集的 702 人 包含 16,522 张图像（随机采样） query 0005_c2_f0046985.jpg 为测试集中的 702 人在每个摄像头中随机选择一张图像作为 query，共有 2,228 张图像 。 命名规则0001_c2_f0046182.jpg1） 0001 表示每个人的标签编号；2） c2 表示来自第二个摄像头(camera2)，共有 8 个摄像头；3） f0046182 表示来自第二个摄像头的第 46182 帧。 CUHK03CUHK03是第一个足以进行深度学习的大规模行人重识别数据集，该数据集的图像采集于香港中文大学（CUHK）校园。数据以”cuhk-03.mat”的 MAT 文件格式存储，含有 1467 个不同的人物，由 5 对摄像头采集。参考链接http://blog.fangchengjin.cn/reid-cuhk03.html 目录结构 detected - 5 * 1 cell 由机器标注，每个 cell 中包含一对摄像头组采集的照片，每个摄像头组由 M x 10 cells 组成，M 为行人索引，前 5 列和后 5 列分别来自同一组的不同摄像头。cell 内每个元素为一幅 H x W x 3 的行人框图像(uint8 数据类型)，个别图像可能空缺，为空集。 843*10 cell 摄像头组pair 1 440*10 cell 摄像头组pair 2 77*10 cell 摄像头组pair 3 58*10 cell 摄像头组pair 4 49*10 cell摄像头组pair 5 labeled - 5 * 1 cell 行人框由人工标注，格式和内容和”detected”相同。 843*10 cell 440*10 cell 77*10 cell 58*10 cell 49*10 cell testsets - 20*1 cell 测试协议，由 20 个 100 x 2 double 类型矩阵组成 (重复二十次) 100*2 double matrix 100 行代表 100 个测试样本，第 1 列为摄像头 pair 索引，第 2 列为行人索引]]></content>
      <categories>
        <category>re-id</category>
      </categories>
      <tags>
        <tag>re-id</tag>
        <tag>data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inception]]></title>
    <url>%2F2018%2F09%2F04%2FInception%2F</url>
    <content type="text"><![CDATA[关于Inception的好的讲解深度学习卷积神经网络——经典网络GoogLeNet(Inception V3)网络的搭建与实现]]></content>
  </entry>
  <entry>
    <title><![CDATA[deconv\dilated conv]]></title>
    <url>%2F2018%2F09%2F04%2Fdeconv%2F</url>
    <content type="text"><![CDATA[反卷积之前一直以为反卷积和空洞卷积是一回事，后来才发现是两个事情，反卷积是为了能够将小图片生成大图片，空洞卷积是为了扩大感受野。 反卷积多用于图像生成中，例如从特征生成图片，GAN，图像分割等等中，常与conv相对应，也有其他名称，比如: Transposed Convolution, Fractional Strided Convolution。 首先定义符号： 假设本文提到的图形和卷积核都是一维的线或者二维的正方形，$x$和$y$轴方向的padding和stride相同 $i,o,k,p,s,i’,o’,k’,p’,s’$分别表示卷积/反卷积输入图片的大小input size, 输出图片的大小 output size，卷积/反卷积的核大小kernel size，padding，stride. 动图演示https://github.com/TJJTJJTJJ/conv_arithmetic 详细解析 卷积的公式如下： o = \lfloor\frac{i-k+2p}{s}\rfloor+1反卷积对应的是直接在原图上填充0 o' = s'(i'-1)+k'-2p'+out\_padding其中涉及到的数学公式非常简单，因为卷积和反卷积都可以对应到一个矩阵相乘上，可以简单地理解成卷积的卷积核矩阵和反卷积的卷积核矩阵互为转置，当然，我们知道，这只是在大小上可以这么理解，其中的数值是不一样的，除非是正交矩阵。 根据动图演示的动态图可以看出： 反卷积的No padding的演示效果就是卷积的full padding，在padding效果上，反卷积和卷积总是以互补的形式出现。 从stride的效果上来看，conv的stride好理解，deconv的stride实际上是在输入图片内加入0 out_padding就是为了应对odd情况，这种情况下，out_padding指得是在padding和stride填充之后，为了能得到预期大小的图片，在输入图片最外面单侧填充0. 空洞卷积扩张卷积的计算公式与上面不一样 扩张卷积扩张的是卷积，在卷积核内部和外部同时填充0.主要是为了扩大感受野，应用在目标分割等上面. 扩张卷积 o'=\lfloor\frac{i'-k+2p-(k-1)*(d-1)}{s}\rfloor+1 感受野的计算公式(a) 普通卷积，1-dilated convolution，卷积核的感受野为$3 \times 3 = 9$。(b) 扩张卷积，2-dilated convolution，卷积核的感受野为$7 \times 7 = 49$。(c) 扩张卷积，4-dilated convolution，卷积核的感受野为$15 \times 15 = 225$。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积\反卷积 空洞卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch]]></title>
    <url>%2F2018%2F08%2F26%2Fpytorch-1.0%2F</url>
    <content type="text"><![CDATA[基础知识魔术方法: P23 调试: P2712import ipdbipdb.set_trace() 带下划线_的函数会修改Tensor本身，比如x.add_(y)和x.add(y)的区别 TensorNumpy与Tensor共享内存b = a.numpy() # Tensor -&gt; Numpya = t.from_numpy(a)# Numpy -&gt; Tensorx = x.cuda()tensor的操作:torch.function,tensor.function.普通索引共享内存高级索引不共享内存线性代数函数 P70自动广播原则: unsqueese(view),expand(expand_as)tensor=Tensor+Storage持久化和加载: t.save(a,’a.pth’) b=t.load(‘a.pth’) P77%timeit -n 10 Variable和autogradfrom torch.autograd import Variable三个属性data: 对应Tensorgrad: 梯度，和data大小一样，也是Variablegrad_fn: 指向Function对象,用于构建计算图。用户创建对应叶子节点,grad_fn=None.记录的是它什么操作的输出。Variable的构造函数的关键字参数:requires_grad(bool):是否需要求导; volatile(bool):True表示之上的计算图都不会求导123456x = Variable(t.ones(2,2),requires_grad = True)y = x.sum()y.grad_fny.backward()x.gradx.grad.data.zero_() # 反向传播清零 variable.backward(grad_variable=None, retain_graph=None, create_graph=None)假设用户输入的数据是真实的不需要求导的。数值在前向传导过程成会保存成buffer,计算梯度之后自动清空。多次反向求导可以使用关键字参数retain_graph=Trueretain_graph=True 实现多次反向传播？？？？ 反向传播过程中非叶子节点的导数在计算完之后就会清空，y=x*w,z=y.sum() 其中y.grad会清空。其对应的方法有两种，P92，t.autograd.grad(z,y)和hook扩展Autograd Function：P95 自己实现前向和反向 nn.Module123456789101112131415161718import torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def \_\_init\_\_(): super(Net,self).__init__() \# 有参数的层的定义 def forward(self,x):net = Net()print(net)list(net.parameters())for name,paramenters in net.named_parameters(): print(name,':',parameters.size())out = net(input)net.zero_grad()out.backwad(Variable(t.ones(1,10)))&gt; ??? nn.Sequential()nn.ModuleList()nn.ParameterList()在优化器中为各层分别设置学习率nn.functional对应nn.Module参数初始化:nn.Conv2d: nSamplesnChannelsHeight*Widh单样本: input.unsqueeze(0)model.train()model.eval()前向或者后向注册钩子函数，P125，可以查看中间层。获取网络的模块属性：getattr(module)P128保存模型：t.save(net.state_dict(), ‘net.pth’)加载模型：net2=Net() net2.load_state_dict(t.load(‘net.pth’))多个GPU并行操作 损失函数12345output = net(input)target = Variable(t.arrange(0,10))criterion = nn.MSELoss()loss = criterion(output, target)loss 1234net.zero_grad()print(net.con1.bias.grad)loss.backward()print(net.conv1.bias.grad) 优化器123learning_rate=0.01for f in net.parameters(): f.data.sub_(f.grad.data*leaning_data) 123456789101112import torch.optim as optim# 优化器optimizer = optim.SGD(net.parameters(), lr=0.01)# 训练过程中 梯度清零optimizer.zero_grad() # 等效于 net.zero_grad()# 损失函数output=net(input)loss = criterion(output, target)# 反向传播loss.backward()# 更新参数optimizer.step() CIFAR-10分类 数据预处理:transform,trainset,trainloader,testset,testloader 定义网络:Net(nn.Module),super(Net,self).init(),forward() 定义损失函数和优化器 训练网络 输入数据 梯度清零 前向传播+反向传播 更新参数 数据处理自定义的数据集需要继承Dataset类，并实现两个Python魔术方法__getitem:返回一个样本。obj[index]=obj.getitem(index)__len:返回样本数量。transform=T.Compose()trans=T.Lambda(lambda img:img.rotate(randonm()*360))ImageFolder(root,transform,target_transform,loader)P139self.class_to_idx了解label和文件夹名的映射关系DataLoader()定义shuffle等P142取样:P146工具包:torchvision P147 models:训练好模型 dataset:数据集加载 transforms:数据预处理操作，主要针对Tensor和PIL Image对象的操作 make_grid和save_img可视化工具 Tensorboard和visdomtensor_board和TensorboardXvisdom:env pane %env LS_COLORS=None!tree —charset ascii data/dogcatTensor—numpy:np.array(Tensor) torch.Tensor(np.darray)PIL.image—numpy:np.asarray(PIL.image) image.fromarray(numpy.ndarray)PIL.image—Tensor:trans = transforms.Compose([transforms.ToTensor()]) tens = trans(img) ToPILImage() GPUP158with t.cuda.device(1):t.cuda.set_device(1)export CUDA_VISIVLE_DEVICES=1b = t.load(‘a.pth’)c = t.load(‘a.pth’,map_location=lamdba storage, loc: storage)d = t.load(‘a.pth’,map_location={‘cuda:1’:’cuda:0’})Module和Optimizer: state_dict Dog.vs.Catcheckpoints/ 中间模型data/ __init__.py dataset.py def init(self,root,transform=None, train=True, test=False): def getitem(self,index): def len(self): get_data.shmodels/ __init__.py AlexNet.py BasicModule.py ResNet34.pyutils/ __init__.py visualize.pyconfig.pymain.pyrequirements.txtREADME.md main.py def train(**kwargs): def test(**kwargs): def val(model,dataloader): def help(): 123if __name__=='main': import fire fire.Fire() pytorch 中文文档https://pytorch-cn.readthedocs.io/zh/latest/package_references/functional/ PyTorch实战指南 第六章 Dog.VS.Cat这是根据深度学习框架：PyTorch入门与实践这本书的第六章写的代码，是关于猫狗识别的，在这个过程中，一边看，一边写，刚开始是运行作者已经写好的代码，后来自己在jupyter上进行复制的复现，发现import无法导入ipynb文件，在使用了Ipynb_importer.py之后可以实现同一文件内导入ipynb模块，如果是在其他文件中进行导入，会有点费事，以下会记录Ipynb_importer.py的用法。因为费事，自己开始开始使用pycharm+jupyter的方式，直接自己根据作者提供的源码进行编写，在编写的过程中接受作者的思想。用pycharm的不方便的地方是无法直接运行测试，所以采取的是对自己不熟悉的模块或者方法，用jupyter进行测试，而直接编写则是pycharm。但是感觉pycharm还是没有那么好用，可能是自己用的少。我是按照data、model、util、main+config、requirement的顺序编写的。在编写函数的过程中，因为刚开始不理解各个模块是怎么组织起来的，所以都是从简单的开始，所以函数的位置和作者的不一样，其中对于model.save和model.load、vis.plot和vis.log的封装让我感觉很有意思，刚开始是编写的时候只能直接打上问号，因为不懂这么编写的意义，但在编写主函数main的时候才感觉到了这种编写的好处，基本把模型训练和对模型、结果的处理完全分离开，避免了耦合性很强的后果。 Ipynb_importer.py我通过几次测试发现，import Ipynb_importer 只需要放在你的当前要运行的文件中即可，然后在其他文件下的init.py 中导入所有的当前文件夹中的Module，就像这样/first/second/models/——-init.py——————- None——-BasicModule.ipynb——-AlexNet.ipynb—————from models.BasicModule import BasicModule /first/main.pyimport Ipynb_importerfrom models import AlexNet 之所以在AlexNet中写models.BasicModule是因为直接导入BasicModule会报错，我根据dict的输出发现有问题，这一点和官网介绍的有一点区别，我没有实现官网说明的跨文件夹导入。因为如果改文件夹导入的话，models.BasicModule要接着换成相应的名字，与我预想的不一致，我预想的是不管在哪里导入，已经导入的应该不受影响才对。 ipynb-py.sh之后发现了这个神器，可以把ipynb转化成.py，还是挺好用的，转化之后也没问题。 同时，借助这次实验，自己对python的掌握也更深了一点。 不过对于网络的构成还是有一些问题，那就是网络为什么这么写，这应该属于理论的东西。还需要进一步加强。 这次实验一共用了三天才完全搞懂，可以说其中涉及到的函数的用法基本都明白了。本意是记录自己，不过如果有任何问题，欢迎交流。 PyTorch实战指南 第七章 DCGAN这一次实现的也比较慢，用了小三天才做完，现在记录一下其中学到的几个东西。 __file__:用来获取模块所在路径 可能是一个相对路径，可能是一个绝对路径，如果当前文件包含在sys.path里面，那么，__file__返回一个相对路径！也可以认为获取模块的名字最后的落脚点一定是XX/XX.py类没有这个属性 12345678In [1]: import numpyIn [3]: numpy.__file__ Out[3]: 'F:\\Programs\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py'In [6]: numpy.random.__file__Out[6]: 'F:\\Programs\\Anaconda3\\lib\\site-packages\\numpy\\random\\__init__.py'$ python test.py ##print(__file__)test.py __name__:__name__就是标识模块的名字的一个系统变量。这里分两种情况：假如当前模块是主模块（也就是调用其他模块的模块），那么此模块名字就是__main__，通过if判断这样就可以执行“__mian__:”后面的主函数内容；假如此模块是被import的，则此模块名字为文件名字（不加后面的.py），通过if判断这样就会跳过“__main__:”后面的内容。这个模块可以是文件夹的名字，可以是类的名字，可以是__mian__”,XX的形式。可以用于获取当前文件的文件名通过上面方式，python就可以分清楚哪些是主函数，进入主函数执行；并且可以调用其他模块的各个函数等等。12345678910111213141516171819# test.py## print(__file__)## print(__name__)# test2.py## import test## print(__file__)## print(__name__)H:\GitHub\pytorch_learn\Chapter7\test.pytesttest2.py__main__## from test import ccc## print(ccc.__name__)## print(__file__)## print(__name__)ccctest2.py__main__ type():返回对象的类型如果是module,则返回module如果是类的实例，则返回类的名称，这个名称以XXX.XXX的形式返回，从import的第一个开始算起。常用于判断数据类型，在pytorch中，用于返回模型名称，这个用法很巧妙，相当于返回了子类的类型名字我觉得没有理解作者是怎么用的。在父类里的type(self) 返回的是子类的类名123456789101112In [1]: import numpyIn [2]: type(numpy)Out[2]: moduleIn [3]: a = numpy.array(1)In [4]: type(a)Out[4]: numpy.ndarrayIn [5]: type(numpy.array)Out[5]: builtin_function_or_method 123456789101112131415161718192021222324252627class A: pass class B(A): pass isinstance(A(), A) # returns Truetype(A()) == A # returns Trueisinstance(B(), A) # returns Truetype(B()) == A # returns Falseclass A(object): def __init__(self): print(type(self)) passclass B(A): def __init__(self): super(B,self).__init__() print(type(self)) passimport testtest.B()&lt;class 'test.B'&gt; &lt;class 'test.B'&gt; __class__:和type类似1234567891011121314151617class A(object): def __init__(self): print(type(self)) print(self.__class__) passclass B(A): def __init__(self): super(B,self).__init__() print(type(self)) print(self.__class__) pass&lt;class 'test.B'&gt;&lt;class 'test.B'&gt;&lt;class 'test.B'&gt;&lt;class 'test.B'&gt; 获取config源码打印参数，方便输入参数inspect.getsource123from inspect import getsourcesource = getsource(opt.__class__)print(source)]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch-learn chenyun</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python]]></title>
    <url>%2F2018%2F08%2F26%2Fpython%2F</url>
    <content type="text"><![CDATA[python的学习过程： 第一个是看着廖雪峰的网站，里面的内容基础，是关于数据结构等的十分基本的内容，适合小白入门 第二个是流畅的python 这本书比python-codebook还深入，更适合当你实现了一个功能之后，还是想知道其具体怎么实现的时候查询。 第三个是python-codebook. 它的组织形式是任务式、问题式的，而且问题也相对而言比较高级，不是算法导论那种以解决某个实际问题，而是在编程上我想实现什么更好的功能那种问题，通过每一个问题，或者说通过每一个你想怎么更优的实现一个方法的思路，来引导如何更好地写代码，实现高级功能。这本书的前提是你已经入门，并且写了一段时间的python代码，在实际写的过程中已经遇到了类似的问题，也勉强实现了，只是苦于没有更好更顺心的方法实现。我现在是个小白，看这本书用了将近两周吧，主要看了第一二三四七八章，里面的代码翔实。其他的也是略微看了看，因为没有实际操作背景，有的时候不懂为什么那样做会更好，可以在以后的编程过程中，遇到这样的情况：这个我能勉强实现，但是感觉不太好，我想实现的更优美。那就应该来看看这本书，说不定这本书的实现能给自己一些思路。不适合为了读而读，因为不是入门。 接下来可以考虑看看那种直接算法任务型的。刚刚看了看python算法教程，估计要跳着看了，因为里面的关于算法的内容已经熟悉了，可以扫描着看 python的点二重列表生成式123456789# 第一种[[i+j for i in range(3)] for j in range(2)][[0, 1, 2], [1, 2, 3]]# 第二种[i+j for i in range(3) for j in range(2)][0, 1, 1, 2, 2, 3]# 按列访问类似列表的结构[a[i][j] for j in range(len(a[0]) for i in range(len(a.size))] 所以对于二重列表生成式，一般可以认为是对于同层的列表，是从前到后，对于不同层的列表，是从外到内 li_ = list(str_)和str_ = ‘’.join(li_)互为相反12345678910111213141516171819202122232425# 一重列表In [1]: str = '我你'In [2]: li = ['我','你']In [3]: list(str)Out[3]: ['我', '你']In [4]: ''.join(li)Out[4]: '我你'# 二重列表In [5]: lli_=[['我','你'],['北','京']]In [6]: str_ = [''.join(li) for li in lli_]In [7]: str_ Out[7]: ['我你', '北京']In [8]: st_ ='/n'.join([''.join(li) for li in lli_])In [9]: st_Out[9]: '我你/n北京'#同理，可以推广到多重列表 list删除元素12345678910111213141516171819# 利用pop，根据位置删除# 存在返回值，与append相对应li_.pop()a = li_.pop(i)# 利用remove，根据值删除# 删除第一个匹配的值aList = [123, 'xyz', 'zara', 'abc', 'xyz'];aList.remove('xyz');aList.remove(aList[1])# 利用del,根据位置删除，没有返回值del(n[4])del n[4]# str.replace()```pythonu'afafafa'.replace('a',u'eee').replace('f',u'rr')'eeerreeerreeerreee' python函数的互换对于类似的函数，并且有相同输入和输出，只是对于实现的功能有一些不一样暂时无法评价这两种写法的优劣123456789# 第一种写法if opt.acrostic: result = gen_acrostic(model, start_words, ix2word, word2ix, prefix_words)else: result = generate(model, start_words, ix2word, word2ix, prefix_words)# 第二种写法gen_poetry = gen_acrostic if opt.acrostic else generateresult = gen_poetry(model, start_words, ix2word, word2ix, prefix_words) 奇怪12345ipdb&gt; y = 10ipdb&gt; [[ 1 for w in range(2)] for j in range(y)][[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]ipdb&gt; [[ 1 for w in range(y)] for j in range(2)]*** NameError: name 'y' is not defined assert123456789101112# 第一种assert 3 &gt;=5,'2不等于1'----&gt; 1 assert 3 &gt;=5AssertionError:# 第二种assert 3 &gt;=5,'3不小于等于5'----&gt; 1 assert 3 &gt;=5,'3不小于等于5'AssertionError: 3不小于等于5 sort和sortedsort是对list的操作sorted是对所有可迭代的序列的操作 requirements.txt12345pip freeze &gt; requirements.txt # 生成requirements.txtpip install -r requirements.txt # 从requirements.txt安装依赖pip install pipreqspipreqs /home/project/location 装饰器第一步：函数也是一个对象，也可以赋值给变量，也可以通过变量来调用函数。12345678910111213141516171819In [1]: def now(): ...: print('now 2020202')In [2]: f = nowIn [3]: f()now 2020202In [4]: now.__name__Out[4]: 'now'In [5]: f.__name__Out[5]: 'now'In [6]: type(f)Out[6]: functionIn [7]: type(now)Out[7]: function 第二步：简单的装饰器，在函数调用前后，进行一些没有参数的操作，在代码运行期间增加功能两层的装饰器,@log123456789101112131415161718192021222324252627In [15]: def log(func): ...: print('wrapper early') ...: print(func.__name__) ...: def wrapper(*args, **kwargs): ...: print('fun early&#123;0&#125;'.format(func.__name__)) ...: func(*args,**kwargs) ...: print('fun later') ...: print('wrapper later') ...: return wrapper ...: ...:In [16]: @log ...: def now(): ...: print('now test3') ...:wrapper earlynowwrapper laterIn [17]: now()fun earlynownow test3fun laterIn [18]: now.__name__Out[18]: 'wrapper' 第三步：复杂的装饰器，在函数调用前后，进行有参数的操作，三层的装饰器，@log()123456789101112131415161718192021222324252627282930313233343536373839404142In [19]: def log(text): ...: print('log----decorator') ...: def decorator(func): ...: print('decorator----wrapper') ...: def wrapper(*args,**kwargs): ...: print('wrapper 1') ...: print(text,func.__name__) ...: print(' wrapper 2') ...: return func(*args, **kwargs) ...: print('wrapper----decorator') ...: return wrapper ...: print('decorator----log') ...: return decorator ...: ...: ...:In [20]: @log('hello') ...: def now(): ...: print('1000') ...: return 1 ...: ...:log----decoratordecorator----logdecorator----wrapperwrapper----decoratorIn [21]: now()wrapper 1hello now wrapper 21000Out[21]: 1In [22]: f = nowIn [23]: f.__name__Out[23]: 'wrapper'In [24]: now.__name__Out[24]: 'wrapper' 第四步：完整的装饰器123456789101112131415161718192021import functoolsdef log(func): @functools.wraps(func) def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper# 或者import functoolsdef log(text): def decorator(func): @functools.wraps(func) def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator等价于wrapper.__name__ = func.__name__ 参数分为位置参数（可以设置为默认参数），可变参数，关键字参数位置参数很简单，跳过可变参数或者关键字参数可以没有值赋予 可变参数 第一步：对于不确定个数的参数，使用list或者tuple作为参数传入缺点：在调用的时候必须先组装成list或者tuple123456789101112131415161718192021In [7]: def calc(number): ...: sum = 0 ...: print(type(number)) ...: for i in number: ...: sum+=i ...: return sum ...: ...:In [8]: calc([1,2,3])&lt;class 'list'&gt;Out[8]: 6In [9]: calc((1,2,3) ...: )&lt;class 'tuple'&gt;Out[9]: 6In [10]: calc((1,2,3))&lt;class 'tuple'&gt;Out[10]: 6 第二步：利用可变参数*number，在第一步的基础上修正缺点，使其调用不再需要先组装缺点：输入参数是list和tuple时就会很麻烦，需要先拆解123456789101112131415161718192021222324252627282930313233In [11]: def calc(*number): ...: sum = 0 ...: print(type(number)) ...: for i in number: ...: sum+=i ...: return sum ...: ...:In [12]: calc(1,2,3)&lt;class 'tuple'&gt;Out[12]: 6In [13]: calc()&lt;class 'tuple'&gt;Out[13]: 0In [14]: num = [1,2,3]In [15]: calc(num)&lt;class 'tuple'&gt;---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-15-bb5c0a89404a&gt; in &lt;module&gt;()----&gt; 1 calc(num)&lt;ipython-input-11-34cdd7bd7eaa&gt; in calc(*number) 3 print(type(number)) 4 for i in number:----&gt; 5 sum+=i 6 return sumTypeError: unsupported operand type(s) for +=: 'int' and 'list' 注意，可变参数在函数内部是作为一个tuple存在的，如果对于tuple进行修改，先变成list 第三步：对第一步的优点和第二步的优点进行整合，使调用时既能直接接收list或者tuple作为输入参数，也能不需要组装成list或者tuple进行输入。也就是在函数定义时注明number,输入直接输入或者list.123456789101112131415161718192021In [11]: def calc(*number): ...: sum = 0 ...: print(type(number)) ...: for i in number: ...: sum+=i ...: return sum ...: ...:# 合理调用的几种方式In [14]: num = [1,2,3]In [16]: calc(*num)&lt;class 'tuple'&gt;Out[16]: 6In [20]: calc(1,2,3)&lt;class 'tuple'&gt;Out[20]: 6In [22]: calc(*(1,2,3))&lt;class 'tuple'&gt;Out[22]: 6 关键字参数类比可变参数，就很容易地理解dict型参数的输入12345678910111213In [18]: def person(name, **kwargs): ...: print(type(kwargs)) ...: print(kwargs)In [23]: person('fa',city='beijing',age='44')&lt;class 'dict'&gt;&#123;'city': 'beijing', 'age': '44'&#125;In [24]: extra = &#123;'city': 'Beijing', 'job': 'Engineer'&#125;In [25]: person('hh',**extra)&lt;class 'dict'&gt;&#123;'city': 'Beijing', 'job': 'Engineer'&#125; 对于可变参数或者关键字参树，或者按照正常地类似位置参数或者默认参数的形式传入，或者按照list或者*dict进行传入。 命名关键字参数1234567891011# 第一种，直接*In [26]: def person(name, * ,city, job): ...: print(name, city, job) ...:# 其调用时必须显示输入参数名，city和job，当然，命名关键字参数也可以设置默认值In [27]: person('11',city='beijing', job='enginner')11 beijing enginner# 第二种 有可变参数的存在def person(name, age, *args, city, job): print(name, age, args, city, job) 参数组合参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。123456789101112131415161718192021222324def f1(a, b, c=0, *args, **kw): print('a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw)def f2(a, b, c=0, *, d, **kw): print('a =', a, 'b =', b, 'c =', c, 'd =', d, 'kw =', kw)In [28]: def f(a,b,c=0,*args,d,**kw): ...: print('a',a,'b',b,'c',c,'args',args,'d',d,'kw',kw) ...:In [29]: f(1,2,3,4,d=5,e=6)a 1 b 2 c 3 args (4,) d 5 kw &#123;'e': 6&#125;# 看一个神奇的东西，输入参数用tuple和dict直接代替In [30]: args = (1,2,3,4)In [31]: dic = &#123;'d':5, 'e':6&#125;In [33]: f(*args,**dic)a 1 b 2 c 3 args (4,) d 5 kw &#123;'e': 6&#125;# 由此引出一个重要结论# 所以，对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的。# 这个结论解释了后期定义装饰器的时候，参数的定义直接是(*args, **kw)的形式，而不用去管函数本身的参数的定义是什么样的，而在调用的时候，按照原函数的参数定义调用即可。 闭包1234567891011121314151617181920212223242526272829303132333435In [29]: def lazy_sum(*args): ...: def sum(): ...: ax = 0 ...: for i in args: ...: ax = ax+i ...: return ax ...: return sum ...: ...:In [30]: f = lazy_sum(1,2,3,4)In [31]: f.__name__Out[31]: 'sum'In [32]: f.__file__---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-32-9c8edc3d9e41&gt; in &lt;module&gt;()----&gt; 1 f.__file__AttributeError: 'function' object has no attribute '__file__'In [33]: fOut[33]: &lt;function __main__.lazy_sum.&lt;locals&gt;.sum()&gt;In [34]: f()Out[34]: 10In [35]: f1 = lazy_sum()In [36]: f2 = lazy_sum()In [37]: f1==f2Out[37]: False 闭包存在的问题123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 返回多个函数In [38]: def count(): ...: fs=[] ...: for i in range(4): ...: def f(): ...: return i*i ...: fs.append(f) ...: return fs ...: ...:In [39]: f1,f2,f3,f4 = count()In [40]: f1()Out[40]: 9In [42]: f2()Out[42]: 9In [43]: f3()Out[43]: 9In [44]: f4()Out[44]: 9In [63]: def count(): ...: def f(): ...: return i*i ...: fs=[] ...: for i in range(4): ...: fs.append(f) ...: print(i) ...: return fsIn [64]: f1, f2, f3, f4 = count()0123In [65]: f1()Out[65]: 9In [66]: f2()Out[66]: 9# 返回单个函数In [48]: def lazy_sum(*args): ...: j=0 ...: def sum(): ...: ax = 0 ...: for i in args: ...: ax = ax+i+j ...: return ax ...: j=100 ...: return sum ...: ...:In [49]: f1 = lazy_sum(1,2,3)In [50]: f1()Out[50]: 306 返回函数不要引用任何循环变量，或者后续会发生变化的变量。应该满足内层函数定义后其所引用的外部变量不发生变化。 针对外部变量发生变化的解决方案就是再用一个函数，令变化的外部变量从隐式参数变成显式参数或者说，对于变化的外部变量，令其执行，不再以变量的形式存在，而是以其值的形式存在。第一种方法1234567891011121314151617181920212223def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1, 4): fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f() return fsIn [52]: f1, f2, f3 = count()In [53]: f1()Out[53]: 1In [60]: f1Out[60]: &lt;function __main__.count.&lt;locals&gt;.f.&lt;locals&gt;.g()&gt;In [61]: f2Out[61]: &lt;function __main__.count.&lt;locals&gt;.f.&lt;locals&gt;.g()&gt;In [62]: f1.__name__Out[62]: 'g' 第二种方法1234567891011121314In [113]: def count(): ...: fs=[] ...: for i in range(2): ...: def f(m=i): ...: return m*m ...: fs.append(f) ...: return fs ...: ...:In [114]: count()Out[114]:[&lt;function __main__.count.&lt;locals&gt;.f(m=0)&gt;, &lt;function __main__.count.&lt;locals&gt;.f(m=1)&gt;] 廖雪峰说可以用lambda函数进行代码缩写，但是没有想通怎么用。 内部函数内修改外部函数局部变量12counterA = createCounter()print(counterA(), counterA(), counterA(), counterA(), counterA()) # 1 2 3 4 5 1234567891011121314151617181920212223242526272829一、在内部函数内修改外部函数局部变量的两种方法1法：把外部变量变成容器或者说可变变量def createCounter(): a = [0] def counter(): a[0] += 1 return a[0] return counter2法：在内部函数里给予外部函数局部变量nonlocal声明，让内部函数去其他领域获取这个变量def createCounter(): a = 0 def counter(): nonlocal a a += 1 return a return counter二、在内部函数内修改全局变量def createCounter(): global a a = 0 def counter(): global a a += 1 return a return counter 快速解压或者list+tuple转置123456789101112li_ = [(1,2,3),(4,5,6)]ll = list(zip(*li_))[(1, 4), (2, 5), (3, 6)]# 甚至可以达到按列取值的效果l1, l2, l3 = list(zip(*li_))l1(1, 4)l2(2, 5)l3(3, 6) 求一个序列中，与固定值之间的最大值，这个最大值不能超过序列的最大值和固定值或者说求两个序列的最小最大值或者说在这个序列中，如果没有比固定值大的数，则取这个序列的最大值作为最大值，如果有，则取固定值作为最大值或者说对这个序列进行截断12345ll = [min(c, max_len) for c in l]max_query = max(ll)# 或者max_l = max(l)max_query = min(max_l, max_len) dict1234567a = 1b = 2d = dict(aa = a, bb = b)&#123;'aa': 1, 'bb': 2&#125;dd = &#123;'aa':a, 'bb':2&#125;dd&#123;'aa': 1, 'bb': 2&#125; 注释文档12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273'''文档快速生成注释的方法介绍,首先我们要用到__all__属性在Py中使用为导出__all__中的所有类、函数、变量成员等在模块使用__all__属性可避免相互引用时命名冲突'''__all__ = ['Login', 'check', 'Shop', 'upDateIt', 'findIt', 'deleteIt']class Login: ''' 测试注释一可以写上此类的作用说明等 例如此方法用来写登录 ''' def __init__(self): ''' 初始化你要的参数说明 那么登录可能要用到 用户名username 密码password ''' pass def check(self): ''' 协商你要实现的功能说明 功能也有很多例如验证 判断语句，验证码之类的 ''' passclass Shop: ''' 商品类所包含的属性及方法 update改/更新 find查找 delete删除 create添加 ''' def __init__(self): ''' 初始化商品的价格、日期、分类等 ''' pass def upDateIt(self): ''' 用来更新商品信息 ''' pass def findIt(self): ''' 查找商品信息 ''' pass def deleteIt(self): ''' 删除过期下架商品信息 ''' pass def createIt(self): ''' 创建新商品及上架信息 ''' passif __name__=="__main__": import test print(help(test)) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283$ python test.pyHelp on module test:NAME testDESCRIPTION 文档快速生成注释的方法介绍,首先我们要用到__all__属性 在Py中使用为导出__all__中的所有类、函数、变量成员等 在模块使用__all__属性可避免相互引用时命名冲突CLASSES builtins.object Login Shop class Login(builtins.object) | 测试注释一可以写上此类的作用说明等 | 例如此方法用来写登录 | | Methods defined here: | | __init__(self) | 初始化你要的参数说明 | 那么登录可能要用到 | 用户名username | 密码password | | check(self) | 协商你要实现的功能说明 | 功能也有很多例如验证 | 判断语句，验证码之类的 | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) class Shop(builtins.object) | 商品类所包含的属性及方法 | update改/更新 | find查找 | delete删除 | create添加 | | Methods defined here: | | __init__(self) | 初始化商品的价格、日期、分类等 | | createIt(self) | 创建新商品及上架信息 | | deleteIt(self) | 删除过期下架商品信息 | | findIt(self) | 查找商品信息 | | upDateIt(self) | 用来更新商品信息 | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined)DATA __all__ = ['Login', 'check', 'Shop', 'upDateIt', 'findIt', 'deleteIt']FILE h:\桌面\test.pyNone 用help或者.doc1print(math.sin.__doc__) all只对from xx import *有作用参考链接https://stackoverflow.com/questions/44834/can-someone-explain-all-in-python12345678910111213# foo.py__all__ = ['bar', 'baz'] waz = 5bar = 10def baz(): return 'baz'# run-foo.pyfrom foo import * print(bar) # 正常print(baz) # 正常print(waz) # 显示错误 from future import absolute_import从python2.1开始以后, 当一个新的语言特性首次出现在发行版中时候, 如果该新特性与以前旧版本python不兼容, 则该特性将会被默认禁用. 如果想启用这个新特性, 则必须使用 “from futureimport *” 语句进行导入.1234from __future__ import absolute_import# https://blog.csdn.net/caiqiiqi/article/details/51050800from __future__ import division# https://blog.csdn.net/feixingfei/article/details/7081446 保持原有维度的取元素 narrow 或者切片1234567tt = t.Tensor([[1,2,3],[3,4,5]])ttt = tt.narrow(0,0,1)ttttensor([[ 1., 2., 3.]])tt[0:1]tensor([[ 1., 2., 3.]]) 堆https://www.bbsmax.com/A/gAJGaGeZdZ/ 生成器函数 迭代器 迭代对象对于生成器函数，可以理解成列表，yield的值就是列表中的元素，用next()或者for in来调用迭代器 Iterator: 可以用于next的，惰性计算迭代对象 Iteratable: 可以用于for的 list, tuple, 生成器迭代对象可以使用iter变成迭代器迭代对象范围更广 对于012345678910a = 0if a ==0: True等价于if not a:并且if a!=0等价于if a 2018-10-05关于Python爬虫涉及到的编码问题计算机内存中，使用Unicode编码硬盘或者传输时，使用UTF-8编码 bytes和strPython3默认的字符串类型是str，在内存中以Unicode表示，如果要想保存到硬盘上或者在网络上传输，就需要把str转换成以字节为单位的bytes。str—encode—&gt;bytes—decode—&gt;str看了一些博客，感觉还是没有讲清楚，不是很清楚。 re参考链接]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cmd Markdown]]></title>
    <url>%2F2018%2F08%2F26%2FCmd%20Markdown%2F</url>
    <content type="text"><![CDATA[欢迎使用 Cmd Markdown 编辑阅读器 我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，Cmd Markdown 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown： 整理知识，学习笔记 发布日记，杂文，所见所想 撰写发布技术文稿（代码支持） 撰写发布学术论文（LaTeX 公式支持） 除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载： Windows/Mac/Linux 全平台客户端 请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 新文稿 或者使用快捷键 Ctrl+Alt+N。 什么是 MarkdownMarkdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，粗体 或者 斜体 某些文字，更棒的是，它还可以 1. 制作一份待办事宜 Todo 列表 [ ] 支持以 PDF 格式导出文稿 [ ] 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 [x] 新增 Todo 列表功能 [x] 修复 LaTex 公式渲染问题 [x] 新增 LaTex 公式编号功能 2. 书写一个质能守恒公式LaTeXE=mc^2x = {-b \pm \sqrt{b^2-4ac} \over 2a}3. 高亮一段代码code1234567@requires_authorizationclass SomeClass: passif __name__ == '__main__': # A comment print 'hello world' 4. 高效绘制 流程图12345678st=&gt;start: Startop=&gt;operation: Your Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 5. 高效绘制 序列图123Alice-&gt;Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob--&gt;Alice: I am good thanks! 6. 高效绘制 甘特图12345678910111213title 项目开发流程section 项目确定 需求分析 :a1, 2016-06-22, 3d 可行性报告 :after a1, 5d 概念验证 : 5dsection 项目实施 概要设计 :2016-07-05 , 5d 详细设计 :2016-07-08, 10d 编码 :2016-07-15, 10d 测试 :2016-07-22, 5dsection 发布验收 发布: 2d 验收: 3d 7. 绘制表格 项目 价格 数量 计算机 $1600 5 手机 $12 12 管线 $1 234 8. 更详细语法说明想要查看更详细的语法说明，可以参考我们准备的 Cmd Markdown 简明语法手册，进阶用户可以参考 Cmd Markdown 高阶语法手册 了解更多高级功能。 总而言之，不同于其它 所见即所得 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。 什么是 Cmd Markdown您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 编辑/发布/阅读 Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。 1. 实时同步预览我们将 Cmd Markdown 的主界面一分为二，左边为编辑区，右边为预览区，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！ 2. 编辑工具栏也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 编辑区 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。 3. 编辑模式完全心无旁骛的方式编辑文字：点击 编辑工具栏 最右侧的拉伸按钮或者按下 Ctrl + M，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！ 4. 实时的云端文稿为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 编辑工具栏 的最右侧提示 已保存 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。 5. 离线模式在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。 6. 管理工具栏为了便于管理您的文稿，在 预览区 的顶部放置了如下所示的 管理工具栏： 通过管理工具栏可以： &lt;/i&gt; 发布：将当前的文稿生成固定链接，在网络上发布，分享 新建：开始撰写一篇新的文稿&lt;/i&gt; 删除：删除当前的文稿 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地&lt;/i&gt; 列表：所有新增和过往的文稿都可以在这里查看、操作 模式：切换 普通/Vim/Emacs 编辑模式 7. 阅读工具栏 通过 预览区 右上角的 阅读工具栏，可以查看当前文稿的目录并增强阅读体验。 工具栏上的五个图标依次为： &lt;/i&gt; 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落 视图：互换左边编辑区和右边预览区的位置&lt;/i&gt; 主题：内置了黑白两种模式的主题，试试 黑色主题，超炫！ 阅读：心无旁骛的阅读模式提供超一流的阅读体验 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境 8. 阅读模式在 阅读工具栏 点击 或者按下 Ctrl+Alt+M 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。 9. 标签、分类和搜索在编辑区任意行首位置输入以下格式的文字可以标签当前文档： 标签： 未分类 标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示： 10. 文稿发布和分享在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 (Ctrl+Alt+P) 发布这份文档给好友吧！ 再一次感谢您花费时间阅读这份欢迎稿，点击 (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！ 作者 @ghosert2016 年 07月 07日 LaTeX. 支持 LaTeX 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 MathJax 参考更多使用方法。 &#8617; code. 代码高亮功能支持包括 Java, Python, JavaScript 在内的，四十一种主流编程语言。 &#8617;]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MdEditor]]></title>
    <url>%2F2018%2F08%2F22%2FMdEditor%2F</url>
    <content type="text"><![CDATA[欢迎使用 Markdown在线编辑器 MdEditorMarkdown是一种轻量级的「标记语言」 Markdown是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面，Markdown文件的后缀名便是“.md” MdEditor是一个在线编辑Markdown文档的编辑器MdEditor扩展了Markdown的功能（如表格、脚注、内嵌HTML等等），以使让Markdown转换成更多的格式，和更丰富的展示效果，这些功能原初的Markdown尚不具备。 Markdown增强版中比较有名的有Markdown Extra、MultiMarkdown、 Maruku等。这些衍生版本要么基于工具，如Pandoc，Pandao；要么基于网站，如GitHub和Wikipedia，在语法上基本兼容，但在一些语法和渲染效果上有改动。 MdEditor源于Pandao的JavaScript开源项目，开源地址Editor.md，并在MIT开源协议的许可范围内进行了优化，以适应广大用户群体的需求。向优秀的markdown开源编辑器原作者Pandao致敬。 MdEditor的功能列表演示标题H1标题H2标题H3标题H4标题H5标题H5字符效果和横线等 删除线 删除线（开启识别HTML标签时） 斜体字 _斜体字_ 粗体 粗体 粗斜体 _粗斜体_ 上标：X2，下标：O2 缩写(同HTML的abbr标签) 即更长的单词或短语的缩写形式，前提是开启识别HTML标签时，已默认开启 The HTML specification is maintained by the W3C. 引用 Blockquotes 引用文本 Blockquotes 引用的行内混合 Blockquotes 引用：如果想要插入空白换行即&lt;br /&gt;标签，在插入处先键入两个以上的空格然后回车即可，普通链接。 锚点与链接 Links普通链接普通链接带标题直接链接：http://www.mdeditor.com锚点链接 mailto:test.test@gmail.comGFM a-tail link @pandao邮箱地址自动链接 test.test@gmail.com www@vip.qq.com @pandao 多语言代码高亮 Codes行内代码 Inline code执行命令：npm install marked 缩进风格即缩进四个空格，也做为实现类似 &lt;pre&gt; 预格式化文本 ( Preformatted Text ) 的功能。 &lt;?php echo &quot;Hello world!&quot;; ?&gt; 预格式化文本： | First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell | JS代码123function test() &#123; console.log("Hello world!");&#125; HTML 代码 HTML codes1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;mate charest="utf-8" /&gt; &lt;meta name="keywords" content="Editor.md, Markdown, Editor" /&gt; &lt;title&gt;Hello world!&lt;/title&gt; &lt;style type="text/css"&gt; body&#123;font-size:14px;color:#444;font-family: "Microsoft Yahei", Tahoma, "Hiragino Sans GB", Arial;background:#fff;&#125; ul&#123;list-style: none;&#125; img&#123;border:none;vertical-align: middle;&#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1 class="text-xxl"&gt;Hello world!&lt;/h1&gt; &lt;p class="text-green"&gt;Plain text&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 图片 Images图片加链接 (Image + Link)： Follow your heart. 列表 Lists无序列表（减号）Unordered Lists (-) 列表一 列表二 列表三 无序列表（星号）Unordered Lists (*) 列表一 列表二 列表三 无序列表（加号和嵌套）Unordered Lists (+) 列表一 列表二 列表二-1 列表二-2 列表二-3 列表三 列表一 列表二 列表三 有序列表 Ordered Lists (-) 第一行 第二行 第三行 GFM task list [x] GFM task list 1 [x] GFM task list 2 [ ] GFM task list 3 [ ] GFM task list 3-1 [ ] GFM task list 3-2 [ ] GFM task list 3-3 [ ] GFM task list 4 [ ] GFM task list 4-1 [ ] GFM task list 4-2 绘制表格 Tables 项目 价格 数量 计算机 $1600 5 手机 $12 12 管线 $1 234 First Header Second Header Content Cell Content Cell Content Cell Content Cell First Header Second Header Content Cell Content Cell Content Cell Content Cell Function name Description help() Display the help window. destroy() Destroy your computer! Left-Aligned Center Aligned Right Aligned col 3 is some wordy text $1600 col 2 is centered $12 zebra stripes are neat $1 Item Value Computer $1600 Phone $12 Pipe $1 特殊符号 HTML Entities Codes&copy; &amp; &uml; &trade; &iexcl; &pound;&amp; &lt; &gt; &yen; &euro; &reg; &plusmn; &para; &sect; &brvbar; &macr; &laquo; &middot; X&sup2; Y&sup3; &frac34; &frac14; &times; &divide; &raquo; 18&ordm;C &quot; &apos; [========] Emoji表情 :smiley: Blockquotes :star: GFM task lists &amp; Emoji &amp; fontAwesome icon emoji &amp; editormd logo emoji :editormd-logo-5x: [x] :smiley: @mentions, :smiley: #refs, links, formatting, and tags supported :editormd-logo:; [x] list syntax required (any unordered or ordered list supported) :editormd-logo-3x:; [x] [ ] :smiley: this is a complete item :smiley:; [ ] []this is an incomplete item test link :fa-star: @pandao; [ ] [ ]this is an incomplete item :fa-star: :fa-gear:; [ ] :smiley: this is an incomplete item test link :fa-star: :fa-gear:; [ ] :smiley: this is :fa-star: :fa-gear: an incomplete item test link; 反斜杠 Escape*literal asterisks* [========] 科学公式 TeX(KaTeX)E=mc^2行内的公式E=mc^2行内的公式，行内的E=mc^2公式。 x > y\(\sqrt{3x-1}+(1+x)^2\)\sin(\alpha)^{\theta}=\sum_{i=0}^{n}(x^i + \cos(f))多行公式： 12345\displaystyle\left( \sum\_&#123;k=1&#125;^n a\_k b\_k \right)^2\leq\left( \sum\_&#123;k=1&#125;^n a\_k^2 \right)\left( \sum\_&#123;k=1&#125;^n b\_k^2 \right) 123456789\displaystyle \frac&#123;1&#125;&#123; \Bigl(\sqrt&#123;\phi \sqrt&#123;5&#125;&#125;-\phi\Bigr) e^&#123; \frac25 \pi&#125;&#125; = 1+\frac&#123;e^&#123;-2\pi&#125;&#125; &#123;1+\frac&#123;e^&#123;-4\pi&#125;&#125; &#123; 1+\frac&#123;e^&#123;-6\pi&#125;&#125; &#123;1+\frac&#123;e^&#123;-8\pi&#125;&#125; &#123;1+\cdots&#125; &#125; &#125; &#125; 123f(x) = \int_&#123;-\infty&#125;^\infty \hat f(\xi)\,e^&#123;2 \pi i \xi x&#125; \,d\xi 分页符 Page break Print Test: Ctrl + P [========] 绘制流程图 Flowchart12345678st=&gt;start: 用户登陆op=&gt;operation: 登陆操作cond=&gt;condition: 登陆成功 Yes or No?e=&gt;end: 进入后台st-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op [========] 绘制序列图 Sequence Diagram1234Andrew-&gt;China: Says HelloNote right of China: China thinks\nabout itChina--&gt;Andrew: How are you?Andrew-&gt;&gt;China: I am good thanks! End]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo]]></title>
    <url>%2F2018%2F08%2F17%2Fhexo%2F</url>
    <content type="text"><![CDATA[常用命令(cmd)hexo n “postName”hexo cleanhexo g 本地生成hexo d 同步到githubhexo d -ghexo new page aboutmehexo s 本地服务器预览hexo s -p 4100 换端口 搭建Github+hexo 博客的过程参考链接如下使用Hexo+Github一步步搭建属于自己的博客（基础）使用Hexo+Github一步步搭建属于自己的博客（进阶）[1] http://blog.haoji.me/build-blog-website-by-hexo-github.html?from=xa[2] https://www.cnblogs.com/fengxiongZz/p/7707568.html[3] https://www.jianshu.com/p/84a8384be1ae[4] ttp://tengj.top/2016/02/22/hexo1/$ npm install -g hexo-cli安装Node.js →安装Hexo → 安装主题 → 本地测试运行 → 注册给github与coding并创建pages仓库 → 部署安装Git →node.js的解释是高并发npm是模块的包管理器,与node.js一起安装的。npm install hexo -g 全局安装hexo是基于node.js的静态博客，所以我们才需要安装node.jsGit是为了让其他人也可以看到你的博客，把本地的内容提交到github上面去常用命令hexo g 生成 generatehexo s 启动服务器预览 serverhexo d 部署 deployhexo clean 清除缓存hexo server -p 4100hexo generate —deploy 完成后部署hexo deploy —generate 完成后部hexo new “postName” node_modules 依赖包public 生成的页面scaffolds 模板文件夹 post draft pagesource 用户资源的地方 hexo解释hexo中_config.ymlmarkdown写博客hexo中的配置信息域名绑定hexo渲染MathJax数学公式markdownpad渲染数学公式只能F6浏览器预览，并且不能实时预览，所以考虑换个markdown编辑器cmd markdown对本地文件支持不友好，并且不能加载本地图片Hexo Markdown 简明语法手册hexo的脚注问题只能实现数字的脚注 2018-09-12 添加功能实现评论功能此次评论功能使用disqus，理由：同学推荐参考链接：Hexo折腾记之科学使用Disqus与Next的集成科学使用 DisqusDisqus PHP API基于disqus-php-api在Hexo博客中使用DisqusDisqus-Proxy 配置说明Github 搭建 hexo （四）——更换主题，disqus，RSS 添加rss功能不知道是干嘛的，好像是为了实现订阅的。暂时不是很清楚。参考链接：最简便的方法搭建Hexo+Github博客,基于Next主题 添加site-map功能参考链接不知道是干嘛的Github 搭建 hexo （五）- 站点地图（sitemap.xml）站点地图还挺高级，以后再说。 百度自动推送参考链接Hexo+Next主题博客提交百度谷歌收录 添加公益404界面参考链接hexo添加404公益界面最简便的方法搭建Hexo+Github博客,基于Next主题 添加搜索参考链接hexo-genarator-search 高级教程以后再说利用Gitpage+hexo开发自己的博客Hexo个人免费博客(三) next主题、评论、阅读量统计和站内搜索官网插件 title: Hello Worldtoc: true Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment hexo实现显示本地图片2018-12-03，文章资源文件夹 在vscode可以显示本地图片，但是上传到blog中后无法显示本地图片。 参考链接https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referral 实际操作 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image —save，这是下载安装一个可以上传本地图片的插件，来自dalao：dalao的git，如果后续步骤报错，可以卸载后执行后续步骤 hexo n后会发现除了生成一个md文件，同目录下还生成了一个同名文件夹，用于放入图片。markdown中引入图片的语法： 1![你想输入的替代文字](xxxx/图片名.jpg) 嗯 最后检查一下，hexo g生成页面后，进入public\2017\02\26\index.html文件中查看相关字段，可以发现，html标签内的语句是 12&lt;img src="2017/02/26/xxxx/图片名.jpg"&gt;而不是&lt;img src="xxxx/图片名.jpg&gt; 这很重要，关乎你的网页是否可以真正加载你想插入的图片。如果发现有问题，可以直接查看源码，查看src，来进行修正。 引用方式二： 123&#123;% asset_path slug %&#125;&#123;% asset_img slug [title] %&#125;&#123;% asset_link slug [title] %&#125; 我最后采用的是 1&#123;% asset_img example.jpg This is an example image %&#125; hexo文件上传到github之后，会发现图片和md文件在同一个目录下。 添加评论 valine2019-05-24 参考链接:https://blog.csdn.net/blue_zy/article/details/79071414]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>github</tag>
        <tag>hexo</tag>
        <tag>npm</tag>
        <tag>基础</tag>
      </tags>
  </entry>
</search>
