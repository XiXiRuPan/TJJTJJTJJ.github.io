<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[neural style]]></title>
    <url>%2F2018%2F09%2F11%2Fneural-style%2F</url>
    <content type="text"><![CDATA[1 前言本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。这是我的代码大神链接：(https://github.com/anishathalye/neural-style)这是论文作者写的 2 问题及其解决我在第六章和第七章的时候还是基于pytorch 0.4.0，而第八章的时候我开始基于pytorch 0.4.1，所以以下的内容介绍都是基于0.4.1 2.1 文件组织形式123456789101112131415161718192021├─checkpoints/├─content_img/│ ├─input.jpg│ ├─output.jpg│ └─style.jpg├─data/│ ├─coco/a.jpg├─dataset/│ ├─__init__.py│ └─dataset.py├─models/│ ├─__init__.py│ └─PackedVGG.py│ └─transformer_net.py└─utils/│ ├─__init__.py│ └─utils.py│ └─visualize.py├─config.py└─main.py 其中，上半部分是对数据和模型的保存组织形式，我们只需要能对应起来即可，其中，checkpoints是为了保存模型，content_img中的style.jpg是训练时候的风格图片，input.jpg是测试的输入，output.jpg是测试的输出，data中的数据是训练数据，主要是因为这个训练数据太整齐，是用ImageFolder读取的，为了避免麻烦，也为了在测试的时候方便观察图片，所以style.jpg我们暂时放在了content中。下半部分是重点，我们需要写的代码，每次都是先从dataset.py和models开始写起，然后导入visualize.py，这个文件基本不会发生改变，然后同时写main.py和config.py，边写边扩展utils中的其他文件，例如main中用到的函数等等。 2.2 modelsPackedVGG.py这里我们主要是取已有的网络，得到中间层的输出models.named_parameters():返回的是一个生成器，每次返回一个参数的关键字和值models.state_dict():返回的是一个字典，记录了参数的关键字和值models.parameters():返回的是变量，没有名字，可以在requires_grad中用到models.features返回的是相对应的模型1234567891011121314151617181920212223242526272829In [7]: from torchvision.models import vgg16In [8]: models = vgg16(pretrained=True)In [9]: model = models.features[:1]In [10]: modelOut[10]: Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))In [11]: models.parameters()Out[11]: &lt;generator object Module.parameters at 0x7f8fad26b3b8&gt;In [12]: models.named_parameters()Out[12]: &lt;generator object Module.named_parameters at 0x7f8f29e99d58&gt;In [13]: model.named_parameters()Out[13]: &lt;generator object Module.named_parameters at 0x7f8fad26b2b0&gt;In [14]: model.parameters()Out[14]: &lt;generator object Module.parameters at 0x7f8fad26b4c0&gt;In [15]: model.state_dict()Out[15]: OrderedDict([('0.weight', tensor([[[[-0.5537, 0.1427, 0.5290], [-0.5831, 0.3566, 0.7657], [-0.6902, -0.0480, 0.4841]], 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107from torchvision.models import vgg16models = vgg16(pretarined = True)In [19]: modelsOut[19]: VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace) (2): Dropout(p=0.5) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace) (5): Dropout(p=0.5) (6): Linear(in_features=4096, out_features=1000, bias=True) ))In [20]: models.featuresOut[20]: Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))In [21]: models.features[1]Out[21]: ReLU(inplace)# listIn [27]: models4 = models2[0:2]In [28]: models4Out[28]: Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace))In [32]: models4listOut[32]: [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace)]In [36]: models4list[1]Out[36]: ReLU(inplace)In [37]: models4list[1].named_parametersOut[37]: &lt;bound method Module.named_parameters of ReLU(inplace)&gt; sequencial是支持索引操作的list(module)会变成一个list，可以通过索引来获取层，注意，nn.ModuleList, nn.Sequential, nn.Conv等都是Module,都可以通过named_parameters来获取参数。为了能够提取出中间层的输出，作者换了一个方法，用的nn.ModuleList,nn.ModuleList和nn.Sequential的区别在此才真正显现，nn.Sequential更有利于直接把输入传给Module，计算是一个整体，写起来更方便，而nn.Modulist则不能直接把输入传给Module，需要用循环传输入，更有利于在层中做一些保留，提取中间层的输出。后面我们会讲到hook。或者说提取中间层的输出我们可以选择在定义网络的forward中进行，另外，就是需要注意的是，这里的输入是一个batch_size大小的矩阵，所以即便像作者这样，用一个列表保存输出，但实际输出的列表中的元素都是(b,n,h,w)大小的。后面我会验证。 提取中间层的输出有两种方法：第二种方法参考链接：https://www.jianshu.com/p/0a23db1df55a12345678910# 第一种方法，这种方法是在前向网络中提取输出，好像也是在反向传播网络中，但这种提取中间层是永久性的，也适合用这些层的做其他运算，这些运算是计算在整体网络框架中的def forward(self, x): results = [] for ii, model in enumerate(self.features): x = model(x) if ii in &#123;3, 8, 15, 22&#125;: results.append(x) vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3']) return vgg_outputs(*results) 12345# 第二种方法，适合在在不影响整体网络的情况下拿出一个分支进行单独计算，现在还不清楚这样子会不会影响backward，个人感觉会，因为也是相当于一个变量对其进行计算，导数为1。def forward(self, x): x= self.model(x) self.fea = x x = self.main(x) transformer.py可参考链接 padding的操作是边界反射补充 放大方法是双线性插值，而不是ConvTransposed2d，即unsample或者说是interpolate， 但是其中的一个参数align_corners一直没有理解，既然是双线性插值，那结果就是固定的，怎么还会因为其他参数发生变化。 其中，写的时候必要的时候可以写写子网络这里我对residualblock提出了疑问，事实上left+right后面可以没有relu层，这一点我们可以从以下链接找到说明。https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.pyhttp://torch.ch/blog/2016/02/04/resnets.html The above result seems to suggest that it’s important to avoid changing data that passes through identity connections only. We can take this philosophy one step further: should we remove the ReLU layers at the end of each residual block? ReLU layers also perturb data that flows through identity connections, but unlike batch normalization, ReLU’s idempotence means that it doesn’t matter if data passes through one ReLU or thirty ReLUs. When we remove ReLU layers at the end of each building block, we observe a small improvement in test performance compared to the paper’s suggested ReLU placement after the addition. However, the effect is fairly minor. More exploration is needed. 对于其他的出现的网络架构，其实都是有理可循的，但暂时不是本篇的重点，所以只做一个记录。上卷积简单地看了看这篇论文，unsample要比ConvTransposed2D要好，但是没有看懂。留作后续。 dataset.py &amp; visualize.py因为加载数据是用的tv.datasets.ImageFolder，所以dataset.py不需要写，visualize.py是第六章的时候写好的，这里只写几个改进的 self.vis = Visdom(env=env,use_incoming_socket=False, **kwargs)，这里的use_incoming_socket是不需要从浏览器接受数据到软件中，如果没有的话会提示 ‘&gt;’ not supported between instances of ‘float’ and ‘NoneType’ 在一个函数前提示输入的大小和类型是一件很重要的事情，必要的时候需要输入分布， 这里的plot用了一个很巧的方法，用字典记录不同的点123self.index = &#123;&#125;x = self.index.get(win,0)self.index[win] = x+1 其他的细节可以看代码中的记录，应该比较清晰了。 main.py &amp; utils.py &amp; config.py其中utils主要为main提供一些用到的函数，config提供参数，main作为主函数，里面主要就是train(),val(),test(),help(),下面记录一些写main函数的一些疑问。 cuda这里写几种怎么从cpu到gpu的方法以及应用场景。123456789101112131415161718192021# 第一种device = t.device('cuda') if opt.use_gpu else t.device('cpu')models.to(device)tensor = tensor.to(device)此时使用默认的cuda，一般是cuda:0，适用于全局# 第二种torch.cuda.current_device() # 查询当前GPUtorch.cuda.set_device(1)device = torch.device('cuda')models.to(device)此时用的是cuda:1，使用于全局#第三种#上下文管理器with torch.cuda.device(1): models.to(device)#第四种import osos.environ["CUDA_VISIBLE_DEVICES"]="2"没用过 tqdm进度条，但是只在jupyter和终端中用的时候效果很明显，在代码中用的效果没有那么好，tqdm试了试，用在enumerate()中时，需要写成这样：123456elements = ('a', 'b', 'c')for count, ele in tqdm(enumerate(elements)): print(count, i)# two argumentsfor count, ele in tqdm(enumerate(elements), total=len(train_ids)): print(count, i) 包括zip也是一样，因为他们返回的是一个生成器，并不知道长度。 反向传播和梯度下降参考链接https://blog.csdn.net/qq_16234613/article/details/80025832这里主要是针对第七章和第八章出现的反向传播和梯度下降出现的问题进行记录。在第七章，是这么实现分别训练的1234567891011121314fake_img = netg(noises).detach() fake_output = netd(fake_img)error_d_fake = criterion(fake_output, fake_labels)error_d_fake.backward()optimizer_d.step()optimizer_g.zero_grad()noises.data.copy_(t.randn(opt.batch_size, opt.nz, 1, 1))fake_img = netg(noises)output = netd(fake_img)error_g = criterion(output, true_labels)error_g.backward()optimizer_g.step() y = x.detach()：表示将生成一个新的叶子节点，值与当前节点的值相同，但是y.requires_grad = False, y.grad_fn=None，此时x和y共享内存，对y数据的操作也会影响x，可以理解为冻结了通过y进行反向传播的路。如果在网络的输出detach，即y= models(x).detach()，可以理解成，models只进行前向传播，grad=None。1234567891011121314151617181920In [17]: a = torch.ones(3,3)In [18]: a.requires_grad=TrueIn [19]: b = a*2In [20]: b.requires_gradOut[20]: TrueIn [21]: b.grad_fnOut[21]: &lt;MulBackward at 0x7f8fac6e40f0&gt;In [22]: c = b.detach()In [23]: c.requires_gradOut[23]: FalseIn [24]: print(c.grad_fn)NoneIn [25]: c.is_leaf 123456789101112131415161718192021222324252627282930313233343536373839In [2]: a = torch.ones(3,3)In [14]: bOut[14]: tensor([[2., 2., 2.], [2., 2., 2.], [2., 2., 2.]], grad_fn=&lt;MulBackward&gt;)In [15]: c = b.detach()In [16]: cOut[16]: tensor([[2., 2., 2.], [2., 2., 2.], [2., 2., 2.]])In [17]: c[0,0]=1In [18]: cOut[18]: tensor([[1., 2., 2.], [2., 2., 2.], [2., 2., 2.]])In [19]: bOut[19]: tensor([[1., 2., 2.], [2., 2., 2.], [2., 2., 2.]], grad_fn=&lt;MulBackward&gt;)In [20]: c.requires_gradOut[20]: FalseIn [21]: b.grad_fnOut[21]: &lt;MulBackward at 0x7f764429ffd0&gt;In [22]: b.grad_fn.next_functionsOut[22]: ((&lt;AccumulateGrad at 0x7f7644428358&gt;, 0),)In [23]: a.grad_fn 在第八章，是这么表示的12for param in vgg16.parameters(): param.requires_grad = False 这种表示可以使得某一个网络不参与梯度下降这个过程，但是对于网络的输入和输出还是支持梯度下降的。requires_grad只是表示当前的变量不再需要梯度下降，综上所述，对于中间变量，需要使用x.detach()，使其变成默认的叶子节点，对于叶子节点，使用x.requires_grad。并且对于中间变量使用requires_grad会报错。 在第八章，还有一种表示方法：1234567with t.no_grad(): features = vgg16(style_img) gram_style = [gram_matrix(feature) for feature in features]@t.no_grad()def stylize(**kwargs): pass 这种方法会使得任何计算得到的结果都是requires_grad = False,暂时不清楚和detach()的区别。也是一种表示只前向传播的方法，不参与反向传播和梯度下降。 train()图片分为两种：风格图片，只需要一张，内容图片，很多，用于训练，这一点没有暂时没有理解为什么这么设置。其中，对输入的图片进行了乘以255，我觉得是因为为了使模型的输出直接就是255，不需要再进行处理，没有验证。ensor.item() tensor.tolist()content_image = tv.datasets.folder.default_loader(opt.content_path)在训练过程中，会发现对于整个训练过程，不仅有神经网络，而且还有自己定义的函数，nn.functional，还有两个损失函数，这是之前没有预料到的。 保存图片1234567# 保存图片的几种方法，第七章的是 # 0-1tv.utils.save_image(fix_fake_imgs,'%s/%s.png' % (opt.img_save_path, epoch),normalize=True, range=(-1,1))# vis.save竟然没找到 我的神 # 0-1vis.img('input')vis.save([opt.env]) utils.py这里的疑问是得到gram矩阵的时候，为什么要除以c*h*w,而不是h*w，虽然源码都是这么写的。 写到这里也还是还要很多疑问，暂时保留。昨天发现训练的过程不对，今天在对比代码的过程中，发现了自己写代码的一些漏洞，主要有 命名不规范：表示同一个东西出现了两个命名，导致了自己在写代码的过程中传参出现了问题，或者是一类东西没有一个规则进行命名，导致自己在写代码的过程中用到之前的变量的时候必须返回去去查找这个变量，效率低且容易出错。 对源码的修改不是很恰当，导致在写上卷积层的输出和源码完全不一致，这个是自己之前没有遇到的。 visdom的运用，我用不同的environment导致结果也不一样，default是之前一直用的，这次换成了test1之后显示的结果就对了。这个暂时还不清楚原因，如果是会保留信息的话，但是plot是重新开始画的，等会测试测试vis的问题。是网络的问题。但是vis.save()的介绍是序列化信息，暂时还没有理解。 贴两个成果图看看效果。 遗留的问题Gram矩阵为什么可以代表图片风格，这里有个解释(https://arxiv.org/pdf/1701.01036.pdf)，还没来得及看。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>neural style transfer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[data]]></title>
    <url>%2F2018%2F09%2F06%2Fdata%2F</url>
    <content type="text"><![CDATA[Market-1501 6个摄像头 1501个人，其中751个人、12936张图片用于训练，750个人、19732张图片用于测试， 3368张查询图片 目录说明 bounding_box_test 19732张测试图片 0000_c1s1_000151_01.jpg 前缀为 0000 表示在提取这 750 人的过程中DPM检测错的图（可能与query是同一个人），-1 表示检测出来其他人的图（不在这 750 人中） DPM检测出的 gallery样本 bounding_box_train 12936张训练图片 0002_c1s1_000451_03.jpg train样本 query 3368张图片，与test的750人对应 但是是人工绘制的 与bounding_box_test中的图片略微有所不同 与gt_bbox中的图片是一样的 0001_c1s1_001051_00.jpg 为 750 人在每个摄像头中随机选择一张图像作为query，因此一个人的query最多有 6 个，共有 3,368 张图像 query样本 gt_bbox 25259张图片 手工绘制 包含1501个行人 0001_c1s1_001051_00.jpg 手工标注的bounding box，用于判断DPM检测的bounding box是不是一个好的box gt_query 是对3368张图片的查询图片的判定，好坏， 0001_c1s1_001051_00_good.mat matlab格式，用于判断一个query的哪些图片是好的匹配（同一个人不同摄像头的图像）和不好的匹配（同一个人同一个摄像头的图像或非同一个人的图像） 命名规则以 0001_c1s1_000151_01.jpg 为例1） 0001 表示每个人的标签编号，从0001到1501；2） c1 表示第一个摄像头(camera1)，共有6个摄像头；3） s1 表示第一个录像片段(sequece1)，每个摄像机都有数个录像段；4） 000151 表示 c1s1 的第000151帧图片，视频帧率25fps；5） 01 表示 c1s1_001051 这一帧上的第1个检测框，由于采用DPM检测器，对于每一帧上的行人可能会框出好几个bbox。00 表示手工标注框 DukeMTMC-reIDDukeMTMC是多目标多摄像机行人跟踪数据集，8个摄像头，2700多个人物，DukeMTMC-reID是DukeMTMC的行人重识别子集，并且提供了人工标注的bounding box。从视频中每 120 帧采样一张图像，得到了 36,411 张图像。一共有 1,404 个人出现在大于两个摄像头下，有 408 个人 (distractor ID) 只出现在一个摄像头下 目录结构 bounding_box_test 0002_c1_f0044158.jpg 用于测试集的 702 人 包含 17,661 张图像（随机采样，702 ID + 408 distractor ID） bounding_box_train 0001_c2_f0046182.jpg 用于训练集的 702 人 包含 16,522 张图像（随机采样） query 0005_c2_f0046985.jpg 为测试集中的 702 人在每个摄像头中随机选择一张图像作为 query，共有 2,228 张图像 。 命名规则0001_c2_f0046182.jpg1） 0001 表示每个人的标签编号；2） c2 表示来自第二个摄像头(camera2)，共有 8 个摄像头；3） f0046182 表示来自第二个摄像头的第 46182 帧。 CUHK03CUHK03是第一个足以进行深度学习的大规模行人重识别数据集，该数据集的图像采集于香港中文大学（CUHK）校园。数据以”cuhk-03.mat”的 MAT 文件格式存储，含有 1467 个不同的人物，由 5 对摄像头采集。 目录结构 detected - 5 * 1 cell 由机器标注，每个 cell 中包含一对摄像头组采集的照片，每个摄像头组由 M x 10 cells 组成，M 为行人索引，前 5 列和后 5 列分别来自同一组的不同摄像头。cell 内每个元素为一幅 H x W x 3 的行人框图像(uint8 数据类型)，个别图像可能空缺，为空集。 843*10 cell 摄像头组pair 1 440*10 cell 摄像头组pair 2 77*10 cell 摄像头组pair 3 58*10 cell 摄像头组pair 4 49*10 cell摄像头组pair 5 labeled - 5 * 1 cell 行人框由人工标注，格式和内容和”detected”相同。 843*10 cell 440*10 cell 77*10 cell 58*10 cell 49*10 cell testsets - 20*1 cell 测试协议，由 20 个 100 x 2 double 类型矩阵组成 (重复二十次) 100*2 double matrix 100 行代表 100 个测试样本，第 1 列为摄像头 pair 索引，第 2 列为行人索引]]></content>
  </entry>
  <entry>
    <title><![CDATA[dataset]]></title>
    <url>%2F2018%2F09%2F04%2Fdataset%2F</url>
    <content type="text"><![CDATA[Person Re-identification数据集描述——Market-1501]]></content>
  </entry>
  <entry>
    <title><![CDATA[Inception]]></title>
    <url>%2F2018%2F09%2F04%2FInception%2F</url>
    <content type="text"><![CDATA[关于Inception的好的讲解深度学习卷积神经网络——经典网络GoogLeNet(Inception V3)网络的搭建与实现]]></content>
  </entry>
  <entry>
    <title><![CDATA[deconv\dilated conv]]></title>
    <url>%2F2018%2F09%2F04%2Fdeconv%2F</url>
    <content type="text"><![CDATA[最近看的一片论文里看到了反卷积，找了一些材料，记录一下，留作纪念。首先定义符号： 假设本文提到的图形和卷积核都是一维的线或者二维的正方形，$x$和$y$轴方向的padding和stride相同 $i,o,k,p,s,i’,o’,k’,p’,s’$分别表示卷积/反卷积输入图片的大小input size, 输出图片的大小 output size，卷积/反卷积的核大小kernel size，padding，stride.动图演示详细解析 反卷积对应的是直接在原图上添加0对应的公式如下：i'=oo'=ik'=ks'=1p'=k-1-pd'=1o' = (s-1)(i'-1)+i'-k'+2p'o' = s(i'-1)+k-2pd' = 2k' = k + (d-1)*(k-1)p' = k-1-p+(d-1)*(k-1)o' = (s-1)(i'-1)+i'-k'+2p'o' = s(i'-1)+k+(d-1)*(k-1)-2p'所以要保持图片图片大小不变，必须令s=1，d = \frac{2}{k-1}*p例如：k=3，d=p, 以上只是先说明了对应尺寸的大小，具体的数学原理留作补充。 扩张卷积的计算公式与上面不一样扩张卷积扩张的是卷积核的大小，在卷积核上添加0扩张卷积 o'=\frac{i'-k+2p-(k-1)*(d-1)}{s}+1]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积\反卷积 扩张卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch]]></title>
    <url>%2F2018%2F08%2F26%2Fpytorch%2F</url>
    <content type="text"><![CDATA[基础知识魔术方法: P23 调试: P2712import ipdbipdb.set_trace() 带下划线_的函数会修改Tensor本身，比如x.add_(y)和x.add(y)的区别 TensorNumpy与Tensor共享内存b = a.numpy() # Tensor -&gt; Numpya = t.from_numpy(a)# Numpy -&gt; Tensorx = x.cuda()tensor的操作:torch.function,tensor.function.普通索引共享内存高级索引不共享内存线性代数函数 P70自动广播原则: unsqueese(view),expand(expand_as)tensor=Tensor+Storage持久化和加载: t.save(a,’a.pth’) b=t.load(‘a.pth’) P77%timeit -n 10 Variable和autogradfrom torch.autograd import Variable三个属性data: 对应Tensorgrad: 梯度，和data大小一样，也是Variablegrad_fn: 指向Function对象,用于构建计算图。用户创建对应叶子节点,grad_fn=None.记录的是它什么操作的输出。Variable的构造函数的关键字参数:requires_grad(bool):是否需要求导; volatile(bool):True表示之上的计算图都不会求导123456x = Variable(t.ones(2,2),requires_grad = True)y = x.sum()y.grad_fny.backward()x.gradx.grad.data.zero_() # 反向传播清零 variable.backward(grad_variable=None, retain_graph=None, create_graph=None)假设用户输入的数据是真实的不需要求导的。数值在前向传导过程成会保存成buffer,计算梯度之后自动清空。多次反向求导可以使用关键字参数retain_graph=Trueretain_graph=True 实现多次反向传播？？？？ 反向传播过程中非叶子节点的导数在计算完之后就会清空，y=x*w,z=y.sum() 其中y.grad会清空。其对应的方法有两种，P92，t.autograd.grad(z,y)和hook扩展Autograd Function：P95 自己实现前向和反向 nn.Module123456789101112131415161718import torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def \_\_init\_\_(): super(Net,self).__init__() \# 有参数的层的定义 def forward(self,x):net = Net()print(net)list(net.parameters())for name,paramenters in net.named_parameters(): print(name,':',parameters.size())out = net(input)net.zero_grad()out.backwad(Variable(t.ones(1,10)))&gt; ??? nn.Sequential()nn.ModuleList()nn.ParameterList()在优化器中为各层分别设置学习率nn.functional对应nn.Module参数初始化:nn.Conv2d: nSamplesnChannelsHeight*Widh单样本: input.unsqueeze(0)model.train()model.eval()前向或者后向注册钩子函数，P125，可以查看中间层。获取网络的模块属性：getattr(module)P128保存模型：t.save(net.state_dict(), ‘net.pth’)加载模型：net2=Net() net2.load_state_dict(t.load(‘net.pth’))多个GPU并行操作 损失函数12345output = net(input)target = Variable(t.arrange(0,10))criterion = nn.MSELoss()loss = criterion(output, target)loss 1234net.zero_grad()print(net.con1.bias.grad)loss.backward()print(net.conv1.bias.grad) 优化器123learning_rate=0.01for f in net.parameters(): f.data.sub_(f.grad.data*leaning_data) 123456789101112import torch.optim as optim# 优化器optimizer = optim.SGD(net.parameters(), lr=0.01)# 训练过程中 梯度清零optimizer.zero_grad() # 等效于 net.zero_grad()# 损失函数output=net(input)loss = criterion(output, target)# 反向传播loss.backward()# 更新参数optimizer.step() CIFAR-10分类 数据预处理:transform,trainset,trainloader,testset,testloader 定义网络:Net(nn.Module),super(Net,self).init(),forward() 定义损失函数和优化器 训练网络 输入数据 梯度清零 前向传播+反向传播 更新参数 数据处理自定义的数据集需要继承Dataset类，并实现两个Python魔术方法__getitem:返回一个样本。obj[index]=obj.getitem(index)__len:返回样本数量。transform=T.Compose()trans=T.Lambda(lambda img:img.rotate(randonm()*360))ImageFolder(root,transform,target_transform,loader)P139self.class_to_idx了解label和文件夹名的映射关系DataLoader()定义shuffle等P142取样:P146工具包:torchvision P147 models:训练好模型 dataset:数据集加载 transforms:数据预处理操作，主要针对Tensor和PIL Image对象的操作 make_grid和save_img可视化工具 Tensorboard和visdomtensor_board和TensorboardXvisdom:env pane %env LS_COLORS=None!tree —charset ascii data/dogcatTensor—numpy:np.array(Tensor) torch.Tensor(np.darray)PIL.image—numpy:np.asarray(PIL.image) image.fromarray(numpy.ndarray)PIL.image—Tensor:trans = transforms.Compose([transforms.ToTensor()]) tens = trans(img) ToPILImage() GPUP158with t.cuda.device(1):t.cuda.set_device(1)export CUDA_VISIVLE_DEVICES=1b = t.load(‘a.pth’)c = t.load(‘a.pth’,map_location=lamdba storage, loc: storage)d = t.load(‘a.pth’,map_location={‘cuda:1’:’cuda:0’})Module和Optimizer: state_dict Dog.vs.Catcheckpoints/ 中间模型data/ __init__.py dataset.py def init(self,root,transform=None, train=True, test=False): def getitem(self,index): def len(self): get_data.shmodels/ __init__.py AlexNet.py BasicModule.py ResNet34.pyutils/ __init__.py visualize.pyconfig.pymain.pyrequirements.txtREADME.md main.py def train(**kwargs): def test(**kwargs): def val(model,dataloader): def help(): 123if __name__=='main': import fire fire.Fire() pytorch 中文文档https://pytorch-cn.readthedocs.io/zh/latest/package_references/functional/ PyTorch实战指南 第六章 Dog.VS.Cat这是根据深度学习框架：PyTorch入门与实践这本书的第六章写的代码，是关于猫狗识别的，在这个过程中，一边看，一边写，刚开始是运行作者已经写好的代码，后来自己在jupyter上进行复制的复现，发现import无法导入ipynb文件，在使用了Ipynb_importer.py之后可以实现同一文件内导入ipynb模块，如果是在其他文件中进行导入，会有点费事，以下会记录Ipynb_importer.py的用法。因为费事，自己开始开始使用pycharm+jupyter的方式，直接自己根据作者提供的源码进行编写，在编写的过程中接受作者的思想。用pycharm的不方便的地方是无法直接运行测试，所以采取的是对自己不熟悉的模块或者方法，用jupyter进行测试，而直接编写则是pycharm。但是感觉pycharm还是没有那么好用，可能是自己用的少。我是按照data、model、util、main+config、requirement的顺序编写的。在编写函数的过程中，因为刚开始不理解各个模块是怎么组织起来的，所以都是从简单的开始，所以函数的位置和作者的不一样，其中对于model.save和model.load、vis.plot和vis.log的封装让我感觉很有意思，刚开始是编写的时候只能直接打上问号，因为不懂这么编写的意义，但在编写主函数main的时候才感觉到了这种编写的好处，基本把模型训练和对模型、结果的处理完全分离开，避免了耦合性很强的后果。 Ipynb_importer.py我通过几次测试发现，import Ipynb_importer 只需要放在你的当前要运行的文件中即可，然后在其他文件下的init.py 中导入所有的当前文件夹中的Module，就像这样/first/second/models/——-init.py——————- None——-BasicModule.ipynb——-AlexNet.ipynb—————from models.BasicModule import BasicModule /first/main.pyimport Ipynb_importerfrom models import AlexNet 之所以在AlexNet中写models.BasicModule是因为直接导入BasicModule会报错，我根据dict的输出发现有问题，这一点和官网介绍的有一点区别，我没有实现官网说明的跨文件夹导入。因为如果改文件夹导入的话，models.BasicModule要接着换成相应的名字，与我预想的不一致，我预想的是不管在哪里导入，已经导入的应该不受影响才对。 ipynb-py.sh之后发现了这个神器，可以把ipynb转化成.py，还是挺好用的，转化之后也没问题。 同时，借助这次实验，自己对python的掌握也更深了一点。 不过对于网络的构成还是有一些问题，那就是网络为什么这么写，这应该属于理论的东西。还需要进一步加强。 这次实验一共用了三天才完全搞懂，可以说其中涉及到的函数的用法基本都明白了。本意是记录自己，不过如果有任何问题，欢迎交流。 PyTorch实战指南 第七章 DCGAN这一次实现的也比较慢，用了小三天才做完，现在记录一下其中学到的几个东西。 __file__:用来获取模块所在路径 可能是一个相对路径，可能是一个绝对路径，如果当前文件包含在sys.path里面，那么，__file__返回一个相对路径！也可以认为获取模块的名字最后的落脚点一定是XX/XX.py类没有这个属性 12345678In [1]: import numpyIn [3]: numpy.__file__ Out[3]: 'F:\\Programs\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py'In [6]: numpy.random.__file__Out[6]: 'F:\\Programs\\Anaconda3\\lib\\site-packages\\numpy\\random\\__init__.py'$ python test.py ##print(__file__)test.py __name__:__name__就是标识模块的名字的一个系统变量。这里分两种情况：假如当前模块是主模块（也就是调用其他模块的模块），那么此模块名字就是__main__，通过if判断这样就可以执行“__mian__:”后面的主函数内容；假如此模块是被import的，则此模块名字为文件名字（不加后面的.py），通过if判断这样就会跳过“__main__:”后面的内容。这个模块可以是文件夹的名字，可以是类的名字，可以是__mian__”,XX的形式。可以用于获取当前文件的文件名通过上面方式，python就可以分清楚哪些是主函数，进入主函数执行；并且可以调用其他模块的各个函数等等。12345678910111213141516171819# test.py## print(__file__)## print(__name__)# test2.py## import test## print(__file__)## print(__name__)H:\GitHub\pytorch_learn\Chapter7\test.pytesttest2.py__main__## from test import ccc## print(ccc.__name__)## print(__file__)## print(__name__)ccctest2.py__main__ type():返回对象的类型如果是module,则返回module如果是类的实例，则返回类的名称，这个名称以XXX.XXX的形式返回，从import的第一个开始算起。常用于判断数据类型，在pytorch中，用于返回模型名称，这个用法很巧妙，相当于返回了子类的类型名字我觉得没有理解作者是怎么用的。在父类里的type(self) 返回的是子类的类名123456789101112In [1]: import numpyIn [2]: type(numpy)Out[2]: moduleIn [3]: a = numpy.array(1)In [4]: type(a)Out[4]: numpy.ndarrayIn [5]: type(numpy.array)Out[5]: builtin_function_or_method 123456789101112131415161718192021222324252627class A: pass class B(A): pass isinstance(A(), A) # returns Truetype(A()) == A # returns Trueisinstance(B(), A) # returns Truetype(B()) == A # returns Falseclass A(object): def __init__(self): print(type(self)) passclass B(A): def __init__(self): super(B,self).__init__() print(type(self)) passimport testtest.B()&lt;class 'test.B'&gt; &lt;class 'test.B'&gt; __class__:和type类似1234567891011121314151617class A(object): def __init__(self): print(type(self)) print(self.__class__) passclass B(A): def __init__(self): super(B,self).__init__() print(type(self)) print(self.__class__) pass&lt;class 'test.B'&gt;&lt;class 'test.B'&gt;&lt;class 'test.B'&gt;&lt;class 'test.B'&gt; 获取config源码打印参数，方便输入参数inspect.getsource123from inspect import getsourcesource = getsource(opt.__class__)print(source)]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch-learn chenyun</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python learning]]></title>
    <url>%2F2018%2F08%2F26%2Fpython-learning%2F</url>
    <content type="text"><![CDATA[python的学习过程： 第一个是看着廖雪峰的网站，里面的内容基础，是关于数据结构等的十分基本的内容，适合小白入门 第二个是流畅的python 这本书比python-codebook还深入，更适合当你实现了一个功能之后，还是想知道其具体怎么实现的时候查询。 第三个是python-codebook. 它的组织形式是任务式、问题式的，而且问题也相对而言比较高级，不是算法导论那种以解决某个实际问题，而是在编程上我想实现什么更好的功能那种问题，通过每一个问题，或者说通过每一个你想怎么更优的实现一个方法的思路，来引导如何更好地写代码，实现高级功能。这本书的前提是你已经入门，并且写了一段时间的python代码，在实际写的过程中已经遇到了类似的问题，也勉强实现了，只是苦于没有更好更顺心的方法实现。我现在是个小白，看这本书用了将近两周吧，主要看了第一二三四七八章，里面的代码翔实。其他的也是略微看了看，因为没有实际操作背景，有的时候不懂为什么那样做会更好，可以在以后的编程过程中，遇到这样的情况：这个我能勉强实现，但是感觉不太好，我想实现的更优美。那就应该来看看这本书，说不定这本书的实现能给自己一些思路。不适合为了读而读，因为不是入门。 接下来可以考虑看看那种直接算法任务型的。刚刚看了看python算法教程，估计要跳着看了，因为里面的关于算法的内容已经熟悉了，可以扫描着看。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MdEditor]]></title>
    <url>%2F2018%2F08%2F22%2FMdEditor%2F</url>
    <content type="text"><![CDATA[欢迎使用 Markdown在线编辑器 MdEditorMarkdown是一种轻量级的「标记语言」 Markdown是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面，Markdown文件的后缀名便是“.md” MdEditor是一个在线编辑Markdown文档的编辑器MdEditor扩展了Markdown的功能（如表格、脚注、内嵌HTML等等），以使让Markdown转换成更多的格式，和更丰富的展示效果，这些功能原初的Markdown尚不具备。 Markdown增强版中比较有名的有Markdown Extra、MultiMarkdown、 Maruku等。这些衍生版本要么基于工具，如Pandoc，Pandao；要么基于网站，如GitHub和Wikipedia，在语法上基本兼容，但在一些语法和渲染效果上有改动。 MdEditor源于Pandao的JavaScript开源项目，开源地址Editor.md，并在MIT开源协议的许可范围内进行了优化，以适应广大用户群体的需求。向优秀的markdown开源编辑器原作者Pandao致敬。 MdEditor的功能列表演示标题H1标题H2标题H3标题H4标题H5标题H5字符效果和横线等 删除线 删除线（开启识别HTML标签时） 斜体字 _斜体字_ 粗体 粗体 粗斜体 _粗斜体_ 上标：X2，下标：O2 缩写(同HTML的abbr标签) 即更长的单词或短语的缩写形式，前提是开启识别HTML标签时，已默认开启 The HTML specification is maintained by the W3C. 引用 Blockquotes 引用文本 Blockquotes 引用的行内混合 Blockquotes 引用：如果想要插入空白换行即&lt;br /&gt;标签，在插入处先键入两个以上的空格然后回车即可，普通链接。 锚点与链接 Links普通链接普通链接带标题直接链接：http://www.mdeditor.com锚点链接 mailto:test.test@gmail.comGFM a-tail link @pandao邮箱地址自动链接 test.test@gmail.com www@vip.qq.com @pandao 多语言代码高亮 Codes行内代码 Inline code执行命令：npm install marked 缩进风格即缩进四个空格，也做为实现类似 &lt;pre&gt; 预格式化文本 ( Preformatted Text ) 的功能。 &lt;?php echo &quot;Hello world!&quot;; ?&gt; 预格式化文本： | First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell | JS代码123function test() &#123; console.log("Hello world!");&#125; HTML 代码 HTML codes1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;mate charest="utf-8" /&gt; &lt;meta name="keywords" content="Editor.md, Markdown, Editor" /&gt; &lt;title&gt;Hello world!&lt;/title&gt; &lt;style type="text/css"&gt; body&#123;font-size:14px;color:#444;font-family: "Microsoft Yahei", Tahoma, "Hiragino Sans GB", Arial;background:#fff;&#125; ul&#123;list-style: none;&#125; img&#123;border:none;vertical-align: middle;&#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1 class="text-xxl"&gt;Hello world!&lt;/h1&gt; &lt;p class="text-green"&gt;Plain text&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 图片 Images图片加链接 (Image + Link)： Follow your heart. 列表 Lists无序列表（减号）Unordered Lists (-) 列表一 列表二 列表三 无序列表（星号）Unordered Lists (*) 列表一 列表二 列表三 无序列表（加号和嵌套）Unordered Lists (+) 列表一 列表二 列表二-1 列表二-2 列表二-3 列表三 列表一 列表二 列表三 有序列表 Ordered Lists (-) 第一行 第二行 第三行 GFM task list [x] GFM task list 1 [x] GFM task list 2 [ ] GFM task list 3 [ ] GFM task list 3-1 [ ] GFM task list 3-2 [ ] GFM task list 3-3 [ ] GFM task list 4 [ ] GFM task list 4-1 [ ] GFM task list 4-2 绘制表格 Tables 项目 价格 数量 计算机 $1600 5 手机 $12 12 管线 $1 234 First Header Second Header Content Cell Content Cell Content Cell Content Cell First Header Second Header Content Cell Content Cell Content Cell Content Cell Function name Description help() Display the help window. destroy() Destroy your computer! Left-Aligned Center Aligned Right Aligned col 3 is some wordy text $1600 col 2 is centered $12 zebra stripes are neat $1 Item Value Computer $1600 Phone $12 Pipe $1 特殊符号 HTML Entities Codes&copy; &amp; &uml; &trade; &iexcl; &pound;&amp; &lt; &gt; &yen; &euro; &reg; &plusmn; &para; &sect; &brvbar; &macr; &laquo; &middot; X&sup2; Y&sup3; &frac34; &frac14; &times; &divide; &raquo; 18&ordm;C &quot; &apos; [========] Emoji表情 :smiley: Blockquotes :star: GFM task lists &amp; Emoji &amp; fontAwesome icon emoji &amp; editormd logo emoji :editormd-logo-5x: [x] :smiley: @mentions, :smiley: #refs, links, formatting, and tags supported :editormd-logo:; [x] list syntax required (any unordered or ordered list supported) :editormd-logo-3x:; [x] [ ] :smiley: this is a complete item :smiley:; [ ] []this is an incomplete item test link :fa-star: @pandao; [ ] [ ]this is an incomplete item :fa-star: :fa-gear:; [ ] :smiley: this is an incomplete item test link :fa-star: :fa-gear:; [ ] :smiley: this is :fa-star: :fa-gear: an incomplete item test link; 反斜杠 Escape*literal asterisks* [========] 科学公式 TeX(KaTeX)E=mc^2行内的公式E=mc^2行内的公式，行内的E=mc^2公式。 x > y\(\sqrt{3x-1}+(1+x)^2\)\sin(\alpha)^{\theta}=\sum_{i=0}^{n}(x^i + \cos(f))多行公式： 12345\displaystyle\left( \sum\_&#123;k=1&#125;^n a\_k b\_k \right)^2\leq\left( \sum\_&#123;k=1&#125;^n a\_k^2 \right)\left( \sum\_&#123;k=1&#125;^n b\_k^2 \right) 123456789\displaystyle \frac&#123;1&#125;&#123; \Bigl(\sqrt&#123;\phi \sqrt&#123;5&#125;&#125;-\phi\Bigr) e^&#123; \frac25 \pi&#125;&#125; = 1+\frac&#123;e^&#123;-2\pi&#125;&#125; &#123;1+\frac&#123;e^&#123;-4\pi&#125;&#125; &#123; 1+\frac&#123;e^&#123;-6\pi&#125;&#125; &#123;1+\frac&#123;e^&#123;-8\pi&#125;&#125; &#123;1+\cdots&#125; &#125; &#125; &#125; 123f(x) = \int_&#123;-\infty&#125;^\infty \hat f(\xi)\,e^&#123;2 \pi i \xi x&#125; \,d\xi 分页符 Page break Print Test: Ctrl + P [========] 绘制流程图 Flowchart12345678st=&gt;start: 用户登陆op=&gt;operation: 登陆操作cond=&gt;condition: 登陆成功 Yes or No?e=&gt;end: 进入后台st-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op [========] 绘制序列图 Sequence Diagram1234Andrew-&gt;China: Says HelloNote right of China: China thinks\nabout itChina--&gt;Andrew: How are you?Andrew-&gt;&gt;China: I am good thanks! End]]></content>
  </entry>
  <entry>
    <title><![CDATA[test1]]></title>
    <url>%2F2018%2F08%2F22%2Ftest1%2F</url>
    <content type="text"><![CDATA[E=mc^2\begin{aligned}\dot{x} &amp; = \sigma(y-x) \\\\dot{y} &amp; = \rho x - y - xz \\\\dot{z} &amp; = -\beta z + xy\end{aligned} 基准测试百度 是一个链接测试百度 第二个链接测试bai du 第三个链接测试badu 第四个测试2 第五个测试这是一个链接到谷歌的脚注。 脚注. http://www.google.com &#8617; 第六个测试 Here is a footnote reference,1 and another.longnote longnote. Here’s one with multiple blocks. &#8617; 1.Here is the footnote. ↩2.http://www.baidu.com ↩]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F21%2F%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8%20Cmd%20Markdown%20%E7%BC%96%E8%BE%91%E9%98%85%E8%AF%BB%E5%99%A8%2F</url>
    <content type="text"><![CDATA[欢迎使用 Cmd Markdown 编辑阅读器 我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，Cmd Markdown 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown： 整理知识，学习笔记 发布日记，杂文，所见所想 撰写发布技术文稿（代码支持） 撰写发布学术论文（LaTeX 公式支持） 除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载： Windows/Mac/Linux 全平台客户端 请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 新文稿 或者使用快捷键 Ctrl+Alt+N。 什么是 MarkdownMarkdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，粗体 或者 斜体 某些文字，更棒的是，它还可以 1. 制作一份待办事宜 Todo 列表 [ ] 支持以 PDF 格式导出文稿 [ ] 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 [x] 新增 Todo 列表功能 [x] 修复 LaTex 公式渲染问题 [x] 新增 LaTex 公式编号功能 2. 书写一个质能守恒公式LaTeXE=mc^2x = {-b \pm \sqrt{b^2-4ac} \over 2a}3. 高亮一段代码code1234567@requires_authorizationclass SomeClass: passif __name__ == '__main__': # A comment print 'hello world' 4. 高效绘制 流程图12345678st=&gt;start: Startop=&gt;operation: Your Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 5. 高效绘制 序列图123Alice-&gt;Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob--&gt;Alice: I am good thanks! 6. 高效绘制 甘特图12345678910111213title 项目开发流程section 项目确定 需求分析 :a1, 2016-06-22, 3d 可行性报告 :after a1, 5d 概念验证 : 5dsection 项目实施 概要设计 :2016-07-05 , 5d 详细设计 :2016-07-08, 10d 编码 :2016-07-15, 10d 测试 :2016-07-22, 5dsection 发布验收 发布: 2d 验收: 3d 7. 绘制表格 项目 价格 数量 计算机 $1600 5 手机 $12 12 管线 $1 234 8. 更详细语法说明想要查看更详细的语法说明，可以参考我们准备的 Cmd Markdown 简明语法手册，进阶用户可以参考 Cmd Markdown 高阶语法手册 了解更多高级功能。 总而言之，不同于其它 所见即所得 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。 什么是 Cmd Markdown您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 编辑/发布/阅读 Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。 1. 实时同步预览我们将 Cmd Markdown 的主界面一分为二，左边为编辑区，右边为预览区，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！ 2. 编辑工具栏也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 编辑区 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。 3. 编辑模式完全心无旁骛的方式编辑文字：点击 编辑工具栏 最右侧的拉伸按钮或者按下 Ctrl + M，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！ 4. 实时的云端文稿为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 编辑工具栏 的最右侧提示 已保存 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。 5. 离线模式在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。 6. 管理工具栏为了便于管理您的文稿，在 预览区 的顶部放置了如下所示的 管理工具栏： 通过管理工具栏可以： &lt;/i&gt; 发布：将当前的文稿生成固定链接，在网络上发布，分享 新建：开始撰写一篇新的文稿&lt;/i&gt; 删除：删除当前的文稿 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地&lt;/i&gt; 列表：所有新增和过往的文稿都可以在这里查看、操作 模式：切换 普通/Vim/Emacs 编辑模式 7. 阅读工具栏 通过 预览区 右上角的 阅读工具栏，可以查看当前文稿的目录并增强阅读体验。 工具栏上的五个图标依次为： &lt;/i&gt; 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落 视图：互换左边编辑区和右边预览区的位置&lt;/i&gt; 主题：内置了黑白两种模式的主题，试试 黑色主题，超炫！ 阅读：心无旁骛的阅读模式提供超一流的阅读体验 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境 8. 阅读模式在 阅读工具栏 点击 或者按下 Ctrl+Alt+M 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。 9. 标签、分类和搜索在编辑区任意行首位置输入以下格式的文字可以标签当前文档： 标签： 未分类 标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示： 10. 文稿发布和分享在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 (Ctrl+Alt+P) 发布这份文档给好友吧！ 再一次感谢您花费时间阅读这份欢迎稿，点击 (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！ 作者 @ghosert2016 年 07月 07日 LaTeX. 支持 LaTeX 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 MathJax 参考更多使用方法。 &#8617; code. 代码高亮功能支持包括 Java, Python, JavaScript 在内的，四十一种主流编程语言。 &#8617;]]></content>
  </entry>
  <entry>
    <title><![CDATA[markdown]]></title>
    <url>%2F2018%2F08%2F20%2Fmarkdown%2F</url>
    <content type="text"><![CDATA[利用git bash生成目录树1cmd //c tree 用下载软件的方法生成的目录树很丑，所以还是继续上述的命令。在linux下 tree是可以生成到文件的。 主标题副标题[TOC] 一级标题二级标题三级标题四级标题大五级标题六级标题 引用 二级引用 删除斜体 粗体 粗斜体 下划 教程2(https://www.cnblogs.com/Jimmy1988/p/7053875.html) 教程2 https://www.cnblogs.com/Jimmy1988/p/7053875.html 无序 列 表 有序 列表 姓名 张三 李四 性别 男 女 one two three four 1 2 3 4 12print("I am studying");print("yes or no"); 代码是hello world 樱桃的简书 行内链接和参考链接]]></content>
      <tags>
        <tag>markdown教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello hexo]]></title>
    <url>%2F2018%2F08%2F17%2Fhello-hexo%2F</url>
    <content type="text"><![CDATA[常用命令(cmd)hexo n “postName”hexo cleanhexo g 本地生成hexo d 同步到githubhexo d -ghexo new page aboutmehexo s 本地服务器预览hexo s -p 4100 换端口 搭建Github+hexo 博客的过程参考链接如下使用Hexo+Github一步步搭建属于自己的博客（基础）使用Hexo+Github一步步搭建属于自己的博客（进阶）1234$ npm install -g hexo-cli安装Node.js →安装Hexo → 安装主题 → 本地测试运行 → 注册给github与coding并创建pages仓库 → 部署安装Git →node.js的解释是高并发npm是模块的包管理器,与node.js一起安装的。npm install hexo -g 全局安装hexo是基于node.js的静态博客，所以我们才需要安装node.jsGit是为了让其他人也可以看到你的博客，把本地的内容提交到github上面去常用命令hexo g 生成 generatehexo s 启动服务器预览 serverhexo d 部署 deployhexo clean 清除缓存hexo server -p 4100hexo generate —deploy 完成后部署hexo deploy —generate 完成后部hexo new “postName” node_modules 依赖包public 生成的页面scaffolds 模板文件夹 post draft pagesource 用户资源的地方 hexo解释hexo中_config.ymlmarkdown写博客hexo中的配置信息域名绑定hexo渲染MathJax数学公式markdownpad渲染数学公式只能F6浏览器预览，并且不能实时预览，所以考虑换个markdown编辑器cmd markdown对本地文件支持不友好，并且不能加载本地图片Hexo Markdown 简明语法手册hexo的脚注问题只能实现数字的脚注 2018-09-12 实现评论功能此次评论功能使用disqus，理由：同学推荐参考链接：Hexo折腾记之科学使用Disqus与Next的集成科学使用 DisqusDisqus PHP API基于disqus-php-api在Hexo博客中使用DisqusDisqus-Proxy 配置说明Github 搭建 hexo （四）——更换主题，disqus，RSS 添加rss功能不知道是干嘛的，好像是为了实现订阅的。暂时不是很清楚。参考链接：最简便的方法搭建Hexo+Github博客,基于Next主题 添加site-map功能参考链接不知道是干嘛的Github 搭建 hexo （五）- 站点地图（sitemap.xml）站点地图还挺高级，以后再说。 百度自动推送参考链接Hexo+Next主题博客提交百度谷歌收录 添加公益404界面参考链接hexo添加404公益界面最简便的方法搭建Hexo+Github博客,基于Next主题 高级教程以后再说利用Gitpage+hexo开发自己的博客Hexo个人免费博客(三) next主题、评论、阅读量统计和站内搜索]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>npm</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello world]]></title>
    <url>%2F2018%2F08%2F17%2Fhello-world-1%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F08%2F17%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. [TOC] Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
